<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250701064159-3d86d3286c41f58cf8fd4322b0c819fba31dd425</doi_batch_id>
    <timestamp>20250701064159</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>111</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>GBNet: Gradient Boosting packages integrated into PyTorch</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Michael</given_name>
            <surname>Horrell</surname>
            <affiliations>
              <institution><institution_name>Independent Researcher, USA</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0001-3091-0342</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>01</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8047</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08047</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.15779198</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8047</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08047</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08047</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08047.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="mccullagh1980regression">
            <article_title>Regression models for ordinal data</article_title>
            <author>McCullagh</author>
            <journal_title>Journal of the Royal Statistical Society. Series B (Methodological)</journal_title>
            <issue>2</issue>
            <volume>42</volume>
            <cYear>1980</cYear>
            <unstructured_citation>McCullagh, P. (1980). Regression models for ordinal data. Journal of the Royal Statistical Society. Series B (Methodological), 42(2), 109–142. http://www.jstor.org/stable/2984952</unstructured_citation>
          </citation>
          <citation key="gagolewski_ordinal_regression">
            <article_title>Ordinal regression</article_title>
            <author>Gagolewski</author>
            <cYear>2022</cYear>
            <unstructured_citation>Gagolewski, M. (2022). Ordinal regression. https://github.com/gagolews/teaching-data/tree/master/ordinal_regression.</unstructured_citation>
          </citation>
          <citation key="mikolov2013distributed">
            <article_title>Distributed representations of words and phrases and their compositionality</article_title>
            <author>Mikolov</author>
            <journal_title>Advances in neural information processing systems</journal_title>
            <volume>26</volume>
            <cYear>2013</cYear>
            <unstructured_citation>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., &amp; Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in Neural Information Processing Systems, 26.</unstructured_citation>
          </citation>
          <citation key="hinton1993autoencoders">
            <article_title>Autoencoders, minimum description length and helmholtz free energy</article_title>
            <author>Hinton</author>
            <journal_title>Advances in neural information processing systems</journal_title>
            <volume>6</volume>
            <cYear>1993</cYear>
            <unstructured_citation>Hinton, G. E., &amp; Zemel, R. (1993). Autoencoders, minimum description length and helmholtz free energy. Advances in Neural Information Processing Systems, 6.</unstructured_citation>
          </citation>
          <citation key="kingma2013auto">
            <article_title>Auto-encoding variational bayes</article_title>
            <author>Kingma</author>
            <cYear>2013</cYear>
            <unstructured_citation>Kingma, D. P., Welling, M., &amp; others. (2013). Auto-encoding variational bayes. Banff, Canada.</unstructured_citation>
          </citation>
          <citation key="scikit-learn">
            <article_title>Scikit-learn: Machine learning in Python</article_title>
            <author>Pedregosa</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <volume>12</volume>
            <cYear>2011</cYear>
            <unstructured_citation>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.</unstructured_citation>
          </citation>
          <citation key="friedman2001gbm">
            <article_title>Greedy function approximation: A gradient boosting machine</article_title>
            <author>Friedman</author>
            <journal_title>Annals of Statistics</journal_title>
            <issue>5</issue>
            <volume>29</volume>
            <doi>10.1214/aos/1013203451</doi>
            <cYear>2001</cYear>
            <unstructured_citation>Friedman, J. H. (2001). Greedy function approximation: A gradient boosting machine. Annals of Statistics, 29(5), 1189–1232. https://doi.org/10.1214/aos/1013203451</unstructured_citation>
          </citation>
          <citation key="chen2016xgboost">
            <article_title>XGBoost: A scalable tree boosting system</article_title>
            <author>Chen</author>
            <journal_title>Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (KDD ’16)</journal_title>
            <doi>10.1145/2939672.2939785</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Chen, T., &amp; Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’16), 785–794. https://doi.org/10.1145/2939672.2939785</unstructured_citation>
          </citation>
          <citation key="ke2017lightgbm">
            <article_title>LightGBM: A highly efficient gradient boosting decision tree</article_title>
            <author>Ke</author>
            <journal_title>Proceedings of the 31st international conference on neural information processing systems (NIPS ’17)</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., &amp; Liu, T.-Y. (2017). LightGBM: A highly efficient gradient boosting decision tree. Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS ’17), 3149–3157.</unstructured_citation>
          </citation>
          <citation key="kaggle2021survey">
            <article_title>2021 kaggle machine learning &amp; data science survey</article_title>
            <author>Kaggle</author>
            <cYear>2021</cYear>
            <unstructured_citation>Kaggle. (2021). 2021 kaggle machine learning &amp; data science survey. Kaggle, https://www.kaggle.com/c/kaggle-survey-2021.</unstructured_citation>
          </citation>
          <citation key="paszke2019pytorch">
            <article_title>PyTorch: An imperative style, high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing systems (NeurIPS 2019)</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems (NeurIPS 2019), 8024–8035. https://papers.nips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library</unstructured_citation>
          </citation>
          <citation key="DBLP:journals/corr/abs-2007-09855">
            <article_title>Wide boosting</article_title>
            <author>Horrell</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/2007.09855</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Horrell, M. (2020). Wide boosting. CoRR, abs/2007.09855. https://arxiv.org/abs/2007.09855</unstructured_citation>
          </citation>
          <citation key="ordinalgbt">
            <article_title>OrdinalGBT - gradient boosting for ordinal regression</article_title>
            <author>Ingas</author>
            <cYear>2024</cYear>
            <unstructured_citation>Ingas, A. (2024). OrdinalGBT - gradient boosting for ordinal regression. https://github.com/adamingas/ordinalgbt</unstructured_citation>
          </citation>
          <citation key="dndf">
            <article_title>Deep neural decision forests</article_title>
            <author>Kontschieder</author>
            <journal_title>Proceedings of the IEEE international conference on computer vision</journal_title>
            <doi>10.1109/ICCV.2015.172</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Kontschieder, P., Fiterau, M., Criminisi, A., &amp; Rota Bulo, S. (2015). Deep neural decision forests. Proceedings of the IEEE International Conference on Computer Vision, 1467–1475. https://doi.org/10.1109/ICCV.2015.172</unstructured_citation>
          </citation>
          <citation key="deepgbm">
            <article_title>DeepGBM: A deep learning framework distilled by GBDT for online prediction tasks</article_title>
            <author>Ke</author>
            <journal_title>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Ke, G., Xu, Z., Zhang, J., Bian, J., &amp; Liu, T.-Y. (2019). DeepGBM: A deep learning framework distilled by GBDT for online prediction tasks. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 384–394.</unstructured_citation>
          </citation>
          <citation key="node">
            <article_title>Neural oblivious decision ensembles for deep learning on tabular data</article_title>
            <author>Popov</author>
            <journal_title>arXiv preprint arXiv:1909.06312</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Popov, S., Morozov, S., &amp; Babenko, A. (2019). Neural oblivious decision ensembles for deep learning on tabular data. arXiv Preprint arXiv:1909.06312.</unstructured_citation>
          </citation>
          <citation key="ndt">
            <article_title>Neural decision trees</article_title>
            <author>Balestriero</author>
            <journal_title>arXiv preprint arXiv:1702.07360</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Balestriero, R. (2017). Neural decision trees. arXiv Preprint arXiv:1702.07360.</unstructured_citation>
          </citation>
          <citation key="taylor2018prophet">
            <article_title>Forecasting at scale</article_title>
            <author>Taylor</author>
            <journal_title>The American Statistician</journal_title>
            <issue>1</issue>
            <volume>72</volume>
            <doi>10.1080/00031305.2017.1380080</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Taylor, S. J., &amp; Letham, B. (2018). Forecasting at scale. The American Statistician, 72(1), 37–45. https://doi.org/10.1080/00031305.2017.1380080</unstructured_citation>
          </citation>
          <citation key="hadsell2006dimensionality">
            <article_title>Dimensionality reduction by learning an invariant mapping</article_title>
            <author>Hadsell</author>
            <journal_title>2006 IEEE computer society conference on computer vision and pattern recognition (CVPR’06)</journal_title>
            <volume>2</volume>
            <doi>10.1109/cvpr.2006.100</doi>
            <cYear>2006</cYear>
            <unstructured_citation>Hadsell, R., Chopra, S., &amp; LeCun, Y. (2006). Dimensionality reduction by learning an invariant mapping. 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06), 2, 1735–1742. https://doi.org/10.1109/cvpr.2006.100</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
