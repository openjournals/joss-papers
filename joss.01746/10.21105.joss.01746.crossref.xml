<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>c74b0473e5fd1aaf7f0d772818ea8b1a</doi_batch_id>
    <timestamp>20200115174108</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2020</year>
        </publication_date>
        <journal_volume>
          <volume>5</volume>
        </journal_volume>
        <issue>45</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Larq: An Open-Source Library for Training Binarized Neural Networks</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Lukas</given_name>
            <surname>Geiger</surname>
            <ORCID>http://orcid.org/0000-0002-8697-9920</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Plumerai</given_name>
            <surname>Team</surname>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>15</day>
          <year>2020</year>
        </publication_date>
        <pages>
          <first_page>1746</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.01746</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.6084/m9.figshare.11619912.v1”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/1746</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.01746</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.01746</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.01746/10.21105.joss.01746.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <unstructured_citation>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems, Martı́n Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Jia, Yangqing and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dandelion Mané and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viégas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng, 2015, https://www.tensorflow.org/, Software available from tensorflow.org</unstructured_citation>
          </citation>
          <citation key="ref2">
            <unstructured_citation>Keras, Chollet, François, 2015, ://keras.io</unstructured_citation>
          </citation>
          <citation key="ref3">
            <unstructured_citation>Automatic Differentiation in PyTorch, Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam, 2017, NIPS Autodiff Workshop</unstructured_citation>
          </citation>
          <citation key="ref4">
            <unstructured_citation>Bethge, Joseph and Bornstein, Marvin and Loy, Adrian and Yang, Haojin and Meinel, Christoph, Training Competitive Binary Neural Networks from Scratch, CoRR, abs/1812.01965, 2018, http://arxiv.org/abs/1812.01965, arXiv, 1812.01965, Tue, 01 Jan 2019 15:01:25 +0100, https://dblp.org/rec/bib/journals/corr/abs-1812-01965, dblp computer science bibliography, https://dblp.org</unstructured_citation>
          </citation>
          <citation key="ref5">
            <unstructured_citation>daBNN: A Super Fast Inference Framework for Binary Neural Networks on ARM devices, Zhang, Jianhao and Pan, Yingwei and Yao, Ting and Zhao, He and Mei, Tao, 2019, arXiv:1908.05858</unstructured_citation>
          </citation>
          <citation key="ref6">
            <unstructured_citation>Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization, Helwegen, Koen and Widdicombe, James and Geiger, Lukas and Liu, Zechun and Cheng, Kwang-Ting and Nusselder, Roeland, 2019, Advances in Neural Information Processing Systems 32, Curran Associates, Inc., 7531–7542, http://papers.nips.cc/paper/8971-latent-weights-do-not-exist-rethinking-binarized-neural-network-optimization.pdf, Wallach, H. and Larochelle, H. and Beygelzimer, A. and d’ Alché-Buc, F. and Fox, E. and Garnett, R.</unstructured_citation>
          </citation>
          <citation key="ref7">
            <unstructured_citation>Binarized Neural Networks, Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua, 2016, Advances in Neural Information Processing Systems 29, Curran Associates, Inc., 4107–4115, http://papers.nips.cc/paper/6573-binarized-neural-networks.pdf, Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.</unstructured_citation>
          </citation>
          <citation key="ref8">
            <unstructured_citation>Back to Simplicity: How to Train Accurate BNNs from Scratch?, Bethge, Joseph and Yang, Haojin and Bornstein, Marvin and Meinel, Christoph, 2019, CoRR, abs/1906.08637, http://arxiv.org/abs/1906.08637, arXiv, 1906.08637, Mon, 24 Jun 2019 17:28:45 +0200, https://dblp.org/rec/bib/journals/corr/abs-1906-08637, dblp computer science bibliography, https://dblp.org</unstructured_citation>
          </citation>
          <citation key="ref9">
            <doi>10.1007/978-3-030-01267-0_44</doi>
          </citation>
          <citation key="ref10">
            <doi>10.1007/978-3-319-46493-0_32</doi>
          </citation>
          <citation key="ref11">
            <unstructured_citation>DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients, Zhou, Shuchang and Ni, Zekun and Zhou, Xinyu and Wen, He and Wu, Yuxin and Zou, Yuheng, 2016, CoRR, abs/1606.06160, http://arxiv.org/abs/1606.06160, arXiv, 1606.06160, Mon, 13 Aug 2018 16:48:10 +0200, https://dblp.org/rec/bib/journals/corr/ZhouNZWWZ16, dblp computer science bibliography, https://dblp.org</unstructured_citation>
          </citation>
          <citation key="ref12">
            <doi>10.1038/323533a0</doi>
          </citation>
          <citation key="ref13">
            <unstructured_citation>Neural networks for machine learning, Hinton, Geoffrey, 2012, Coursera Video Lectures</unstructured_citation>
          </citation>
          <citation key="ref14">
            <unstructured_citation>BinaryConnect: Training Deep Neural Networks with binary weights during propagations, Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre, 2015, Advances in Neural Information Processing Systems 28, Curran Associates, Inc., 3123–3131, http://papers.nips.cc/paper/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf, Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.</unstructured_citation>
          </citation>
          <citation key="ref15">
            <unstructured_citation>An Empirical study of Binary Neural Networks’ Optimisation, Alizadeh, Milad and Fernández-Marqués, Javier and Lane, Nicholas D. and Gal, Yarin, 2019, 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, https://openreview.net/forum?id=rJfUCoR5KX, DBLP:conf/iclr/2019, Thu, 25 Jul 2019 14:25:40 +0200, https://dblp.org/rec/bib/conf/iclr/AlizadehFLG19, dblp computer science bibliography, https://dblp.org</unstructured_citation>
          </citation>
          <citation key="ref16">
            <doi>10.1109/CVPR.2019.00506</doi>
          </citation>
          <citation key="ref17">
            <doi>10.1109/CVPR.2019.00050</doi>
          </citation>
          <citation key="ref18">
            <unstructured_citation>Stochastic gradient learning in neural networks, Bottou, Léon, 1991, Proceedings of Neuro Nimes, 91, 8, 12</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
