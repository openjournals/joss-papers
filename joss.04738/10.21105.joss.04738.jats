<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4738</article-id>
<article-id pub-id-type="doi">10.21105/joss.04738</article-id>
<title-group>
<article-title>py2lispIDyOM: A Python package for the information
dynamics of music (IDyOM) model</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0002-4570-906X</contrib-id>
<name>
<surname>Guan</surname>
<given-names>Xinyi</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9097-2633</contrib-id>
<name>
<surname>Ren</surname>
<given-names>Zeng</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-5960-8174</contrib-id>
<name>
<surname>Pelofi</surname>
<given-names>Claire</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Max Planck NYU Center for Language, Music and Emotion, New
York, NY 10003 USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Digital and Cognitive Musicology Lab, École Polytechnique
Fédérale de Lausanne, Lausanne, VD 1015 Switzerland</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-06-15">
<day>15</day>
<month>6</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>79</issue>
<fpage>4738</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>music cognition</kwd>
<kwd>IDyOM</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Music is a complex, multi-layered signal that displays structures
  along a variety of dimensions - among which melodic and rhythmic
  sequences play a crucial role across styles and cultures
  (<xref alt="Pearce &amp; Wiggins, 2006" rid="ref-PearceWiggins2006" ref-type="bibr">Pearce
  &amp; Wiggins, 2006</xref>). Empirical studies have consistently
  demonstrated that listeners have strong and well-defined musical
  predictions that reflect the long-range statistical regularities
  present in the music they have heard across their lifespan
  (<xref alt="Margulis, 2005" rid="ref-Margulis2005" ref-type="bibr">Margulis,
  2005</xref>;
  <xref alt="Morgan et al., 2019" rid="ref-Morgan2019" ref-type="bibr">Morgan
  et al., 2019</xref>). These statistics are learned through passive
  exposure to the music in everyday life
  (<xref alt="Bigand &amp; Poulin-Charronnat, 2006" rid="ref-Bigand2006" ref-type="bibr">Bigand
  &amp; Poulin-Charronnat, 2006</xref>;
  <xref alt="Eerola et al., 2009" rid="ref-Eerola2009" ref-type="bibr">Eerola
  et al., 2009</xref>;
  <xref alt="Rohrmeier et al., 2011" rid="ref-Rohrmeier2011" ref-type="bibr">Rohrmeier
  et al., 2011</xref>).</p>
  <p>The Information Dynamics of Music (IDyOM) has been a
  well-established computational model for melodic expectation in the
  music cognition community and has been empirically tested in various
  behavioral and neural studies
  (<xref alt="Di Liberto et al., 2020" rid="ref-Di2020" ref-type="bibr">Di
  Liberto et al., 2020</xref>;
  <xref alt="Pearce &amp; Wiggins, 2012" rid="ref-PearceWiggins2012" ref-type="bibr">Pearce
  &amp; Wiggins, 2012</xref>;
  <xref alt="Politimou et al., 2021" rid="ref-Politimou2021" ref-type="bibr">Politimou
  et al., 2021</xref>). IDyOM models listeners’ online expectations of
  musical events using predictions from variable-order Markov chains
  (<xref alt="Pearce, 2005" rid="ref-Pearce2005" ref-type="bibr">Pearce,
  2005</xref>,
  <xref alt="2018" rid="ref-Pearce2018" ref-type="bibr">2018</xref>).
  Its long-term component is pre-trained on a large musical corpus and
  its short-term component dynamically learns the local statistics of a
  melody, simulating long-term learning of musical statistics and
  short-term learning of musical patterns respectively.</p>
  <p>For each note in a melody, IDyOM outputs a probability derived by
  merging the long-term and short-term distributions. From this
  distribution, two information-theoretic measures characterize the
  predictions of the model. Surprisal (or Information Content)
  represents the expectedness (i.e. the note predicted matched the note
  heard) of each note given the long term (corpus statistics) and short
  term (melody statistics) context. Entropy corresponds to the degree of
  uncertainty of the prediction being made. Entropy and surprisal are
  intended to simulate listeners’ dynamical updating expectations–their
  predictions–when listening to music.</p>
  <p>Although the model has been firmly established as a powerful tool
  to model listener’s experience, the Common Lisp ecosystem, in which
  the IDyOM model is built in, entails a significant entry barrier for
  researchers who intend to use the model. On the one hand, Common Lisp
  is a fairly niche programming language for data analysis and gathers a
  rather small community of users in music psychology and music
  cognition in this day and age. On the other hand, to use the IDyOM
  model, it is assumed that users, who are often new to the Lisp
  language, are familiar with Emacs and SLIME, which themselves can take
  up a lot of time and energy to learn. Therefore, obtaining the IDyOM
  outputs can be discouraging and time-consuming.</p>
  <p>To help researchers further bring insights in the music cognition
  domain, we introduce the <monospace>py2lispIDyOM</monospace> package
  which aims to fill this gap by providing an easy-to-use Python-based
  interface to run IDyOM model and harness the extensive support
  libraries in Python to conduct IDyOM-based analysis. With
  <monospace>py2lispIDyOM</monospace>, we circumvent the challenge of
  writing Lisp codes and hide the complexities of the necessary
  interactions with Lisp in Emacs from the users.</p>
</sec>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>py2lispIDyOM</monospace> is an open-source Python
  package that serves as a unifying Python interface that simplifies and
  streamlines the research workflow for running the IDyOM model and
  analyzing output data. It is broadly aimed at researchers conducting
  IDyOM-based analysis in Python. This package makes it easier to do the
  following two tasks: (i) configuring and running the IDyOM model, and
  (ii) processing and analyzing the IDyOM output data.</p>
  <p>Users can now configure the IDyOM experiments in Python using the
  <monospace>IDyOMExperiment</monospace> object and, based on it,
  <monospace>py2lispIDyOM</monospace> will generate a Lisp script to run
  the IDyOM. To encourage an organized workflow and to improve the
  reproducibility of data, all data related to an experiment will be
  logged in a structured folder, which can be shared with other
  researchers to verify and replicate the experiment results. The logged
  data includes, but is not limited to, the datasets used for the
  experiment, the generated Lisp script, and the IDyOM output file (in
  <monospace>.dat</monospace> format).</p>
  <p>For the processing and analysis of the IDyOM outputs, we
  implemented three functionalities as modules:
  <monospace>extract</monospace>, <monospace>export</monospace>, and
  <monospace>visualization</monospace>. Each module contains methods
  that we frequently used in our previous research projects. In the
  current version, we provided several common types of figures as shown
  below in the <monospace>visualization</monospace> module. For example,
  for each test melody, we can show the piano roll plots for pitch
  prediction distribution
  (<xref alt="Figure 1" rid="figU003AFig1">Figure 1</xref>, upper panel)
  and visually compare it to the ground truth
  (<xref alt="Figure 1" rid="figU003AFig1">Figure 1</xref>, lower panel)
  for this particular melody. The surprisal values can be visualized
  along with the corresponding piano roll, as illustrated in
  <xref alt="Figure 2" rid="figU003AFig2">Figure 2</xref>. The model’s
  output for distinct viewpoints can be visualized in
  <xref alt="Figure 3" rid="figU003AFig3">Figure 3</xref> (upper panel:
  pitch only, middle panel: onset only, lower panel: pitch and onset
  combined). <xref alt="Figure 4" rid="figU003AFig4">Figure 4</xref>
  displays the predicted surprisal and entropy values (based on the
  pitch and onset combined model). All plotting functions in this
  package follow the same syntax, and therefore users can easily
  customize their own figures by using the provided examples. The
  package includes tutorials in the form of Jupyter notebooks on the
  GitHub repository. These tutorials present examples using the
  aforementioned functionalities.</p>
  <p>The package has been used in several ongoing research projects at
  the Max Planck - NYU Center for Language, Music and Emotion.
  Therefore, we hope this package can bring similar values to other
  research groups working on IDyOM-based analysis.</p>
</sec>
<sec id="figures">
  <title>Figures</title>
  <fig>
    <caption><p>Upper panel: Model’s prediction of pitch distribution
    for melody “chor-003”. Lower panel: display of the ground truth
    piano roll for melody “chor-003”.
    <styled-content id="figU003AFig1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/pitch-pred-chor-003.png" xlink:title="" />
  </fig>
  <fig>
    <caption><p>Upper panel: Display of ground truth piano roll plot of
    for melody “chor-003”. Lower panel: model’s surprisal output plot
    for melody “chor-003” is
    aligned.<styled-content id="figU003AFig2"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/groundtruth-surprisal-chor-003.png" xlink:title="" />
  </fig>
  <fig>
    <caption><p>Model’s surprisal outputs of each target viewpoints for
    melody “chor-003” is displayed: pitch (upper panel), onset (middle
    panel) and overall/combined (lower
    panel).<styled-content id="figU003AFig3"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/all-surprisals-chor-003.png" xlink:title="" />
  </fig>
  <fig>
    <caption><p>Model’s overall/combined surprisal and entropy outputs
    for melody “chor-003” is
    displayed.<styled-content id="figU003AFig4"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/surprisal-entropy-chor-003.png" xlink:title="" />
  </fig>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-PearceWiggins2006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pearce</surname><given-names>Marcus T</given-names></name>
        <name><surname>Wiggins</surname><given-names>Geraint A</given-names></name>
      </person-group>
      <article-title>Expectation in melody: The influence of context and learning</article-title>
      <source>Music Perception</source>
      <year iso-8601-date="2006">2006</year>
      <volume>23</volume>
      <issue>5</issue>
      <uri>https://online.ucpress.edu/mp/article/23/5/377/62287/Expectation-in-Melody-The-Influence-of-Context-and</uri>
      <pub-id pub-id-type="doi">10.1525/mp.2006.23.5.377</pub-id>
      <fpage>377</fpage>
      <lpage>405</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Margulis2005">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Margulis</surname><given-names>Elizabeth Hellmuth</given-names></name>
      </person-group>
      <article-title>A model of melodic expectation</article-title>
      <source>Music Perception</source>
      <year iso-8601-date="2005">2005</year>
      <volume>22</volume>
      <issue>4</issue>
      <uri>https://online.ucpress.edu/mp/article/22/4/663/62196/A-Model-of-Melodic-Expectation</uri>
      <pub-id pub-id-type="doi">10.1525/mp.2005.22.4.663</pub-id>
      <fpage>663</fpage>
      <lpage>714</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Morgan2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Morgan</surname><given-names>Emily</given-names></name>
        <name><surname>Fogel</surname><given-names>Allison</given-names></name>
        <name><surname>Nair</surname><given-names>Anjali</given-names></name>
        <name><surname>Patel</surname><given-names>Aniruddh D</given-names></name>
      </person-group>
      <article-title>Statistical learning and gestalt-like principles predict melodic expectations</article-title>
      <source>Cognition</source>
      <year iso-8601-date="2019">2019</year>
      <volume>189</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010027718303317</uri>
      <pub-id pub-id-type="doi">10.1016/j.cognition.2018.12.015</pub-id>
      <fpage>23</fpage>
      <lpage>34</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bigand2006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bigand</surname><given-names>Emmanuel</given-names></name>
        <name><surname>Poulin-Charronnat</surname><given-names>Bénédicte</given-names></name>
      </person-group>
      <article-title>Are we “experienced listeners”? A review of the musical capacities that do not depend on formal musical training</article-title>
      <source>Cognition</source>
      <year iso-8601-date="2006">2006</year>
      <volume>100</volume>
      <issue>1</issue>
      <uri>https://www.sciencedirect.com/science/article/pii/S0010027705002234</uri>
      <pub-id pub-id-type="doi">10.1016/j.cognition.2005.11.007</pub-id>
      <fpage>100</fpage>
      <lpage>130</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Eerola2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eerola</surname><given-names>Tuomas</given-names></name>
        <name><surname>Louhivuori</surname><given-names>Jukka</given-names></name>
        <name><surname>Lebaka</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>Expectancy in sami yoiks revisited: The role of data-driven and schema-driven knowledge in the formation of melodic expectations</article-title>
      <source>Musicae Scientiae</source>
      <year iso-8601-date="2009">2009</year>
      <volume>13</volume>
      <issue>2</issue>
      <uri>https://doi.org/10.1177/102986490901300203</uri>
      <pub-id pub-id-type="doi">10.1177/102986490901300203</pub-id>
      <fpage>231</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Rohrmeier2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rohrmeier</surname><given-names>Martin</given-names></name>
        <name><surname>Rebuschat</surname><given-names>Patrick</given-names></name>
        <name><surname>Cross</surname><given-names>Ian</given-names></name>
      </person-group>
      <article-title>Incidental and online learning of melodic structure</article-title>
      <source>Consciousness and cognition</source>
      <year iso-8601-date="2011">2011</year>
      <volume>20</volume>
      <issue>2</issue>
      <uri>https://www.sciencedirect.com/science/article/pii/S1053810010001467</uri>
      <pub-id pub-id-type="doi">10.1016/j.concog.2010.07.004</pub-id>
      <fpage>214</fpage>
      <lpage>222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PearceWiggins2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pearce</surname><given-names>Marcus T</given-names></name>
        <name><surname>Wiggins</surname><given-names>Geraint A</given-names></name>
      </person-group>
      <article-title>Auditory expectation: The information dynamics of music perception and cognition</article-title>
      <source>Topics in cognitive science</source>
      <year iso-8601-date="2012">2012</year>
      <volume>4</volume>
      <issue>4</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1756-8765.2012.01214.x</uri>
      <pub-id pub-id-type="doi">10.1111/j.1756-8765.2012.01214.x</pub-id>
      <fpage>625</fpage>
      <lpage>652</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Di2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Di Liberto</surname><given-names>Giovanni M</given-names></name>
        <name><surname>Pelofi</surname><given-names>Claire</given-names></name>
        <name><surname>Bianco</surname><given-names>Roberta</given-names></name>
        <name><surname>Patel</surname><given-names>Prachi</given-names></name>
        <name><surname>Mehta</surname><given-names>Ashesh D</given-names></name>
        <name><surname>Herrero</surname><given-names>Jose L</given-names></name>
        <name><surname>Cheveigné</surname><given-names>Alain de</given-names></name>
        <name><surname>Shamma</surname><given-names>Shihab</given-names></name>
        <name><surname>Mesgarani</surname><given-names>Nima</given-names></name>
      </person-group>
      <article-title>Cortical encoding of melodic expectations in human temporal cortex</article-title>
      <source>Elife</source>
      <year iso-8601-date="2020">2020</year>
      <volume>9</volume>
      <uri>https://elifesciences.org/articles/51784</uri>
      <pub-id pub-id-type="doi">10.7554/eLife.51784</pub-id>
      <fpage>e51784</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Politimou2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Politimou</surname><given-names>Nina</given-names></name>
        <name><surname>Douglass-Kirk</surname><given-names>Pedro</given-names></name>
        <name><surname>Pearce</surname><given-names>Marcus</given-names></name>
        <name><surname>Stewart</surname><given-names>Lauren</given-names></name>
        <name><surname>Franco</surname><given-names>Fabia</given-names></name>
      </person-group>
      <article-title>Melodic expectations in 5-and 6-year-old children</article-title>
      <source>Journal of Experimental Child Psychology</source>
      <year iso-8601-date="2021">2021</year>
      <volume>203</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0022096520304744</uri>
      <pub-id pub-id-type="doi">10.1016/j.jecp.2020.105020</pub-id>
      <fpage>105020</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Pearce2005">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Pearce</surname><given-names>Marcus T</given-names></name>
      </person-group>
      <article-title>The construction and evaluation of statistical models of melodic structure in music perception and composition</article-title>
      <publisher-name>City University London</publisher-name>
      <year iso-8601-date="2005">2005</year>
      <uri>https://openaccess.city.ac.uk/id/eprint/8459/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Pearce2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pearce</surname><given-names>Marcus T</given-names></name>
      </person-group>
      <article-title>Statistical learning and probabilistic prediction in music cognition: Mechanisms of stylistic enculturation</article-title>
      <source>Annals of the New York Academy of Sciences</source>
      <year iso-8601-date="2018">2018</year>
      <volume>1423</volume>
      <issue>1</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/nyas.13654</uri>
      <pub-id pub-id-type="doi">10.1111/nyas.13654</pub-id>
      <fpage>378</fpage>
      <lpage>395</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
