<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250607132004-7f0c50737683b3d1e8c60b2936b493db52a55f49</doi_batch_id>
    <timestamp>20250607132004</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>06</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>110</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>gibbonNetR: an R Package for the Use of Convolutional Neural Networks for Automated Detection of Acoustic Data</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Dena Jane</given_name>
            <surname>Clink</surname>
            <affiliations>
              <institution><institution_name>K. Lisa Yang Center for Conservation Bioacoustics, Cornell Lab of Ornithology, Cornell University, Ithaca, New York, United States</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0003-0363-5581</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Abdul Hamid</given_name>
            <surname>Ahmad</surname>
            <affiliations>
              <institution><institution_name>Institute for Tropical Biology and Conservation, Universiti Malaysia Sabah (UMS), Kota Kinabalu, Sabah, Malaysia</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-9658-8600</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>06</month>
          <day>07</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>7250</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.07250</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.15608992</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/7250</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.07250</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.07250</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.07250.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ROCR">
            <article_title>ROCR: Visualizing classifier performance in r</article_title>
            <author>Sing</author>
            <journal_title>Bioinformatics</journal_title>
            <issue>20</issue>
            <volume>21</volume>
            <doi>10.1093/bioinformatics/bti623</doi>
            <cYear>2005</cYear>
            <unstructured_citation>Sing, T., Sander, O., Beerenwinkel, N., &amp; Lengauer, T. (2005). ROCR: Visualizing classifier performance in r. Bioinformatics, 21(20), 7881. https://doi.org/10.1093/bioinformatics/bti623</unstructured_citation>
          </citation>
          <citation key="seewave2008">
            <article_title>Seewave: A free modular tool for sound analysis and synthesis</article_title>
            <author>Sueur</author>
            <journal_title>Bioacoustics</journal_title>
            <volume>18</volume>
            <doi>10.1080/09524622.2008.9753600</doi>
            <cYear>2008</cYear>
            <unstructured_citation>Sueur, J., Aubin, T., &amp; Simonis, C. (2008). Seewave: A free modular tool for sound analysis and synthesis. Bioacoustics, 18, 213–226. https://doi.org/10.1080/09524622.2008.9753600</unstructured_citation>
          </citation>
          <citation key="clink2019gibbonfindr">
            <article_title>gibbonR: An r package for the detection and classification of acoustic signals</article_title>
            <author>Clink</author>
            <journal_title>arXiv preprint arXiv:1906.02572</journal_title>
            <doi>10.48550/arXiv.1906.02572</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Clink, D. J., &amp; Klinck, H. (2019). gibbonR: An r package for the detection and classification of acoustic signals. arXiv Preprint arXiv:1906.02572. https://doi.org/10.48550/arXiv.1906.02572</unstructured_citation>
          </citation>
          <citation key="clinkzenodo2024">
            <article_title>A labelled dataset of the loud calls of four vertebrates collected using passive acoustic monitoring in malaysian borneo</article_title>
            <author>Clink</author>
            <doi>10.5281/zenodo.14213067</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Clink, D. J., &amp; Hamid Ahmad, A. (2024). A labelled dataset of the loud calls of four vertebrates collected using passive acoustic monitoring in malaysian borneo. https://doi.org/10.5281/zenodo.14213067</unstructured_citation>
          </citation>
          <citation key="araya2017warbler">
            <article_title>warbleR: An r package to streamline analysis of animal acoustic signals</article_title>
            <author>Araya-Salas</author>
            <journal_title>Methods in Ecology and Evolution</journal_title>
            <issue>2</issue>
            <volume>8</volume>
            <doi>10.1111/2041-210X.12624</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Araya-Salas, M., &amp; Smith-Vidaurre, G. (2017). warbleR: An r package to streamline analysis of animal acoustic signals. Methods in Ecology and Evolution, 8(2), 184–191. https://doi.org/10.1111/2041-210X.12624</unstructured_citation>
          </citation>
          <citation key="clink2023">
            <article_title>A workflow for the automated detection and classification of female gibbon calls from long-term acoustic recordings</article_title>
            <author>Clink</author>
            <journal_title>Frontiers in Ecology and Evolution</journal_title>
            <volume>11</volume>
            <doi>10.3389/fevo.2023.1071640</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Clink, D. J., Kier, I., Ahmad, A. H., &amp; Klinck, H. (2023). A workflow for the automated detection and classification of female gibbon calls from long-term acoustic recordings. Frontiers in Ecology and Evolution, 11. https://doi.org/10.3389/fevo.2023.1071640</unstructured_citation>
          </citation>
          <citation key="sugai2019">
            <article_title>Terrestrial passive acoustic monitoring: Review and perspectives</article_title>
            <author>Sugai</author>
            <journal_title>BioScience</journal_title>
            <issue>1</issue>
            <volume>69</volume>
            <doi>10.1093/biosci/biy147</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Sugai, L. S. M., Silva, T. S. F., Ribeiro, J. W., &amp; Llusia, D. (2019). Terrestrial passive acoustic monitoring: Review and perspectives. BioScience, 69(1), 1525. https://doi.org/10.1093/biosci/biy147</unstructured_citation>
          </citation>
          <citation key="gibb2018">
            <article_title>Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring</article_title>
            <author>Gibb</author>
            <journal_title>Methods in Ecology and Evolution</journal_title>
            <doi>10.1111/2041-210X.13101</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Gibb, R., Browning, E., Glover-Kapfer, P., &amp; Jones, K. E. (2018). Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210X.13101</unstructured_citation>
          </citation>
          <citation key="katz2016">
            <article_title>Assessment of error rates in acoustic monitoring with the r package monitoR</article_title>
            <author>Katz</author>
            <journal_title>Bioacoustics</journal_title>
            <issue>2</issue>
            <volume>25</volume>
            <doi>10.1080/09524622.2015.1133320</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Katz, J., Hafner, S. D., &amp; Donovan, T. (2016). Assessment of error rates in acoustic monitoring with the r package monitoR. Bioacoustics, 25(2), 177196. https://doi.org/10.1080/09524622.2015.1133320</unstructured_citation>
          </citation>
          <citation key="balantic2020">
            <article_title>AMMonitor: Remote monitoring of biodiversity in an adaptive framework with r</article_title>
            <author>Balantic</author>
            <journal_title>Methods in Ecology and Evolution</journal_title>
            <issue>7</issue>
            <volume>11</volume>
            <doi>10.1111/2041-210X.13397</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Balantic, C., &amp; Donovan, T. (2020). AMMonitor: Remote monitoring of biodiversity in an adaptive framework with r. Methods in Ecology and Evolution, 11(7), 869877. https://doi.org/10.1111/2041-210X.13397</unstructured_citation>
          </citation>
          <citation key="kalan2015">
            <article_title>Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring</article_title>
            <author>Kalan</author>
            <journal_title>Ecological Indicators</journal_title>
            <issue>July 2015</issue>
            <volume>54</volume>
            <doi>10.1016/j.ecolind.2015.02.023</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Kalan, A. K., Mundry, R., Wagner, O. J. J., Heinicke, S., Boesch, C., &amp; Kühl, H. S. (2015). Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring. Ecological Indicators, 54(July 2015), 217226. https://doi.org/10.1016/j.ecolind.2015.02.023</unstructured_citation>
          </citation>
          <citation key="lecun2015">
            <article_title>Deep learning</article_title>
            <author>LeCun</author>
            <journal_title>Nature</journal_title>
            <issue>7553</issue>
            <volume>521</volume>
            <doi>10.1038/nature14539</doi>
            <cYear>2015</cYear>
            <unstructured_citation>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. Nature, 521(7553), 436–444. https://doi.org/10.1038/nature14539</unstructured_citation>
          </citation>
          <citation key="scavetta2021">
            <volume_title>Python and r for the modern data scientist</volume_title>
            <author>Scavetta</author>
            <doi>10.18637/jss.v103.b02</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Scavetta, R. J., &amp; Angelov, B. (2021). Python and r for the modern data scientist. O’Reilly Media, Inc. https://doi.org/10.18637/jss.v103.b02</unstructured_citation>
          </citation>
          <citation key="lawlor2022">
            <article_title>Ten simple rules for teaching yourself R</article_title>
            <author>Lawlor</author>
            <journal_title>PLOS Computational Biology</journal_title>
            <issue>9</issue>
            <volume>18</volume>
            <doi>10.1371/journal.pcbi.1010372</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Lawlor, J., Banville, F., Forero-Muñoz, N.-R., Hébert, K., Martínez-Lanfranco, J. A., Rogy, P., &amp; MacDonald, A. A. M. (2022). Ten simple rules for teaching yourself R. PLOS Computational Biology, 18(9), e1010372. https://doi.org/10.1371/journal.pcbi.1010372</unstructured_citation>
          </citation>
          <citation key="chollet2015">
            <article_title>Keras</article_title>
            <author>Chollet</author>
            <doi>10.1163/1574-9347_bnp_e612900</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Chollet, F., &amp; others. (2015). Keras. https://doi.org/10.1163/1574-9347_bnp_e612900</unstructured_citation>
          </citation>
          <citation key="paszke2019">
            <article_title>PyTorch: An imperative style, high-performance deep learning library</article_title>
            <author>Paszke</author>
            <doi>10.48550/arXiv.1912.01703</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library (p. 80248035). Curran Associates, Inc. https://doi.org/10.48550/arXiv.1912.01703</unstructured_citation>
          </citation>
          <citation key="martínabadi2015">
            <article_title>TensorFlow: Large-scale machine learning on heterogeneous systems</article_title>
            <author>Martín Abadi</author>
            <doi>10.48550/arXiv.1605.08695</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Jia, Y., Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, … Xiaoqiang Zheng. (2015). TensorFlow: Large-scale machine learning on heterogeneous systems. https://doi.org/10.48550/arXiv.1605.08695</unstructured_citation>
          </citation>
          <citation key="ushey2022">
            <volume_title>Reticulate: Interface to ’python’</volume_title>
            <author>Ushey</author>
            <doi>10.32614/CRAN.package.reticulat</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Ushey, K., Allaire, J. J., &amp; Tang, Y. (2022). Reticulate: Interface to ’python’. https://doi.org/10.32614/CRAN.package.reticulat</unstructured_citation>
          </citation>
          <citation key="falbel2023">
            <volume_title>Luz: Higher level ’API’ for ’torch’</volume_title>
            <author>Falbel</author>
            <doi>10.32614/CRAN.package.luz</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Falbel, D. (2023). Luz: Higher level ’API’ for ’torch’. https://doi.org/10.32614/CRAN.package.luz</unstructured_citation>
          </citation>
          <citation key="stevens2020">
            <volume_title>Deep Learning with PyTorch</volume_title>
            <author>Stevens</author>
            <cYear>2020</cYear>
            <unstructured_citation>Stevens, E., Antiga, L., &amp; Viehmann, T. (2020). Deep Learning with PyTorch. Simon; Schuster.</unstructured_citation>
          </citation>
          <citation key="goodfellow2016">
            <volume_title>Deep learning</volume_title>
            <author>Goodfellow</author>
            <cYear>2016</cYear>
            <unstructured_citation>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. MIT Press.</unstructured_citation>
          </citation>
          <citation key="deng2009">
            <article_title>Imagenet: A large-scale hierarchical image database</article_title>
            <author>Deng</author>
            <doi>10.1109/cvpr.2009.5206848</doi>
            <cYear>2009</cYear>
            <unstructured_citation>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., &amp; Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. 248255. https://doi.org/10.1109/cvpr.2009.5206848</unstructured_citation>
          </citation>
          <citation key="dufourq2022">
            <article_title>Passive acoustic monitoring of animal populations with transfer learning</article_title>
            <author>Dufourq</author>
            <journal_title>Ecological Informatics</journal_title>
            <volume>70</volume>
            <doi>10.1016/j.ecoinf.2022.101688</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Dufourq, E., Batist, C., Foquet, R., &amp; Durbach, I. (2022). Passive acoustic monitoring of animal populations with transfer learning. Ecological Informatics, 70, 101688. https://doi.org/10.1016/j.ecoinf.2022.101688</unstructured_citation>
          </citation>
          <citation key="ruan2022">
            <article_title>ResNet-based bio-acoustics presence detection technology of hainan gibbon calls</article_title>
            <author>Ruan</author>
            <journal_title>Applied Acoustics</journal_title>
            <volume>198</volume>
            <doi>10.1016/j.apacoust.2022.108939</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Ruan, W., Wu, K., Chen, Q., &amp; Zhang, C. (2022). ResNet-based bio-acoustics presence detection technology of hainan gibbon calls. Applied Acoustics, 198, 108939. https://doi.org/10.1016/j.apacoust.2022.108939</unstructured_citation>
          </citation>
          <citation key="keydana2023">
            <volume_title>Deep learning and scientific computing with r torch</volume_title>
            <author>Keydana</author>
            <doi>10.1201/9781003275923</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Keydana, S. (2023). Deep learning and scientific computing with r torch. CRC Press. https://doi.org/10.1201/9781003275923</unstructured_citation>
          </citation>
          <citation key="lecun1995">
            <volume_title>Convolutional networks for images, speech, and time series</volume_title>
            <author>LeCun</author>
            <series_title>The handbook of brain theory and neural networks</series_title>
            <volume>3361</volume>
            <cYear>1995</cYear>
            <unstructured_citation>LeCun, Y., Bengio, Y., &amp; others. (1995). Convolutional networks for images, speech, and time series. In The handbook of brain theory and neural networks (Vol. 3361, p. 1995).</unstructured_citation>
          </citation>
          <citation key="gu2018">
            <article_title>Recent advances in convolutional neural networks</article_title>
            <author>Gu</author>
            <journal_title>Pattern recognition</journal_title>
            <volume>77</volume>
            <doi>10.1016/j.patcog.2017.10.013</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J., &amp; others. (2018). Recent advances in convolutional neural networks. Pattern Recognition, 77, 354377. https://doi.org/10.1016/j.patcog.2017.10.013</unstructured_citation>
          </citation>
          <citation key="krizhevsky2017">
            <article_title>Imagenet classification with deep convolutional neural networks</article_title>
            <author>Krizhevsky</author>
            <journal_title>Communications of the ACM</journal_title>
            <issue>6</issue>
            <volume>60</volume>
            <doi>10.1145/3065386</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2017). Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6), 8490. https://doi.org/10.1145/3065386</unstructured_citation>
          </citation>
          <citation key="simonyan2014">
            <article_title>Very deep convolutional networks for large-scale image recognition</article_title>
            <author>Simonyan</author>
            <journal_title>arXiv preprint arXiv:1409.1556</journal_title>
            <doi>10.48550/arXiv.1409.1556</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Simonyan, K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv Preprint arXiv:1409.1556. https://doi.org/10.48550/arXiv.1409.1556</unstructured_citation>
          </citation>
          <citation key="he2016">
            <article_title>Deep residual learning for image recognition</article_title>
            <author>He</author>
            <doi>10.1109/cvpr.2016.90</doi>
            <cYear>2016</cYear>
            <unstructured_citation>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep residual learning for image recognition. 770778. https://doi.org/10.1109/cvpr.2016.90</unstructured_citation>
          </citation>
          <citation key="lakdari2024mel">
            <article_title>Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons</article_title>
            <author>Lakdari</author>
            <journal_title>Ecological Informatics</journal_title>
            <volume>80</volume>
            <doi>10.1016/j.ecoinf.2023.102457</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Lakdari, M. W., Ahmad, A. H., Sethi, S., Bohn, G. A., &amp; Clink, D. J. (2024). Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons. Ecological Informatics, 80, 102457. https://doi.org/10.1016/j.ecoinf.2023.102457</unstructured_citation>
          </citation>
          <citation key="ghani2023">
            <article_title>Global birdsong embeddings enable superior transfer learning for bioacoustic classification</article_title>
            <author>Ghani</author>
            <journal_title>Scientific Reports</journal_title>
            <issue>1</issue>
            <volume>13</volume>
            <doi>10.1038/s41598-023-49989-z</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Ghani, B., Denton, T., Kahl, S., &amp; Klinck, H. (2023). Global birdsong embeddings enable superior transfer learning for bioacoustic classification. Scientific Reports, 13(1), 22876. https://doi.org/10.1038/s41598-023-49989-z</unstructured_citation>
          </citation>
          <citation key="clink2021not">
            <article_title>Not by the light of the moon: Investigating circadian rhythms and environmental predictors of calling in bornean great argus</article_title>
            <author>Clink</author>
            <journal_title>Plos one</journal_title>
            <issue>2</issue>
            <volume>16</volume>
            <doi>10.1371/journal.pone.0246564</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Clink, D. J., Groves, T., Ahmad, A. H., &amp; Klinck, H. (2021). Not by the light of the moon: Investigating circadian rhythms and environmental predictors of calling in bornean great argus. Plos One, 16(2), e0246564. https://doi.org/10.1371/journal.pone.0246564</unstructured_citation>
          </citation>
          <citation key="kennedy2023evidence">
            <article_title>Evidence for acoustic niche partitioning depends on the temporal scale in two sympatric bornean hornbill species</article_title>
            <author>Kennedy</author>
            <journal_title>Biotropica</journal_title>
            <issue>2</issue>
            <volume>55</volume>
            <doi>10.1111/btp.13205</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Kennedy, A. G., Ahmad, A. H., Klinck, H., Johnson, L. M., &amp; Clink, D. J. (2023). Evidence for acoustic niche partitioning depends on the temporal scale in two sympatric bornean hornbill species. Biotropica, 55(2), 517–528. https://doi.org/10.1111/btp.13205</unstructured_citation>
          </citation>
          <citation key="stowell2022">
            <article_title>Computational bioacoustics with deep learning: a review and roadmap</article_title>
            <author>Stowell</author>
            <journal_title>PeerJ</journal_title>
            <volume>10</volume>
            <doi>10.7717/peerj.13152</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Stowell, D. (2022). Computational bioacoustics with deep learning: a review and roadmap. PeerJ, 10, e13152. https://doi.org/10.7717/peerj.13152</unstructured_citation>
          </citation>
          <citation key="10.1371/journal.pone.0283396">
            <article_title>Deep audio embeddings for vocalisation clustering</article_title>
            <author>Best</author>
            <journal_title>PLOS ONE</journal_title>
            <issue>7</issue>
            <volume>18</volume>
            <doi>10.1371/journal.pone.0283396</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Best, P., Paris, S., Glotin, H., &amp; Marxer, R. (2023). Deep audio embeddings for vocalisation clustering. PLOS ONE, 18(7), 1–18. https://doi.org/10.1371/journal.pone.0283396</unstructured_citation>
          </citation>
          <citation key="dbscan">
            <article_title>dbscan: Fast density-based clustering with R</article_title>
            <author>Hahsler</author>
            <journal_title>Journal of Statistical Software</journal_title>
            <issue>1</issue>
            <volume>91</volume>
            <doi>10.18637/jss.v091.i01</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Hahsler, M., Piekenbrock, M., &amp; Doran, D. (2019). dbscan: Fast density-based clustering with R. Journal of Statistical Software, 91(1), 1–30. https://doi.org/10.18637/jss.v091.i01</unstructured_citation>
          </citation>
          <citation key="kuhn2008">
            <article_title>Caret package</article_title>
            <author>Kuhn</author>
            <journal_title>Journal of Statistical Software</journal_title>
            <issue>5</issue>
            <volume>28</volume>
            <doi>10.18637/jss.v028.i05</doi>
            <cYear>2008</cYear>
            <unstructured_citation>Kuhn, M. (2008). Caret package. Journal of Statistical Software, 28(5), 126. https://doi.org/10.18637/jss.v028.i05</unstructured_citation>
          </citation>
          <citation key="ruff2021">
            <article_title>Workflow and convolutional neural network for automated identification of animal sounds</article_title>
            <author>Ruff</author>
            <journal_title>Ecological Indicators</journal_title>
            <volume>124</volume>
            <doi>10.1016/j.ecolind.2021.107419</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Ruff, Z. J., Lesmeister, D. B., Appel, C. L., &amp; Sullivan, C. M. (2021). Workflow and convolutional neural network for automated identification of animal sounds. Ecological Indicators, 124, 107419. https://doi.org/10.1016/j.ecolind.2021.107419</unstructured_citation>
          </citation>
          <citation key="silva2022soundclass">
            <article_title>soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning</article_title>
            <author>Silva</author>
            <journal_title>Methods in Ecology and Evolution</journal_title>
            <doi>10.1111/2041-210X.13964</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Silva, B., Mestre, F., Barreiro, S., Alves, P. J., &amp; Herrera, J. M. (2022). soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning. Methods in Ecology and Evolution. https://doi.org/10.1111/2041-210X.13964</unstructured_citation>
          </citation>
          <citation key="Clink2024benchmark">
            <article_title>Benchmarking for the automated detection and classification of southern yellow-cheeked crested gibbon calls from passive acoustic monitoring data</article_title>
            <author>Clink</author>
            <journal_title>bioRxiv</journal_title>
            <doi>10.1101/2024.08.17.608420</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Clink, D. J., Cross-Jaya, H., Kim, J., Ahmad, A. H., Hong, M., Sala, R., Birot, H., Agger, C., Vu, T. T., Thi, H. N., Chi, T. N., &amp; Klinck, H. (2024). Benchmarking for the automated detection and classification of southern yellow-cheeked crested gibbon calls from passive acoustic monitoring data. bioRxiv. https://doi.org/10.1101/2024.08.17.608420</unstructured_citation>
          </citation>
          <citation key="clink2024automated">
            <article_title>Automated detection of gibbon calls from passive acoustic monitoring data using convolutional neural networks in the" torch for r" ecosystem</article_title>
            <author>Clink</author>
            <journal_title>arXiv preprint arXiv:2407.09976</journal_title>
            <doi>10.48550/arXiv.2407.09976</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Clink, D. J., Kim, J., Cross-Jaya, H., Ahmad, A. H., Hong, M., Sala, R., Birot, H., Agger, C., Vu, T. T., Thi, H. N., &amp; others. (2024). Automated detection of gibbon calls from passive acoustic monitoring data using convolutional neural networks in the" torch for r" ecosystem. arXiv Preprint arXiv:2407.09976. https://doi.org/10.48550/arXiv.2407.09976</unstructured_citation>
          </citation>
          <citation key="Vu2024">
            <article_title>Investigating hunting in a protected area in southeast asia using passive acoustic monitoring with mobile smartphones and transfer learning</article_title>
            <author>Vu</author>
            <journal_title>Ecological Indicators</journal_title>
            <doi>10.1016/j.ecolind.2024.112501</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Vu, T. T., Phan, D. V., Le, T. S., &amp; Clink, D. J. (2024). Investigating hunting in a protected area in southeast asia using passive acoustic monitoring with mobile smartphones and transfer learning. Ecological Indicators. https://doi.org/10.1016/j.ecolind.2024.112501</unstructured_citation>
          </citation>
          <citation key="pytorch_quantized_transfer_learning">
            <article_title>Quantized transfer learning tutorial</article_title>
            <author>Takhirov</author>
            <cYear>2021</cYear>
            <unstructured_citation>Takhirov, Z. (2021). Quantized transfer learning tutorial. https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
