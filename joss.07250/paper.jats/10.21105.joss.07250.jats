<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7250</article-id>
<article-id pub-id-type="doi">10.21105/joss.07250</article-id>
<title-group>
<article-title>gibbonNetR: an R Package for the Use of Convolutional
Neural Networks for Automated Detection of Acoustic Data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0363-5581</contrib-id>
<name>
<surname>Clink</surname>
<given-names>Dena Jane</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9658-8600</contrib-id>
<name>
<surname>Ahmad</surname>
<given-names>Abdul Hamid</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>K. Lisa Yang Center for Conservation Bioacoustics,Cornell
Lab of Ornithology, Cornell University, Ithaca, New York, United
States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute for Tropical Biology and Conservation,Universiti
Malaysia Sabah (UMS), Kota Kinabalu, Sabah, Malaysia</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-01-01">
<day>1</day>
<month>1</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>110</issue>
<fpage>7250</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>deep learning</kwd>
<kwd>passive acoustic monitoring</kwd>
<kwd>gibbon</kwd>
<kwd>automated detection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Automated detection of acoustic signals is crucial for effectively
  monitoring vocal animals and their habitats across large spatial and
  temporal scales. Recent advances in deep learning have made
  high-performance automated detection approaches accessible to more
  practitioners. However, there are few deep learning approaches that
  can be implemented natively in R. The ‘torch for R’ ecosystem has made
  the use of convolutional neural networks (CNNs) accessible for R
  users. Here, we provide an R package and workflow to use CNNs for
  automated detection and classification of acoustics signals from
  passive acoustic monitoring data. We provide examples using data
  collected in Sabah, Malaysia. The package provides functions to create
  spectrogram images from labeled data, compare the performance of
  different CNN architectures, deploy trained models over directories of
  sound files, and extract embeddings from trained models. The R
  programming language remains one of the most commonly used languages
  among ecologists, and we hope that this package makes deep learning
  approaches more accessible to this audience. In addition, these models
  can serve as important benchmarks for future automated detection
  work.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <sec id="passive-acoustic-monitoring">
    <title><italic>Passive acoustic monitoring</italic></title>
    <p>We are in a biodiversity crisis, and there is a great need for
    the ability to rapidly assess biodiversity in order to understand
    and mitigate anthropogenic impacts. One approach that can be
    especially effective for monitoring of sound-producing yet cryptic
    animals is the use of passive acoustic monitoring
    (<xref alt="Gibb et al., 2018" rid="ref-gibb2018" ref-type="bibr">Gibb
    et al., 2018</xref>), a technique that relies on autonomous acoustic
    recording units. PAM allows researchers to monitor acoustically
    active animals and their habitats at temporal and spatial scales
    that are impossible to achieve using only human observers. Interest
    in use of PAM in terrestrial environments has increased
    substantially in recent years
    (<xref alt="Sugai et al., 2019" rid="ref-sugai2019" ref-type="bibr">Sugai
    et al., 2019</xref>), due to the reduced price of autonomous
    recording units and improved battery life and data storage
    capabilities. However, the use of PAM often leads to the collection
    of terabytes of data that is time- and cost-prohibitive to analyze
    manually.</p>
  </sec>
  <sec id="automated-detection">
    <title><italic>Automated detection</italic></title>
    <p>Automated detection for PAM data refers to identifying the start
    and stop time of signals of interest within a longer sound recording
    (<xref alt="Stowell, 2022" rid="ref-stowell2022" ref-type="bibr">Stowell,
    2022</xref>). Some of the early non-deep learning approaches for the
    automated detection of acoustic signals in terrestrial PAM data
    include binary point matching
    (<xref alt="Katz et al., 2016" rid="ref-katz2016" ref-type="bibr">Katz
    et al., 2016</xref>), spectrogram cross-correlation
    (<xref alt="Balantic &amp; Donovan, 2020" rid="ref-balantic2020" ref-type="bibr">Balantic
    &amp; Donovan, 2020</xref>), or the use of a band-limited energy
    detector and subsequent classifier, such as support vector machine
    (<xref alt="Clink et al., 2023" rid="ref-clink2023" ref-type="bibr">Clink
    et al., 2023</xref>;
    <xref alt="Kalan et al., 2015" rid="ref-kalan2015" ref-type="bibr">Kalan
    et al., 2015</xref>). Recent advances in deep learning have
    revolutionized image and speech recognition
    (<xref alt="LeCun et al., 2015" rid="ref-lecun2015" ref-type="bibr">LeCun
    et al., 2015</xref>), with important cross-over for the analysis of
    PAM data. Traditional approaches to machine learning relied heavily
    on feature engineering, since early machine learning algorithms
    required a reduced set of representative features that were manually
    chosen by researchers, such as features estimated from the
    spectrogram.</p>
    <p>Deep learning does not require feature engineering
    (<xref alt="Stevens et al., 2020" rid="ref-stevens2020" ref-type="bibr">Stevens
    et al., 2020</xref>), as the algorithms include a step that
    identifies relevant features from the input. This can lead to faster
    development time and increased ability to represent complex patterns
    typically seen in image and acoustic data. Convolutional neural
    networks (CNNs) — one of the most widely used deep learning
    algorithms—are useful for processing data that have a ‘grid-like
    topology’, such as image data that can be considered a 2-dimensional
    grid of pixels
    (<xref alt="Goodfellow et al., 2016" rid="ref-goodfellow2016" ref-type="bibr">Goodfellow
    et al., 2016</xref>). The ‘convolutional’ layer learns the feature
    representations of the inputs; these convolutional layers consist of
    a set of filters, which are two-dimensional matrices of numbers, and
    the primary parameter is the number of filters
    (<xref alt="Gu et al., 2018" rid="ref-gu2018" ref-type="bibr">Gu et
    al., 2018</xref>). If training data are scarce, overfitting may
    occur as representations of images tend to be large with many
    variables
    (<xref alt="LeCun et al., 1995" rid="ref-lecun1995" ref-type="bibr">LeCun
    et al., 1995</xref>).</p>
  </sec>
  <sec id="transfer-learning">
    <title><italic>Transfer learning</italic></title>
    <p>Training deep learning models generally requires a large amount
    of training data and substantial computing resources. Transfer
    learning is an approach wherein the architecture of a pretrained CNN
    (which is generally trained on a very large dataset) is applied to a
    new classification problem. For example, CNNs trained on the
    ImageNet dataset of &gt; 1 million images
    (<xref alt="Deng et al., 2009" rid="ref-deng2009" ref-type="bibr">Deng
    et al., 2009</xref>) such as ResNet have been applied to automated
    detection/classification of primate and bird species from PAM data
    (<xref alt="Dufourq et al., 2022" rid="ref-dufourq2022" ref-type="bibr">Dufourq
    et al., 2022</xref>;
    <xref alt="Ruan et al., 2022" rid="ref-ruan2022" ref-type="bibr">Ruan
    et al., 2022</xref>). Generally, very few practitioners train a CNN
    from scratch, and there are two common approaches for transfer
    learning. The first option is to use the CNN as a feature extractor,
    and train only the last classification layer. The second option is
    known as ‘fine-tuning’, where instead of initializing a neural
    network with random weights, the initialization is done using the
    pre-trained network. Using these pre-trained weights are valuable
    because the model has already learned useful feature representations
    (<xref alt="Takhirov, 2021" rid="ref-pytorch_quantized_transfer_learning" ref-type="bibr">Takhirov,
    2021</xref>). Both approaches require substantially less computing
    power than training from scratch. The functions in the ‘gibbonNetR’
    package allow users to train models using both types of transfer
    learning.</p>
  </sec>
  <sec id="state-of-the-field">
    <title>State of the field</title>
    <p>The two most popular open-source programming languages are R and
    Python
    (<xref alt="Scavetta &amp; Angelov, 2021" rid="ref-scavetta2021" ref-type="bibr">Scavetta
    &amp; Angelov, 2021</xref>). Python has surpassed R in terms of
    overall popularity, but R remains an important language for the life
    sciences
    (<xref alt="Lawlor et al., 2022" rid="ref-lawlor2022" ref-type="bibr">Lawlor
    et al., 2022</xref>). ‘Keras’
    (<xref alt="Chollet &amp; others, 2015" rid="ref-chollet2015" ref-type="bibr">Chollet
    &amp; others, 2015</xref>), ‘PyTorch’
    (<xref alt="Paszke et al., 2019" rid="ref-paszke2019" ref-type="bibr">Paszke
    et al., 2019</xref>) and ‘Tensorflow’
    (<xref alt="Martín Abadi et al., 2015" rid="ref-martínabadi2015" ref-type="bibr">Martín
    Abadi et al., 2015</xref>) are some of the more popular neural
    network libraries; these libraries were all initially developed for
    the Python programming language. One of the earliest implementations
    of automated detection using R was the ‘monitoR’ package, which
    included functions for template detection
    (<xref alt="Katz et al., 2016" rid="ref-katz2016" ref-type="bibr">Katz
    et al., 2016</xref>). The ‘warbleR’ package included functions for
    energy-based detection, which identifies signals of interest in a
    certain frequency range above specified energy thresholds
    (<xref alt="Araya-Salas &amp; Smith-Vidaurre, 2017" rid="ref-araya2017warbler" ref-type="bibr">Araya-Salas
    &amp; Smith-Vidaurre, 2017</xref>). The ‘gibbonR’ package combined
    energy-based detection with traditional machine learning
    classification
    (<xref alt="Clink &amp; Klinck, 2019" rid="ref-clink2019gibbonfindr" ref-type="bibr">Clink
    &amp; Klinck, 2019</xref>).</p>
    <p>Until recently, deep learning implementations in R relied on the
    ‘reticulate’ package, which served as an interface to Python
    (<xref alt="Ushey et al., 2022" rid="ref-ushey2022" ref-type="bibr">Ushey
    et al., 2022</xref>). Early implementations of automated detection
    using deep learning in R relied on the ‘reticulate’ package Silva et
    al.
    (<xref alt="2022" rid="ref-silva2022soundclass" ref-type="bibr">2022</xref>).
    However, the recent release of the ‘torch for R’ ecosystem provides
    a framework based on ‘PyTorch’ that runs natively in R and has no
    dependency on Python
    (<xref alt="Falbel, 2023" rid="ref-falbel2023" ref-type="bibr">Falbel,
    2023</xref>). Running natively in R means more straightforward
    installation, and higher accessibility for users of the R
    programming environment. Keydana
    (<xref alt="2023" rid="ref-keydana2023" ref-type="bibr">2023</xref>)
    provides tutorials for image and audio classification in the ‘torch
    for R’ ecosystem, and the functionality in ‘gibbonNetR’ relies
    heavily on these tutorials. Variations of the transfer learning
    approaches included in this package have already been implemented in
    Python
    (<xref alt="Dufourq et al., 2022" rid="ref-dufourq2022" ref-type="bibr">Dufourq
    et al., 2022</xref>). Recent advances have used embeddings from
    audio classification models trained on bird songs for new
    classification problems, and in many cases, these embeddings led to
    better performance than general audio or image datasets
    (<xref alt="Ghani et al., 2023" rid="ref-ghani2023" ref-type="bibr">Ghani
    et al., 2023</xref>).</p>
  </sec>
</sec>
<sec id="overview">
  <title>Overview</title>
  <p>The package ‘gibbonNetR’ provides functions to create spectrogram
  images using the ‘seewave’ package
  (<xref alt="J. Sueur et al., 2008" rid="ref-seewave2008" ref-type="bibr">J.
  Sueur et al., 2008</xref>), and train and deploy six CNN
  architectures: AlexNet
  (<xref alt="Krizhevsky et al., 2017" rid="ref-krizhevsky2017" ref-type="bibr">Krizhevsky
  et al., 2017</xref>), VGG16, VGG19
  (<xref alt="Simonyan &amp; Zisserman, 2014" rid="ref-simonyan2014" ref-type="bibr">Simonyan
  &amp; Zisserman, 2014</xref>), ResNet18, ResNet50, and ResNet152
  (<xref alt="He et al., 2016" rid="ref-he2016" ref-type="bibr">He et
  al., 2016</xref>)) trained on the ImageNet dataset
  (<xref alt="Deng et al., 2009" rid="ref-deng2009" ref-type="bibr">Deng
  et al., 2009</xref>). This package has been used for automated
  detection of gunshots
  (<xref alt="Vu et al., 2024" rid="ref-Vu2024" ref-type="bibr">Vu et
  al., 2024</xref>) and the calls of two gibbon species
  (<xref alt="Clink, Kim, et al., 2024" rid="ref-clink2024automated" ref-type="bibr">Clink,
  Kim, et al., 2024</xref>;
  <xref alt="Clink, Cross-Jaya, et al., 2024" rid="ref-Clink2024benchmark" ref-type="bibr">Clink,
  Cross-Jaya, et al., 2024</xref>). The package also has functions to
  evaluate model performance, deploy the highest-performing model over a
  directory of sound files, and extract embeddings from trained models
  to visualize acoustic data. We provide an example dataset that
  consists of labelled vocalizations of the loud calls of four
  vertebrates (see detailed description below) from Danum Valley
  Conservation Area, Sabah, Malaysia
  (<xref alt="Clink &amp; Hamid Ahmad, 2024" rid="ref-clinkzenodo2024" ref-type="bibr">Clink
  &amp; Hamid Ahmad, 2024</xref>). Detailed usage instructions for
  ‘gibbonNetR’ can be found on the
  <ext-link ext-link-type="uri" xlink:href="https://denajgibbon.github.io/gibbonNetR/">gibbonNetR
  documentation site</ext-link></p>
  <sec id="data-summary">
    <title>Data summary</title>
    <p>We include sound files and spectrogram images of five sound
    classes: great argus pheasant (<italic>Argusianus argus</italic>)
    long calls
    (<xref alt="Clink et al., 2021" rid="ref-clink2021not" ref-type="bibr">Clink
    et al., 2021</xref>), helmeted hornbills (<italic>Rhinoplax
    vigil</italic>), and rhinoceros hornbills (<italic>Buceros
    rhinoceros</italic>)
    (<xref alt="Kennedy et al., 2023" rid="ref-kennedy2023evidence" ref-type="bibr">Kennedy
    et al., 2023</xref>), female gibbons (<italic>Hylobates
    funereus</italic>) and a catch-all “noise” category. The data come
    from two separate PAM arrays in Danum Valley Conservation Area,
    Sabah, Malaysia. The training and validation data come from a wide
    array of Swift autonomous recording units placed on ~750 m spacing
    (<xref alt="Clink et al., 2023" rid="ref-clink2023" ref-type="bibr">Clink
    et al., 2023</xref>), and the test data come from a different,
    smaller array (~250 m spacing) within the same area. We used a
    band-limited energy detector to identify signals that were 3-sec or
    longer duration within the 400-1600 Hz range, and then a single
    observer (DJC) manually sorted the detections into their respective
    categories
    (<xref alt="Clink et al., 2023" rid="ref-clink2023" ref-type="bibr">Clink
    et al., 2023</xref>).</p>
  </sec>
  <sec id="preparing-training-validation-and-test-data">
    <title>Preparing training, validation, and test data</title>
    <p>The package currently uses spectrogram images (Figure 1) to train
    and evaluate CNN model performance, and we include a function that
    can be used to create spectrogram images from Waveform Audio File
    Formant (.wav) files. The .wav files should be organized into
    separate folders, with each folder named according to the class
    label of the files it contains. We highly recommend that your test
    data come from a different recording time and/or location to better
    understand the generalizability of the models
    (<xref alt="Stowell, 2022" rid="ref-stowell2022" ref-type="bibr">Stowell,
    2022</xref>).</p>
    <fig>
      <caption><p>Spectrograms of training clips for CNNs.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="spectro.png" />
    </fig>
  </sec>
  <sec id="model-training">
    <title>Model training</title>
    <p>The package currently allows for the training of six different
    CNN architectures (‘alexnet’, ‘vgg16’, ‘vgg19’, ‘resnet18’,
    ‘resnet50’, or ‘resnet152’), and the user can specify if they want
    to freeze the feature extraction layers or not. There is also the
    option to train a binary or multi-class classifer.</p>
  </sec>
  <sec id="evaluate-model-performance">
    <title>Evaluate model performance</title>
    <p>We can compare the performance of different CNN architectures
    (Figure 2). Using the ‘get_best_performance’ function, we can
    evaluate the performance of different model architectures on the
    test dataset for the specified class. We can calculate the best F1,
    precision, recall using the ‘caret’ package
    (<xref alt="Kuhn, 2008" rid="ref-kuhn2008" ref-type="bibr">Kuhn,
    2008</xref>), and the area under the ROC (Receiver Operating
    Characteristic) curve using the ‘ROCR’ package
    (<xref alt="Sing et al., 2005" rid="ref-ROCR" ref-type="bibr">Sing
    et al., 2005</xref>), which is a threshold or confidence independent
    metric that evaluates the classifier’s ability to discriminate
    between positive and negative classes.</p>
    <code language="r script">PerformanceOutput &lt;- get_best_performance(
  performancetables.dir =
    performancetables.dir,
  class = 'female.gibbon',
  model.type = &quot;multi&quot;,
  Thresh.val = 0
)

PerformanceOutput$f1_plot</code>
    <fig>
      <caption><p>Evaluating performance of pretrained
      CNNs.</p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="modelperformance.pdf" />
    </fig>
  </sec>
  <sec id="extract-embeddings">
    <title>Extract embeddings</title>
    <p>Embeddings from deep learning models can be used as features in
    unsupervised approaches, with promising results for call repertoires
    (<xref alt="Best et al., 2023" rid="ref-10.1371U002Fjournal.pone.0283396" ref-type="bibr">Best
    et al., 2023</xref>) and individual identity
    (<xref alt="Lakdari et al., 2024" rid="ref-lakdari2024mel" ref-type="bibr">Lakdari
    et al., 2024</xref>). This package contains a function to use
    pretrained CNNs to extract embeddings, where the trained model path,
    along with test data location and target class are specified.
    Depending on the research question, this output could be used to
    visualize true and false positives from automated detection, or to
    explore differences in call types or potential number of individuals
    in the dataset.</p>
  </sec>
  <sec id="we-can-plot-the-unsupervised-clustering-results">
    <title>We can plot the unsupervised clustering results</title>
    <p>In Figure 3, the top plot is a Uniform Manifold Approximation and
    Projection (UMAP) where each point represents one call, and the
    colors indicate the original class label. The bottom plot is the
    same UMAP plot, but with points colored based on cluster assignment
    by the ‘hdbscan’ algorithm
    (<xref alt="Hahsler et al., 2019" rid="ref-dbscan" ref-type="bibr">Hahsler
    et al., 2019</xref>).</p>
    <fig>
      <caption><p>UMAP plot of embeddings from test data set colored by
      actual label (top) and unsupervised cluster assignment
      (bottom).</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="embeddings.png" />
    </fig>
    <sec id="explore-the-unsupervised-clustering-results">
      <title>Explore the unsupervised clustering results</title>
      <p>We can calculate the Normalize Mutual Information score, which
      provides a value between 0 and 1, indicating the match between
      cluster labels and actual labels. We also create a confusion
      matrix using the ‘caret’ package
      (<xref alt="Kuhn, 2008" rid="ref-kuhn2008" ref-type="bibr">Kuhn,
      2008</xref>), which returns the results when we use the
      unsupervised clustering algorithm function ‘hdbscan’
      (<xref alt="Hahsler et al., 2019" rid="ref-dbscan" ref-type="bibr">Hahsler
      et al., 2019</xref>) to match the target class to the cluster with
      the largest number of observations of that particular class.</p>
    </sec>
  </sec>
</sec>
<sec id="future-directions">
  <title>Future directions</title>
  <p>There have been huge advances in the fields of deep learning and
  automated detection for PAM data in recent years. The approach
  presented in this package is one of the first to use the ‘torch for R’
  ecosystem and to employ automated detection using deep learning
  natively in R. More recent approaches that use models that are
  explicitly trained on bioacoustics data, such as BirdNET
  (<xref alt="Ghani et al., 2023" rid="ref-ghani2023" ref-type="bibr">Ghani
  et al., 2023</xref>), have been introduced. There is a huge need in
  the field of bioacoustics to do benchmarking, wherein different model
  architectures and performance are compared across diverse datasets.
  The methods presented here can provide important benchmarks for future
  work and for understanding how and if deep learning advances improve
  performance over more traditional methods. In addition, this package
  provides a comprehensive suite of tools for processing, analyzing, and
  visualizing acoustic data, providing robust support for tasks such as
  automated detection, feature extraction, classification, and data
  visualization, which are critical for conservation work using PAM. The
  R package is available on
  <ext-link ext-link-type="uri" xlink:href="https://github.com/DenaJGibbon/gibbonNetR">Github</ext-link>,
  where issues can be opened.</p>
</sec>
<sec id="ethical-statement">
  <title>Ethical statement</title>
  <p>The research presented here adhered to all local and international
  laws. Institutional approval was provided by Cornell University (IACUC
  2017–0098). Sabah Biodiversity Centre and the Danum Valley Management
  Committee provided permission for the collection of acoustic
  recordings.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We would like to thank the Sabah Biodiversity Centre and Danum
  Valley Conservation Area for granting us permission to conduct
  research. We are incredibly grateful for the detailed comments
  provided by Steffi LaZerte and Camille Desjonquères, which
  substantially improved the package and documentation.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-ROCR">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sing</surname><given-names>T.</given-names></name>
        <name><surname>Sander</surname><given-names>O.</given-names></name>
        <name><surname>Beerenwinkel</surname><given-names>N.</given-names></name>
        <name><surname>Lengauer</surname><given-names>T.</given-names></name>
      </person-group>
      <article-title>ROCR: Visualizing classifier performance in r</article-title>
      <source>Bioinformatics</source>
      <year iso-8601-date="2005">2005</year>
      <volume>21</volume>
      <issue>20</issue>
      <uri>http://rocr.bioinf.mpi-sb.mpg.de</uri>
      <pub-id pub-id-type="doi">10.1093/bioinformatics/bti623</pub-id>
      <fpage>7881</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-seewave2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>J. Sueur</string-name>
        <string-name>T. Aubin</string-name>
        <string-name>C. Simonis</string-name>
      </person-group>
      <article-title>Seewave: A free modular tool for sound analysis and synthesis</article-title>
      <source>Bioacoustics</source>
      <year iso-8601-date="2008">2008</year>
      <volume>18</volume>
      <pub-id pub-id-type="doi">10.1080/09524622.2008.9753600</pub-id>
      <fpage>213</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-clink2019gibbonfindr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
      </person-group>
      <article-title>gibbonR: An r package for the detection and classification of acoustic signals</article-title>
      <source>arXiv preprint arXiv:1906.02572</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1906.02572</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-clinkzenodo2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Hamid Ahmad</surname><given-names>Abdul</given-names></name>
      </person-group>
      <article-title>A labelled dataset of the loud calls of four vertebrates collected using passive acoustic monitoring in malaysian borneo</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2024-11">2024</year><month>11</month>
      <uri>10.5281/zenodo.14213067</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.14213067</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-araya2017warbler">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Araya-Salas</surname><given-names>Marcelo</given-names></name>
        <name><surname>Smith-Vidaurre</surname><given-names>Grace</given-names></name>
      </person-group>
      <article-title>warbleR: An r package to streamline analysis of animal acoustic signals</article-title>
      <source>Methods in Ecology and Evolution</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>8</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1111/2041-210X.12624</pub-id>
      <fpage>184</fpage>
      <lpage>191</lpage>
    </element-citation>
  </ref>
  <ref id="ref-clink2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Kier</surname><given-names>Isabel</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
      </person-group>
      <article-title>A workflow for the automated detection and classification of female gibbon calls from long-term acoustic recordings</article-title>
      <source>Frontiers in Ecology and Evolution</source>
      <year iso-8601-date="2023">2023</year>
      <volume>11</volume>
      <pub-id pub-id-type="doi">10.3389/fevo.2023.1071640</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-sugai2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sugai</surname><given-names>Larissa Sayuri Moreira</given-names></name>
        <name><surname>Silva</surname><given-names>Thiago Sanna Freire</given-names></name>
        <name><surname>Ribeiro</surname><given-names>José Wagner</given-names></name>
        <name><surname>Llusia</surname><given-names>Diego</given-names></name>
      </person-group>
      <article-title>Terrestrial passive acoustic monitoring: Review and perspectives</article-title>
      <source>BioScience</source>
      <year iso-8601-date="2019">2019</year>
      <volume>69</volume>
      <issue>1</issue>
      <uri>https://academic.oup.com/bioscience/article/69/1/15/5193506</uri>
      <pub-id pub-id-type="doi">10.1093/biosci/biy147</pub-id>
      <fpage>1525</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gibb2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gibb</surname><given-names>Rory</given-names></name>
        <name><surname>Browning</surname><given-names>Ella</given-names></name>
        <name><surname>Glover-Kapfer</surname><given-names>Paul</given-names></name>
        <name><surname>Jones</surname><given-names>Kate E.</given-names></name>
      </person-group>
      <article-title>Emerging opportunities and challenges for passive acoustics in ecological assessment and monitoring</article-title>
      <source>Methods in Ecology and Evolution</source>
      <year iso-8601-date="2018-10">2018</year><month>10</month>
      <uri>http://doi.wiley.com/10.1111/2041-210X.13101</uri>
      <pub-id pub-id-type="doi">10.1111/2041-210X.13101</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-katz2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Katz</surname><given-names>Jonathan</given-names></name>
        <name><surname>Hafner</surname><given-names>Sasha D</given-names></name>
        <name><surname>Donovan</surname><given-names>Therese</given-names></name>
      </person-group>
      <article-title>Assessment of error rates in acoustic monitoring with the r package monitoR</article-title>
      <source>Bioacoustics</source>
      <year iso-8601-date="2016">2016</year>
      <volume>25</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1080/09524622.2015.1133320</pub-id>
      <fpage>177196</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-balantic2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Balantic</surname><given-names>Cathleen</given-names></name>
        <name><surname>Donovan</surname><given-names>Therese</given-names></name>
      </person-group>
      <article-title>AMMonitor: Remote monitoring of biodiversity in an adaptive framework with r</article-title>
      <source>Methods in Ecology and Evolution</source>
      <year iso-8601-date="2020">2020</year>
      <volume>11</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.1111/2041-210X.13397</pub-id>
      <fpage>869877</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kalan2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kalan</surname><given-names>Ammie K.</given-names></name>
        <name><surname>Mundry</surname><given-names>Roger</given-names></name>
        <name><surname>Wagner</surname><given-names>Oliver J J</given-names></name>
        <name><surname>Heinicke</surname><given-names>Stefanie</given-names></name>
        <name><surname>Boesch</surname><given-names>Christophe</given-names></name>
        <name><surname>Kühl</surname><given-names>Hjalmar S.</given-names></name>
      </person-group>
      <article-title>Towards the automated detection and occupancy estimation of primates using passive acoustic monitoring</article-title>
      <source>Ecological Indicators</source>
      <year iso-8601-date="2015">2015</year>
      <volume>54</volume>
      <issue>July 2015</issue>
      <pub-id pub-id-type="doi">10.1016/j.ecolind.2015.02.023</pub-id>
      <fpage>217226</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lecun2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
      </person-group>
      <article-title>Deep learning</article-title>
      <source>Nature</source>
      <year iso-8601-date="2015-05">2015</year><month>05</month>
      <volume>521</volume>
      <issue>7553</issue>
      <uri>https://www.nature.com/articles/nature14539</uri>
      <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
      <fpage>436</fpage>
      <lpage>444</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scavetta2021">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Scavetta</surname><given-names>Rick J</given-names></name>
        <name><surname>Angelov</surname><given-names>Boyan</given-names></name>
      </person-group>
      <source>Python and r for the modern data scientist</source>
      <publisher-name>O’Reilly Media, Inc.</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.18637/jss.v103.b02</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lawlor2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lawlor</surname><given-names>Jake</given-names></name>
        <name><surname>Banville</surname><given-names>Francis</given-names></name>
        <name><surname>Forero-Muñoz</surname><given-names>Norma-Rocio</given-names></name>
        <name><surname>Hébert</surname><given-names>Katherine</given-names></name>
        <name><surname>Martínez-Lanfranco</surname><given-names>Juan Andrés</given-names></name>
        <name><surname>Rogy</surname><given-names>Pierre</given-names></name>
        <name><surname>MacDonald</surname><given-names>A. Andrew M.</given-names></name>
      </person-group>
      <article-title>Ten simple rules for teaching yourself R</article-title>
      <source>PLOS Computational Biology</source>
      <year iso-8601-date="2022-09-01">2022</year><month>09</month><day>01</day>
      <volume>18</volume>
      <issue>9</issue>
      <uri>https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010372</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010372</pub-id>
      <fpage>e1010372</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-chollet2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Keras</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://github.com/fchollet/keras</uri>
      <pub-id pub-id-type="doi">10.1163/1574-9347_bnp_e612900</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
      <fpage>80248035</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-martínabadi2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Martín Abadi</surname></name>
        <name><surname>Ashish Agarwal</surname></name>
        <name><surname>Paul Barham</surname></name>
        <name><surname>Eugene Brevdo</surname></name>
        <name><surname>Zhifeng Chen</surname></name>
        <name><surname>Craig Citro</surname></name>
        <name><surname>Greg S. Corrado</surname></name>
        <name><surname>Andy Davis</surname></name>
        <name><surname>Jeffrey Dean</surname></name>
        <name><surname>Matthieu Devin</surname></name>
        <name><surname>Sanjay Ghemawat</surname></name>
        <name><surname>Ian Goodfellow</surname></name>
        <name><surname>Andrew Harp</surname></name>
        <name><surname>Geoffrey Irving</surname></name>
        <name><surname>Michael Isard</surname></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Rafal Jozefowicz</surname></name>
        <name><surname>Lukasz Kaiser</surname></name>
        <name><surname>Manjunath Kudlur</surname></name>
        <name><surname>Josh Levenberg</surname></name>
        <name><surname>Dandelion Mané</surname></name>
        <name><surname>Rajat Monga</surname></name>
        <name><surname>Sherry Moore</surname></name>
        <name><surname>Derek Murray</surname></name>
        <name><surname>Chris Olah</surname></name>
        <name><surname>Mike Schuster</surname></name>
        <name><surname>Jonathon Shlens</surname></name>
        <name><surname>Benoit Steiner</surname></name>
        <name><surname>Ilya Sutskever</surname></name>
        <name><surname>Kunal Talwar</surname></name>
        <name><surname>Paul Tucker</surname></name>
        <name><surname>Vincent Vanhoucke</surname></name>
        <name><surname>Vijay Vasudevan</surname></name>
        <name><surname>Fernanda Viégas</surname></name>
        <name><surname>Oriol Vinyals</surname></name>
        <name><surname>Pete Warden</surname></name>
        <name><surname>Martin Wattenberg</surname></name>
        <name><surname>Martin Wicke</surname></name>
        <name><surname>Yuan Yu</surname></name>
        <name><surname>Xiaoqiang Zheng</surname></name>
      </person-group>
      <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://www.tensorflow.org/</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1605.08695</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ushey2022">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ushey</surname><given-names>Kevin</given-names></name>
        <name><surname>Allaire</surname><given-names>J. J.</given-names></name>
        <name><surname>Tang</surname><given-names>Yuan</given-names></name>
      </person-group>
      <source>Reticulate: Interface to ’python’</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.32614/CRAN.package.reticulat</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-falbel2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Falbel</surname><given-names>Daniel</given-names></name>
      </person-group>
      <source>Luz: Higher level ’API’ for ’torch’</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://CRAN.R-project.org/package=luz</uri>
      <pub-id pub-id-type="doi">10.32614/CRAN.package.luz</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-stevens2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Stevens</surname><given-names>Eli</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Viehmann</surname><given-names>Thomas</given-names></name>
      </person-group>
      <source>Deep Learning with PyTorch</source>
      <publisher-name>Simon; Schuster</publisher-name>
      <year iso-8601-date="2020-08-04">2020</year><month>08</month><day>04</day>
    </element-citation>
  </ref>
  <ref id="ref-goodfellow2016">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Courville</surname><given-names>Aaron</given-names></name>
      </person-group>
      <source>Deep learning</source>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-deng2009">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Deng</surname><given-names>Jia</given-names></name>
        <name><surname>Dong</surname><given-names>Wei</given-names></name>
        <name><surname>Socher</surname><given-names>Richard</given-names></name>
        <name><surname>Li</surname><given-names>Li-Jia</given-names></name>
        <name><surname>Li</surname><given-names>Kai</given-names></name>
        <name><surname>Fei-Fei</surname><given-names>Li</given-names></name>
      </person-group>
      <article-title>Imagenet: A large-scale hierarchical image database</article-title>
      <publisher-name>Ieee</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <pub-id pub-id-type="doi">10.1109/cvpr.2009.5206848</pub-id>
      <fpage>248255</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-dufourq2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dufourq</surname><given-names>Emmanuel</given-names></name>
        <name><surname>Batist</surname><given-names>Carly</given-names></name>
        <name><surname>Foquet</surname><given-names>Ruben</given-names></name>
        <name><surname>Durbach</surname><given-names>Ian</given-names></name>
      </person-group>
      <article-title>Passive acoustic monitoring of animal populations with transfer learning</article-title>
      <source>Ecological Informatics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>70</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S1574954122001388</uri>
      <pub-id pub-id-type="doi">10.1016/j.ecoinf.2022.101688</pub-id>
      <fpage>101688</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ruan2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ruan</surname><given-names>Wenda</given-names></name>
        <name><surname>Wu</surname><given-names>Keyi</given-names></name>
        <name><surname>Chen</surname><given-names>Qingchun</given-names></name>
        <name><surname>Zhang</surname><given-names>Chengyun</given-names></name>
      </person-group>
      <article-title>ResNet-based bio-acoustics presence detection technology of hainan gibbon calls</article-title>
      <source>Applied Acoustics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>198</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0003682X22003139</uri>
      <pub-id pub-id-type="doi">10.1016/j.apacoust.2022.108939</pub-id>
      <fpage>108939</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-keydana2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Keydana</surname><given-names>Sigrid</given-names></name>
      </person-group>
      <source>Deep learning and scientific computing with r torch</source>
      <publisher-name>CRC Press</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1201/9781003275923</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lecun1995">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <source>Convolutional networks for images, speech, and time series</source>
      <year iso-8601-date="1995">1995</year>
      <volume>3361</volume>
      <fpage>1995</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gu2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gu</surname><given-names>Jiuxiang</given-names></name>
        <name><surname>Wang</surname><given-names>Zhenhua</given-names></name>
        <name><surname>Kuen</surname><given-names>Jason</given-names></name>
        <name><surname>Ma</surname><given-names>Lianyang</given-names></name>
        <name><surname>Shahroudy</surname><given-names>Amir</given-names></name>
        <name><surname>Shuai</surname><given-names>Bing</given-names></name>
        <name><surname>Liu</surname><given-names>Ting</given-names></name>
        <name><surname>Wang</surname><given-names>Xingxing</given-names></name>
        <name><surname>Wang</surname><given-names>Gang</given-names></name>
        <name><surname>Cai</surname><given-names>Jianfei</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Recent advances in convolutional neural networks</article-title>
      <source>Pattern recognition</source>
      <year iso-8601-date="2018">2018</year>
      <volume>77</volume>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2017.10.013</pub-id>
      <fpage>354377</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-krizhevsky2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Krizhevsky</surname><given-names>Alex</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name>
      </person-group>
      <article-title>Imagenet classification with deep convolutional neural networks</article-title>
      <source>Communications of the ACM</source>
      <year iso-8601-date="2017">2017</year>
      <volume>60</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1145/3065386</pub-id>
      <fpage>8490</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-simonyan2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Simonyan</surname><given-names>Karen</given-names></name>
        <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Very deep convolutional networks for large-scale image recognition</article-title>
      <source>arXiv preprint arXiv:1409.1556</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-he2016">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>Kaiming</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Ren</surname><given-names>Shaoqing</given-names></name>
        <name><surname>Sun</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Deep residual learning for image recognition</article-title>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>
      <fpage>770778</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lakdari2024mel">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lakdari</surname><given-names>Mohamed Walid</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Sethi</surname><given-names>Sarab</given-names></name>
        <name><surname>Bohn</surname><given-names>Gabriel A</given-names></name>
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
      </person-group>
      <article-title>Mel-frequency cepstral coefficients outperform embeddings from pre-trained convolutional neural networks under noisy conditions for discrimination tasks of individual gibbons</article-title>
      <source>Ecological Informatics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>80</volume>
      <pub-id pub-id-type="doi">10.1016/j.ecoinf.2023.102457</pub-id>
      <fpage>102457</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ghani2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ghani</surname><given-names>Burooj</given-names></name>
        <name><surname>Denton</surname><given-names>Tom</given-names></name>
        <name><surname>Kahl</surname><given-names>Stefan</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
      </person-group>
      <article-title>Global birdsong embeddings enable superior transfer learning for bioacoustic classification</article-title>
      <source>Scientific Reports</source>
      <year iso-8601-date="2023">2023</year>
      <volume>13</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-023-49989-z</pub-id>
      <fpage>22876</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-clink2021not">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Groves</surname><given-names>Tom</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
      </person-group>
      <article-title>Not by the light of the moon: Investigating circadian rhythms and environmental predictors of calling in bornean great argus</article-title>
      <source>Plos one</source>
      <publisher-name>Public Library of Science San Francisco, CA USA</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>16</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0246564</pub-id>
      <fpage>e0246564</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kennedy2023evidence">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kennedy</surname><given-names>Amy G</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
        <name><surname>Johnson</surname><given-names>Lynn M</given-names></name>
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
      </person-group>
      <article-title>Evidence for acoustic niche partitioning depends on the temporal scale in two sympatric bornean hornbill species</article-title>
      <source>Biotropica</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>55</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1111/btp.13205</pub-id>
      <fpage>517</fpage>
      <lpage>528</lpage>
    </element-citation>
  </ref>
  <ref id="ref-stowell2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stowell</surname><given-names>Dan</given-names></name>
      </person-group>
      <article-title>Computational bioacoustics with deep learning: a review and roadmap</article-title>
      <source>PeerJ</source>
      <year iso-8601-date="2022-03-21">2022</year><month>03</month><day>21</day>
      <volume>10</volume>
      <uri>https://peerj.com/articles/13152</uri>
      <pub-id pub-id-type="doi">10.7717/peerj.13152</pub-id>
      <fpage>e13152</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1371U002Fjournal.pone.0283396">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Best</surname><given-names>Paul</given-names></name>
        <name><surname>Paris</surname><given-names>Sébastien</given-names></name>
        <name><surname>Glotin</surname><given-names>Hervé</given-names></name>
        <name><surname>Marxer</surname><given-names>Ricard</given-names></name>
      </person-group>
      <article-title>Deep audio embeddings for vocalisation clustering</article-title>
      <source>PLOS ONE</source>
      <publisher-name>Public Library of Science</publisher-name>
      <year iso-8601-date="2023-07">2023</year><month>07</month>
      <volume>18</volume>
      <issue>7</issue>
      <uri>10.1371/journal.pone.0283396</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0283396</pub-id>
      <fpage>1</fpage>
      <lpage>18</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dbscan">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hahsler</surname><given-names>Michael</given-names></name>
        <name><surname>Piekenbrock</surname><given-names>Matthew</given-names></name>
        <name><surname>Doran</surname><given-names>Derek</given-names></name>
      </person-group>
      <article-title>dbscan: Fast density-based clustering with R</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2019">2019</year>
      <volume>91</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v091.i01</pub-id>
      <fpage>1</fpage>
      <lpage>30</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kuhn2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kuhn</surname><given-names>Max</given-names></name>
      </person-group>
      <article-title>Caret package</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2008">2008</year>
      <volume>28</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v028.i05</pub-id>
      <fpage>126</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ruff2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ruff</surname><given-names>Zachary J.</given-names></name>
        <name><surname>Lesmeister</surname><given-names>Damon B.</given-names></name>
        <name><surname>Appel</surname><given-names>Cara L.</given-names></name>
        <name><surname>Sullivan</surname><given-names>Christopher M.</given-names></name>
      </person-group>
      <article-title>Workflow and convolutional neural network for automated identification of animal sounds</article-title>
      <source>Ecological Indicators</source>
      <year iso-8601-date="2021-05-01">2021</year><month>05</month><day>01</day>
      <volume>124</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S1470160X21000844</uri>
      <pub-id pub-id-type="doi">10.1016/j.ecolind.2021.107419</pub-id>
      <fpage>107419</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-silva2022soundclass">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Silva</surname><given-names>Bruno</given-names></name>
        <name><surname>Mestre</surname><given-names>Frederico</given-names></name>
        <name><surname>Barreiro</surname><given-names>Sílvia</given-names></name>
        <name><surname>Alves</surname><given-names>Pedro J</given-names></name>
        <name><surname>Herrera</surname><given-names>José M</given-names></name>
      </person-group>
      <article-title>soundClass: An automatic sound classification tool for biodiversity monitoring using machine learning</article-title>
      <source>Methods in Ecology and Evolution</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1111/2041-210X.13964</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Clink2024benchmark">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Cross-Jaya</surname><given-names>Hope</given-names></name>
        <name><surname>Kim</surname><given-names>Jinsung</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Hong</surname><given-names>Moeurk</given-names></name>
        <name><surname>Sala</surname><given-names>Roeun</given-names></name>
        <name><surname>Birot</surname><given-names>Hélène</given-names></name>
        <name><surname>Agger</surname><given-names>Cain</given-names></name>
        <name><surname>Vu</surname><given-names>Thinh Tien</given-names></name>
        <name><surname>Thi</surname><given-names>Hoa Nguyen</given-names></name>
        <name><surname>Chi</surname><given-names>Thanh Nguyen</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
      </person-group>
      <article-title>Benchmarking for the automated detection and classification of southern yellow-cheeked crested gibbon calls from passive acoustic monitoring data</article-title>
      <source>bioRxiv</source>
      <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://www.biorxiv.org/content/early/2024/08/19/2024.08.17.608420</uri>
      <pub-id pub-id-type="doi">10.1101/2024.08.17.608420</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-clink2024automated">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
        <name><surname>Kim</surname><given-names>Jinsung</given-names></name>
        <name><surname>Cross-Jaya</surname><given-names>Hope</given-names></name>
        <name><surname>Ahmad</surname><given-names>Abdul Hamid</given-names></name>
        <name><surname>Hong</surname><given-names>Moeurk</given-names></name>
        <name><surname>Sala</surname><given-names>Roeun</given-names></name>
        <name><surname>Birot</surname><given-names>Hélène</given-names></name>
        <name><surname>Agger</surname><given-names>Cain</given-names></name>
        <name><surname>Vu</surname><given-names>Thinh Tien</given-names></name>
        <name><surname>Thi</surname><given-names>Hoa Nguyen</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Automated detection of gibbon calls from passive acoustic monitoring data using convolutional neural networks in the&quot; torch for r&quot; ecosystem</article-title>
      <source>arXiv preprint arXiv:2407.09976</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2407.09976</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Vu2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vu</surname><given-names>Thinh Tien</given-names></name>
        <name><surname>Phan</surname><given-names>Dai Viet</given-names></name>
        <name><surname>Le</surname><given-names>Thai Son</given-names></name>
        <name><surname>Clink</surname><given-names>D. J.</given-names></name>
      </person-group>
      <article-title>Investigating hunting in a protected area in southeast asia using passive acoustic monitoring with mobile smartphones and transfer learning</article-title>
      <source>Ecological Indicators</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1016/j.ecolind.2024.112501</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pytorch_quantized_transfer_learning">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Takhirov</surname><given-names>Zafar</given-names></name>
      </person-group>
      <article-title>Quantized transfer learning tutorial</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
