<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5395</article-id>
<article-id pub-id-type="doi">10.21105/joss.05395</article-id>
<title-group>
<article-title>onsetsync: An R Package for Onset Synchrony
Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2896-929X</contrib-id>
<name>
<surname>Eerola</surname>
<given-names>Tuomas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9670-5077</contrib-id>
<name>
<surname>Clayton</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Music, Durham University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-01-17">
<day>17</day>
<month>1</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>93</issue>
<fpage>5395</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>music</kwd>
<kwd>entrainment</kwd>
<kwd>periodicity</kwd>
<kwd>synchrony</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Music performance relies on tight yet flexible timing coordination
  between performers. An important aspect of this interpersonal
  entrainment is the synchronization of musical events between
  performers, and this can be analysed from the note onsets obtained
  from recorded performances. Analysis of the asynchronies between
  onsets gives an insight into synchronization between performers, and
  the extent to which it is accurate (how close together the events are
  in time) and precise (how stable the relationship is). The synchrony
  between performers is influenced by various factors such as the genre
  of music, performer skill level, intention, and phrase and beat
  structures of the music. The analysis of synchrony in music benefits
  from shared tools as there are a number of common operations that need
  to be carried out in every dataset (e.g., calculating pairwise
  synchrony, or assessing the synchrony across other variables such as
  tempo, metrical hierarchy, or phrasing) and the number of datasets
  containing onset timing information has recently increased.</p>
  <sec id="statement-of-need">
    <title>Statement of need</title>
    <p><monospace>onsetsync</monospace> is a R package for assessment of
    musical synchrony through note onsets. Data is ingested in the form
    of CSV files with individual instrument onset times aligned with
    labelled beat positions; separate metre annotations comprising lists
    of time estimates for the downbeats are also used for some
    functions. The package includes functions for common operations such
    as adding isochronous beats based on metrical structure, adding
    annotations, calculating classic measures of synchrony between two
    performers, assessing the periodicity of the onsets, and visualising
    synchrony across metrical cycles, time, or another property. These
    functions make the analysis of onset corpora transparent and will
    allow more scholars to carry out investigations of entrainment in
    music.</p>
    <p><monospace>onsetsync</monospace> package comes with example
    performances from two traditions, Cuban Son and Salsa performances
    (<xref alt="Poole et al., 2022" rid="ref-Poole_Tarsitani_Clayton_2022" ref-type="bibr">Poole
    et al., 2022</xref>) and North Hindustani performances
    (<xref alt="Clayton, Leante, et al., 2021" rid="ref-Clayton_Leante_Tarsitani_2021" ref-type="bibr">Clayton,
    Leante, et al., 2021</xref>). The examples are taken from the
    Interpersonal Entrainment in Music Performance (IEMP) project
    collection that has compiled small ensemble performance data from
    six traditions (North Indian ragas, Malian jembe, Tunisian stambeli,
    Uruguayan candombe, European string quartet, and Cuban son and
    salsa). These collections contain tens of hours annotated and
    verified onsets and are available from Open Science Framework (OSF).
    There are also noteworthy datasets containing audio and annotated
    contents of music relevant to assessing synchrony such as
    <italic>Carnatic Music Rhythm Dataset</italic>
    (<xref alt="Srinivasamurthy et al., 2015" rid="ref-srinivasamurthy2015particle" ref-type="bibr">Srinivasamurthy
    et al., 2015</xref>), <italic>Hindustani Music Rhythm
    Dataset</italic>
    (<xref alt="Srinivasamurthy et al., 2016" rid="ref-srinivasamurthy2016generalized" ref-type="bibr">Srinivasamurthy
    et al., 2016</xref>), <italic>Arab-Andalusian (Maghreb)
    dataset</italic>
    (<xref alt="Sordo et al., 2014" rid="ref-sordo2014creating" ref-type="bibr">Sordo
    et al., 2014</xref>), <italic>Scandinavian Fiddle Tunes with
    Accompanying Foot-Tapping</italic>
    (<xref alt="Lordelo et al., 2020" rid="ref-lordelo2020adversarial" ref-type="bibr">Lordelo
    et al., 2020</xref>) and <italic>MAESTRO (MIDI and Audio Edited for
    Synchronous Tracks and Organisation)</italic> dataset which is an
    extensive collection (200+ hours) of professional piano performances
    (<xref alt="Hawthorne et al., 2018" rid="ref-hawthorne2018enabling" ref-type="bibr">Hawthorne
    et al., 2018</xref>). Some datasets such as Chopin performances
    compiled by Goebl et al.
    (<xref alt="2010" rid="ref-goebl2010investigations" ref-type="bibr">2010</xref>)
    facilitate study of the synchrony between the two hands of the
    pianist.</p>
    <sec id="terminology-and-measures-of-synchrony">
      <title>Terminology and measures of synchrony</title>
      <p>Much of the research on synchrony in music has focused on
      <italic>sensorimotor synchronization</italic> (SMS) or just
      synchronization, a process occurring within 100-2000 ms
      timescales, which has commonly been studied with tapping
      experiments
      (<xref alt="Repp &amp; Keller, 2008" rid="ref-repp2008sensorimotor" ref-type="bibr">Repp
      &amp; Keller, 2008</xref>;
      <xref alt="Repp &amp; Su, 2013" rid="ref-repp2013sensorimotor" ref-type="bibr">Repp
      &amp; Su, 2013</xref>) but also through analyses of music corpora,
      from string quartets
      (<xref alt="Wing et al., 2014" rid="ref-wing2014optimal" ref-type="bibr">Wing
      et al., 2014</xref>) to drum ensembles
      (<xref alt="Polak et al., 2016" rid="ref-polak2016both" ref-type="bibr">Polak
      et al., 2016</xref>), and to Indian instrumental music
      performances
      (<xref alt="Clayton et al., 2019" rid="ref-clayton_et_al_2018b" ref-type="bibr">Clayton
      et al., 2019</xref>).</p>
      <p>A number of different measures of synchrony have been proposed.
      We follow the definitions from Clayton et al.
      (<xref alt="2020" rid="ref-clayton2020" ref-type="bibr">2020</xref>)
      who adopt and develop the measures defined by Rasch
      (<xref alt="1979" rid="ref-rasch1979synchronization" ref-type="bibr">1979</xref>,
      <xref alt="1988" rid="ref-rasch1988timing" ref-type="bibr">1988</xref>):</p>
      <p>Here we first define the onset time differences (or
      asynchronies), <inline-formula><alternatives>
      <tex-math><![CDATA[d]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[I_{1, i}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[I_{2, i}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      refer to an onset at time point <inline-formula><alternatives>
      <tex-math><![CDATA[i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
      for instrument 1 and 2:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[d_{i} = I_{1, i} - I_{2, i}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p><bold>Precision measures</bold> estimate how consistent the two
      parts are:</p>
      <p><italic>Pairwise asynchronization</italic>: standard deviation
      of the asynchronies of any pair of instruments, which is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[SD = \sqrt\frac{\sum_{i=1}{(d_i-\bar{d})^2}}{n-1}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mo>√</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover><mml:mi>d</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p><italic>Groupwise asynchronization</italic>: Root mean square
      (RMS) of the pairwise asynchronizations calculated as</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[RMS = \sqrt{\Sigma_{i=1}^{n}{\Big(\frac{d_{i}}{n}\Big)^2}}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>R</mml:mi><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msubsup><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo minsize="1.8" maxsize="1.8" stretchy="false" form="prefix">(</mml:mo><mml:mfrac><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>n</mml:mi></mml:mfrac><mml:msup><mml:mo minsize="1.8" maxsize="1.8" stretchy="false" form="postfix">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[n]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      is the number of onset time differences.</p>
      <p><italic>Mean absolute asynchrony</italic>: Mean of all unsigned
      asynchrony values which is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\bar{|s|} = \frac{1}{n} \sum_{i=1}^{n} |d_{i}|]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo accent="true">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p><bold>Accuracy measures</bold> estimate how far one part is
      ahead of or behind the other(s):</p>
      <p><italic>Mean pairwise asynchrony</italic>: mean difference in
      the onset values of two instruments,
      <inline-formula><alternatives>
      <tex-math><![CDATA[\bar{d}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>d</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover></mml:math></alternatives></inline-formula>,
      i.e., the signed asynchrony values.</p>
      <p><italic>Mean relative asynchrony</italic>: mean position of an
      instrument’s onsets relative to average position
      (<inline-formula><alternatives>
      <tex-math><![CDATA[\bar{I}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>I</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover></mml:math></alternatives></inline-formula>)
      of the group, where the average position is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\bar{I} = \frac{I_{1} + I_{2} + \ldots + I_{n}}{n}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>I</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>…</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>and the relative onset time <inline-formula><alternatives>
      <tex-math><![CDATA[R]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>R</mml:mi></mml:math></alternatives></inline-formula>
      is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[R_i = I_i - \bar{I}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mover><mml:mi>I</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>and therefore mean relative asynchrony is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\bar{s} = \frac{1}{n} \sum_{i=1}^{n} R_{i}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>s</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>These methods are based on asynchronies between onsets
      occurring at the same beat positions; other methods exist for
      assessing synchrony such as utilising circular statistics
      (<xref alt="Berens, 2009" rid="ref-berens2009circstat" ref-type="bibr">Berens,
      2009</xref>), recurrence quantification analysis
      (<xref alt="Wallot &amp; Leonardi, 2018" rid="ref-wallot2018analyzing" ref-type="bibr">Wallot
      &amp; Leonardi, 2018</xref>), or calculating the synchrony of
      spike trains
      (<xref alt="Kreuz et al., 2007" rid="ref-kreuz2007measuring" ref-type="bibr">Kreuz
      et al., 2007</xref>).</p>
    </sec>
  </sec>
  <sec id="availability-and-functionality">
    <title>Availability and functionality</title>
    <p><monospace>onsetsync</monospace> is available on GitHub and can
    be loaded and installed using the code below. The current version is
    0.5.0.</p>
    <code language="r script">library(devtools)
devtools::install_github(&quot;tuomaseerola/onsetsync&quot;)
library(onsetsync)</code>
    <p>To carry out such comparisons, <monospace>onsetsync</monospace>
    has several auxiliary functions. There are functions that help
    assess relative timings, to associate onsets with beat subdivisions
    and cycles in music structure, and to select and compare the
    instruments. The library has seven categories of functions
    summarised in Table 1.</p>
    <table-wrap>
      <caption>
        <p>Function categories and example functions or datasets.</p>
      </caption>
      <table>
        <thead>
          <tr>
            <th>Category</th>
            <th>Examples</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Input/Output</td>
            <td><monospace>get_OSF_csv</monospace>,
            <monospace>synthesise_onsets</monospace></td>
          </tr>
          <tr>
            <td>Annotation</td>
            <td><monospace>add_annotation</monospace>,
            <monospace>add_isobeats</monospace></td>
          </tr>
          <tr>
            <td>Visualise</td>
            <td><monospace>plot_by_beat</monospace>,
            <monospace>plot_by_pair</monospace>,
            <monospace>plot_by_variable</monospace></td>
          </tr>
          <tr>
            <td>Synchrony</td>
            <td><monospace>sync_sample_paired</monospace>,
            <monospace>sync_execute_pairs</monospace></td>
          </tr>
          <tr>
            <td>Periodicity</td>
            <td><monospace>periodicity</monospace>,
            <monospace>period_to_BPM</monospace>,
            <monospace>period_nPVI</monospace></td>
          </tr>
          <tr>
            <td>Summarise</td>
            <td><monospace>summarise_onsets</monospace>,
            <monospace>summarise_sync</monospace></td>
          </tr>
          <tr>
            <td>Datasets</td>
            <td><monospace>Asere_OU_2</monospace>,
            <monospace>DebBh_Drut</monospace>,
            <monospace>CSS_IEMP</monospace></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>In the next sections, we will demonstrate the analysis processes
    related to asynchrony between several performers. It is also worth
    pointing out that the library is not dedicated to extraction of
    onsets from audio as that can be done in other packages
    (e.g. <ext-link ext-link-type="uri" xlink:href="https://librosa.org">Librosa</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://essentia.upf.edu">Essentia</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://www.jyu.fi/hytk/fi/laitokset/mutku/en/research/materials/mirtoolbox">MIR
    toolbox for Matlab</ext-link>, or
    <ext-link ext-link-type="uri" xlink:href="https://www.sonicvisualiser.org">Sonic
    Visualiser</ext-link> using well-known onset detection algorithms).
    Here we take it for granted that the onsets have been extracted
    already, someone has carried out a quality check, assigned relevant
    onsets to beat positions and saved them in csv files. For solid
    overviews on computational onset extraction from audio, see Schlüter
    &amp; Böck
    (<xref alt="2014" rid="ref-schluter2014improved" ref-type="bibr">2014</xref>)
    or Böck et al.
    (<xref alt="2016" rid="ref-bock2016madmom" ref-type="bibr">2016</xref>),
    and for a full workflow of how to combine onset detection and
    annotation of the musical information, see Clayton, Tarsitani, et
    al.
    (<xref alt="2021" rid="ref-Clayton2020emr" ref-type="bibr">2021</xref>).</p>
    <sec id="onset-representation">
      <title>Onset representation</title>
      <p>Here we will take one Cuban Son performance that has been
      included in the library. This song, <italic>Palo Santo</italic>,
      has been performed by seven performers using the following
      combination of instruments: bass, guitar, tres, and trumpet, and
      five percussion instruments, clave, bongo, bell, cajon, and conga
      (the instrumentation varies slightly between songs and sections
      within songs). The full data including other Cuban salsa and son
      performances is available from an open access repository at
      <italic>Open Science Framework</italic> (OSF) and has been
      annotated and processed as part of the
      <ext-link ext-link-type="uri" xlink:href="https://musicscience.net/projects/timing/iemp/">Interpersonal
      Entrainment in Music Performance</ext-link> project, available at
      <ext-link ext-link-type="uri" xlink:href="https://osf.io/sfxa2/">https://osf.io/sfxa2/</ext-link>.</p>
      <p>The code example loads the onset data and annotations of metre
      of the son performance. The library comes with a set of five Cuban
      salsa and son (CSS) performances and this is the second example
      from the IEMP <monospace>CSS_IEMP</monospace> mini-corpus. The
      second line of the code selects the nine columns of interest for
      the analysis; the first two columns are meta-data, followed by
      four instruments, and three timing-related columns are included at
      the end.</p>
      <code language="r script">CSS_Song2 &lt;- dplyr::select(onsetsync::CSS_IEMP[[2]],
  Piece, Section, Clave, Bass, Guitar, Tres, 
  SD, Cycle, Isochronous.SD.Time)
print(knitr::kable(head(CSS_Song2),digits = 2,
  caption = 'Onset data structure example.'))</code>
      <table-wrap>
        <caption>
          <p>Onset data structure example.</p>
        </caption>
        <table>
          <colgroup>
            <col width="10%" />
            <col width="12%" />
            <col width="9%" />
            <col width="7%" />
            <col width="10%" />
            <col width="7%" />
            <col width="4%" />
            <col width="9%" />
            <col width="30%" />
          </colgroup>
          <thead>
            <tr>
              <th align="left">Piece</th>
              <th align="left">Section</th>
              <th align="right">Clave</th>
              <th align="right">Bass</th>
              <th align="right">Guitar</th>
              <th align="right">Tres</th>
              <th align="right">SD</th>
              <th align="right">Cycle</th>
              <th align="right">Isochronous.SD.Time</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">1</td>
              <td align="right">1</td>
              <td align="right">5.04</td>
            </tr>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">5.28</td>
              <td align="right">NA</td>
              <td align="right">2</td>
              <td align="right">1</td>
              <td align="right">5.26</td>
            </tr>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">5.48</td>
              <td align="right">NA</td>
              <td align="right">3</td>
              <td align="right">1</td>
              <td align="right">5.48</td>
            </tr>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">5.71</td>
              <td align="right">5.71</td>
              <td align="right">5.73</td>
              <td align="right">4</td>
              <td align="right">1</td>
              <td align="right">5.71</td>
            </tr>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">5.93</td>
              <td align="right">5.94</td>
              <td align="right">5.92</td>
              <td align="right">5</td>
              <td align="right">1</td>
              <td align="right">5.93</td>
            </tr>
            <tr>
              <td align="left">Song_2</td>
              <td align="left">Son</td>
              <td align="right">NA</td>
              <td align="right">NA</td>
              <td align="right">6.15</td>
              <td align="right">6.14</td>
              <td align="right">6</td>
              <td align="right">1</td>
              <td align="right">6.15</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>Table 2 shows a portion of the onset data structure, just 6
      rows out of 1568 rows in the data containing 3286 onsets. The
      first two columns are <italic>meta-data</italic>, referring to the
      piece and section. The next four columns represent the onset times
      in seconds (rounded to two decimals for convenience) of four
      selected instruments, <monospace>Clave</monospace>,
      <monospace>Bass</monospace>, <monospace>Guitar</monospace> and
      <monospace>Tres</monospace>. The onset times for the instruments
      are available in seconds (with the precision of 6 digits, although
      we prefer to use milliseconds (ms) in the analyses of synchrony to
      avoid reporting unnecessarily many digits). The last three columns
      refer to information about the relative time:
      <monospace>SD</monospace> represents the beat subdivision within
      the cycle. In this music, there are 16 subdivisions per cycle
      (which can be felt as 4 beats () 4 subdivisions).
      <monospace>Cycle</monospace> refers to a running number of the
      beat cycles (each will have 16 beats in this music).
      <monospace>Isochronous.SD.Time</monospace> is an evenly temporally
      spaced time point for each beat subdivision (calculated by
      dividing each cycle duration by 16) that can act as a temporal
      reference grid. The beat subdivisions and cycles are manually
      annotated and can be integrated to the data frame with a specific
      function (<monospace>add_annotation</monospace>) if given
      separately. Also, a reference timing
      (<monospace>Isochronous.SD.Time</monospace>) has been provided for
      the cycles but it can also be estimated from the cycles and beat
      subdivisions using a function
      (<monospace>add_isobeats</monospace>) in the library. Information
      about cycles and beat subdivisions is generally based on manual
      annotation, and originates separately from the automatically
      extracted onsets; the two are combined to produce tables with
      onsets assigned to beat positions. This explains why the first row
      of the table has no onsets but it contains annotation information
      (Cycle 1 and beat sub-division 1). It is also possible to read the
      csv files directly from OSF using
      <monospace>get_OSF_csv</monospace> function to allow transparent
      and reproducible analysis workflows with published datasets.</p>
    </sec>
    <sec id="onset-summaries">
      <title>Onset summaries</title>
      <p>Let’s explore the overall structure of the onsets in the
      example piece using the function
      <monospace>plot_by_beat</monospace> to visualise the asynchrony
      relative to an equal division subdivision of the beats for each
      instrument across the time. This visualisation gives a summary of
      when the instruments are playing relative to the beat division and
      the sections of the song, shown in Figure 1.</p>
      <code language="r script">library(dplyr)
fig1 &lt;- plot_by_beat(df = CSS_Song2, 
                     instr = c('Bass','Clave','Guitar','Tres'), 
                     beat = 'SD', 
                     virtual='Isochronous.SD.Time',
                     pcols=2)
print(fig1)</code>
      <fig>
        <caption><p>Onsets arranged for beat sub-divisions for four
        instruments across the whole piece.</p></caption>
        <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/paper_files/figure-latex/figure1-1.pdf" />
      </fig>
      <p>At first glance at Figure 1, it is easy to spot the distinctive
      clave pattern going through the song (the beginning of the song is
      at the bottom of the graphs, and every cycle is cascaded on top of
      the previous cycle). Notice how the bass plays mainly on
      subdivisions 5, 7, 13, 15, except in the last minute when the
      pattern changes into playing subdivisions 3, 4, 7, 8, 15, 16. The
      guitar plays almost on every beat subdivision, and towards the end
      of the piece, the tres – a cuban guitar with 6-strings tuned to 3
      pitches – plays a pattern of 3 subdivisions and a break.</p>
    </sec>
  </sec>
</sec>
<sec id="analysis-of-synchrony">
  <title>Analysis of synchrony</title>
  <p>Moving on to the analysis of synchrony, we can first explore how
  much the onsets deviate from the isochronous beats or from the mean
  onset times. Figure 2 shows an example of the former for two guitars,
  guitar and tres.</p>
  <code language="r script">print(plot_by_beat(df = CSS_Song2,
  instr = c(&quot;Guitar&quot;,&quot;Tres&quot;),
  beat = &quot;SD&quot;,
  virtual = &quot;Isochronous.SD.Time&quot;, 
  griddeviations = TRUE))</code>
  <fig>
    <caption><p>Relative timing deviations from beat sub-divisions
    across the performance for guitar and tres.</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/paper_files/figure-latex/figure2-1.pdf" />
  </fig>
  <p>Overall this suggests that these two instruments tend to play
  earlier when compared to the isochronous beat division of the cycle,
  although the guitar is tightly aligned (&lt;2%) with the reference
  beat on every fourth beat subdivisions (subdivisions of 1, 5, 9, and
  13), which are also the salient metrical positions. Similar
  fluctuations across the beat subdivisions are not evident in the tres,
  but the graph reveals that the tres tends to play a little earlier
  than the guitar.</p>
  <sec id="synchrony-between-the-instruments">
    <title>Synchrony between the instruments</title>
    <p>Let’s determine the overall synchrony between specific
    instruments. Here we continue the analysis of the guitar and the
    tres and calculate the asynchrony between them.</p>
    <code language="r script">d1 &lt;- sync_sample_paired(CSS_Song2, instr1 = &quot;Guitar&quot;, instr2 = &quot;Tres&quot;, 
  beat = &quot;SD&quot;)
dplyr::summarise(data.frame(d1), 
  N = n(), Mean.ms = mean(asynch*1000), Sd.ms = sd(asynch*1000))</code>
    <preformat>##     N  Mean.ms    Sd.ms
## 1 853 12.53126 26.74134</preformat>
    <p>This analysis indicates that on average, the tres plays 13 ms
    ahead of the guitar, with a standard deviation of 27 ms. When one is
    comparing instruments that have radically different numbers of joint
    onsets (such as the clave and the bass in this example), it is
    possible to make the comparisons between the pairs of instruments
    easier by specifying a sample of onsets that will be taken from each
    instrument for the analysis. It also possible to establish the
    confidence intervals by bootstrapping the asynchrony
    calculations.</p>
    <p>To carry out the comparison for all possible pairings of the
    instruments is possible with
    <monospace>sync_execute_pairs</monospace> function and the results
    can be visualised with a related function
    (<monospace>plot_by_pair</monospace>).</p>
    <code language="r script">inst &lt;- c(&quot;Clave&quot;,&quot;Bass&quot;,&quot;Guitar&quot;,&quot;Tres&quot;)
dn &lt;- sync_execute_pairs(CSS_Song2, inst, beat = &quot;SD&quot;)
print(plot_by_pair(dn))</code>
    <fig>
      <caption><p>Asynchronies across multiple instrument
      pairs.</p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/paper_files/figure-latex/figure3-1.pdf" />
    </fig>
    <p>In Figure 3 we see how different pairs of instruments have
    different asynchrony relationships; the bass is consistently ahead
    of the guitar, and the clave and the guitar play behind the tres, to
    pick some of the extreme examples from the visualisation. The
    vertical lines stand for median asynchrony and the violin plot shows
    the distribution of onset time differences.</p>
    <p>We can calculate the classic measures of synchronization once the
    pairing has been done. Here are these measures calculated for the
    synchrony between the bass and the clave.</p>
    <code language="r script">d &lt;- sync_sample_paired(CSS_Song2,&quot;Clave&quot;,&quot;Bass&quot;, beat = &quot;SD&quot;)
print(t(summarise_sync(d)))</code>
    <preformat>##                               [,1]
## Pairwise asynchronization 19.58636
## Mean absolute asynchrony  20.66440
## Mean pairwise asynchrony  16.23288</preformat>
    <p>The output suggests that at least in this piece, these two
    instruments tend to have the synchronization precision around 20 ms,
    depending on the method of calculation. Pairwise asynchronization is
    the standard deviation of the signed asynchrony; this tends to be
    highly correlated with the mean absolute asynchrony. The Mean
    pairwise asynchrony measure shows that on average, the clave plays
    16 ms after the bass.</p>
    <p>We can also calculate the relative asynchrony by comparing the
    onset times to the mean of the other instruments that we define.
    Here we compare the asynchrony of the bass and the clave to the mean
    of the rhythm section as defined by the mean onset times of the four
    instruments.</p>
    <code language="r script">dl &lt;- sync_sample_paired_relative(df = CSS_Song2, 
  instr = &quot;Bass&quot;, instr_ref = c(&quot;Guitar&quot;,&quot;Bass&quot;,&quot;Tres&quot;,&quot;Clave&quot;),
  beat = &quot;SD&quot;)
print(dl$`Mean pairwise asynchrony`)</code>
    <preformat>## [1] -18.72675</preformat>
    <code language="r script">dl &lt;- sync_sample_paired_relative(df = CSS_Song2,
  instr = &quot;Clave&quot;, instr_ref = c(&quot;Guitar&quot;,&quot;Bass&quot;,&quot;Tres&quot;,&quot;Clave&quot;),
  beat = &quot;SD&quot;)
print(dl$`Mean pairwise asynchrony`)</code>
    <preformat>## [1] 0.8796104</preformat>
    <p>The relative measure (mean relative asynchrony) shows that the
    bass is ahead (i.e. -19 ms) of the mean of other the instruments
    whereas the clave is almost exactly in synchrony with the other
    instruments (&lt;1 ms late). In different types of performances,
    musical pieces, instruments, and genres, the magnitude of these
    differences is highly variable
    (<xref alt="Clayton et al., 2020" rid="ref-clayton2020" ref-type="bibr">Clayton
    et al., 2020</xref>).</p>
    <p>It is also possible to
    <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/articles/advanced_topics.html">calculate
    synchrony between instruments across beat sub-divisions</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/articles/advanced_topics.html">assess
    synchrony across multiple performances</ext-link>, and
    <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/articles/advanced_topics.html">estimate
    and visualise synchrony with other variables</ext-link>. The library
    also comes with functionality to
    <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/articles/analysisis_of_periodicity.html">analyse
    the periodicity of the onsets</ext-link> and
    <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/articles/synthesise_onsets.html">synthesising
    the onsets</ext-link>.</p>
  </sec>
</sec>
<sec id="conclusions">
  <title>Conclusions</title>
  <p><monospace>onsetsync</monospace> provides representations,
  visualisations and calculations of key measures relating to onset
  structures in music. The library does not take a stance on how the
  onset times have been estimated but assumes that onset times are
  mapped onto beat positions and that metre annotations are available.
  Future development needs lie in creating a specific corpus format that
  would be open to many uses, testing the framework with other datasets,
  and incorporating other analytical options such as circular
  statistics, Granger causality or cross-recurrence analyses.</p>
  <p>We hope scholars and students of music, music information retrieval
  and psychology can pursue topics related to synchrony in music with a
  transparent workflow assisted by the library and capitalise on
  existing datasets that are compatible with the library. Eventually, we
  hope that tools such as these will allow a larger number of people to
  create and utilise open datasets related to musical behaviours,
  helping the topic become data-driven, empirical and collaborative.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from IEMP Fellows, specifically Adrian
  Poole, who created Cuban Son and Salsa dataset with Simone Tarsitani
  and Martin Clayton, but also Rainer Polak,
  <ext-link ext-link-type="uri" xlink:href="https://as.tufts.edu/music/people/faculty/richard-jankowsky">Richard
  Jankowsky</ext-link>,
  <ext-link ext-link-type="uri" xlink:href="http://www.norijacoby.com">Nori
  Jacoby</ext-link>, Mark Doffman, Luis Jure, Andy McGuiness, Nikki
  Moran, and
  <ext-link ext-link-type="uri" xlink:href="https://iie.fing.edu.uy/~rocamora/">Martín
  Rocamora</ext-link>). We are thankful for technical support by
  <ext-link ext-link-type="uri" xlink:href="https://www.durham.ac.uk/staff/simone-tarsitani/">Simone
  Tarsitani</ext-link> and useful feedback by Juliano Abramovay. For
  funding and time, we acknowledge EU Horizon 2020 FET project
  <ext-link ext-link-type="uri" xlink:href="https://entimement.dibris.unige.it/">EnTimeMent
  - ENtrainment &amp; synchronization at multiple TIME scales in the
  MENTal foundations of expressive gesture</ext-link> that was
  instrumental in turning the idea about the shared resource for
  temporal analyses such as musical onsets into an open access
  library.</p>
  <p>The package is available from
  <ext-link ext-link-type="uri" xlink:href="https://tuomaseerola.github.io/onsetsync/">https://tuomaseerola.github.io/onsetsync/</ext-link>.
  The analyses shown in the article are contained in the vignette and
  article titled “Analysis Example”.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-berens2009circstat">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Berens</surname><given-names>Philipp</given-names></name>
      </person-group>
      <article-title>CircStat: A MATLAB toolbox for circular statistics</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2009">2009</year>
      <volume>31</volume>
      <fpage>1</fpage>
      <lpage>21</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bock2016madmom">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
        <name><surname>Korzeniowski</surname><given-names>Filip</given-names></name>
        <name><surname>Schlüter</surname><given-names>Jan</given-names></name>
        <name><surname>Krebs</surname><given-names>Florian</given-names></name>
        <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
      </person-group>
      <article-title>Madmom: A new python audio and music signal processing library</article-title>
      <source>Proceedings of the 24th ACM international conference on multimedia</source>
      <year iso-8601-date="2016">2016</year>
      <fpage>1174</fpage>
      <lpage>1178</lpage>
    </element-citation>
  </ref>
  <ref id="ref-clayton2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clayton</surname><given-names>M.</given-names></name>
        <name><surname>Jakubowski</surname><given-names>K.</given-names></name>
        <name><surname>Eerola</surname><given-names>T.</given-names></name>
        <name><surname>Keller</surname><given-names>P.</given-names></name>
        <name><surname>Camurri</surname><given-names>A.</given-names></name>
        <name><surname>Volpe</surname><given-names>G.</given-names></name>
        <name><surname>Alborno</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Interpersonal entrainment in music performance: Theory, method and model</article-title>
      <source>Music Perception</source>
      <year iso-8601-date="2020">2020</year>
      <volume>38</volume>
      <issue>2</issue>
      <fpage>136</fpage>
      <lpage>194</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goebl2010investigations">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goebl</surname><given-names>Werner</given-names></name>
        <name><surname>Flossmann</surname><given-names>Sebastian</given-names></name>
        <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
      </person-group>
      <article-title>Investigations of between-hand synchronization in Magaloff’s Chopin</article-title>
      <source>Computer Music Journal</source>
      <publisher-name>The MIT Press</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>34</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1162/COMJ_a_00002</pub-id>
      <fpage>35</fpage>
      <lpage>44</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hawthorne2018enabling">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hawthorne</surname><given-names>Curtis</given-names></name>
        <name><surname>Stasyuk</surname><given-names>Andriy</given-names></name>
        <name><surname>Roberts</surname><given-names>Adam</given-names></name>
        <name><surname>Simon</surname><given-names>Ian</given-names></name>
        <name><surname>Huang</surname><given-names>Cheng-Zhi Anna</given-names></name>
        <name><surname>Dieleman</surname><given-names>Sander</given-names></name>
        <name><surname>Elsen</surname><given-names>Erich</given-names></name>
        <name><surname>Engel</surname><given-names>Jesse</given-names></name>
        <name><surname>Eck</surname><given-names>Douglas</given-names></name>
      </person-group>
      <article-title>Enabling factorized piano music modeling and generation with the MAESTRO dataset</article-title>
      <source>arXiv preprint arXiv:1810.12247</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-kreuz2007measuring">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kreuz</surname><given-names>Thomas</given-names></name>
        <name><surname>Haas</surname><given-names>Julie S</given-names></name>
        <name><surname>Morelli</surname><given-names>Alice</given-names></name>
        <name><surname>Abarbanel</surname><given-names>Henry DI</given-names></name>
        <name><surname>Politi</surname><given-names>Antonio</given-names></name>
      </person-group>
      <article-title>Measuring spike train synchrony</article-title>
      <source>Journal of Neuroscience Methods</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>165</volume>
      <issue>1</issue>
      <fpage>151</fpage>
      <lpage>161</lpage>
    </element-citation>
  </ref>
  <ref id="ref-schluter2014improved">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schlüter</surname><given-names>Jan</given-names></name>
        <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Improved musical onset detection with convolutional neural networks</article-title>
      <source>2014 IEEE international conference on acoustics, speech and signal processing (icassp)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.1109/ICASSP.2014.6854953</pub-id>
      <fpage>6979</fpage>
      <lpage>6983</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wallot2018analyzing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wallot</surname><given-names>Sebastian</given-names></name>
        <name><surname>Leonardi</surname><given-names>Giuseppe</given-names></name>
      </person-group>
      <article-title>Analyzing multivariate dynamics using cross-recurrence quantification analysis (CRQA), diagonal-cross-recurrence profiles (dcrp), and multidimensional recurrence quantification analysis (MDRQA)– A tutorial in R</article-title>
      <source>Frontiers in Psychology</source>
      <publisher-name>Frontiers Media SA</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.3389/fpsyg.2018.02232</pub-id>
      <fpage>2232</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lordelo2020adversarial">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lordelo</surname><given-names>Carlos</given-names></name>
        <name><surname>Benetos</surname><given-names>Emmanouil</given-names></name>
        <name><surname>Dixon</surname><given-names>Simon</given-names></name>
        <name><surname>Ahlbäck</surname><given-names>Sven</given-names></name>
        <name><surname>Ohlsson</surname><given-names>Patrik</given-names></name>
      </person-group>
      <article-title>Adversarial unsupervised domain adaptation for harmonic-percussive source separation</article-title>
      <source>IEEE Signal Processing Letters</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>28</volume>
      <pub-id pub-id-type="doi">10.1109/LSP.2020.3045915</pub-id>
      <fpage>81</fpage>
      <lpage>85</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sordo2014creating">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sordo</surname><given-names>Mohamed</given-names></name>
        <name><surname>Chaachoo</surname><given-names>Amin</given-names></name>
        <name><surname>Serra</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>Creating corpora for computational research in Arab-Andalusian music</article-title>
      <source>Proceedings of the 1st international workshop on digital libraries for musicology</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.1145/2660168.2660182</pub-id>
      <fpage>1</fpage>
      <lpage>3</lpage>
    </element-citation>
  </ref>
  <ref id="ref-srinivasamurthy2016generalized">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Srinivasamurthy</surname><given-names>Ajay</given-names></name>
        <name><surname>Holzapfel</surname><given-names>Andre</given-names></name>
        <name><surname>Cemgil</surname><given-names>Ali Taylan</given-names></name>
        <name><surname>Serra</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>A generalized Bayesian model for tracking long metrical cycles in acoustic music signals</article-title>
      <source>2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/ICASSP.2016.7471640</pub-id>
      <fpage>76</fpage>
      <lpage>80</lpage>
    </element-citation>
  </ref>
  <ref id="ref-srinivasamurthy2015particle">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Srinivasamurthy</surname><given-names>Ajay</given-names></name>
        <name><surname>Holzapfel</surname><given-names>Andre</given-names></name>
        <name><surname>Cemgil</surname><given-names>Ali Taylan</given-names></name>
        <name><surname>Serra</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>Particle filters for efficient meter tracking with dynamic Bayesian networks</article-title>
      <source>Proceedings of the 16th international society for music information retrieval conference; 2015 oct 26-30</source>
      <person-group person-group-type="editor">
        <name><surname>M.</surname><given-names>Müller</given-names></name>
        <name><surname>F.</surname><given-names>Wiering</given-names></name>
      </person-group>
      <publisher-name>International Society for Music Information Retrieval (ISMIR)</publisher-name>
      <publisher-loc>Málaga, Spain</publisher-loc>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-Poole_Tarsitani_Clayton_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Poole</surname><given-names>Adrian</given-names></name>
        <name><surname>Tarsitani</surname><given-names>Simone</given-names></name>
        <name><surname>Clayton</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>IEMP Cuban Son and Salsa</article-title>
      <publisher-name>OSF</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>osf.io/sfxa2</uri>
      <pub-id pub-id-type="doi">10.17605/OSF.IO/SFXA2</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Clayton_Leante_Tarsitani_2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Clayton</surname><given-names>Martin</given-names></name>
        <name><surname>Leante</surname><given-names>Laura</given-names></name>
        <name><surname>Tarsitani</surname><given-names>Simone</given-names></name>
      </person-group>
      <article-title>IEMP North Indian Raga</article-title>
      <publisher-name>OSF</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>osf.io/ks325</uri>
      <pub-id pub-id-type="doi">10.17605/OSF.IO/KS325</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wing2014optimal">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wing</surname><given-names>Alan M</given-names></name>
        <name><surname>Endo</surname><given-names>Satoshi</given-names></name>
        <name><surname>Bradbury</surname><given-names>Adrian</given-names></name>
        <name><surname>Vorberg</surname><given-names>Dirk</given-names></name>
      </person-group>
      <article-title>Optimal feedback correction in string quartet synchronization</article-title>
      <source>Journal of The Royal Society Interface</source>
      <publisher-name>The Royal Society</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>11</volume>
      <issue>93</issue>
      <pub-id pub-id-type="doi">10.1098/rsif.2013.1125</pub-id>
      <fpage>20131125</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-repp2013sensorimotor">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Repp</surname><given-names>Bruno H</given-names></name>
        <name><surname>Su</surname><given-names>Yi-Huang</given-names></name>
      </person-group>
      <article-title>Sensorimotor synchronization: A review of recent research (2006–2012)</article-title>
      <source>Psychonomic Bulletin &amp; Review</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>20</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.3758/s13423-012-0371-2</pub-id>
      <fpage>403</fpage>
      <lpage>452</lpage>
    </element-citation>
  </ref>
  <ref id="ref-clayton_et_al_2018b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clayton</surname><given-names>M.</given-names></name>
        <name><surname>Jakubowski</surname><given-names>K.</given-names></name>
        <name><surname>Eerola</surname><given-names>T.</given-names></name>
      </person-group>
      <article-title>Interpersonal entrainment in Indian instrumental music performance: Synchronization and movement coordination relate to tempo, dynamics, metrical and cadential structure</article-title>
      <source>Musicae Scientiae</source>
      <year iso-8601-date="2019">2019</year>
      <volume>23</volume>
      <pub-id pub-id-type="doi">10.1177/1029864919844809</pub-id>
      <fpage>304</fpage>
      <lpage>331</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rasch1979synchronization">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rasch</surname><given-names>Rudolf A</given-names></name>
      </person-group>
      <article-title>Synchronization in performed ensemble music</article-title>
      <source>Acta Acustica united with Acustica</source>
      <publisher-name>S. Hirzel Verlag</publisher-name>
      <year iso-8601-date="1979">1979</year>
      <volume>43</volume>
      <issue>2</issue>
      <fpage>121</fpage>
      <lpage>131</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rasch1988timing">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Rasch</surname><given-names>Rudolf A</given-names></name>
      </person-group>
      <article-title>Timing and synchronization in ensemble performance</article-title>
      <source>Generative processes in music: The psychology of performance, improvisation, and composition</source>
      <person-group person-group-type="editor">
        <name><surname>Sloboda</surname><given-names>J. A.</given-names></name>
      </person-group>
      <publisher-name>Clarendon Press/Oxford University Press</publisher-name>
      <year iso-8601-date="1988">1988</year>
      <fpage>70</fpage>
      <lpage>90</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Clayton2020emr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Clayton</surname><given-names>M.</given-names></name>
        <name><surname>Tarsitani</surname><given-names>S.</given-names></name>
        <name><surname>Jankowsky</surname><given-names>R.</given-names></name>
        <name><surname>Jure</surname><given-names>L.</given-names></name>
        <name><surname>Leante</surname><given-names>L.</given-names></name>
        <name><surname>Polak</surname><given-names>R.</given-names></name>
        <name><surname>Poole</surname><given-names>A.</given-names></name>
        <name><surname>Rocamora</surname><given-names>M.</given-names></name>
        <name><surname>Alborno</surname><given-names>P.</given-names></name>
        <name><surname>Camurri</surname><given-names>A.</given-names></name>
        <name><surname>Eerola</surname><given-names>T.</given-names></name>
        <name><surname>Jacoby</surname><suffix>N.</suffix></name>
        <name><surname>Jakubowski</surname><given-names>K.</given-names></name>
      </person-group>
      <article-title>The interpersonal entrainment in music performance data collection</article-title>
      <source>Empirical Musicology Review</source>
      <year iso-8601-date="2021">2021</year>
      <volume>16</volume>
      <issue>1</issue>
      <uri>10.18061/emr.v16i1.7555</uri>
      <pub-id pub-id-type="doi">10.18061/emr.v16i1.7555</pub-id>
      <fpage>65</fpage>
      <lpage>84</lpage>
    </element-citation>
  </ref>
  <ref id="ref-polak2016both">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Polak</surname><given-names>Rainer</given-names></name>
        <name><surname>London</surname><given-names>Justin</given-names></name>
        <name><surname>Jacoby</surname><given-names>Nori</given-names></name>
      </person-group>
      <article-title>Both isochronous and non-isochronous metrical subdivision afford precise and stable ensemble entrainment: A corpus study of malian jembe drumming</article-title>
      <source>Frontiers in Neuroscience</source>
      <publisher-name>Frontiers</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.3389/fnins.2016.00285</pub-id>
      <fpage>285</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-repp2008sensorimotor">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Repp</surname><given-names>Bruno H</given-names></name>
        <name><surname>Keller</surname><given-names>Peter E</given-names></name>
      </person-group>
      <article-title>Sensorimotor synchronization with adaptively timed sequences</article-title>
      <source>Human Movement Science</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>27</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1016/j.humov.2008.02.016</pub-id>
      <fpage>423</fpage>
      <lpage>456</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
