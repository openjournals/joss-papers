<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8253</article-id>
<article-id pub-id-type="doi">10.21105/joss.08253</article-id>
<title-group>
<article-title>Explabox: A Python Toolkit for Standardized Auditing and
Explanation of Text Models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6430-9774</contrib-id>
<name>
<surname>Robeer</surname>
<given-names>Marcel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4823-6085</contrib-id>
<name>
<surname>Bron</surname>
<given-names>Michiel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2729-6599</contrib-id>
<name>
<surname>Herrewijnen</surname>
<given-names>Elize</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hoeseni</surname>
<given-names>Riwish</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5699-9656</contrib-id>
<name>
<surname>Bex</surname>
<given-names>Floris</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>National Police Lab AI, Utrecht University, The
Netherlands</institution>
<institution-id institution-id-type="ROR">04pp8hn57</institution-id>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Netherlands National Police, The Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>School of Law, Utrecht University, The
Netherlands</institution>
<institution-id institution-id-type="ROR">04pp8hn57</institution-id>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-08-29">
<day>29</day>
<month>8</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>114</issue>
<fpage>8253</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>AI auditing</kwd>
<kwd>explainable AI (XAI)</kwd>
<kwd>interpretability</kwd>
<kwd>fairness</kwd>
<kwd>robustness</kwd>
<kwd>AI safety</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Developed to meet the practical machine learning (ML) auditing
  requirements of the Netherlands National Police,
  <monospace>Explabox</monospace> is an open-source Python toolkit that
  implements a standardized four-step analysis workflow:
  <italic>explore</italic>, <italic>examine</italic>,
  <italic>explain</italic>, and <italic>expose</italic>. The framework
  transforms models and data (<italic>ingestibles</italic>) into
  interpretable reports and visualizations
  (<italic>digestibles</italic>), covering everything from data
  statistics and performance metrics to local and global explanations,
  and sensitivity testing for fairness, robustness, and security.
  Designed for developers, testers, and auditors,
  <monospace>Explabox</monospace> operationalizes the entire audit
  lifecycle in a reproducible manner. The initial release is focused on
  text classification and regression models, with plans for future
  expansion. Code and documentation are available open-source at
  <ext-link ext-link-type="uri" xlink:href="https://explabox.readthedocs.io/en/stable">https://explabox.readthedocs.io</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In high-stakes environments like law enforcement, machine learning
  (ML) models are subject to intense scrutiny and must comply with
  emerging regulations like the EU AI Act
  (<xref alt="Edwards, 2022" rid="ref-Edwards2022" ref-type="bibr">Edwards,
  2022</xref>). <monospace>Explabox</monospace> was developed to address
  the operational challenges of ML auditing at the Netherlands National
  Police, where models for text classification and regression require
  standardized, reproducible, and holistic evaluation to satisfy diverse
  stakeholders—from developers and internal auditors to legal and
  ethical oversight bodies. Existing tools, while powerful, were often
  fragmented, focusing on a single aspect of analysis (e.g., only
  explainability or testing) and lacking a unified framework for
  conducting a complete audit from data exploration to final
  reporting.</p>
  <p>To solve this workflow problem, we developed Explabox around a
  four-step analysis strategy—<italic>explore</italic>,
  <italic>examine</italic>, <italic>explain</italic>, and
  <italic>expose</italic>—inspired by similar conceptualizations of the
  analytical process
  (<xref alt="Biecek &amp; Burzykowski, 2021" rid="ref-Biecek2021" ref-type="bibr">Biecek
  &amp; Burzykowski, 2021</xref>). While comprehensive libraries like
  <monospace>OmniXAI</monospace>
  (<xref alt="Yang et al., 2022" rid="ref-Yang2022" ref-type="bibr">Yang
  et al., 2022</xref>) offer a broad, multi-modal collection of
  explainers and <monospace>dalex</monospace>
  (<xref alt="Baniecki et al., 2021" rid="ref-Baniecki2021" ref-type="bibr">Baniecki
  et al., 2021</xref>) provides a mature, research-driven framework for
  model exploration, <monospace>Explabox</monospace> was developed to
  fill a specific operational gap. Practitioners seeking to conduct a
  full audit in a model-agnostic manner often have to combine multiple,
  highly-specialized libraries, such as <monospace>AIF360</monospace>
  (<xref alt="Bellamy et al., 2018" rid="ref-Bellamy2018" ref-type="bibr">Bellamy
  et al., 2018</xref>) for fairness metrics,
  <monospace>alibi explain</monospace>
  (<xref alt="Klaise et al., 2021" rid="ref-Klaise2021" ref-type="bibr">Klaise
  et al., 2021</xref>) or <monospace>AIX360</monospace>
  (<xref alt="Arya et al., 2019" rid="ref-Arya2019" ref-type="bibr">Arya
  et al., 2019</xref>) for local explanations, and
  <monospace>CheckList</monospace>
  (<xref alt="Ribeiro et al., 2020" rid="ref-Ribeiro2020" ref-type="bibr">Ribeiro
  et al., 2020</xref>) for behavioral testing.</p>
  <p>This fragmentation introduces significant challenges, particularly
  regarding <italic>reproducibility</italic> and <italic>flexibility in
  communicating results</italic>. <monospace>Explabox</monospace>
  addresses the reproducibility challenge by providing a unified
  pipeline that not only offers centralized control over random seeds,
  but also tracks the specific data subsets and parameters used for each
  function call, ensuring full traceability. Furthermore, it provides
  flexibility through our <italic>digestible</italic> object system,
  which is designed to generate outputs tailored to diverse
  stakeholders. By integrating these critical components into a single,
  cohesive workflow, <monospace>Explabox</monospace> provides a
  practical framework that enhances the efficiency and methodological
  rigor of the ML auditing lifecycle, making it directly applicable to
  other high-stakes domains where model validation is critical, such as
  finance, healthcare, and law.</p>
</sec>
<sec id="explore-examine-explain-expose-your-ml-models">
  <title>Explore, Examine, Explain &amp; Expose your ML models</title>
  <p><monospace>Explabox</monospace> transforms opaque
  <italic>ingestibles</italic> into transparent
  <italic>digestibles</italic> through four types of
  <italic>analyses</italic> to enhance explainability and aid fairness,
  robustness, and security audits.</p>
  <sec id="ingestibles">
    <title>Ingestibles</title>
    <p>Ingestibles provide a unified import interface for data and
    models, where layers abstract away access
    (<xref alt="[fig:layers]" rid="figU003Alayers">[fig:layers]</xref>)
    to allow optimized processing. <monospace>Explabox</monospace> uses
    <monospace>instancelib</monospace>
    (<xref alt="Bron, 2023" rid="ref-instancelib" ref-type="bibr">Bron,
    2023</xref>) for fast model and data encapsulation. The model can be
    any Python <monospace>Callable</monospace> containing a regression
    or (binary and multi-class) classification model. While this
    interface is model-agnostic, the current release provides data
    handling and analysis modules optimized specifically for text-based
    tasks. <monospace>scikit-learn</monospace> or
    <monospace>ONNX</monospace> models (e.g.,
    <monospace>PyTorch</monospace>, <monospace>TensorFlow</monospace>,
    or <monospace>Keras</monospace>) import directly with optimizations
    and automatic input/output interpretation. Data can be automatically
    downloaded, extracted and loaded. Data inputs include
    <monospace>NumPy</monospace>, <monospace>Pandas</monospace>,
    <monospace>Hugging Face</monospace>, raw files (e.g., HDF5, CSV, or
    TSV), and (compressed) file folders. Data can be subdivided into
    named splits (e.g., train-test-validation), and instance vectors and
    tokens can be precomputed and optionally saved for fast
    inferencing.</p>
    <fig>
      <caption><p>Logical separation of <monospace>Explabox</monospace>
      into layers with
      interfaces.<styled-content id="figU003Alayers"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figure1.png" />
    </fig>
  </sec>
  <sec id="analyses">
    <title>Analyses</title>
    <p><monospace>Explabox</monospace> turns these
    <italic>ingestibles</italic> into <italic>digestibles</italic>
    (transparency-increasing information on ingestibles) through four
    <italic>analyses</italic> types: <bold>explore</bold>,
    <bold>examine</bold>, <bold>explain</bold>, and
    <bold>expose</bold>.</p>
    <p><bold>Explore</bold> allows data slicing, dicing and sorting, and
    provides descriptive statistics (dataset sizes, label distributions,
    and text string/token lengths).</p>
    <p><bold>Examine</bold> shows model performance metrics, summarized
    in a table or shown graphically, with computation and interpretation
    references. For further analysis, <bold>examine</bold> also supports
    drilling down into (in)correct predictions.</p>
    <p><bold>Explain</bold> uses model-agnostic techniques
    (<xref alt="Ribeiro et al., 2016a" rid="ref-Ribeiro2016b" ref-type="bibr">Ribeiro
    et al., 2016a</xref>) to explain model behavior
    (<italic>global</italic>) and individual predictions
    (<italic>local</italic>). It summarizes model-labelled data, through
    prototypes (<monospace>K-Medoids</monospace>), prototypes with
    criticisms (<monospace>MMDCritic</monospace>
    (<xref alt="Kim et al., 2016" rid="ref-Kim2016" ref-type="bibr">Kim
    et al., 2016</xref>)), token distributions
    (<monospace>TokenFrequency</monospace>), and token informativeness
    (<monospace>TokenInformation</monospace>). Local explanations use
    popular techniques: feature attribution scores
    (<monospace>LIME</monospace>
    (<xref alt="Ribeiro et al., 2016b" rid="ref-Ribeiro2016a" ref-type="bibr">Ribeiro
    et al., 2016b</xref>), <monospace>KernelSHAP</monospace>
    (<xref alt="Lundberg &amp; Lee, 2017" rid="ref-Lundberg2017" ref-type="bibr">Lundberg
    &amp; Lee, 2017</xref>)), feature subsets
    (<monospace>Anchors</monospace>
    (<xref alt="Ribeiro et al., 2018" rid="ref-Ribeiro2018" ref-type="bibr">Ribeiro
    et al., 2018</xref>)), local rule-based models
    (<monospace>LORE</monospace>
    (<xref alt="Guidotti et al., 2018" rid="ref-Guidotti2018" ref-type="bibr">Guidotti
    et al., 2018</xref>)), and counterfactual or contrastive
    explanations (<monospace>FoilTrees</monospace>
    (<xref alt="Waa et al., 2018" rid="ref-Waa2018" ref-type="bibr">Waa
    et al., 2018</xref>)). Built from generic components separating
    global and local explanation steps, these methods allow
    customization and enable scientific advances to be quickly
    integrated into operational processes (e.g., combine
    <monospace>KernelShap</monospace> sampling with
    <monospace>imodels</monospace>
    (<xref alt="Singh et al., 2021" rid="ref-Singh2021" ref-type="bibr">Singh
    et al., 2021</xref>) surrogate rules). Example configurations, such
    as <monospace>LIME</monospace> with default hyperparameters, ease
    adoption. <bold>Explain</bold> is provided by subpackage
    <monospace>text_explainability</monospace>
    (<xref alt="Robeer, 2021a" rid="ref-text_explainability" ref-type="bibr">Robeer,
    2021a</xref>), which doubles as a standalone tool.</p>
    <p><bold>Expose</bold> gathers sensitivity insights via local and
    global testing regimes. These insights can be used to, through
    relevant attributes, assess the <italic>robustness</italic> (e.g.,
    the effect of typos on model performance), <italic>security</italic>
    (e.g., if inputs containing certain characters crash the model), and
    <italic>fairness</italic> (e.g., subgroup performance for protected
    attributes such as country of origin, gender, race or socioeconomic
    status) of the model. Relevant attributes can either be observed in
    the current data or generated from user-provided templates
    (<xref alt="Ribeiro et al., 2020" rid="ref-Ribeiro2020" ref-type="bibr">Ribeiro
    et al., 2020</xref>) filled with multi-lingual data generation
    (<xref alt="Faraglia &amp; Other Contributors, 2021" rid="ref-Faker" ref-type="bibr">Faraglia
    &amp; Other Contributors, 2021</xref>). These attributes are then
    either summarized in performance metrics, compared to expected
    behavior
    (<xref alt="Ribeiro et al., 2020" rid="ref-Ribeiro2020" ref-type="bibr">Ribeiro
    et al., 2020</xref>), or assessed with fairness metrics for
    classification
    (<xref alt="Mehrabi et al., 2021" rid="ref-Mehrabi2021" ref-type="bibr">Mehrabi
    et al., 2021</xref>) and regression
    (<xref alt="Agarwal et al., 2019" rid="ref-Agarwal2019" ref-type="bibr">Agarwal
    et al., 2019</xref>). Like <bold>explain</bold>, <bold>expose</bold>
    is also made from generic components, which allows users to
    customize data generation and tests. <bold>Expose</bold> is provided
    by the <monospace>text_sensitivity</monospace> subpackage
    (<xref alt="Robeer, 2021b" rid="ref-text_sensitivity" ref-type="bibr">Robeer,
    2021b</xref>), which also doubles as a standalone tool.</p>
  </sec>
  <sec id="digestibles">
    <title>Digestibles</title>
    <p>Digestibles serve stakeholders—such as creators, auditors,
    applicants, end-users, or clients
    (<xref alt="Tomsett et al., 2018" rid="ref-Tomsett2018" ref-type="bibr">Tomsett
    et al., 2018</xref>)—via a Jupyter Notebook or Web UI
    (<xref alt="[fig:ui]" rid="figU003Aui">[fig:ui]</xref>) (using
    <monospace>plotly</monospace>
    (<xref alt="Plotly Technologies Inc., 2015" rid="ref-plotly" ref-type="bibr">Plotly
    Technologies Inc., 2015</xref>) visuals), integrated API, and static
    reporting.</p>
    <fig>
      <caption><p>UI elements from the Jupyter Notebook interface,
      designed to present audit results to diverse
      stakeholders.<styled-content id="figU003Aui"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figure2.png" />
    </fig>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Development was supported by the Netherlands National Police. The
  authors thank contributors within the Police for development, testing,
  and usage, and participants from ICT.OPEN 2022, the UU NPAI, and UMC
  Utrecht demos for their valuable feedback.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Ribeiro2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ribeiro</surname><given-names>Marco Tulio</given-names></name>
        <name><surname>Singh</surname><given-names>Sameer</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
      </person-group>
      <article-title>Anchors: High-Precision Model-Agnostic Explanations</article-title>
      <source>AAAI conference on artificial intelligence, proceedings</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1609/aaai.v32i1.11491</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Ribeiro2016a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ribeiro</surname><given-names>Marco Tulio</given-names></name>
        <name><surname>Singh</surname><given-names>Sameer</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
      </person-group>
      <article-title>“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</article-title>
      <source>22nd ACM SIGKDD international conference on knowledge discovery in data mining (KDD’16), proceedings</source>
      <year iso-8601-date="2016">2016</year>
      <isbn>9781450321389</isbn>
      <pub-id pub-id-type="doi">10.18653/v1/n16-3020</pub-id>
      <fpage>1135</fpage>
      <lpage>1144</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Lundberg2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lundberg</surname><given-names>Scott M.</given-names></name>
        <name><surname>Lee</surname><given-names>Su-In</given-names></name>
      </person-group>
      <article-title>A Unified Approach to Interpreting Model Predictions</article-title>
      <source>Advances in neural information processing systems 30 (NIPS 2017)</source>
      <year iso-8601-date="2017">2017</year>
      <fpage>4765</fpage>
      <lpage>4774</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Ribeiro2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ribeiro</surname><given-names>Marco Tulio</given-names></name>
        <name><surname>Wu</surname><given-names>Tongshuang</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
        <name><surname>Singh</surname><given-names>Sameer</given-names></name>
      </person-group>
      <article-title>Beyond Accuracy: Behavioral Testing of NLP models with CheckList</article-title>
      <source>Association for computational linguistics (ACL)</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.18653/v1/2020.acl-main.442</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Waa2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Waa</surname><given-names>Jasper van der</given-names></name>
        <name><surname>Robeer</surname><given-names>Marcel</given-names></name>
        <name><surname>Diggelen</surname><given-names>Jurriaan van</given-names></name>
        <name><surname>Neerincx</surname><given-names>Mark</given-names></name>
        <name><surname>Brinkhuis</surname><given-names>Matthieu</given-names></name>
      </person-group>
      <article-title>Contrastive Explanations with Local Foil Trees</article-title>
      <source>2018 workshop on human interpretability in machine learning (WHI 2018)</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-Guidotti2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Guidotti</surname><given-names>Riccardo</given-names></name>
        <name><surname>Monreale</surname><given-names>Anna</given-names></name>
        <name><surname>Ruggieri</surname><given-names>Salvatore</given-names></name>
        <name><surname>Pedreschi</surname><given-names>Dino</given-names></name>
        <name><surname>Turini</surname><given-names>Franco</given-names></name>
        <name><surname>Giannotti</surname><given-names>Fosca</given-names></name>
      </person-group>
      <article-title>Local Rule-Based Explanations of Black Box Decision Systems</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1805.10820</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kim2016">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kim</surname><given-names>Been</given-names></name>
        <name><surname>Khanna</surname><given-names>Rajiv</given-names></name>
        <name><surname>Koyejo</surname><given-names>Oluwasanmi</given-names></name>
      </person-group>
      <article-title>Examples are not Enough, Learn to Criticize! Criticism for Interpretability</article-title>
      <source>29th conference on neural information processing systems (NIPS 2016)</source>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-Mehrabi2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mehrabi</surname><given-names>Ninareh</given-names></name>
        <name><surname>Morstatter</surname><given-names>Fred</given-names></name>
        <name><surname>Saxena</surname><given-names>Nripsuta</given-names></name>
        <name><surname>Lerman</surname><given-names>Kristina</given-names></name>
        <name><surname>Galstyan</surname><given-names>Aram</given-names></name>
      </person-group>
      <article-title>A Survey on Bias and Fairness in Machine Learning</article-title>
      <source>ACM Computing Surveys (CSUR)</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>54</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1145/3457607</pub-id>
      <fpage>1</fpage>
      <lpage>35</lpage>
    </element-citation>
  </ref>
  <ref id="ref-instancelib">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bron</surname><given-names>Michiel P.</given-names></name>
      </person-group>
      <article-title>Python Package instancelib</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023-09">2023</year><month>09</month>
      <uri>https://doi.org/10.5281/zenodo.8308017</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.8308017</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-text_explainability">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Robeer</surname><given-names>Marcel</given-names></name>
      </person-group>
      <article-title>Python package text_explainability</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://doi.org/10.5281/zenodo.14192126</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.14192126</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-text_sensitivity">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Robeer</surname><given-names>Marcel</given-names></name>
      </person-group>
      <article-title>Python package text_sensitivity</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://doi.org/10.5281/zenodo.14192940</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.14192940</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Tomsett2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tomsett</surname><given-names>Richard</given-names></name>
        <name><surname>Braines</surname><given-names>Dave</given-names></name>
        <name><surname>Harborne</surname><given-names>Dan</given-names></name>
        <name><surname>Preece</surname><given-names>Alun</given-names></name>
        <name><surname>Chakraborty</surname><given-names>Supriyo</given-names></name>
      </person-group>
      <article-title>Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems</article-title>
      <source>2018 ICML workshop on human interpretability in machine learning (WHI 2018)</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-Agarwal2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Agarwal</surname><given-names>Alekh</given-names></name>
        <name><surname>Dudik</surname><given-names>Miroslav</given-names></name>
        <name><surname>Wu</surname><given-names>Zhiwei Steven</given-names></name>
      </person-group>
      <article-title>Fair Regression: Quantitative Definitions and Reduction-Based Algorithms</article-title>
      <source>Proceedings of the 36th international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>Chaudhuri</surname><given-names>Kamalika</given-names></name>
        <name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>97</volume>
      <fpage>120</fpage>
      <lpage>129</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Ribeiro2016b">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ribeiro</surname><given-names>Marco Tulio</given-names></name>
        <name><surname>Singh</surname><given-names>Sameer</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
      </person-group>
      <article-title>Model-Agnostic Interpretability of Machine Learning</article-title>
      <source>2016 ICML workshop on human interpretability in machine learning (WHI 2016)</source>
      <year iso-8601-date="2016">2016</year>
      <isbn>9781450336642</isbn>
      <pub-id pub-id-type="doi">10.1145/2858036.2858529</pub-id>
      <fpage>91</fpage>
      <lpage>95</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Singh2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Singh</surname><given-names>Chandan</given-names></name>
        <name><surname>Nasseri</surname><given-names>Keyan</given-names></name>
        <name><surname>Tan</surname><given-names>Yan Shuo</given-names></name>
        <name><surname>Tang</surname><given-names>Tiffany</given-names></name>
        <name><surname>Yu</surname><given-names>Bin</given-names></name>
      </person-group>
      <article-title>imodels: A python package for fitting interpretable models</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>61</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03192</pub-id>
      <fpage>3192</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Faker">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Faraglia</surname><given-names>Daniele</given-names></name>
        <string-name>Other Contributors</string-name>
      </person-group>
      <article-title>Python package Faker</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/joke2k/faker</uri>
    </element-citation>
  </ref>
  <ref id="ref-Baniecki2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baniecki</surname><given-names>Hubert</given-names></name>
        <name><surname>Kretowicz</surname><given-names>Wojciech</given-names></name>
        <name><surname>Piatyszek</surname><given-names>Piotr</given-names></name>
        <name><surname>Wisniewski</surname><given-names>Jakub</given-names></name>
        <name><surname>Biecek</surname><given-names>Przemyslaw</given-names></name>
      </person-group>
      <article-title>dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>214</issue>
      <uri>http://jmlr.org/papers/v22/20-1473.html</uri>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Arya2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arya</surname><given-names>Vijay</given-names></name>
        <name><surname>Bellamy</surname><given-names>Rachel K. E.</given-names></name>
        <name><surname>Chen</surname><given-names>Pin-Yu</given-names></name>
        <name><surname>Dhurandhar</surname><given-names>Amit</given-names></name>
        <name><surname>Hind</surname><given-names>Michael</given-names></name>
        <name><surname>Hoffman</surname><given-names>Samuel C.</given-names></name>
        <name><surname>Houde</surname><given-names>Stephanie</given-names></name>
        <name><surname>Liao</surname><given-names>Q. Vera</given-names></name>
        <name><surname>Luss</surname><given-names>Ronny</given-names></name>
        <name><surname>Mojsilović</surname><given-names>Aleksandra</given-names></name>
        <name><surname>Mourad</surname><given-names>Sami</given-names></name>
        <name><surname>Pedemonte</surname><given-names>Pablo</given-names></name>
        <name><surname>Raghavendra</surname><given-names>Ramya</given-names></name>
        <name><surname>Richards</surname><given-names>John</given-names></name>
        <name><surname>Sattigeri</surname><given-names>Prasanna</given-names></name>
        <name><surname>Shanmugam</surname><given-names>Karthikeyan</given-names></name>
        <name><surname>Singh</surname><given-names>Moninder</given-names></name>
        <name><surname>Varshney</surname><given-names>Kush R.</given-names></name>
        <name><surname>Wei</surname><given-names>Dennis</given-names></name>
        <name><surname>Zhang</surname><given-names>Yunfeng</given-names></name>
      </person-group>
      <article-title>One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2019-09">2019</year><month>09</month>
      <pub-id pub-id-type="doi">10.48550/arXiv.1909.03012</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Bellamy2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bellamy</surname><given-names>Rachel K. E.</given-names></name>
        <name><surname>Dey</surname><given-names>Kuntal</given-names></name>
        <name><surname>Hind</surname><given-names>Michael</given-names></name>
        <name><surname>Hoffman</surname><given-names>Samuel C.</given-names></name>
        <name><surname>Houde</surname><given-names>Stephanie</given-names></name>
        <name><surname>Kannan</surname><given-names>Kalapriya</given-names></name>
        <name><surname>Lohia</surname><given-names>Pranay</given-names></name>
        <name><surname>Martino</surname><given-names>Jacquelyn</given-names></name>
        <name><surname>Mehta</surname><given-names>Sameep</given-names></name>
        <name><surname>Mojsilovic</surname><given-names>Aleksandra</given-names></name>
        <name><surname>Nagar</surname><given-names>Seema</given-names></name>
        <name><surname>Ramamurthy</surname><given-names>Karthikeyan Natesan</given-names></name>
        <name><surname>Richards</surname><given-names>John</given-names></name>
        <name><surname>Saha</surname><given-names>Diptikalyan</given-names></name>
        <name><surname>Sattigeri</surname><given-names>Prasanna</given-names></name>
        <name><surname>Singh</surname><given-names>Moninder</given-names></name>
        <name><surname>Varshney</surname><given-names>Kush R.</given-names></name>
        <name><surname>Zhang</surname><given-names>Yunfeng</given-names></name>
      </person-group>
      <article-title>AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2018-10">2018</year><month>10</month>
      <pub-id pub-id-type="doi">10.48550/arXiv.1810.01943</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Klaise2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Klaise</surname><given-names>Janis</given-names></name>
        <name><surname>Looveren</surname><given-names>Arnaud Van</given-names></name>
        <name><surname>Vacanti</surname><given-names>Giovanni</given-names></name>
        <name><surname>Coca</surname><given-names>Alexandru</given-names></name>
      </person-group>
      <article-title>Alibi Explain: Algorithms for Explaining Machine Learning Models</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>181</issue>
      <uri>http://jmlr.org/papers/v22/21-0017.html</uri>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-plotly">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Plotly Technologies Inc.</string-name>
      </person-group>
      <article-title>Collaborative data science</article-title>
      <publisher-name>Plotly Technologies Inc.</publisher-name>
      <publisher-loc>Montreal, QC</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <uri>https://plot.ly</uri>
    </element-citation>
  </ref>
  <ref id="ref-Edwards2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Edwards</surname><given-names>Lilian</given-names></name>
      </person-group>
      <article-title>The EU AI Act: A summary of its significance and scope</article-title>
      <source>Ada Lovelace Institute, Expert Explainer Report</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://www.adalovelaceinstitute.org/wp-content/uploads/2022/04/Expert-explainer-The-EU-AI-Act-11-April-2022.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-Biecek2021">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Biecek</surname><given-names>Przemyslaw</given-names></name>
        <name><surname>Burzykowski</surname><given-names>Tomasz</given-names></name>
      </person-group>
      <source>Explanatory Model Analysis: Explore, Explain and Examine Predictive Models</source>
      <publisher-name>Chapman; Hall/CRC</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>http://dx.doi.org/10.1201/9780429027192</uri>
      <pub-id pub-id-type="doi">10.1201/9780429027192</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Yang2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Wenzhuo</given-names></name>
        <name><surname>Le</surname><given-names>Hung</given-names></name>
        <name><surname>Savarese</surname><given-names>Silvio</given-names></name>
        <name><surname>Hoi</surname><given-names>Steven</given-names></name>
      </person-group>
      <article-title>OmniXAI: A Library for Explainable AI</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2206.01612</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
