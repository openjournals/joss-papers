<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20251007143050-fe6b76654071678afad5c10893c9d3cdd11f2b53</doi_batch_id>
    <timestamp>20251007143050</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>10</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>114</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Explabox: A Python Toolkit for Standardized Auditing and Explanation of Text Models</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Marcel</given_name>
            <surname>Robeer</surname>
            <affiliations>
              <institution><institution_name>National Police Lab AI, Utrecht University, The Netherlands</institution_name><institution_id type="ror">https://ror.org/04pp8hn57</institution_id></institution>
              <institution><institution_name>Netherlands National Police, The Netherlands</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-6430-9774</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Michiel</given_name>
            <surname>Bron</surname>
            <affiliations>
              <institution><institution_name>National Police Lab AI, Utrecht University, The Netherlands</institution_name><institution_id type="ror">https://ror.org/04pp8hn57</institution_id></institution>
              <institution><institution_name>Netherlands National Police, The Netherlands</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-4823-6085</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Elize</given_name>
            <surname>Herrewijnen</surname>
            <affiliations>
              <institution><institution_name>National Police Lab AI, Utrecht University, The Netherlands</institution_name><institution_id type="ror">https://ror.org/04pp8hn57</institution_id></institution>
              <institution><institution_name>Netherlands National Police, The Netherlands</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-2729-6599</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Riwish</given_name>
            <surname>Hoeseni</surname>
            <affiliations>
              <institution><institution_name>Netherlands National Police, The Netherlands</institution_name></institution>
            </affiliations>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Floris</given_name>
            <surname>Bex</surname>
            <affiliations>
              <institution><institution_name>National Police Lab AI, Utrecht University, The Netherlands</institution_name><institution_id type="ror">https://ror.org/04pp8hn57</institution_id></institution>
              <institution><institution_name>School of Law, Utrecht University, The Netherlands</institution_name><institution_id type="ror">https://ror.org/04pp8hn57</institution_id></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-5699-9656</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>10</month>
          <day>07</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8253</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08253</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.17256331</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8253</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08253</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08253</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08253.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="Ribeiro2018">
            <article_title>Anchors: High-Precision Model-Agnostic Explanations</article_title>
            <author>Ribeiro</author>
            <journal_title>AAAI conference on artificial intelligence, proceedings</journal_title>
            <doi>10.1609/aaai.v32i1.11491</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2018). Anchors: High-Precision Model-Agnostic Explanations. AAAI Conference on Artificial Intelligence, Proceedings. https://doi.org/10.1609/aaai.v32i1.11491</unstructured_citation>
          </citation>
          <citation key="Ribeiro2016a">
            <article_title>“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</article_title>
            <author>Ribeiro</author>
            <journal_title>22nd ACM SIGKDD international conference on knowledge discovery in data mining (KDD’16), proceedings</journal_title>
            <doi>10.18653/v1/n16-3020</doi>
            <isbn>9781450321389</isbn>
            <cYear>2016</cYear>
            <unstructured_citation>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. 22nd ACM SIGKDD International Conference on Knowledge Discovery in Data Mining (KDD’16), Proceedings, 1135–1144. https://doi.org/10.18653/v1/n16-3020</unstructured_citation>
          </citation>
          <citation key="Lundberg2017">
            <article_title>A Unified Approach to Interpreting Model Predictions</article_title>
            <author>Lundberg</author>
            <journal_title>Advances in neural information processing systems 30 (NIPS 2017)</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Lundberg, S. M., &amp; Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems 30 (NIPS 2017), 4765–4774.</unstructured_citation>
          </citation>
          <citation key="Ribeiro2020">
            <article_title>Beyond Accuracy: Behavioral Testing of NLP models with CheckList</article_title>
            <author>Ribeiro</author>
            <journal_title>Association for computational linguistics (ACL)</journal_title>
            <doi>10.18653/v1/2020.acl-main.442</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Ribeiro, M. T., Wu, T., Guestrin, C., &amp; Singh, S. (2020). Beyond Accuracy: Behavioral Testing of NLP models with CheckList. Association for Computational Linguistics (ACL). https://doi.org/10.18653/v1/2020.acl-main.442</unstructured_citation>
          </citation>
          <citation key="Waa2018">
            <article_title>Contrastive Explanations with Local Foil Trees</article_title>
            <author>Waa</author>
            <journal_title>2018 workshop on human interpretability in machine learning (WHI 2018)</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Waa, J. van der, Robeer, M., Diggelen, J. van, Neerincx, M., &amp; Brinkhuis, M. (2018). Contrastive Explanations with Local Foil Trees. 2018 Workshop on Human Interpretability in Machine Learning (WHI 2018).</unstructured_citation>
          </citation>
          <citation key="Guidotti2018">
            <article_title>Local Rule-Based Explanations of Black Box Decision Systems</article_title>
            <author>Guidotti</author>
            <journal_title>arXiv preprint</journal_title>
            <doi>arXiv.1805.10820</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Guidotti, R., Monreale, A., Ruggieri, S., Pedreschi, D., Turini, F., &amp; Giannotti, F. (2018). Local Rule-Based Explanations of Black Box Decision Systems. arXiv Preprint. https://doi.org/arXiv.1805.10820</unstructured_citation>
          </citation>
          <citation key="Kim2016">
            <article_title>Examples are not Enough, Learn to Criticize! Criticism for Interpretability</article_title>
            <author>Kim</author>
            <journal_title>29th conference on neural information processing systems (NIPS 2016)</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Kim, B., Khanna, R., &amp; Koyejo, O. (2016). Examples are not Enough, Learn to Criticize! Criticism for Interpretability. 29th Conference on Neural Information Processing Systems (NIPS 2016).</unstructured_citation>
          </citation>
          <citation key="Mehrabi2021">
            <article_title>A Survey on Bias and Fairness in Machine Learning</article_title>
            <author>Mehrabi</author>
            <journal_title>ACM Computing Surveys (CSUR)</journal_title>
            <issue>6</issue>
            <volume>54</volume>
            <doi>10.1145/3457607</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2021). A Survey on Bias and Fairness in Machine Learning. ACM Computing Surveys (CSUR), 54(6), 1–35. https://doi.org/10.1145/3457607</unstructured_citation>
          </citation>
          <citation key="instancelib">
            <article_title>Python Package instancelib</article_title>
            <author>Bron</author>
            <doi>10.5281/zenodo.8308017</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Bron, M. P. (2023). Python Package instancelib (Version 0.5.0). Zenodo. https://doi.org/10.5281/zenodo.8308017</unstructured_citation>
          </citation>
          <citation key="text_explainability">
            <article_title>Python package text_explainability</article_title>
            <author>Robeer</author>
            <doi>10.5281/zenodo.14192126</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Robeer, M. (2021). Python package text_explainability. https://doi.org/10.5281/zenodo.14192126</unstructured_citation>
          </citation>
          <citation key="text_sensitivity">
            <article_title>Python package text_sensitivity</article_title>
            <author>Robeer</author>
            <doi>10.5281/zenodo.14192940</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Robeer, M. (2021). Python package text_sensitivity. https://doi.org/10.5281/zenodo.14192940</unstructured_citation>
          </citation>
          <citation key="Tomsett2018">
            <article_title>Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems</article_title>
            <author>Tomsett</author>
            <journal_title>2018 ICML workshop on human interpretability in machine learning (WHI 2018)</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Tomsett, R., Braines, D., Harborne, D., Preece, A., &amp; Chakraborty, S. (2018). Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems. 2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018).</unstructured_citation>
          </citation>
          <citation key="Agarwal2019">
            <article_title>Fair Regression: Quantitative Definitions and Reduction-Based Algorithms</article_title>
            <author>Agarwal</author>
            <journal_title>Proceedings of the 36th international conference on machine learning</journal_title>
            <volume>97</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Agarwal, A., Dudik, M., &amp; Wu, Z. S. (2019). Fair Regression: Quantitative Definitions and Reduction-Based Algorithms. In K. Chaudhuri &amp; R. Salakhutdinov (Eds.), Proceedings of the 36th international conference on machine learning (Vol. 97, pp. 120–129). PMLR.</unstructured_citation>
          </citation>
          <citation key="Ribeiro2016b">
            <article_title>Model-Agnostic Interpretability of Machine Learning</article_title>
            <author>Ribeiro</author>
            <journal_title>2016 ICML workshop on human interpretability in machine learning (WHI 2016)</journal_title>
            <doi>10.1145/2858036.2858529</doi>
            <isbn>9781450336642</isbn>
            <cYear>2016</cYear>
            <unstructured_citation>Ribeiro, M. T., Singh, S., &amp; Guestrin, C. (2016). Model-Agnostic Interpretability of Machine Learning. 2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), 91–95. https://doi.org/10.1145/2858036.2858529</unstructured_citation>
          </citation>
          <citation key="Singh2021">
            <article_title>imodels: A python package for fitting interpretable models</article_title>
            <author>Singh</author>
            <journal_title>Journal of Open Source Software</journal_title>
            <issue>61</issue>
            <volume>6</volume>
            <doi>10.21105/joss.03192</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Singh, C., Nasseri, K., Tan, Y. S., Tang, T., &amp; Yu, B. (2021). imodels: A python package for fitting interpretable models. Journal of Open Source Software, 6(61), 3192. https://doi.org/10.21105/joss.03192</unstructured_citation>
          </citation>
          <citation key="Faker">
            <article_title>Python package Faker</article_title>
            <author>Faraglia</author>
            <cYear>2021</cYear>
            <unstructured_citation>Faraglia, D., &amp; Other Contributors. (2021). Python package Faker. https://github.com/joke2k/faker</unstructured_citation>
          </citation>
          <citation key="Baniecki2021">
            <article_title>dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python</article_title>
            <author>Baniecki</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <issue>214</issue>
            <volume>22</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Baniecki, H., Kretowicz, W., Piatyszek, P., Wisniewski, J., &amp; Biecek, P. (2021). dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python. Journal of Machine Learning Research, 22(214), 1–7. http://jmlr.org/papers/v22/20-1473.html</unstructured_citation>
          </citation>
          <citation key="Arya2019">
            <article_title>One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques</article_title>
            <author>Arya</author>
            <journal_title>arXiv preprint</journal_title>
            <doi>arXiv.1909.03012</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Arya, V., Bellamy, R. K. E., Chen, P.-Y., Dhurandhar, A., Hind, M., Hoffman, S. C., Houde, S., Liao, Q. V., Luss, R., Mojsilović, A., Mourad, S., Pedemonte, P., Raghavendra, R., Richards, J., Sattigeri, P., Shanmugam, K., Singh, M., Varshney, K. R., Wei, D., &amp; Zhang, Y. (2019). One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques. arXiv Preprint. https://doi.org/arXiv.1909.03012</unstructured_citation>
          </citation>
          <citation key="Bellamy2018">
            <article_title>AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias</article_title>
            <author>Bellamy</author>
            <journal_title>arXiv preprint</journal_title>
            <doi>arXiv.1810.01943</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Bellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., &amp; Zhang, Y. (2018). AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv Preprint. https://doi.org/arXiv.1810.01943</unstructured_citation>
          </citation>
          <citation key="Klaise2021">
            <article_title>Alibi Explain: Algorithms for Explaining Machine Learning Models</article_title>
            <author>Klaise</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <issue>181</issue>
            <volume>22</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Klaise, J., Looveren, A. V., Vacanti, G., &amp; Coca, A. (2021). Alibi Explain: Algorithms for Explaining Machine Learning Models. Journal of Machine Learning Research, 22(181), 1–7. http://jmlr.org/papers/v22/21-0017.html</unstructured_citation>
          </citation>
          <citation key="plotly">
            <article_title>Collaborative data science</article_title>
            <author>Plotly Technologies Inc.</author>
            <cYear>2015</cYear>
            <unstructured_citation>Plotly Technologies Inc. (2015). Collaborative data science. Plotly Technologies Inc. https://plot.ly</unstructured_citation>
          </citation>
          <citation key="Edwards2022">
            <article_title>The EU AI Act: A summary of its significance and scope</article_title>
            <author>Edwards</author>
            <journal_title>Ada Lovelace Institute, Expert Explainer Report</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Edwards, L. (2022). The EU AI Act: A summary of its significance and scope. Ada Lovelace Institute, Expert Explainer Report. https://www.adalovelaceinstitute.org/wp-content/uploads/2022/04/Expert-explainer-The-EU-AI-Act-11-April-2022.pdf</unstructured_citation>
          </citation>
          <citation key="Biecek2021">
            <volume_title>Explanatory Model Analysis: Explore, Explain and Examine Predictive Models</volume_title>
            <author>Biecek</author>
            <doi>10.1201/9780429027192</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Biecek, P., &amp; Burzykowski, T. (2021). Explanatory Model Analysis: Explore, Explain and Examine Predictive Models. Chapman; Hall/CRC. https://doi.org/10.1201/9780429027192</unstructured_citation>
          </citation>
          <citation key="Yang2022">
            <article_title>OmniXAI: A Library for Explainable AI</article_title>
            <author>Yang</author>
            <cYear>2022</cYear>
            <unstructured_citation>Yang, W., Le, H., Savarese, S., &amp; Hoi, S. (2022). OmniXAI: A Library for Explainable AI. https://arxiv.org/abs/2206.01612</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
