<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5361</article-id>
<article-id pub-id-type="doi">10.21105/joss.05361</article-id>
<title-group>
<article-title>normflows: A PyTorch Package for Normalizing
Flows</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4965-4297</contrib-id>
<name>
<surname>Stimper</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liu</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Campbell</surname>
<given-names>Andrew</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berenz</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ryll</surname>
<given-names>Lukas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8177-0925</contrib-id>
<name>
<surname>Schölkopf</surname>
<given-names>Bernhard</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hernández-Lobato</surname>
<given-names>José Miguel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Cambridge, Cambridge, United
Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Max Planck Institute for Intelligent Systems, Tübingen,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-02-17">
<day>17</day>
<month>2</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>5361</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PyTorch</kwd>
<kwd>Machine Learning</kwd>
<kwd>Normalizing Flows</kwd>
<kwd>Density Estimation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Normalizing flows model probability distributions through an
  expressive tractable density
  (<xref alt="D. Rezende &amp; Mohamed, 2015" rid="ref-rezende2015variational" ref-type="bibr">D.
  Rezende &amp; Mohamed, 2015</xref>;
  <xref alt="Esteban G. Tabak &amp; Turner, 2013" rid="ref-tabak2013family" ref-type="bibr">Esteban
  G. Tabak &amp; Turner, 2013</xref>;
  <xref alt="Esteban G. Tabak &amp; Vanden-Eijnden, 2010" rid="ref-Tabak2010" ref-type="bibr">Esteban
  G. Tabak &amp; Vanden-Eijnden, 2010</xref>). They transform a simple
  base distribution, such as a Gaussian, through a sequence of
  invertible functions, which are referred to as layers. These layers
  typically use neural networks to become very expressive. Flows are
  ubiquitous in machine learning and have been applied to image
  generation
  (<xref alt="Grcić et al., 2021" rid="ref-grcic2021" ref-type="bibr">Grcić
  et al., 2021</xref>;
  <xref alt="Kingma &amp; Dhariwal, 2018" rid="ref-kingma2018glow" ref-type="bibr">Kingma
  &amp; Dhariwal, 2018</xref>), text modeling
  (<xref alt="Wang &amp; Wang, 2019" rid="ref-wang2019text" ref-type="bibr">Wang
  &amp; Wang, 2019</xref>), variational inference
  (<xref alt="D. Rezende &amp; Mohamed, 2015" rid="ref-rezende2015variational" ref-type="bibr">D.
  Rezende &amp; Mohamed, 2015</xref>), approximating Boltzmann
  distributions
  (<xref alt="Noé et al., 2019" rid="ref-noe2019boltzmann" ref-type="bibr">Noé
  et al., 2019</xref>), and many other problems
  (<xref alt="Kobyzev et al., 2021" rid="ref-kobyzev2021" ref-type="bibr">Kobyzev
  et al., 2021</xref>;
  <xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>).</p>
  <p>Here, we present <monospace>normflows</monospace>, a Python package
  for normalizing flows. It allows to build normalizing flow models from
  a suite of base distributions, flow layers, and neural networks. The
  package is implemented in the popular deep learning framework PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>), which simplifies the integration of flows in
  larger machine learning models or pipelines. It supports most of the
  common normalizing flow architectures, such as Real NVP
  (<xref alt="Dinh et al., 2017" rid="ref-dinh2017RealNVP" ref-type="bibr">Dinh
  et al., 2017</xref>), Glow
  (<xref alt="Kingma &amp; Dhariwal, 2018" rid="ref-kingma2018glow" ref-type="bibr">Kingma
  &amp; Dhariwal, 2018</xref>), Masked Autoregressive Flows
  (<xref alt="Papamakarios et al., 2017" rid="ref-papamakarios2017" ref-type="bibr">Papamakarios
  et al., 2017</xref>), Neural Spline Flows
  (<xref alt="Durkan et al., 2019" rid="ref-durkan2019neuralspline" ref-type="bibr">Durkan
  et al., 2019</xref>;
  <xref alt="Müller et al., 2019" rid="ref-muller2019neural" ref-type="bibr">Müller
  et al., 2019</xref>), Residual Flows
  (<xref alt="Chen et al., 2019" rid="ref-chen2019residual" ref-type="bibr">Chen
  et al., 2019</xref>), and many more. The package can be easily
  installed via <monospace>pip</monospace> and the code is publicly
  available on
  <ext-link ext-link-type="uri" xlink:href="https://github.com/VincentStimper/normalizing-flows">GitHub</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>normflows</monospace> focuses on flows that are composed
  of discrete transformations, as opposed to continuous normalizing
  flows
  (<xref alt="Chen et al., 2018" rid="ref-Chen2018a" ref-type="bibr">Chen
  et al., 2018</xref>;
  <xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>). There are several other packages implementing
  discrete normalizing flows, such as TensorFlow Probability
  (<xref alt="Dillon et al., 2017" rid="ref-dillon2017" ref-type="bibr">Dillon
  et al., 2017</xref>) for TensorFlow, <monospace>distrax</monospace>
  (<xref alt="Babuschkin et al., 2020" rid="ref-deepmind2020jax" ref-type="bibr">Babuschkin
  et al., 2020</xref>) for JAX, and <monospace>nflows</monospace>
  (<xref alt="Durkan et al., 2020" rid="ref-nflows" ref-type="bibr">Durkan
  et al., 2020</xref>) and <monospace>FrEIA</monospace>
  (<xref alt="Ardizzone et al., 2018-2022" rid="ref-freia" ref-type="bibr">Ardizzone
  et al., 2018-2022</xref>) for PyTorch. However, none of them support
  the two popular flow architectures, residual and autoregressive flows,
  within a single package, while we do so.</p>
  <p>Moreover, <monospace>normflows</monospace> stands out by providing
  tools that are often used when approximating Boltzmann distributions.
  First, sampling layers needed for Stochastic Normalizing Flows
  (<xref alt="Nielsen et al., 2020" rid="ref-nielsen2020" ref-type="bibr">Nielsen
  et al., 2020</xref>;
  <xref alt="Wu et al., 2020" rid="ref-wu2020stochasticNF" ref-type="bibr">Wu
  et al., 2020</xref>) are included. Second, Neural Spline Flows on
  circular coordinates are supported
  (<xref alt="D. J. Rezende et al., 2020" rid="ref-rezende2020" ref-type="bibr">D.
  J. Rezende et al., 2020</xref>), which can be combined with standard
  coordinates on bounded or unbounded intervals. They are needed when
  modeling the internal coordinates of molecules consisting of angles
  and lengths
  (<xref alt="Midgley et al., 2023" rid="ref-Midgley2023" ref-type="bibr">Midgley
  et al., 2023</xref>). Furthermore, there is an extension for
  <monospace>normflows</monospace> that adds Boltzmann distributions as
  targets as well as flow layers converting between Cartesian and
  internal coordinates
  (<xref alt="Stimper et al., 2023" rid="ref-boltzgen" ref-type="bibr">Stimper
  et al., 2023</xref>).</p>
  <p>Our package has already been used in several scientific projects
  and publications
  (<xref alt="Campbell et al., 2021" rid="ref-campbell2021gradient" ref-type="bibr">Campbell
  et al., 2021</xref>;
  <xref alt="Midgley et al., 2023" rid="ref-Midgley2023" ref-type="bibr">Midgley
  et al., 2023</xref>;
  <xref alt="Stimper et al., 2022" rid="ref-stimper2021" ref-type="bibr">Stimper
  et al., 2022</xref>). Due to its modular nature,
  <monospace>normflows</monospace> can be easily extended to house new
  flow layers, base distributions, or other tools. For instance,
  (<xref alt="Stimper et al., 2022" rid="ref-stimper2021" ref-type="bibr">Stimper
  et al., 2022</xref>) extends the package by adding resampled base
  distributions, which overcome an architectural weakness of normalizing
  flows and make them more expressive.</p>
</sec>
<sec id="examples">
  <title>Examples</title>
  <fig>
    <caption><p>Target density defined on a cylinder surface, having an
    unbounded coordinate <inline-formula><alternatives>
    <tex-math><![CDATA[x]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
    and a circular coordinate <inline-formula><alternatives>
    <tex-math><![CDATA[\phi]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϕ</mml:mi></mml:math></alternatives></inline-formula>.
    A Neural Spline Flow is fit to it, being almost indistinguishable
    from the target. (a) shows the densities in 2D and (b) is a
    visualization on the cylinder surface.
    <styled-content id="figU003Aexample"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/nsf_cylinder_2d_3d.png" />
  </fig>
  <p>In the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/VincentStimper/normalizing-flows">GitHub
  repository of our package</ext-link>, we provide various examples
  illustrating how to use it. We show how to build a flow model from a
  base distribution, a list of flow layers, and, optionally, a target
  distribution. They can be trained by computing a loss through the
  respective methods provided and minimizing it with the standard
  PyTorch optimizers. We show how to approximate simple 2D
  distributions, but, moreover, apply flows to images through the
  multiscale architecture
  (<xref alt="Dinh et al., 2017" rid="ref-dinh2017RealNVP" ref-type="bibr">Dinh
  et al., 2017</xref>), which <monospace>normflows</monospace> provides
  as well. Furthermore, there is an example of how to build a
  variational autoencoder with normalizing flows as well.</p>
  <p>Here, we want to illustrate a strength of
  <monospace>normflows</monospace>, i.e. that it can deal with
  combinations of standard and circular coordinates. Therefore, we
  consider a distribution of two random variables,
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\phi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϕ</mml:mi></mml:math></alternatives></inline-formula>.
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  follows a Gaussian distribution with density
  <inline-formula><alternatives>
  <tex-math><![CDATA[p(x) = \mathcal{N}(x|0, 1)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>𝒩</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\phi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ϕ</mml:mi></mml:math></alternatives></inline-formula>
  has a circular von Mises distribution such that
  <inline-formula><alternatives>
  <tex-math><![CDATA[p(\phi|x) = \mathcal{M}(\phi|\mu(x), 1)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ℳ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  with <inline-formula><alternatives>
  <tex-math><![CDATA[\mu(x) = 3x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>3</mml:mn><mml:mi>x</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  We train a Neural Spline Flow with an unbound and a circular
  coordinate to approximate the target distribution
  <inline-formula><alternatives>
  <tex-math><![CDATA[p(x, \phi) = p(x) p(\phi|x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  by minimizing the reverse Kullback-Leibler divergence. As shown in
  <xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>,
  the density of the flow is almost indistinguishable from the
  target.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Laurence Midgley and Timothy Gebhard for their valuable
  contributions to the package. Moreover, we thank everyone who
  contacted us via mail or on GitHub for the valuable feedback and
  spotting bugs.</p>
  <p>José Miguel Hernández-Lobato acknowledges support from a Turing AI
  Fellowship under grant EP/V023756/1. This work was supported by the
  German Federal Ministry of Education and Research (BMBF): Tübingen AI
  Center, FKZ: 01IS18039B; and by the Machine Learning Cluster of
  Excellence, EXC number 2064/1 - Project number 390727645.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Tabak2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tabak</surname><given-names>Esteban G.</given-names></name>
        <name><surname>Vanden-Eijnden</surname><given-names>Eric</given-names></name>
      </person-group>
      <article-title>Density estimation by dual ascent of the log-likelihood</article-title>
      <source>Communications in Mathematical Sciences</source>
      <year iso-8601-date="2010">2010</year>
      <volume>8</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.4310/CMS.2010.v8.n1.a11</pub-id>
      <fpage>217</fpage>
      <lpage>233</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tabak2013family">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tabak</surname><given-names>Esteban G</given-names></name>
        <name><surname>Turner</surname><given-names>Cristina V</given-names></name>
      </person-group>
      <article-title>A family of nonparametric density estimation algorithms</article-title>
      <source>Communications on Pure and Applied Mathematics</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>66</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1002/cpa.21423</pub-id>
      <fpage>145</fpage>
      <lpage>164</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rezende2015variational">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rezende</surname><given-names>Danilo</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
      </person-group>
      <article-title>Variational inference with normalizing flows</article-title>
      <source>Proceedings of the 32nd international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <fpage>1530</fpage>
      <lpage>1538</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kingma2018glow">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
      </person-group>
      <article-title>Glow: Generative flow with invertible 1x1 convolutions</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
    </element-citation>
  </ref>
  <ref id="ref-grcic2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Grcić</surname><given-names>Matej</given-names></name>
        <name><surname>Grubišić</surname><given-names>Ivan</given-names></name>
        <name><surname>Šegvić</surname><given-names>Siniša</given-names></name>
      </person-group>
      <article-title>Densely connected normalizing flows</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
    </element-citation>
  </ref>
  <ref id="ref-wang2019text">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Prince Zizhuang</given-names></name>
        <name><surname>Wang</surname><given-names>William Yang</given-names></name>
      </person-group>
      <article-title>Riemannian normalizing flow on variational Wasserstein autoencoder for text modeling</article-title>
      <source>Proceedings of the 2019 conference of the north American chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-noe2019boltzmann">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Noé</surname><given-names>Frank</given-names></name>
        <name><surname>Olsson</surname><given-names>Simon</given-names></name>
        <name><surname>Köhler</surname><given-names>Jonas</given-names></name>
        <name><surname>Wu</surname><given-names>Hao</given-names></name>
      </person-group>
      <article-title>Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning</article-title>
      <source>Science</source>
      <publisher-name>American Association for the Advancement of Science</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>365</volume>
      <issue>6457</issue>
      <pub-id pub-id-type="doi">10.1126/science.aaw1147</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kobyzev2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kobyzev</surname><given-names>Ivan</given-names></name>
        <name><surname>Prince</surname><given-names>Simon J. D.</given-names></name>
        <name><surname>Brubaker</surname><given-names>Marcus A.</given-names></name>
      </person-group>
      <article-title>Normalizing flows: An introduction and review of current methods</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>43</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2020.2992934</pub-id>
      <fpage>3964</fpage>
      <lpage>3979</lpage>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2021normalizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>57</issue>
      <fpage>1</fpage>
      <lpage>64</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <year iso-8601-date="2019">2019</year>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dinh2017RealNVP">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dinh</surname><given-names>Laurent</given-names></name>
        <name><surname>Sohl-Dickstein</surname><given-names>Jascha</given-names></name>
        <name><surname>Bengio</surname><given-names>Samy</given-names></name>
      </person-group>
      <article-title>Density estimation using Real NVP</article-title>
      <source>International Conference on Learning Representations</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Pavlakou</surname><given-names>Theo</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
      </person-group>
      <article-title>Masked autoregressive flow for density estimation</article-title>
      <source>Proceedings of the 31st International Conference on Neural Information Processing Systems</source>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <publisher-loc>Red Hook, NY, USA</publisher-loc>
      <year iso-8601-date="2017-12">2017</year><month>12</month>
      <fpage>2335</fpage>
      <lpage>2344</lpage>
    </element-citation>
  </ref>
  <ref id="ref-muller2019neural">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Müller</surname><given-names>Thomas</given-names></name>
        <name><surname>McWilliams</surname><given-names>Brian</given-names></name>
        <name><surname>Rousselle</surname><given-names>Fabrice</given-names></name>
        <name><surname>Gross</surname><given-names>Markus</given-names></name>
        <name><surname>Novák</surname><given-names>Jan</given-names></name>
      </person-group>
      <article-title>Neural importance sampling</article-title>
      <source>ACM Transactions on Graphics (TOG)</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>38</volume>
      <issue>5</issue>
      <fpage>1</fpage>
      <lpage>19</lpage>
    </element-citation>
  </ref>
  <ref id="ref-durkan2019neuralspline">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        <name><surname>Bekasov</surname><given-names>Artur</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>Neural spline flows</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <fpage>7511</fpage>
      <lpage>7522</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chen2019residual">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Behrmann</surname><given-names>Jens</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David K</given-names></name>
        <name><surname>Jacobsen</surname><given-names>Joern-Henrik</given-names></name>
      </person-group>
      <article-title>Residual flows for invertible generative modeling</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-Chen2018a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Rubanova</surname><given-names>Yulia</given-names></name>
        <name><surname>Bettencourt</surname><given-names>Jesse</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Neural Ordinary Differential Equations</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
    </element-citation>
  </ref>
  <ref id="ref-dillon2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dillon</surname><given-names>Joshua V.</given-names></name>
        <name><surname>Langmore</surname><given-names>Ian</given-names></name>
        <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Srinivas</given-names></name>
        <name><surname>Moore</surname><given-names>Dave</given-names></name>
        <name><surname>Patton</surname><given-names>Brian</given-names></name>
        <name><surname>Alemi</surname><given-names>Alex</given-names></name>
        <name><surname>Hoffman</surname><given-names>Matt</given-names></name>
        <name><surname>Saurous</surname><given-names>Rif A.</given-names></name>
      </person-group>
      <article-title>TensorFlow Distributions</article-title>
      <source>arXiv preprint arXiv:1711.10604</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1711.10604</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-deepmind2020jax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Babuschkin</surname><given-names>Igor</given-names></name>
        <name><surname>Baumli</surname><given-names>Kate</given-names></name>
        <name><surname>Bell</surname><given-names>Alison</given-names></name>
        <name><surname>Bhupatiraju</surname><given-names>Surya</given-names></name>
        <name><surname>Bruce</surname><given-names>Jake</given-names></name>
        <name><surname>Buchlovsky</surname><given-names>Peter</given-names></name>
        <name><surname>Budden</surname><given-names>David</given-names></name>
        <name><surname>Cai</surname><given-names>Trevor</given-names></name>
        <name><surname>Clark</surname><given-names>Aidan</given-names></name>
        <name><surname>Danihelka</surname><given-names>Ivo</given-names></name>
        <name><surname>Fantacci</surname><given-names>Claudio</given-names></name>
        <name><surname>Godwin</surname><given-names>Jonathan</given-names></name>
        <name><surname>Jones</surname><given-names>Chris</given-names></name>
        <name><surname>Hemsley</surname><given-names>Ross</given-names></name>
        <name><surname>Hennigan</surname><given-names>Tom</given-names></name>
        <name><surname>Hessel</surname><given-names>Matteo</given-names></name>
        <name><surname>Hou</surname><given-names>Shaobo</given-names></name>
        <name><surname>Kapturowski</surname><given-names>Steven</given-names></name>
        <name><surname>Keck</surname><given-names>Thomas</given-names></name>
        <name><surname>Kemaev</surname><given-names>Iurii</given-names></name>
        <name><surname>King</surname><given-names>Michael</given-names></name>
        <name><surname>Kunesch</surname><given-names>Markus</given-names></name>
        <name><surname>Martens</surname><given-names>Lena</given-names></name>
        <name><surname>Merzic</surname><given-names>Hamza</given-names></name>
        <name><surname>Mikulik</surname><given-names>Vladimir</given-names></name>
        <name><surname>Norman</surname><given-names>Tamara</given-names></name>
        <name><surname>Quan</surname><given-names>John</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Ring</surname><given-names>Roman</given-names></name>
        <name><surname>Ruiz</surname><given-names>Francisco</given-names></name>
        <name><surname>Sanchez</surname><given-names>Alvaro</given-names></name>
        <name><surname>Schneider</surname><given-names>Rosalia</given-names></name>
        <name><surname>Sezener</surname><given-names>Eren</given-names></name>
        <name><surname>Spencer</surname><given-names>Stephen</given-names></name>
        <name><surname>Srinivasan</surname><given-names>Srivatsan</given-names></name>
        <name><surname>Wang</surname><given-names>Luyu</given-names></name>
        <name><surname>Stokowiec</surname><given-names>Wojciech</given-names></name>
        <name><surname>Viola</surname><given-names>Fabio</given-names></name>
      </person-group>
      <article-title>The DeepMind JAX Ecosystem</article-title>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-nflows">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        <name><surname>Bekasov</surname><given-names>Artur</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>nflows: Normalizing flows in PyTorch</article-title>
      <source>Zenodo</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://doi.org/10.5281/zenodo.4296287</uri>
    </element-citation>
  </ref>
  <ref id="ref-freia">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Ardizzone</surname><given-names>Lynton</given-names></name>
        <name><surname>Bungert</surname><given-names>Till</given-names></name>
        <name><surname>Draxler</surname><given-names>Felix</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Kruse</surname><given-names>Jakob</given-names></name>
        <name><surname>Schmier</surname><given-names>Robert</given-names></name>
        <name><surname>Sorrenson</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Framework for Easily Invertible Architectures (FrEIA)</article-title>
      <uri>https://github.com/vislearn/FrEIA</uri>
    </element-citation>
  </ref>
  <ref id="ref-wu2020stochasticNF">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Hao</given-names></name>
        <name><surname>Köhler</surname><given-names>Jonas</given-names></name>
        <name><surname>Noe</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>Stochastic normalizing flows</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <fpage>5933</fpage>
      <lpage>5944</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nielsen2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nielsen</surname><given-names>Didrik</given-names></name>
        <name><surname>Jaini</surname><given-names>Priyank</given-names></name>
        <name><surname>Hoogeboom</surname><given-names>Emiel</given-names></name>
        <name><surname>Winther</surname><given-names>Ole</given-names></name>
        <name><surname>Welling</surname><given-names>Max</given-names></name>
      </person-group>
      <article-title>SurVAE flows: Surjections to bridge the gap between VAEs and flows</article-title>
      <source>Advances in Neural Information Processing Systems 33</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-rezende2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Racanière</surname><given-names>Sébastien</given-names></name>
        <name><surname>Albergo</surname><given-names>Michael S.</given-names></name>
        <name><surname>Kanwar</surname><given-names>Gurtej</given-names></name>
        <name><surname>Shanahan</surname><given-names>Phiala E.</given-names></name>
        <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
      </person-group>
      <article-title>Normalizing flows on tori and spheres</article-title>
      <source>Proceedings of the 37th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>119</volume>
      <fpage>8083</fpage>
      <lpage>8092</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Midgley2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Midgley</surname><given-names>Laurence Illing</given-names></name>
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Simm</surname><given-names>Gregor N. C.</given-names></name>
        <name><surname>Schölkopf</surname><given-names>Bernhard</given-names></name>
        <name><surname>Hernández-Lobato</surname><given-names>José Miguel</given-names></name>
      </person-group>
      <article-title>Flow Annealed Importance Sampling Bootstrap</article-title>
      <source>International Conference on Learning Representations</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-boltzgen">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Campbell</surname><given-names>Andrew</given-names></name>
        <name><surname>Hernández-Lobato</surname><given-names>José Miguel</given-names></name>
      </person-group>
      <article-title>Implementing Boltzmann generators with normflows</article-title>
      <source>Zenodo</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://doi.org/10.5281/zenodo.7565800</uri>
    </element-citation>
  </ref>
  <ref id="ref-campbell2021gradient">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Campbell</surname><given-names>Andrew</given-names></name>
        <name><surname>Chen</surname><given-names>Wenlong</given-names></name>
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Hernandez-Lobato</surname><given-names>Jose Miguel</given-names></name>
        <name><surname>Zhang</surname><given-names>Yichuan</given-names></name>
      </person-group>
      <article-title>A gradient based strategy for Hamiltonian Monte Carlo hyperparameter optimization</article-title>
      <source>Proceedings of the 38th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <fpage>1238</fpage>
      <lpage>1248</lpage>
    </element-citation>
  </ref>
  <ref id="ref-stimper2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Schölkopf</surname><given-names>Bernhard</given-names></name>
        <name><surname>Hernández-Lobato</surname><given-names>José Miguel</given-names></name>
      </person-group>
      <article-title>Resampling Base Distributions of Normalizing Flows</article-title>
      <source>Proceedings of the 25th international conference on artificial intelligence and statistics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>151</volume>
      <fpage>4915</fpage>
      <lpage>4936</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
