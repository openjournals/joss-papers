<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5023</article-id>
<article-id pub-id-type="doi">10.21105/joss.05023</article-id>
<title-group>
<article-title>High-performance neural population dynamics modeling
enabled by scalable computational infrastructure</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Patel</surname>
<given-names>Aashish N.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9480-0698</contrib-id>
<name>
<surname>Sedler</surname>
<given-names>Andrew R.</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Huang</surname>
<given-names>Jingya</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pandarinath</surname>
<given-names>Chethan</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Gilja</surname>
<given-names>Vikash</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-6"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Electrical and Computer Engineering,
University of California San Diego, United States of
America</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute for Neural Computation, University of California
San Diego, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Center for Machine Learning, Georgia Institute of
Technology, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Biomedical Engineering, Georgia Institute of
Technology, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Department of Neurosurgery, Emory University, United States
of America</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>These authors contributed equally</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-06-30">
<day>30</day>
<month>6</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>83</issue>
<fpage>5023</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>autolfads</kwd>
<kwd>kubeflow</kwd>
<kwd>ray</kwd>
<kwd>neuroscience</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Advances in neural interface technology are facilitating parallel,
  high-dimensional time series measurements of the brain in action. A
  powerful strategy for analyzing these measurements is to apply
  unsupervised learning techniques to uncover lower-dimensional latent
  dynamics that explain much of the variance in the high-dimensional
  measurements
  (<xref alt="Cunningham &amp; Yu, 2014" rid="ref-cunningham2014dimensionality" ref-type="bibr">Cunningham
  &amp; Yu, 2014</xref>;
  <xref alt="Golub et al., 2018" rid="ref-golub2018learning" ref-type="bibr">Golub
  et al., 2018</xref>;
  <xref alt="Vyas et al., 2020" rid="ref-vyas2020computation" ref-type="bibr">Vyas
  et al., 2020</xref>). Latent factor analysis via dynamical systems
  (LFADS)
  (<xref alt="Pandarinath et al., 2018" rid="ref-pandarinath2018inferring" ref-type="bibr">Pandarinath
  et al., 2018</xref>) provides a deep learning approach for extracting
  estimates of these latent dynamics from neural population data. The
  recently developed AutoLFADS framework
  (<xref alt="Keshtkaran et al., 2022" rid="ref-keshtkaran2021large" ref-type="bibr">Keshtkaran
  et al., 2022</xref>) extends LFADS by using Population Based Training
  (PBT)
  (<xref alt="Jaderberg et al., 2017" rid="ref-jaderberg2017population" ref-type="bibr">Jaderberg
  et al., 2017</xref>) to effectively and scalably tune model
  hyperparameters, a critical step for accurate modeling of neural
  population data. As hyperparameter sweeps are one of the most
  computationally demanding processes in model development, these
  workflows should be deployed in a computationally efficient and cost
  effective manner given the compute resources available (e.g., local,
  institutionally-supported, or commercial computing clusters). The
  initial implementation of AutoLFADS used the Ray library
  (<xref alt="Moritz et al., 2018" rid="ref-moritz2018ray" ref-type="bibr">Moritz
  et al., 2018</xref>) to enable support for specific local and
  commercial cloud workflows. We extend this support, by providing
  additional options for training AutoLFADS models using local clusters
  in a container-native approach (e.g., Docker, Podman), unmanaged
  compute clusters leveraging Ray, and managed compute clusters
  leveraging KubeFlow and Kubernetes orchestration.</p>
  <p>As the neurosciences increasingly employ deep learning based models
  that require compute intensive hyperparameter optimization
  (<xref alt="Keshtkaran &amp; Pandarinath, 2019" rid="ref-keshtkaran2019enabling" ref-type="bibr">Keshtkaran
  &amp; Pandarinath, 2019</xref>;
  <xref alt="Willett et al., 2021" rid="ref-willett2021high" ref-type="bibr">Willett
  et al., 2021</xref>;
  <xref alt="Yu et al., 2021" rid="ref-yu2021fast" ref-type="bibr">Yu et
  al., 2021</xref>), standardization and dissemination of computational
  methods becomes increasingly challenging. Although this work
  specifically provides implementations of AutoLFADS, the tooling
  provided demonstrates strategies for employing computation at scale
  while facilitating dissemination and reproducibility.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Machine learning models enable neuroscience researchers to uncover
  new insights regarding the neural basis of perception, cognition, and
  behavior
  (<xref alt="Vu et al., 2018" rid="ref-vu2018shared" ref-type="bibr">Vu
  et al., 2018</xref>). However, models are often developed with
  hyperparameters tuned for a specific dataset, despite their intended
  generality. Application to new datasets requires computationally
  intensive hyperparameter searches for model tuning. Given the
  diversity of data across tasks, species, neural interface
  technologies, and brain regions, hyperparameter tuning is common and
  presents a significant barrier to evaluation and adoption of new
  algorithms. With the maturation of “AutoML” hyperparameter exploration
  libraries (HyperOpt, SkOpt, Ray), it is now easier to effectively
  search an extensive hyperparameter space. Solutions like KubeFlow
  (<xref alt="Kubeflow, 2018" rid="ref-kubeflow" ref-type="bibr"><italic>Kubeflow</italic>,
  2018</xref>) additionally enable scaling on managed clusters and
  provide near codeless workflows for the entire machine learning
  lifecycle. This lifecycle typically begins with data ingest and
  initial evaluation of machine learning algorithms with respect to
  data, and then matures to compute intensive model training and tuning.
  Building upon these tools, we empower researchers with multiple
  deployment strategies for leveraging AutoLFADS on local compute, on
  ad-hoc or unmanaged compute, and on managed or cloud compute, as
  illustrated in
  <xref alt="[fig:solutions]" rid="figU003Asolutions">[fig:solutions]</xref>.</p>
  <table-wrap>
    <caption>
      <p>Workflow overview. Summary of user burdens related to the three
      available deployment strategies.
      <styled-content id="tableU003Aworkflow_overview"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="25%" />
        <col width="25%" />
        <col width="25%" />
        <col width="25%" />
      </colgroup>
      <thead>
        <tr>
          <th>Task</th>
          <th>Local Solution</th>
          <th>Unmanaged Solution</th>
          <th>Managed Solution (this work)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Cluster management</td>
          <td>• Routine maintenance of software and hardware</td>
          <td><italic>Same as Local Solution</italic></td>
          <td>N/A</td>
        </tr>
        <tr>
          <td>Cluster orchestration</td>
          <td>N/A</td>
          <td>• Interacting with Ray clusters via CLI, manually setting
          up and tearing down clusters as needed<break/>• Maintenance of
          YAML configuration files that specify network locations and
          access credentials for machines in the cluster</td>
          <td>• Interacting with Kubernetes clusters via user interfaces
          (GUI or CLI)</td>
        </tr>
        <tr>
          <td>Dependency management</td>
          <td>• Running container images via Docker, Podman, or
          containerd</td>
          <td>• Installing dependencies in a virtual environment via
          Conda or pip</td>
          <td>• Specifying location of desired container image</td>
        </tr>
        <tr>
          <td>Running distributed jobs</td>
          <td>• Developing YAML model configurations and running Python
          scripts that use Ray to sweep hyperparameters</td>
          <td><italic>Same as Local Solution</italic></td>
          <td>• Providing model and hyperparameter sweep configurations
          to KubeFlow via YAML or code-less UI</td>
        </tr>
        <tr>
          <td>Evaluating and tuning models</td>
          <td>• Visualizing loss curves and intermediate output in
          TensorBoard<break/>• Flexible post-hoc analysis</td>
          <td><italic>Same as Local Solution</italic></td>
          <td>• Visualizing loss curves and intermediate output in
          KubeFlow<break/>• Flexible post-hoc analysis</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
<sec id="solutions">
  <title>Solutions</title>
  <fig>
    <caption><p>Solutions for running AutoLFADS on various compute
    infrastructures. (Left) This column depicts a local workflow: users
    leverage a container image that bundles all the AutoLFADS software
    dependencies and provides an entrypoint directly to the LFADS
    package. Container images are run with a supported container runtime
    (e.g., Docker, Podman, containerd). Users can interact with this
    workflow by providing YAML model configuration files and command
    line (CLI) arguments. (Middle) This column depicts a scalable
    solution using Ray: users start a Ray cluster by specifying the
    configuration (network location and authentication) as a YAML file.
    They install AutoLFADS locally or in a virtual environment, update
    YAML model configurations and hyperparameter sweep scripts, and then
    run the experiment code. (Right) This column depicts a scalable
    solution using KubeFlow: users provide an experiment specification
    that includes model configuration and hyperparameter sweep
    specifications either as a YAML file or using a code-less UI-based
    workflow. After experiment submission, the KubeFlow service spawns
    workers across the cluster that use the same container images as the
    local
    workflow.<styled-content id="figU003Asolutions"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/solutions.png" />
  </fig>
  <p>When training models on a novel dataset, it is often helpful to
  probe hyperparameters and investigate model performance locally prior
  to conducting a more exhaustive, automated hyperparameter search. This
  need can be met by installing the LFADS package locally or in a
  virtual environment. Isolating the workflow from local computational
  environments, we provide a pair of reference container images
  targeting CPU and GPU architectures. This allows users to treat the
  bundled algorithm as a portable executable for which they simply
  provide the input neural data and desired LFADS model configuration to
  initiate model training. This approach eliminates the need for users
  to configure their environments with compatible interpreters and
  dependencies. Instead, the user installs a container runtime engine
  (e.g., Docker, Podman), which are generally well-supported
  cross-platform tools, to run the image based solution. In addition to
  streamlining configuration, this approach enables reproducibility as
  the software environment employed for computation is fully defined and
  version controlled.</p>
  <p>Scaling initial investigations may involve evaluating data on
  internal lab resources, which may comprise a set of loosely connected
  compute devices. In such a heterogeneous environment, we leverage Ray
  to efficiently create processing jobs. In this approach Ray spawns a
  set of workers on compute nodes that the primary spawner is then able
  to send jobs to. This approach requires users to provide a mapping of
  machine locations (e.g., IP, hostname) and access credentials. It
  provides useful flexibility beyond single node local compute, but
  requires users to manage compute cluster configuration details. Ray
  can also be deployed in managed compute environments, but similarly
  requires users to have knowledge of the underlying compute
  infrastructure configuration defined by the managed environment. In
  short, the Ray based solution requires researchers to specifically
  target, and potentially modify, compute cluster configuration.</p>
  <p>To more effectively leverage large scale compute in managed
  infrastructure, such as those provided by commercial and academic
  cloud providers, we use KubeFlow which is a comprehensive machine
  learning solution designed to be operated as a service on top of
  Kubernetes based orchestration. This approach enables code-less
  workflows and provides a rich set of tooling around development (e.g.,
  notebooks, algorithm exploration) and automation (e.g., Pipelines)
  that reduces research iteration time. In contrast to Ray,
  configuration requirements are algorithm focused and are generally
  agnostic to the lower level details related to compute cluster
  configuration. With this solution, Kubernetes manages the underlying
  compute resource pool and is able to efficiently schedule compute
  jobs. Within KubeFlow, we leverage Katib
  (<xref alt="George et al., 2020" rid="ref-george2020katib" ref-type="bibr">George
  et al., 2020</xref>) – KubeFlow’s “AutoML” framework – to efficiently
  explore the hyperparameter space and specify individual sweeps. As
  KubeFlow is an industry-grade tool, many cloud providers offer
  KubeFlow as a service or provide supported pathways for deploying a
  KubeFlow cluster, facilitating replication and compute resource
  scaling.</p>
  <p>The two distributed workflows provided, Ray and KubeFlow based,
  each have their respective advantages and disadvantages. The correct
  choice for a specific research end user is dependent upon their
  requirements and access to compute resources. Thus we provide an
  evaluation in
  <xref alt="Table 1" rid="tableU003Aworkflow_overview">Table 1</xref>
  as a starting point in this decision making process.</p>
</sec>
<sec id="evaluation">
  <title>Evaluation</title>
  <p>A core innovation of AutoLFADS is the integration of PBT for
  hyperparameter exploration. As the underlying job scheduler and PBT
  implementation are unique in KubeFlow, we used the MC Maze dataset
  (<xref alt="Churchland &amp; Kaufman, 2021" rid="ref-churchland2021mc_maze" ref-type="bibr">Churchland
  &amp; Kaufman, 2021</xref>) from the Neural Latents Benchmark
  (<xref alt="Pei et al., 2021" rid="ref-pei2021neural" ref-type="bibr">Pei
  et al., 2021</xref>) to train and evaluate two AutoLFADS models. One
  model was trained with the Ray solution and the other with the
  KubeFlow solution using matching PBT hyperparameters and model
  configurations to ensure that models of comparable quality can be
  learned across both solutions. A comprehensive description of the
  AutoLFADS algorithm and results applying the algorithm to neural data
  using Ray can be found in Keshtkaran et al.
  (<xref alt="2022" rid="ref-keshtkaran2021large" ref-type="bibr">2022</xref>).
  We demonstrate similar converged model performances on metrics
  relevant to the quality of inferred firing rates in
  <xref alt="Table 2" rid="tableU003Amodel_performances">Table 2</xref>
  (<xref alt="Pei et al., 2021" rid="ref-pei2021neural" ref-type="bibr">Pei
  et al., 2021</xref>). In
  <xref alt="[fig:inferred_rates]" rid="figU003Ainferred_rates">[fig:inferred_rates]</xref>,
  inferred firing rates from the KubeFlow trained AutoLFADS model are
  shown along with conventional firing rate estimation strategies.
  Qualitatively, these example inferences are similar to those described
  in Keshtkaran et al.
  (<xref alt="2022" rid="ref-keshtkaran2021large" ref-type="bibr">2022</xref>),
  showing similar consistency across trials and resemblance to
  peristimulus time histograms (PSTH). In
  <xref alt="[fig:hp_progression]" rid="figU003Ahp_progression">[fig:hp_progression]</xref>,
  we plot the hyperparameter and associated loss values for the KubeFlow
  based implementation of AutoLFADS to provide a visualization of the
  PBT based optimization process on these data. These results
  demonstrate that although PBT is stochastic, both the original Ray and
  novel KubeFlow implementations are converging to stable, comparable
  solutions.</p>
  <table-wrap>
    <caption>
      <p>AutoLFADS Performance. An evaluation of AutoLFADS performance
      on Ray and KubeFlow. Test trial performance comparison on four
      neurally relevant metrics for evaluating latent variable models:
      co-smoothing on held-out neurons (co-bps), hand trajectory
      decoding on held-out neurons (vel R2), match to peristimulus time
      histogram (PSTH) on held-out neurons (psth R2), forward prediction
      on held-in neurons (fp-bps). The trained models converge with less
      than 5% difference between the frameworks on the above metrics.
      The percent difference is calculated with respect to the Ray
      framework.
      <styled-content id="tableU003Amodel_performances"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="41%" />
        <col width="16%" />
        <col width="14%" />
        <col width="16%" />
        <col width="14%" />
      </colgroup>
      <thead>
        <tr>
          <th>Framework</th>
          <th>co-bps (↑)</th>
          <th>vel R2 (↑)</th>
          <th>psth R2 (↑)</th>
          <th>fp-bps (↑)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Ray</td>
          <td>0.3364</td>
          <td>0.9097</td>
          <td>0.6360</td>
          <td>0.2349</td>
        </tr>
        <tr>
          <td>KubeFlow</td>
          <td>0.35103</td>
          <td>0.9099</td>
          <td>0.6339</td>
          <td>0.2405</td>
        </tr>
        <tr>
          <td>Percent difference (%)</td>
          <td>+4.35</td>
          <td>+0.03</td>
          <td>-0.33</td>
          <td>+2.38</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <fig>
    <caption><p>Evolution of loss curves and hyperparameters for
    AutoLFADS using KubeFlow. Each point represents each individual
    training trial. Points are colored by the exponentially-smoothed
    negative log-likelihood (NLL) loss at the end of each
    generation.<styled-content id="figU003Ahp_progression"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/hp_progression.png" />
  </fig>
  <fig>
    <caption><p>AutoLFADS inferred firing rates, relative to
    conventional estimation strategies, aligned to movement onset time
    (dashed vertical line at 250ms) for 3 example neurons (columns) and
    6 example conditions (colors; out of 108 conditions). Smoothed spike
    rates calculated with a Gaussian kernel, 50 ms st.dev. (top),
    trial-averaged peristimulus time histogram (PSTH) (middle), and
    inferred firing rates of AutoLFADS on KubeFlow (bottom). The
    smoothed spikes and inferred firing rates are shown for
    corresponding trials in the validation set (trial count range from 2
    to 6 per condition, shown as individual traces for each trial),
    while the PSTHs are calculated for all trials (trial counts range
    from 19 to 24 per condition, one trace per condition). In addition,
    the PSTHs are calculated based on the averaged spikes smoothed with
    a 70 ms st.dev Gaussian kernel, which corresponds to the definition
    of PSTHs used for the evaluation metric PSTH R2 (Table 1). In all
    subfigures, the inferred rates are calculated with a time resolution
    of 5ms.
    <styled-content id="figU003Ainferred_rates"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/inferred_rates.png" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Funding for this project was provided by: NIH-NIDCD 1R01DC018446,
  NSF EFRI 2223822, UCSD ORA Center Launch Program (VG); NIH-BRAIN/NIDA
  1RF1DA055667, NIH-NINDS/OD DP2NS127291, NSF NCS 1835364, the Alfred P.
  Sloan Foundation (CP); and NSF Graduate Research Fellowship
  DGE-1650044 (ARS).</p>
</sec>
<sec id="author-contributions">
  <title>Author contributions</title>
  <p>We describe contributions to this paper using the CRediT taxonomy
  (<xref alt="Brand et al., 2015" rid="ref-credit" ref-type="bibr">Brand
  et al., 2015</xref>).</p>
  <p>Writing and Visualization: ANP, ARS, JH, CP, VG; Software: ANP,
  ARS; Validation: JH, ARS, ANP; Supervision: CP, VG.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-cunningham2014dimensionality">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cunningham</surname><given-names>John P</given-names></name>
        <name><surname>Yu</surname><given-names>Byron M</given-names></name>
      </person-group>
      <article-title>Dimensionality reduction for large-scale neural recordings</article-title>
      <source>Nature Neuroscience</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>17</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1038/nn.3776</pub-id>
      <fpage>1500</fpage>
      <lpage>1509</lpage>
    </element-citation>
  </ref>
  <ref id="ref-keshtkaran2021large">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Keshtkaran</surname><given-names>Mohammad Reza</given-names></name>
        <name><surname>Sedler</surname><given-names>Andrew R</given-names></name>
        <name><surname>Chowdhury</surname><given-names>Raeed H</given-names></name>
        <name><surname>Tandon</surname><given-names>Raghav</given-names></name>
        <name><surname>Basrai</surname><given-names>Diya</given-names></name>
        <name><surname>Nguyen</surname><given-names>Sarah L</given-names></name>
        <name><surname>Sohn</surname><given-names>Hansem</given-names></name>
        <name><surname>Jazayeri</surname><given-names>Mehrdad</given-names></name>
        <name><surname>Miller</surname><given-names>Lee E</given-names></name>
        <name><surname>Pandarinath</surname><given-names>Chethan</given-names></name>
      </person-group>
      <article-title>A large-scale neural network training framework for generalized estimation of single-trial population dynamics</article-title>
      <source>Nature Methods</source>
      <publisher-name>Springer Nature</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>19</volume>
      <issue>12</issue>
      <uri>https://doi.org/10.1038/s41592-022-01675-0</uri>
      <pub-id pub-id-type="doi">10.1038/s41592-022-01675-0</pub-id>
      <fpage>1572</fpage>
      <lpage>1577</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pandarinath2018inferring">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pandarinath</surname><given-names>Chethan</given-names></name>
        <name><surname>O’Shea</surname><given-names>Daniel J</given-names></name>
        <name><surname>Collins</surname><given-names>Jasmine</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Stavisky</surname><given-names>Sergey D</given-names></name>
        <name><surname>Kao</surname><given-names>Jonathan C</given-names></name>
        <name><surname>Trautmann</surname><given-names>Eric M</given-names></name>
        <name><surname>Kaufman</surname><given-names>Matthew T</given-names></name>
        <name><surname>Ryu</surname><given-names>Stephen I</given-names></name>
        <name><surname>Hochberg</surname><given-names>Leigh R</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title>
      <source>Nature Methods</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>15</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id>
      <fpage>805</fpage>
      <lpage>815</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jaderberg2017population">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jaderberg</surname><given-names>Max</given-names></name>
        <name><surname>Dalibard</surname><given-names>Valentin</given-names></name>
        <name><surname>Osindero</surname><given-names>Simon</given-names></name>
        <name><surname>Czarnecki</surname><given-names>Wojciech M</given-names></name>
        <name><surname>Donahue</surname><given-names>Jeff</given-names></name>
        <name><surname>Razavi</surname><given-names>Ali</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Green</surname><given-names>Tim</given-names></name>
        <name><surname>Dunning</surname><given-names>Iain</given-names></name>
        <name><surname>Simonyan</surname><given-names>Karen</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Population based training of neural networks</article-title>
      <year iso-8601-date="2017">2017</year>
      <uri>http://arxiv.org/abs/1711.09846</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2201.11941</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-moritz2018ray">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
        <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
        <name><surname>Wang</surname><given-names>Stephanie</given-names></name>
        <name><surname>Tumanov</surname><given-names>Alexey</given-names></name>
        <name><surname>Liaw</surname><given-names>Richard</given-names></name>
        <name><surname>Liang</surname><given-names>Eric</given-names></name>
        <name><surname>Elibol</surname><given-names>Melih</given-names></name>
        <name><surname>Yang</surname><given-names>Zongheng</given-names></name>
        <name><surname>Paul</surname><given-names>William</given-names></name>
        <name><surname>Jordan</surname><given-names>Michael I</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Ray: A distributed framework for emerging \{AI\} applications</article-title>
      <source>13th USENIX symposium on operating systems design and implementation (OSDI 18)</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1712.05889</pub-id>
      <fpage>561</fpage>
      <lpage>577</lpage>
    </element-citation>
  </ref>
  <ref id="ref-george2020katib">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>George</surname><given-names>Johnu</given-names></name>
        <name><surname>Gao</surname><given-names>Ce</given-names></name>
        <name><surname>Liu</surname><given-names>Richard</given-names></name>
        <name><surname>Liu</surname><given-names>Hou Gang</given-names></name>
        <name><surname>Tang</surname><given-names>Yuan</given-names></name>
        <name><surname>Pydipaty</surname><given-names>Ramdoot</given-names></name>
        <name><surname>Saha</surname><given-names>Amit Kumar</given-names></name>
      </person-group>
      <article-title>A scalable and cloud-native hyperparameter tuning system</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2006.02085</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.02085</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-churchland2021mc_maze">
    <element-citation publication-type="dataset">
      <person-group person-group-type="author">
        <name><surname>Churchland</surname><given-names>Mark</given-names></name>
        <name><surname>Kaufman</surname><given-names>Matthew</given-names></name>
      </person-group>
      <article-title>MC_maze: Macaque primary motor and dorsal premotor cortex spiking activity during delayed reaching</article-title>
      <publisher-name>DANDI archive</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://dandiarchive.org/dandiset/000128/0.220113.0400</uri>
    </element-citation>
  </ref>
  <ref id="ref-pei2021neural">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pei</surname><given-names>Felix</given-names></name>
        <name><surname>Ye</surname><given-names>Joel</given-names></name>
        <name><surname>Zoltowski</surname><given-names>David</given-names></name>
        <name><surname>Zoltowski</surname><given-names>David</given-names></name>
        <name><surname>Wu</surname><given-names>Anqi</given-names></name>
        <name><surname>Chowdhury</surname><given-names>Raeed</given-names></name>
        <name><surname>Sohn</surname><given-names>Hansem</given-names></name>
        <name><surname>ODoherty</surname><given-names>Joseph</given-names></name>
        <name><surname>Shenoy</surname><given-names>Krishna V</given-names></name>
        <name><surname>Kaufman</surname><given-names>Matthew</given-names></name>
        <name><surname>Churchland</surname><given-names>Mark</given-names></name>
        <name><surname>Jazayeri</surname><given-names>Mehrdad</given-names></name>
        <name><surname>Miller</surname><given-names>Lee</given-names></name>
        <name><surname>Pillow</surname><given-names>Jonathan</given-names></name>
        <name><surname>Park</surname><given-names>Il Memming</given-names></name>
        <name><surname>Dyer</surname><given-names>Eva</given-names></name>
        <name><surname>Pandarinath</surname><given-names>Chethan</given-names></name>
      </person-group>
      <article-title>Neural latents benchmark ‘21: Evaluating latent variable models of neural population activity</article-title>
      <source>Proceedings of the neural information processing systems track on datasets and benchmarks</source>
      <person-group person-group-type="editor">
        <name><surname>Vanschoren</surname><given-names>J.</given-names></name>
        <name><surname>Yeung</surname><given-names>S.</given-names></name>
      </person-group>
      <year iso-8601-date="2021">2021</year>
      <volume>1</volume>
      <uri>https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/file/979d472a84804b9f647bc185a877a8b5-Paper-round2.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2109.04463</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-keshtkaran2019enabling">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Keshtkaran</surname><given-names>Mohammad Reza</given-names></name>
        <name><surname>Pandarinath</surname><given-names>Chethan</given-names></name>
      </person-group>
      <article-title>Enabling hyperparameter optimization in sequential autoencoders for spiking neural data</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <uri>https://proceedings.neurips.cc/paper/2019/file/6948bd44c91acd2b54ecdd1b132f10fb-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-willett2021high">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Willett</surname><given-names>Francis R</given-names></name>
        <name><surname>Avansino</surname><given-names>Donald T</given-names></name>
        <name><surname>Hochberg</surname><given-names>Leigh R</given-names></name>
        <name><surname>Henderson</surname><given-names>Jaimie M</given-names></name>
        <name><surname>Shenoy</surname><given-names>Krishna V</given-names></name>
      </person-group>
      <article-title>High-performance brain-to-text communication via handwriting</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>593</volume>
      <issue>7858</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-021-03506-2</pub-id>
      <fpage>249</fpage>
      <lpage>254</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yu2021fast">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yu</surname><given-names>Xinwei</given-names></name>
        <name><surname>Creamer</surname><given-names>Matthew S</given-names></name>
        <name><surname>Randi</surname><given-names>Francesco</given-names></name>
        <name><surname>Sharma</surname><given-names>Anuj K</given-names></name>
        <name><surname>Linderman</surname><given-names>Scott W</given-names></name>
        <name><surname>Leifer</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Fast deep neural correspondence for tracking and identifying neurons in c. Elegans using semi-synthetic training</article-title>
      <source>eLife</source>
      <publisher-name>eLife Sciences Publications Limited</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.7554/eLife.66410</pub-id>
      <fpage>e66410</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vyas2020computation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vyas</surname><given-names>Saurabh</given-names></name>
        <name><surname>Golub</surname><given-names>Matthew D</given-names></name>
        <name><surname>Sussillo</surname><given-names>David</given-names></name>
        <name><surname>Shenoy</surname><given-names>Krishna V</given-names></name>
      </person-group>
      <article-title>Computation through neural population dynamics</article-title>
      <source>Annual Review of Neuroscience</source>
      <publisher-name>NIH Public Access</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>43</volume>
      <pub-id pub-id-type="doi">10.1146/annurev-neuro-092619-094115</pub-id>
      <fpage>249</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-golub2018learning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Golub</surname><given-names>Matthew D</given-names></name>
        <name><surname>Sadtler</surname><given-names>Patrick T</given-names></name>
        <name><surname>Oby</surname><given-names>Emily R</given-names></name>
        <name><surname>Quick</surname><given-names>Kristin M</given-names></name>
        <name><surname>Ryu</surname><given-names>Stephen I</given-names></name>
        <name><surname>Tyler-Kabara</surname><given-names>Elizabeth C</given-names></name>
        <name><surname>Batista</surname><given-names>Aaron P</given-names></name>
        <name><surname>Chase</surname><given-names>Steven M</given-names></name>
        <name><surname>Yu</surname><given-names>Byron M</given-names></name>
      </person-group>
      <article-title>Learning by neural reassociation</article-title>
      <source>Nature Neuroscience</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>21</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1038/s41593-018-0095-3</pub-id>
      <fpage>607</fpage>
      <lpage>616</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kubeflow">
    <element-citation>
      <article-title>Kubeflow: Machine learning toolkit for kubernetes</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/kubeflow/kubeflow</uri>
    </element-citation>
  </ref>
  <ref id="ref-vu2018shared">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vu</surname><given-names>Mai-Anh T</given-names></name>
        <name><surname>Adalı</surname><given-names>Tülay</given-names></name>
        <name><surname>Ba</surname><given-names>Demba</given-names></name>
        <name><surname>Buzsáki</surname><given-names>György</given-names></name>
        <name><surname>Carlson</surname><given-names>David</given-names></name>
        <name><surname>Heller</surname><given-names>Katherine</given-names></name>
        <name><surname>Liston</surname><given-names>Conor</given-names></name>
        <name><surname>Rudin</surname><given-names>Cynthia</given-names></name>
        <name><surname>Sohal</surname><given-names>Vikaas S</given-names></name>
        <name><surname>Widge</surname><given-names>Alik S</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>A shared vision for machine learning in neuroscience</article-title>
      <source>Journal of Neuroscience</source>
      <publisher-name>Soc Neuroscience</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>38</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.1523/JNEUROSCI.0508-17.2018</pub-id>
      <fpage>1601</fpage>
      <lpage>1607</lpage>
    </element-citation>
  </ref>
  <ref id="ref-credit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brand</surname><given-names>Amy</given-names></name>
        <name><surname>Allen</surname><given-names>Liz</given-names></name>
        <name><surname>Altman</surname><given-names>Micah</given-names></name>
        <name><surname>Hlava</surname><given-names>Marjorie</given-names></name>
        <name><surname>Scott</surname><given-names>Jo</given-names></name>
      </person-group>
      <article-title>Beyond authorship: Attribution, contribution, collaboration, and credit</article-title>
      <source>Learned Publishing</source>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <issue>2</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1087/20150211</uri>
      <pub-id pub-id-type="doi">10.1087/20150211</pub-id>
      <fpage>151</fpage>
      <lpage>155</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
