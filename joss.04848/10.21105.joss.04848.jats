<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4848</article-id>
<article-id pub-id-type="doi">10.21105/joss.04848</article-id>
<title-group>
<article-title>UBayFS: An R Package for User Guided Feature
Selection</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6919-3483</contrib-id>
<name>
<surname>Jenul</surname>
<given-names>Anna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1327-4855</contrib-id>
<name>
<surname>Schrunner</surname>
<given-names>Stefan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Norwegian University of Life Sciences, Ås,
Norway</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-10-01">
<day>1</day>
<month>10</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>81</issue>
<fpage>4848</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>feature selection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Feature selection, also known as variable selection in statistics,
  is the process of selecting important variables (features) from a list
  of variables in a dataset. When training predictive models, the
  intention behind removing the least informative features from a
  dataset beforehand is (a) to reduce the computational burden and
  mathematical limitations associated with the curse of dimensionality,
  and (b) to increase interpretability of the model by allowing the user
  to obtain insights into the relevant input variables. In particular,
  feature selection speeds up the training process of machine learning
  models, especially when the dataset is high-dimensional.</p>
  <p>The <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  (<xref alt="R Core Team, 2022" rid="ref-R" ref-type="bibr">R Core
  Team, 2022</xref>) package UBayFS implements the user-guided framework
  for feature selection proposed in Jenul et al.
  (<xref alt="2022" rid="ref-jenul2022ubayfs" ref-type="bibr">2022</xref>),
  which incorporates information from the data and prior knowledge from
  domain experts.
  <xref alt="Figure 1" rid="figU003AUBayFS">Figure 1</xref> demonstrates
  the framework. Different approaches for integrating prior knowledge in
  feature selection exist, though there is a lack of general and
  sophisticated frameworks that deliver stable and reproducible feature
  selection along with implementations. With its generic setup and the
  possibilities to specify prior weights as well as side constraints,
  UBayFS shows the flexibility to be applied in a broad range of
  application scenarios, which exceed the capabilities of conventional
  feature selectors while preserving large model generality. Besides
  side constraints, such as the option to specify a maximum number of
  features, the user can add must-link constraints (features must be
  selected together) or cannot-link constraints (features must not be
  selected together). In addition, constraints can be defined on
  feature-block level, as well. Thus, UBayFS is also capable of solving
  more general problems such as block feature selection. A parameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\rho]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>
  regulates the shape of a penalty term accounting for side constraints,
  where feature sets that violate constraints lead to a lower target
  value. State-of-the-art methods do not cover such scenarios.</p>
  <p>The presented <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  package UBayFS provides an implementation along with an interactive
  Shiny dashboard, which makes feature selection available to
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>-users
  with different levels of expertise. The implementation allows the user
  to define their own feature selectors via a function interface or to
  use one out of three state-of-the-art feature selectors for building
  the generic ensemble of feature selectors covering the data-driven
  component of UBayFS. State-of-the-art choices include:</p>
  <list list-type="bullet">
    <list-item>
      <p>Laplacian score</p>
    </list-item>
    <list-item>
      <p>Fisher score</p>
    </list-item>
    <list-item>
      <p>mRMR</p>
    </list-item>
  </list>
  <p><inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  offers multiple packages implementing feature selection methodology.
  To name a few, <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{caret}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚌</mml:mi><mml:mi>𝚊</mml:mi><mml:mi>𝚛</mml:mi><mml:mi>𝚎</mml:mi><mml:mi>𝚝</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  (<xref alt="Kuhn, 2022" rid="ref-caret" ref-type="bibr">Kuhn,
  2022</xref>) is an essential machine learning repository, containing
  models with built-in feature selection such as tree based methods (for
  instance <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{rpart2}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚛</mml:mi><mml:mi>𝚙</mml:mi><mml:mi>𝚊</mml:mi><mml:mi>𝚛</mml:mi><mml:mi>𝚝</mml:mi><mml:mn>2</mml:mn></mml:mstyle></mml:math></alternatives></inline-formula>),
  regularized approaches like <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{lasso}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚕</mml:mi><mml:mi>𝚊</mml:mi><mml:mi>𝚜</mml:mi><mml:mi>𝚜</mml:mi><mml:mi>𝚘</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  and non-integrated feature selectors such as recursive feature
  elimination <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{rfe}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚛</mml:mi><mml:mi>𝚏</mml:mi><mml:mi>𝚎</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
  Other examples are the <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{Boruta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝙱</mml:mi><mml:mi>𝚘</mml:mi><mml:mi>𝚛</mml:mi><mml:mi>𝚞</mml:mi><mml:mi>𝚝</mml:mi><mml:mi>𝚊</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  (<xref alt="Kursa &amp; Rudnicki, 2010" rid="ref-boruta" ref-type="bibr">Kursa
  &amp; Rudnicki, 2010</xref>) package implementing the
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{Boruta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝙱</mml:mi><mml:mi>𝚘</mml:mi><mml:mi>𝚛</mml:mi><mml:mi>𝚞</mml:mi><mml:mi>𝚝</mml:mi><mml:mi>𝚊</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  feature selector or the <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{GSelection}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝙶</mml:mi><mml:mi>𝚂</mml:mi><mml:mi>𝚎</mml:mi><mml:mi>𝚕</mml:mi><mml:mi>𝚎</mml:mi><mml:mi>𝚌</mml:mi><mml:mi>𝚝</mml:mi><mml:mi>𝚒</mml:mi><mml:mi>𝚘</mml:mi><mml:mi>𝚗</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  (<xref alt="Majumdar et al., 2019" rid="ref-gselection" ref-type="bibr">Majumdar
  et al., 2019</xref>) package containing <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{hsic\;lasso}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚑</mml:mi><mml:mi>𝚜</mml:mi><mml:mi>𝚒</mml:mi><mml:mi>𝚌</mml:mi><mml:mspace width="0.278em"></mml:mspace><mml:mi>𝚕</mml:mi><mml:mi>𝚊</mml:mi><mml:mi>𝚜</mml:mi><mml:mi>𝚜</mml:mi><mml:mi>𝚘</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  feature selection. All feature selectors available in
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  can be used as underlying ensemble feature selectors in UBayFS. Prior
  weights can be specified for single features or whole blocks as weight
  vectors. Linear side constraints are implemented via a matrix
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{A}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐀</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  and a right side <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{b}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐛</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  or with a customized function for specific constraint types. Hence,
  the sophisticated statistical model is summarized in a user-friendly
  and easy-to-use package.</p>
  <fig>
    <caption><p>At first, UBayFS elaborates information directly from
    data via ensemble feature selection. This information is merged with
    prior expert knowledge (a-priori feature weights) in a Bayesian
    model framework. Additionally, the user can include further side
    constraints such as a maximum number of features or cannot-link
    constraints between features. The final step comprises the
    optimization with respect to the model’s utility function, including
    the side
    constraints.<styled-content id="figU003AUBayFS"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/UBayFS_concept.png" xlink:title="" />
  </fig>
</sec>
<sec id="concept-of-ubayfs">
  <title>Concept of UBayFS</title>
  <p>As described in Jenul et al.
  (<xref alt="2022" rid="ref-jenul2022ubayfs" ref-type="bibr">2022</xref>),
  UBayFS is a Bayesian ensemble feature selection framework. The
  methodology is based on quantifying a random variable
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  representing feature importances, given evidence collected from the
  data, denoted as <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
  In particular, <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  counts the number of elementary models in the generic ensemble of
  feature selectors, which select a particular feature. Statistically,
  we interpret the result from each elementary feature selector as a
  realization from a multinomial distribution with parameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\theta}\in[0,1]^N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  defines the success probabilities of sampling each feature in an
  individual feature selection and thus the success probability in the
  ensemble. Both sources of information are combined using Bayes’
  Theorem:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[p(\boldsymbol{\theta}|\boldsymbol{y})\propto p(\boldsymbol{y}|\boldsymbol{\theta})\cdot p(\boldsymbol{\theta}).]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>In the framework of UBayFS, <inline-formula><alternatives>
  <tex-math><![CDATA[p(\boldsymbol{y}|\boldsymbol{\theta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  represents the data-driven component (implemented via a multinomial
  likelihood), while <inline-formula><alternatives>
  <tex-math><![CDATA[p(\boldsymbol{\theta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  describes the user knowledge part modeled with a Dirichlet
  distribution. Due to the conjugate prior property of the Dirichlet
  distribution, the posterior parameter update has a tractable form and
  can be computed analytically. Side constraints are represented by a
  system of linear inequalities <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{A}\cdot \boldsymbol{\delta}-\boldsymbol{b}\leq \boldsymbol{0}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝐀</mml:mi></mml:mstyle><mml:mo>⋅</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>−</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐛</mml:mi></mml:mstyle><mml:mo>≤</mml:mo><mml:mstyle mathvariant="bold"><mml:mn>0</mml:mn></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{A}\in\mathbb{R}^{K\times N}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝐀</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mrow><mml:mi>K</mml:mi><mml:mo>×</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{b}\in\mathbb{R}^K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝐛</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{0}\in\mathbb{R}^K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="bold"><mml:mn>0</mml:mn></mml:mstyle><mml:mo>∈</mml:mo><mml:msup><mml:mstyle mathvariant="double-struck"><mml:mi>ℝ</mml:mi></mml:mstyle><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is the <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>-dimensional
  vector of zeros. <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  is defined as the total number of constraints. The comparison is
  performed elementwise.</p>
  <p>In UBayFS, a relaxed inadmissibility function
  <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa_{k,\rho}(\boldsymbol{\delta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is used as a penalization for the violation of a given side constraint
  <inline-formula><alternatives>
  <tex-math><![CDATA[k=1,...,K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  The joint inadmissibility function <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>κ</mml:mi></mml:math></alternatives></inline-formula>
  pursues the idea that <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa = 1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  (maximum penalization) if at least one <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa_{k,\rho}=1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  while <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  (no penalization) if all <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa_{k,\rho}=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>κ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  A more detailed description is provided in the original paper
  (<xref alt="Jenul et al., 2022" rid="ref-jenul2022ubayfs" ref-type="bibr">Jenul
  et al., 2022</xref>).</p>
  <p>To obtain an optimal feature set <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\delta}^\star]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>⋆</mml:mo></mml:msup></mml:math></alternatives></inline-formula>,
  we use a target function <inline-formula><alternatives>
  <tex-math><![CDATA[U(\boldsymbol{\delta}, \boldsymbol{\theta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  which represents a posterior expected utility of feature sets
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\delta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  given the posterior feature importance parameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  regularized by the inadmissibility function
  <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa(.)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>κ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>.</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\delta}^\star = \underset{\boldsymbol{\delta}\in\{0,1\}^N}{\text{max}} \left(\mathbb{E}_{\boldsymbol{\theta}|\boldsymbol{y}}[U(\boldsymbol{\delta}, \boldsymbol{\theta}(\boldsymbol{y}))] \right) = \underset{\boldsymbol{\delta}\in\{0,1\}^N}{\text{max}}\left(\boldsymbol{\delta}^T \mathbb{E}_{\boldsymbol{\theta}|\boldsymbol{y}}[\boldsymbol{\theta}(\boldsymbol{y} )]-\lambda\kappa(\boldsymbol{\delta})\right).
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>⋆</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext mathvariant="normal">max</mml:mtext><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false" form="postfix">}</mml:mo><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mstyle mathvariant="double-struck"><mml:mi>𝔼</mml:mi></mml:mstyle><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>,</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mtext mathvariant="normal">max</mml:mtext><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo>∈</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:msup><mml:mo stretchy="false" form="postfix">}</mml:mo><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mi>T</mml:mi></mml:msup><mml:msub><mml:mstyle mathvariant="double-struck"><mml:mi>𝔼</mml:mi></mml:mstyle><mml:mrow><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛉</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝐲</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mi>κ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mstyle mathvariant="bold"><mml:mi>𝛅</mml:mi></mml:mstyle><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>The optimization is implemented via a genetic algorithm along with
  a greedy algorithm for initialization, suggested by Jenul et al.
  (<xref alt="2022" rid="ref-jenul2022ubayfs" ref-type="bibr">2022</xref>)
  to find a proper start vector for the optimization.</p>
</sec>
<sec id="package-summary">
  <title>Package Summary</title>
  <p>The function <monospace>build.UBaymodel()</monospace> initializes
  an S3 class object UBaymodel and computes the ensemble of elementary
  feature selectors. In the current version, linear feature selectors
  such as Fisher score, Laplacian score
  (<xref alt="You &amp; Shung, 2022" rid="ref-you2022rdimtools" ref-type="bibr">You
  &amp; Shung, 2022</xref>), and mRMR
  (<xref alt="Jay et al., 2013" rid="ref-de2013mrmre" ref-type="bibr">Jay
  et al., 2013</xref>) are supported as integrated options. Any
  arbitrary feature selector can be defined manually and used as input.
  In addition, the number of elementary models
  <inline-formula><alternatives>
  <tex-math><![CDATA[M]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>
  is specified. The user can directly set prior weights inside the
  <monospace>build</monospace> function. Constraints are either provided
  as a matrix <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{A}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐀</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  and a right side <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{b}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="bold"><mml:mi>𝐛</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
  or built using the <monospace>buildConstraints()</monospace> function,
  which supports max-size, must-link, and cannot-link constraints on
  both feature and block level. UBayFS requires at least one constraint
  limiting the total number of features to be selected (“max-size”). The
  level of constraint-relaxation is steered with an input parameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\rho]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>.
  In addition, the weights for single features or feature blocks are set
  with <monospace>setWeights()</monospace>.</p>
  <p>The function <monospace>admissibility()</monospace> allows the user
  to evaluate the penalty term for a given feature set under a set of
  constraints. After initializing the model and computing the ensembles,
  the <monospace>train.UBaymodel()</monospace> function optimizes the
  feature set via a genetic algorithm
  (<xref alt="Scrucca, 2013" rid="ref-scrucca2013ga" ref-type="bibr">Scrucca,
  2013</xref>) with greedy initialization. According to empirical
  evaluations, the greedy initialization decreases the runtime and leads
  to faster convergence towards an optimal feature set. Finally, the
  package implements the generic functions
  <monospace>print.UBaymodel()</monospace>,
  <monospace>plot.UBaymodel()</monospace>, and
  <monospace>summary.UBaymodel()</monospace> as well as an evaluation
  function <monospace>evaluteFS()</monospace> to report and visualize
  results. Two vignettes guide the user through the package and
  demonstrate how the method can be deployed in common application
  scenarios, including how user knowledge is specified and how feature-
  and block-wise constraints are set.</p>
</sec>
<sec id="interactive-shiny-dashboard">
  <title>Interactive Shiny Dashboard</title>
  <p>The function <monospace>runInteractive()</monospace> opens an
  interactive Shiny dashboard allowing the user to load and analyze data
  interactively. However, due to computational limitations, it is not
  recommended to use the HTML interface for larger datasets
  (<inline-formula><alternatives>
  <tex-math><![CDATA[> 100]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  features or <inline-formula><alternatives>
  <tex-math><![CDATA[>1000]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  samples). Instead, functions should be called from the
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  console in such cases.
  <xref alt="Figure 2" rid="figU003AShiny">Figure 2</xref> shows the
  dashboard with the different tabs:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>data</bold>: Load the dataset and specify whether row
      names, column names, or a block structure is present. A demo
      dataset is ready to be loaded and used for a first touch on the
      package.</p>
    </list-item>
    <list-item>
      <p><bold>likelihood</bold>: Select elementary feature selectors
      for ensemble feature selection, the number of models
      <inline-formula><alternatives>
      <tex-math><![CDATA[M]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>,
      the number of features in each model, and the ratio of the
      train-test split. Further, the dashboard allows the user to mix
      different elementary feature selectors, although this option is
      not recommended due to limited stability
      (<xref alt="Seijo-Pardo et al., 2017" rid="ref-seijoU003AensembleSurvey" ref-type="bibr">Seijo-Pardo
      et al., 2017</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>weights</bold>: The prior feature weights are set by the
      user. For block feature selection, it is possible to set weights
      for blocks; otherwise, for a single feature.</p>
    </list-item>
    <list-item>
      <p><bold>constraints / block constraints</bold>: In this task, the
      user sets different constraints (at least a max-size constraint).
      The penalty <inline-formula><alternatives>
      <tex-math><![CDATA[\rho]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>
      can be varied here as well.</p>
    </list-item>
    <list-item>
      <p><bold>feature selection</bold>: In the dashboard’s last step,
      an optimization procedure determines the final feature set. A plot
      of the final result is produced - also, the model can be saved as
      an Rdata file and loaded to the dashboard again.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Illustration of the Shiny HTML
    dashboard.<styled-content id="figU003AShiny"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/UBay_Shiny_Screenshot.png" xlink:title="" />
  </fig>
</sec>
<sec id="ongoing-research">
  <title>Ongoing research</title>
  <p>Based on the present UBayFS package, ongoing work focuses on the
  implementation of even more types of expert constraints and elementary
  feature selection models. Moreover, a Python package with similar
  functionality is planned for the future.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank Prof. Jürgen Pilz (University of Klagenfurt)
  and Prof. Oliver Tomic (Norwegian University of Life Sciences), who
  contributed to the development of the methodology and supported us
  with ideas and fruitful discussions, as well as Kristian Hovde Liland
  (Norwegian University of Life Sciences) for testing the
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathtt{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="monospace"><mml:mi>𝚁</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
  package.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-jenul2022ubayfs">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jenul</surname><given-names>Anna</given-names></name>
        <name><surname>Schrunner</surname><given-names>Stefan</given-names></name>
        <name><surname>Pilz</surname><given-names>Jürgen</given-names></name>
        <name><surname>Tomic</surname><given-names>Oliver</given-names></name>
      </person-group>
      <article-title>A user-guided Bayesian framework for ensemble feature selection in life science applications (UBayFS)</article-title>
      <source>Machine Learning</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2022-08">2022</year><month>08</month>
      <volume>111</volume>
      <issue>10</issue>
      <uri>https://doi.org/10.1007/s10994-022-06221-9</uri>
      <pub-id pub-id-type="doi">10.1007/s10994-022-06221-9</pub-id>
      <fpage>3897</fpage>
      <lpage>3923</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scrucca2013ga">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Scrucca</surname><given-names>Luca</given-names></name>
      </person-group>
      <article-title>GA: A package for genetic algorithms in R</article-title>
      <source>Journal of Statistical Software</source>
      <publisher-name>Foundation for Open Access Statistic</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>53</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.18637/jss.v053.i04</uri>
      <pub-id pub-id-type="doi">10.18637/jss.v053.i04</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-you2022rdimtools">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>You</surname><given-names>Kisung</given-names></name>
        <name><surname>Shung</surname><given-names>Dennis</given-names></name>
      </person-group>
      <article-title>Rdimtools: An R package for dimension reduction and intrinsic dimension estimation</article-title>
      <source>Software Impacts</source>
      <publisher-name>Elsevier BV</publisher-name>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <volume>14</volume>
      <uri>https://doi.org/10.1016/j.simpa.2022.100414</uri>
      <pub-id pub-id-type="doi">10.1016/j.simpa.2022.100414</pub-id>
      <fpage>100414</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-R">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>R Core Team</string-name>
      </person-group>
      <source>R: A language and environment for statistical computing</source>
      <publisher-name>R Foundation for Statistical Computing</publisher-name>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <uri>https://www.R-project.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-de2013mrmre">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jay</surname><given-names>Nicolas De</given-names></name>
        <name><surname>Papillon-Cavanagh</surname><given-names>Simon</given-names></name>
        <name><surname>Olsen</surname><given-names>Catharina</given-names></name>
        <name><surname>El-Hachem</surname><given-names>Nehme</given-names></name>
        <name><surname>Bontempi</surname><given-names>Gianluca</given-names></name>
        <name><surname>Haibe-Kains</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>mRMRe: An R package for parallelized mRMR ensemble feature selection</article-title>
      <source>Bioinformatics</source>
      <publisher-name>Oxford University Press (OUP)</publisher-name>
      <year iso-8601-date="2013-07">2013</year><month>07</month>
      <volume>29</volume>
      <issue>18</issue>
      <uri>https://doi.org/10.1093/bioinformatics/btt383</uri>
      <pub-id pub-id-type="doi">10.1093/bioinformatics/btt383</pub-id>
      <fpage>2365</fpage>
      <lpage>2368</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gselection">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Majumdar</surname><given-names>Sayanti Guha</given-names></name>
        <name><surname>Rai</surname><given-names>Anil</given-names></name>
        <name><surname>Mishra</surname><given-names>Dwijesh Chandra</given-names></name>
      </person-group>
      <source>GSelection: Genomic selection</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://CRAN.R-project.org/package=GSelection</uri>
    </element-citation>
  </ref>
  <ref id="ref-boruta">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kursa</surname><given-names>Miron B.</given-names></name>
        <name><surname>Rudnicki</surname><given-names>Witold R.</given-names></name>
      </person-group>
      <article-title>Feature selection with the Boruta package</article-title>
      <source>Journal of Statistical Software</source>
      <publisher-name>Foundation for Open Access Statistic</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>36</volume>
      <issue>11</issue>
      <uri>https://doi.org/10.18637/jss.v036.i11</uri>
      <pub-id pub-id-type="doi">10.18637/jss.v036.i11</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-caret">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Kuhn</surname><given-names>Max</given-names></name>
      </person-group>
      <source>Caret: Classification and regression training</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://CRAN.R-project.org/package=caret</uri>
    </element-citation>
  </ref>
  <ref id="ref-seijoU003AensembleSurvey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Seijo-Pardo</surname><given-names>B.</given-names></name>
        <name><surname>Porto-Dı́az</surname><given-names>I.</given-names></name>
        <name><surname>Bolón-Canedo</surname><given-names>V.</given-names></name>
        <name><surname>Alonso-Betanzos</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Ensemble feature selection: Homogeneous and heterogeneous approaches</article-title>
      <source>Knowledge-Based Systems</source>
      <publisher-name>Elsevier BV</publisher-name>
      <year iso-8601-date="2017-02">2017</year><month>02</month>
      <volume>118</volume>
      <uri>https://doi.org/10.1016/j.knosys.2016.11.017</uri>
      <pub-id pub-id-type="doi">10.1016/j.knosys.2016.11.017</pub-id>
      <fpage>124</fpage>
      <lpage>139</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
