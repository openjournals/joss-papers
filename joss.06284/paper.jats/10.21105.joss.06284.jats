<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6284</article-id>
<article-id pub-id-type="doi">10.21105/joss.06284</article-id>
<title-group>
<article-title>LabelProp: A semi-automatic segmentation tool for 3D
medical images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6911-6373</contrib-id>
<name>
<surname>Decaux</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2214-3654</contrib-id>
<name>
<surname>Conze</surname>
<given-names>Pierre-Henri</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7467-759X</contrib-id>
<name>
<surname>Ropars</surname>
<given-names>Juliette</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>He</surname>
<given-names>Xinyan</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sheehan</surname>
<given-names>Frances T.</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3924-6035</contrib-id>
<name>
<surname>Pons</surname>
<given-names>Christelle</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5532-2208</contrib-id>
<name>
<surname>Salem</surname>
<given-names>Douraied Ben</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4950-1696</contrib-id>
<name>
<surname>Brochard</surname>
<given-names>Sylvain</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9837-7487</contrib-id>
<name>
<surname>Rousseau</surname>
<given-names>François</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>LaTIM UMR 1101, Inserm, Brest, France</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>IMT Atlantique, Brest, France</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University Hospital of Brest, Brest, France</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Rehabilitation Medicine, NIH, Bethesda, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Fondation ILDYS, Brest, France</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-01-08">
<day>8</day>
<month>1</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>109</issue>
<fpage>6284</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>segmentation</kwd>
<kwd>deep learning</kwd>
<kwd>medical images</kwd>
<kwd>musculoskeletal</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>LabelProp is a tool that provides a semi-automated method to
  segment 3D medical images with multiple labels. It is a convenient
  implementation of our peer-reviewed method designed to assist medical
  professionals in segmenting musculoskeletal structures on scans based
  on a small number of annotated slices
  (<xref alt="Decaux et al., 2023" rid="ref-decaux_semi-automatic_2023" ref-type="bibr">Decaux
  et al., 2023</xref>). LabelProp leverages deep learning techniques,
  but can be used without a training set. It is available as a PyPi
  package and offers both a command-line interface (CLI) and an API.
  Additionally, LabelProp provides two plugins, namely
  ‘napari-labelprop’ and ‘napari-labelprop-remote’, which facilitate
  training and inference on a single scan within the multi-dimensional
  viewer Napari. It is available on GitHub with pretrained weights
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/nathandecaux/napari-labelprop">https://github.com/nathandecaux/napari-labelprop</ext-link>)</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Segmenting musculoskeletal structures from MR images is crucial for
  clinical research, diagnosis, and treatment planning. However,
  challenges arise from the limited availability of annotated datasets,
  particularly in rare diseases or pediatric cohorts
  (<xref alt="Conze et al., 2020" rid="ref-conze2020healthy" ref-type="bibr">Conze
  et al., 2020</xref>). While manual segmentation ensures accuracy, it
  is labor-intensive and prone to observer variability
  (<xref alt="Vădineanu et al., 2022" rid="ref-vadineanu_analysis_2022" ref-type="bibr">Vădineanu
  et al., 2022</xref>). Existing semi-automatic methods based on point
  and scribbles require minimal interactions but often lack
  reproducibility
  (<xref alt="Chanti et al., 2021" rid="ref-chanti_ifss-net_2021" ref-type="bibr">Chanti
  et al., 2021</xref>;
  <xref alt="Lee &amp; Jeong, 2020" rid="ref-lee_scribble2label_2020" ref-type="bibr">Lee
  &amp; Jeong, 2020</xref>;
  <xref alt="Sakinis et al., 2019" rid="ref-sakinis2019interactive" ref-type="bibr">Sakinis
  et al., 2019</xref>;
  <xref alt="Zhang et al., 2021" rid="ref-zhang2021interactive" ref-type="bibr">Zhang
  et al., 2021</xref>).</p>
  <p>LabelProp addresses these challenges with a novel deep
  registration-based label propagation method. This approach efficiently
  adapts to various musculoskeletal structures, leveraging image
  intensity and muscle shape for improved segmentation accuracy.</p>
  <p>A key innovation of LabelProp is its minimal reliance on manual
  annotations. Demonstrating the capability for accurate 3D segmentation
  from as few as three annotated slices per MR volume
  (<xref alt="Decaux et al., 2023" rid="ref-decaux_semi-automatic_2023" ref-type="bibr">Decaux
  et al., 2023</xref>), it significantly reduces the workload for
  medical professionals and is particularly beneficial where extensive
  annotated data is scarce. This feature aligns with the method of
  slice-to-slice registration
  (<xref alt="Ogier et al., 2017" rid="ref-ogier2017individual" ref-type="bibr">Ogier
  et al., 2017</xref>), but is further enhanced by deep learning
  techniques.</p>
  <p>Similar to VoxelMorph, the underlying model in this approach learns
  to generate deformations without supervision
  (<xref alt="Balakrishnan et al., 2019" rid="ref-balakrishnan2019voxelmorph" ref-type="bibr">Balakrishnan
  et al., 2019</xref>). However, it specifically focuses on aligning
  adjacent 2D slices and can be trained directly on the scan that needs
  to be segmented or on a complete dataset for optimal results. When
  training the model with at least two annotations for a scan, a
  constraint is added to ensure that the generated deformations are
  consistent from both an image and segmentation perspective.
  Additionally, weak annotations in the form of scribbles can be
  provided during training on intermediate slices to provide additional
  guidance for propagation. Examples of manual annotations and scribbles
  are shown in Fig. 1.</p>
  <p>During the inference phase, each annotation is propagated to its
  nearest neighboring annotation, resulting in two predictions for each
  intermediate slice from different source annotations. The label fusion
  process involves weighting each prediction based on their distance to
  the source annotation or an estimate of the registration accuracy.
  Importantly, the propagation method is label-agnostic, allowing for
  the simultaneous segmentation of multiple structures, regardless of
  whether they are manually annotated on the same slice or not.</p>
  <fig>
    <caption><p>Example of propagation from 3 manual annotations of the
    right deltoid muscle in an MRI, in the axial plane. Optional
    scribbles (yellow) can be provided, without plane constraints, for
    further
    guidance.<styled-content id="figU003Apropagation"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="propagation.pdf" />
  </fig>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>In a previous study, we evaluated our method against various
  approaches in a shoulder muscle MRI dataset and the publicly
  accessible
  <ext-link ext-link-type="uri" xlink:href="https://osf.io/svwa7/?view_only=c2c980c17b3a40fca35d088a3cdd83e2">MyoSegmenTUM</ext-link>
  dataset. Specifically, we focused on intra-subject segmentation using
  only 3 annotated slices
  (<xref alt="Decaux et al., 2023" rid="ref-decaux_semi-automatic_2023" ref-type="bibr">Decaux
  et al., 2023</xref>). The reference methods were the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/KitwareMedical/ITKMorphologicalContourInterpolation">ITKMorphologicalContourInterpolation</ext-link>
  approach
  (<xref alt="Albu et al., 2008" rid="ref-albu2008morphology" ref-type="bibr">Albu
  et al., 2008</xref>), a well-known implementation of
  <ext-link ext-link-type="uri" xlink:href="https://github.com/milesial/Pytorch-UNet">UNet</ext-link>
  (<xref alt="Ronneberger et al., 2015" rid="ref-ronneberger2015u" ref-type="bibr">Ronneberger
  et al., 2015</xref>), and a
  <ext-link ext-link-type="uri" xlink:href="https://github.com/ajabri/videowalk">semi-automatic
  image sequence segmentation approach</ext-link>
  (<xref alt="Jabri et al., 2020" rid="ref-jabri_space-time_2020" ref-type="bibr">Jabri
  et al., 2020</xref>). Our results showed that in this particular
  configuration, our method (Labelprop) outperformed all of these
  methods. Additionally, our method also demonstrated competitive
  performance compared to a leave-one-out trained UNet for the shoulder
  dataset
  (<xref alt="Conze et al., 2020" rid="ref-conze2020healthy" ref-type="bibr">Conze
  et al., 2020</xref>).</p>
</sec>
<sec id="software-details">
  <title>Software Details</title>
  <p>LabelProp is composed of three main components: labelprop,
  napari-labelprop, and napari-labelprop-remote. The labelprop algorithm
  is accompanied by a command-line interface (CLI) and a REST API. The
  CLI enables unsupervised pretraining or training with sparse
  annotations on a dataset, and inference on a single volume. The API
  provides access to training with annotations and inference on a single
  subject via HTTP requests. It is used in the napari-labelprop-remote
  plugin, but can be adapted to other extendable viewer/segmentation
  tools such as
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Slicer/Slicer">3D
  Slicer</ext-link> or
  <ext-link ext-link-type="uri" xlink:href="https://github.com/MITK/MITK">MITK</ext-link>.
  The napari-labelprop plugin brings the labelprop algorithm into the
  interactive Napari platform, allowing users to conduct both the
  training and inference stages of label propagation directly within the
  Napari environment. The napari-labelprop-remote plugin extends the
  functionality of napari-labelprop, allowing users to perform training
  and inference on a remote server through the labelprop API. These
  tools provide a versatile and user-friendly toolkit for 3D image
  segmentation, offering the flexibility to work locally or remotely,
  and leveraging deep learning to efficiently generate 3D delineations
  from slice annotations.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was partially funded by ANR (AI4Child project, grant
  ANR-19-CHIA-0015), Innoveo from CHU de Brest and Fondation de
  l’avenir.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-albu2008morphology">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Albu</surname><given-names>Alexandra Branzan</given-names></name>
        <name><surname>Beugeling</surname><given-names>Trevor</given-names></name>
        <name><surname>Laurendeau</surname><given-names>Denis</given-names></name>
      </person-group>
      <article-title>A morphology-based approach for interslice interpolation of anatomical slices from volumetric images</article-title>
      <source>IEEE Transactions on Biomedical Engineering</source>
      <year iso-8601-date="2008">2008</year>
      <volume>55</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1109/TBME.2008.921158</pub-id>
      <fpage>2022</fpage>
      <lpage>2038</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ronneberger2015u">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ronneberger</surname><given-names>Olaf</given-names></name>
        <name><surname>Fischer</surname><given-names>Philipp</given-names></name>
        <name><surname>Brox</surname><given-names>Thomas</given-names></name>
      </person-group>
      <article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>
      <source>Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, munich, germany, october 5-9, 2015, proceedings, part III 18</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.1007/978-3-319-24574-4_28</pub-id>
      <fpage>234</fpage>
      <lpage>241</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jabri_space-time_2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Jabri</surname><given-names>Allan</given-names></name>
        <name><surname>Owens</surname><given-names>Andrew</given-names></name>
        <name><surname>Efros</surname><given-names>Alexei A.</given-names></name>
      </person-group>
      <article-title>Space-time correspondence as a contrastive random walk</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020-12">2020</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-09-08">2023</year><month>09</month><day>08</day></date-in-citation>
      <uri>http://arxiv.org/abs/2006.14613</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.14613</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-decaux_semi-automatic_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Decaux</surname><given-names>Nathan</given-names></name>
        <name><surname>Conze</surname><given-names>Pierre-Henri</given-names></name>
        <name><surname>Ropars</surname><given-names>Juliette</given-names></name>
        <name><surname>He</surname><given-names>Xinyan</given-names></name>
        <name><surname>Sheehan</surname><given-names>Frances T.</given-names></name>
        <name><surname>Pons</surname><given-names>Christelle</given-names></name>
        <name><surname>Ben Salem</surname><given-names>Douraied</given-names></name>
        <name><surname>Brochard</surname><given-names>Sylvain</given-names></name>
        <name><surname>Rousseau</surname><given-names>François</given-names></name>
      </person-group>
      <article-title>Semi-automatic muscle segmentation in MR images using deep registration-based label propagation</article-title>
      <source>Pattern Recognition</source>
      <year iso-8601-date="2023-08">2023</year><month>08</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-10-16">2023</year><month>10</month><day>16</day></date-in-citation>
      <volume>140</volume>
      <issn>0031-3203</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0031320323002297</uri>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2023.109529</pub-id>
      <fpage>109529</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-conze2020healthy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Conze</surname><given-names>Pierre-Henri</given-names></name>
        <name><surname>Brochard</surname><given-names>Sylvain</given-names></name>
        <name><surname>Burdin</surname><given-names>Valérie</given-names></name>
        <name><surname>Sheehan</surname><given-names>Frances T</given-names></name>
        <name><surname>Pons</surname><given-names>Christelle</given-names></name>
      </person-group>
      <article-title>Healthy versus pathological learning transferability in shoulder muscle MRI segmentation using deep convolutional encoder-decoders</article-title>
      <source>Computerized Medical Imaging and Graphics</source>
      <year iso-8601-date="2020">2020</year>
      <volume>83</volume>
      <pub-id pub-id-type="doi">10.1016/j.compmedimag.2020.101733</pub-id>
      <fpage>101733</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vadineanu_analysis_2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vădineanu</surname><given-names>Şerban</given-names></name>
        <name><surname>Pelt</surname><given-names>Daniël Maria</given-names></name>
        <name><surname>Dzyubachyk</surname><given-names>Oleh</given-names></name>
        <name><surname>Batenburg</surname><given-names>Kees Joost</given-names></name>
      </person-group>
      <article-title>An analysis of the impact of annotation errors on the accuracy of deep learning for cell segmentation</article-title>
      <source>Proceedings of the 5th international conference on medical imaging with deep learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-12-15">2023</year><month>12</month><day>15</day></date-in-citation>
      <uri>https://proceedings.mlr.press/v172/vadineanu22a.html</uri>
      <fpage>1251</fpage>
      <lpage>1267</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sakinis2019interactive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sakinis</surname><given-names>Tomas</given-names></name>
        <name><surname>Milletari</surname><given-names>Fausto</given-names></name>
        <name><surname>Roth</surname><given-names>Holger</given-names></name>
        <name><surname>Korfiatis</surname><given-names>Panagiotis</given-names></name>
        <name><surname>Kostandy</surname><given-names>Petro</given-names></name>
        <name><surname>Philbrick</surname><given-names>Kenneth</given-names></name>
        <name><surname>Akkus</surname><given-names>Zeynettin</given-names></name>
        <name><surname>Xu</surname><given-names>Ziyue</given-names></name>
        <name><surname>Xu</surname><given-names>Daguang</given-names></name>
        <name><surname>Erickson</surname><given-names>Bradley J</given-names></name>
      </person-group>
      <article-title>Interactive segmentation of medical images through fully convolutional neural networks</article-title>
      <source>arXiv preprint arXiv:1903.08205</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1903.08205</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zhang2021interactive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Jian</given-names></name>
        <name><surname>Shi</surname><given-names>Yinghuan</given-names></name>
        <name><surname>Sun</surname><given-names>Jinquan</given-names></name>
        <name><surname>Wang</surname><given-names>Lei</given-names></name>
        <name><surname>Zhou</surname><given-names>Luping</given-names></name>
        <name><surname>Gao</surname><given-names>Yang</given-names></name>
        <name><surname>Shen</surname><given-names>Dinggang</given-names></name>
      </person-group>
      <article-title>Interactive medical image segmentation via a point-based interaction</article-title>
      <source>Artificial Intelligence in Medicine</source>
      <year iso-8601-date="2021">2021</year>
      <volume>111</volume>
      <pub-id pub-id-type="doi">10.1016/j.artmed.2020.101998</pub-id>
      <fpage>101998</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lee_scribble2label_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Hyeonsoo</given-names></name>
        <name><surname>Jeong</surname><given-names>Won-Ki</given-names></name>
      </person-group>
      <article-title>Scribble2Label: Scribble-supervised cell segmentation via self-generating pseudo-labels with consistency</article-title>
      <source>arXiv:2006.12890 [cs]</source>
      <year iso-8601-date="2020-06">2020</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-11-25">2020</year><month>11</month><day>25</day></date-in-citation>
      <uri>http://arxiv.org/abs/2006.12890</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.12890</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chanti_ifss-net_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chanti</surname><given-names>Dawood Al</given-names></name>
        <name><surname>Duque</surname><given-names>Vanessa Gonzalez</given-names></name>
        <name><surname>Crouzier</surname><given-names>Marion</given-names></name>
        <name><surname>Nordez</surname><given-names>Antoine</given-names></name>
        <name><surname>Lacourpaille</surname><given-names>Lilian</given-names></name>
        <name><surname>Mateus</surname><given-names>Diana</given-names></name>
      </person-group>
      <article-title>IFSS-net: Interactive few-shot siamese network for faster muscle segmentation and propagation in volumetric ultrasound</article-title>
      <source>IEEE Transactions on Medical Imaging</source>
      <year iso-8601-date="2021-10">2021</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-01-06">2022</year><month>01</month><day>06</day></date-in-citation>
      <volume>40</volume>
      <issue>10</issue>
      <issn>0278-0062</issn>
      <uri>http://arxiv.org/abs/2011.13246</uri>
      <pub-id pub-id-type="doi">10.1109/TMI.2021.3069150</pub-id>
      <fpage>2615</fpage>
      <lpage>2628</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ogier2017individual">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ogier</surname><given-names>Augustin</given-names></name>
        <name><surname>Sdika</surname><given-names>Michael</given-names></name>
        <name><surname>Foure</surname><given-names>Alexandre</given-names></name>
        <name><surname>Le Troter</surname><given-names>Arnaud</given-names></name>
        <name><surname>Bendahan</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Individual muscle segmentation in MR images: A 3D propagation through 2D non-linear registration approaches</article-title>
      <source>International conference of the IEEE engineering in medicine and biology society</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.1109/EMBC.2017.8036826</pub-id>
      <fpage>317</fpage>
      <lpage>320</lpage>
    </element-citation>
  </ref>
  <ref id="ref-balakrishnan2019voxelmorph">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Balakrishnan</surname><given-names>Guha</given-names></name>
        <name><surname>Zhao</surname><given-names>Amy</given-names></name>
        <name><surname>Sabuncu</surname><given-names>Mert R</given-names></name>
        <name><surname>Guttag</surname><given-names>John</given-names></name>
        <name><surname>Dalca</surname><given-names>Adrian V</given-names></name>
      </person-group>
      <article-title>VoxelMorph: A learning framework for deformable medical image registration</article-title>
      <source>IEEE Transactions on Medical Imaging</source>
      <year iso-8601-date="2019">2019</year>
      <volume>38</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1109/TMI.2019.2897538</pub-id>
      <fpage>1788</fpage>
      <lpage>1800</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
