<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20220719T163512-58bbb3598ee368598d2bb26ecc42ee6d8059a03c</doi_batch_id>
    <timestamp>20220719163512</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2022</year>
        </publication_date>
        <journal_volume>
          <volume>7</volume>
        </journal_volume>
        <issue>75</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>swyft: Truncated Marginal Neural Ratio Estimation in
Python</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Benjamin Kurt</given_name>
            <surname>Miller</surname>
            <ORCID>https://orcid.org/0000-0003-0387-8727</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Alex</given_name>
            <surname>Cole</surname>
            <ORCID>https://orcid.org/0000-0001-8035-4308</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Christoph</given_name>
            <surname>Weniger</surname>
            <ORCID>https://orcid.org/0000-0001-7579-8684</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Francesco</given_name>
            <surname>Nattino</surname>
            <ORCID>https://orcid.org/0000-0003-3286-0139</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Ou</given_name>
            <surname>Ku</surname>
            <ORCID>https://orcid.org/0000-0002-5373-5209</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Meiert W.</given_name>
            <surname>Grootes</surname>
            <ORCID>https://orcid.org/0000-0002-5733-4795</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>19</day>
          <year>2022</year>
        </publication_date>
        <pages>
          <first_page>4205</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.04205</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.6412465</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/4205</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.04205</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.04205</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.04205.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="pytorch">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc,
E. Fox, &amp; R. Garnett (Eds.), Advances in neural information
processing systems 32 (pp. 8024–8035). Curran Associates, Inc.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
          <citation key="harris2020array">
            <article_title>Array programming with NumPy</article_title>
            <author>Harris</author>
            <journal_title>Nature</journal_title>
            <issue>7825</issue>
            <volume>585</volume>
            <doi>10.1038/s41586-020-2649-2</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Harris, C. R., Millman, K. J., Walt,
S. J. van der, Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E.,
Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S.,
Kerkwijk, M. H. van, Brett, M., Haldane, A., R’ıo, J. F. del, Wiebe, M.,
Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy.
Nature, 585(7825), 357–362.
https://doi.org/10.1038/s41586-020-2649-2</unstructured_citation>
          </citation>
          <citation key="2020SciPy-NMeth">
            <article_title>SciPy 1.0: Fundamental Algorithms for
Scientific Computing in Python</article_title>
            <author>Virtanen</author>
            <journal_title>Nature Methods</journal_title>
            <volume>17</volume>
            <doi>10.1038/s41592-019-0686-2</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Virtanen, P., Gommers, R., Oliphant,
T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson,
P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson,
J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R.,
Larson, E., … SciPy 1.0 Contributors. (2020). SciPy 1.0: Fundamental
Algorithms for Scientific Computing in Python. Nature Methods, 17,
261–272.
https://doi.org/10.1038/s41592-019-0686-2</unstructured_citation>
          </citation>
          <citation key="Waskom2021">
            <article_title>Seaborn: Statistical data
visualization</article_title>
            <author>Waskom</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>60</issue>
            <volume>6</volume>
            <doi>10.21105/joss.03021</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Waskom, M. L. (2021). Seaborn:
Statistical data visualization. Journal of Open Source Software, 6(60),
3021. https://doi.org/10.21105/joss.03021</unstructured_citation>
          </citation>
          <citation key="Hunter:2007">
            <article_title>Matplotlib: A 2D graphics
environment</article_title>
            <author>Hunter</author>
            <journal_title>Computing in Science &amp;
Engineering</journal_title>
            <issue>3</issue>
            <volume>9</volume>
            <doi>10.1109/MCSE.2007.55</doi>
            <cYear>2007</cYear>
            <unstructured_citation>Hunter, J. D. (2007). Matplotlib: A
2D graphics environment. Computing in Science &amp; Engineering, 9(3),
90–95. https://doi.org/10.1109/MCSE.2007.55</unstructured_citation>
          </citation>
          <citation key="jupyter">
            <article_title>Jupyter notebooks - a publishing format for
reproducible computational workflows</article_title>
            <author>Kluyver</author>
            <journal_title>Positioning and power in academic publishing:
Players, agents and agendas</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Kluyver, T., Ragan-Kelley, B., Pérez,
F., Granger, B., Bussonnier, M., Frederic, J., Kelley, K., Hamrick, J.,
Grout, J., Corlay, S., Ivanov, P., Avila, D., Abdalla, S., Willing, C.,
&amp; team, J. development. (2016). Jupyter notebooks - a publishing
format for reproducible computational workflows. In F. Loizides &amp; B.
Scmidt (Eds.), Positioning and power in academic publishing: Players,
agents and agendas (pp. 87–90). IOS Press.
https://eprints.soton.ac.uk/403913/</unstructured_citation>
          </citation>
          <citation key="reback2020pandas">
            <article_title>Pandas-dev/pandas</article_title>
            <author>Reback</author>
            <doi>10.5281/zenodo.3509134</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Reback, J., jbrockmendel, McKinney,
W., Bossche, J. V. den, Augspurger, T., Cloud, P., Hawkins, S.,
Roeschke, M., gfyoung, Sinhrks, &amp; al., et. (2021).
Pandas-dev/pandas.
https://doi.org/10.5281/zenodo.3509134</unstructured_citation>
          </citation>
          <citation key="mckinney-proc-scipy-2010">
            <article_title>Data Structures for Statistical Computing in
Python</article_title>
            <author>McKinney</author>
            <journal_title>Proceedings of the 9th Python in Science
Conference</journal_title>
            <doi>10.25080/Majora-92bf1922-00a</doi>
            <cYear>2010</cYear>
            <unstructured_citation>McKinney, Wes. (2010). Data
Structures for Statistical Computing in Python. In Stéfan van der Walt
&amp; Jarrod Millman (Eds.), Proceedings of the 9th Python in Science
Conference (pp. 56–61).
https://doi.org/10.25080/Majora-92bf1922-00a</unstructured_citation>
          </citation>
          <citation key="dask">
            <volume_title>Dask: Library for dynamic task
scheduling</volume_title>
            <author>Dask Development Team</author>
            <cYear>2016</cYear>
            <unstructured_citation>Dask Development Team. (2016). Dask:
Library for dynamic task scheduling.
https://dask.org</unstructured_citation>
          </citation>
          <citation key="zarr">
            <article_title>Zarr-developers/zarr-python:</article_title>
            <author>Miles</author>
            <doi>10.5281/zenodo.5712786</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Miles, A., jakirkham, Bussonnier, M.,
Moore, J., Fulton, A., Bourbeau, J., Onalan, T., Hamman, J., Patel, Z.,
Rocklin, M., &amp; al., et. (2021). Zarr-developers/zarr-python:
https://doi.org/10.5281/zenodo.5712786</unstructured_citation>
          </citation>
          <citation key="miller2021truncated">
            <article_title>Truncated marginal neural ratio
estimation</article_title>
            <author>Miller</author>
            <journal_title>Advances in Neural Information Processing
Systems</journal_title>
            <volume>34</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Miller, B. K., Cole, A., Forré, P.,
Louppe, G., &amp; Weniger, C. (2021). Truncated marginal neural ratio
estimation. Advances in Neural Information Processing Systems,
34.</unstructured_citation>
          </citation>
          <citation key="swyft">
            <article_title>Simulation-efficient marginal posterior
estimation with swyft: Stop wasting your precious time</article_title>
            <author>Miller</author>
            <journal_title>Machine Learning and the Physical Sciences:
Workshop at the 34th Conference on Neural Information Processing Systems
(NeurIPS)</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Miller, B. K., Cole, A., Louppe, G.,
&amp; Weniger, C. (2020). Simulation-efficient marginal posterior
estimation with swyft: Stop wasting your precious time. Machine Learning
and the Physical Sciences: Workshop at the 34th Conference on Neural
Information Processing Systems (NeurIPS).</unstructured_citation>
          </citation>
          <citation key="batista2021eucapt">
            <article_title>EuCAPT white paper: Opportunities and
challenges for theoretical astroparticle physics in the next
decade</article_title>
            <author>Batista</author>
            <journal_title>arXiv preprint
arXiv:2110.10074</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Batista, R. A., Amin, M., Barenboim,
G., Bartolo, N., Baumann, D., Bauswein, A., Bellini, E., Benisty, D.,
Bertone, G., Blasi, P., &amp; others. (2021). EuCAPT white paper:
Opportunities and challenges for theoretical astroparticle physics in
the next decade. arXiv Preprint
arXiv:2110.10074.</unstructured_citation>
          </citation>
          <citation key="coogan2020targeted">
            <article_title>Targeted likelihood-free inference of dark
matter substructure in strongly-lensed galaxies</article_title>
            <author>Coogan</author>
            <journal_title>Machine Learning and the Physical Sciences:
Workshop at the 34th Conference on Neural Information Processing Systems
(NeurIPS)</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Coogan, A., Karchev, K., &amp;
Weniger, C. (2020). Targeted likelihood-free inference of dark matter
substructure in strongly-lensed galaxies. Machine Learning and the
Physical Sciences: Workshop at the 34th Conference on Neural Information
Processing Systems (NeurIPS).</unstructured_citation>
          </citation>
          <citation key="cole2021fast">
            <article_title>Fast and credible likelihood-free cosmology
with truncated marginal neural ratio estimation</article_title>
            <author>Cole</author>
            <journal_title>arXiv preprint
arXiv:2111.08030</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Cole, A., Miller, B. K., Witte, S.
J., Cai, M. X., Grootes, M. W., Nattino, F., &amp; Weniger, C. (2021).
Fast and credible likelihood-free cosmology with truncated marginal
neural ratio estimation. arXiv Preprint
arXiv:2111.08030.</unstructured_citation>
          </citation>
          <citation key="sbibm">
            <article_title>Benchmarking simulation-based
inference</article_title>
            <author>Lueckmann</author>
            <journal_title>Proceedings of the 24th international
conference on artificial intelligence and statistics</journal_title>
            <volume>130</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Lueckmann, J.-M., Boelts, J.,
Greenberg, D., Goncalves, P., &amp; Macke, J. (2021). Benchmarking
simulation-based inference. In A. Banerjee &amp; K. Fukumizu (Eds.),
Proceedings of the 24th international conference on artificial
intelligence and statistics (Vol. 130, pp. 343–351). PMLR.
http://proceedings.mlr.press/v130/lueckmann21a.html</unstructured_citation>
          </citation>
          <citation key="hermans2021averting">
            <article_title>Averting a crisis in simulation-based
inference</article_title>
            <author>Hermans</author>
            <journal_title>arXiv preprint
arXiv:2110.06581</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Hermans, J., Delaunoy, A., Rozet, F.,
Wehenkel, A., &amp; Louppe, G. (2021). Averting a crisis in
simulation-based inference. arXiv Preprint
arXiv:2110.06581.</unstructured_citation>
          </citation>
          <citation key="Cranmer2020">
            <article_title>The frontier of simulation-based
inference</article_title>
            <author>Cranmer</author>
            <journal_title>Proc. Natl. Acad. Sci. U. S.
A.</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Cranmer, K., Brehmer, J., &amp;
Louppe, G. (2020). The frontier of simulation-based inference. Proc.
Natl. Acad. Sci. U. S. A.</unstructured_citation>
          </citation>
          <citation key="sbi">
            <article_title>Sbi: A toolkit for simulation-based
inference</article_title>
            <author>Tejero-Cantero</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>52</issue>
            <volume>5</volume>
            <doi>10.21105/joss.02505</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Tejero-Cantero, A., Boelts, J.,
Deistler, M., Lueckmann, J.-M., Durkan, C., Gonçalves, P. J., Greenberg,
D. S., &amp; Macke, J. H. (2020). Sbi: A toolkit for simulation-based
inference. Journal of Open Source Software, 5(52), 2505.
https://doi.org/10.21105/joss.02505</unstructured_citation>
          </citation>
          <citation key="elfi2018">
            <article_title>ELFI: Engine for likelihood-free
inference</article_title>
            <author>Lintusaari</author>
            <journal_title>The Journal of Machine Learning
Research</journal_title>
            <issue>1</issue>
            <volume>19</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Lintusaari, J., Vuollekoski, H.,
Kangasrääsiö, A., Skytén, K., Järvenpää, M., Marttinen, P., Gutmann, M.
U., Vehtari, A., Corander, J., &amp; Kaski, S. (2018). ELFI: Engine for
likelihood-free inference. The Journal of Machine Learning Research,
19(1), 643–649.</unstructured_citation>
          </citation>
          <citation key="hypothesis-repo">
            <article_title>Hypothesis</article_title>
            <author>Hermans</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Hermans, J. (2019). Hypothesis. In
GitHub repository. https://github.com/montefiore-ai/hypothesis;
GitHub.</unstructured_citation>
          </citation>
          <citation key="pydelfi-repo">
            <article_title>pydelfi: Density estimation likelihood-free
inference</article_title>
            <author>Alsing</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Alsing, J. (2019). pydelfi: Density
estimation likelihood-free inference. In GitHub repository.
https://github.com/justinalsing/pydelfi; GitHub.</unstructured_citation>
          </citation>
          <citation key="louppe2016">
            <article_title>carl: A likelihood-free inference
toolbox</article_title>
            <author>Louppe</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>1</issue>
            <volume>1</volume>
            <doi>10.21105/joss.00011</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Louppe, G., Cranmer, K., &amp; Pavez,
J. (2016). carl: A likelihood-free inference toolbox. Journal of Open
Source Software, 1(1), 11.
https://doi.org/10.21105/joss.00011</unstructured_citation>
          </citation>
          <citation key="Klinger2018">
            <article_title>pyABC: Distributed, likelihood-free
inference</article_title>
            <author>Klinger</author>
            <journal_title>Bioinformatics</journal_title>
            <issue>20</issue>
            <volume>34</volume>
            <doi>10.1093/bioinformatics/bty361</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Klinger, E., Rickert, D., &amp;
Hasenauer, J. (2018). pyABC: Distributed, likelihood-free inference.
Bioinformatics, 34(20), 3591–3593.
https://doi.org/10.1093/bioinformatics/bty361</unstructured_citation>
          </citation>
          <citation key="dutta2017">
            <article_title>ABCpy: A user-friendly, extensible, and
parallel library for approximate bayesian computation</article_title>
            <author>Dutta</author>
            <journal_title>Proceedings of the platform for advanced
scientific computing conference</journal_title>
            <doi>10.1145/3093172.3093233</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Dutta, R., Schoengens, M., Onnela,
J.-P., &amp; Mira, A. (2017). ABCpy: A user-friendly, extensible, and
parallel library for approximate bayesian computation. Proceedings of
the Platform for Advanced Scientific Computing Conference, 8:1–8:9.
https://doi.org/10.1145/3093172.3093233</unstructured_citation>
          </citation>
          <citation key="greenberg2019automatic">
            <article_title>Automatic posterior transformation for
likelihood-free inference</article_title>
            <author>Greenberg</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Greenberg, D., Nonnenmacher, M.,
&amp; Macke, J. (2019). Automatic posterior transformation for
likelihood-free inference. International Conference on Machine Learning,
2404–2414.</unstructured_citation>
          </citation>
          <citation key="rozet2021arbitrary">
            <article_title>Arbitrary marginal neural ratio estimation
for simulation-based inference</article_title>
            <author>Rozet</author>
            <journal_title>Machine Learning and the Physical Sciences:
Workshop at the 35th Conference on Neural Information Processing Systems
(NeurIPS)</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Rozet, F., &amp; Louppe, G. (2021).
Arbitrary marginal neural ratio estimation for simulation-based
inference. Machine Learning and the Physical Sciences: Workshop at the
35th Conference on Neural Information Processing Systems
(NeurIPS).</unstructured_citation>
          </citation>
          <citation key="Hermans2019">
            <article_title>Likelihood-free mcmc with amortized
approximate ratio estimators</article_title>
            <author>Hermans</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Hermans, J., Begy, V., &amp; Louppe,
G. (2020). Likelihood-free mcmc with amortized approximate ratio
estimators. International Conference on Machine Learning,
4239–4248.</unstructured_citation>
          </citation>
          <citation key="papamakarios2019sequential">
            <article_title>Sequential neural likelihood: Fast
likelihood-free inference with autoregressive flows</article_title>
            <author>Papamakarios</author>
            <journal_title>The 22nd international conference on
artificial intelligence and statistics</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Papamakarios, G., Sterratt, D., &amp;
Murray, I. (2019). Sequential neural likelihood: Fast likelihood-free
inference with autoregressive flows. The 22nd International Conference
on Artificial Intelligence and Statistics,
837–848.</unstructured_citation>
          </citation>
          <citation key="Durkan2020">
            <article_title>On contrastive learning for likelihood-free
inference</article_title>
            <author>Durkan</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Durkan, C., Murray, I., &amp;
Papamakarios, G. (2020). On contrastive learning for likelihood-free
inference. International Conference on Machine Learning,
2771–2781.</unstructured_citation>
          </citation>
          <citation key="epsilon_free">
            <article_title>Fast \epsilon-free inference of simulation
models with bayesian conditional density estimation</article_title>
            <author>Papamakarios</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>29</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Papamakarios, G., &amp; Murray, I.
(2016). Fast \epsilon-free inference of simulation models with bayesian
conditional density estimation. In D. Lee, M. Sugiyama, U. Luxburg, I.
Guyon, &amp; R. Garnett (Eds.), Advances in neural information
processing systems (Vol. 29). Curran Associates, Inc.
https://proceedings.neurips.cc/paper/2016/file/6aca97005c68f1206823815f66102863-Paper.pdf</unstructured_citation>
          </citation>
          <citation key="lueckmann2017flexible">
            <article_title>Flexible statistical inference for
mechanistic models of neural dynamics</article_title>
            <author>Lueckmann</author>
            <journal_title>Proceedings of the 31st international
conference on neural information processing systems</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Lueckmann, J.-M., Gonçalves, P. J.,
Bassetto, G., Öcal, K., Nonnenmacher, M., &amp; Macke, J. H. (2017).
Flexible statistical inference for mechanistic models of neural
dynamics. Proceedings of the 31st International Conference on Neural
Information Processing Systems, 1289–1299.</unstructured_citation>
          </citation>
          <citation key="lueckmann2019likelihood">
            <article_title>Likelihood-free inference with emulator
networks</article_title>
            <author>Lueckmann</author>
            <journal_title>Symposium on advances in approximate bayesian
inference</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Lueckmann, J.-M., Bassetto, G.,
Karaletsos, T., &amp; Macke, J. H. (2019). Likelihood-free inference
with emulator networks. Symposium on Advances in Approximate Bayesian
Inference, 32–53.</unstructured_citation>
          </citation>
          <citation key="alsing2019fast">
            <article_title>Fast likelihood-free cosmology with neural
density estimators and active learning</article_title>
            <author>Alsing</author>
            <journal_title>Monthly Notices of the Royal Astronomical
Society</journal_title>
            <issue>3</issue>
            <volume>488</volume>
            <doi>10.1093/mnras/stz1960</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Alsing, J., Charnock, T., Feeney, S.,
&amp; Wandelt, B. (2019). Fast likelihood-free cosmology with neural
density estimators and active learning. Monthly Notices of the Royal
Astronomical Society, 488(3), 4440–4458.
https://doi.org/10.1093/mnras/stz1960</unstructured_citation>
          </citation>
          <citation key="alsing2018massive">
            <article_title>Massive optimal data compression and density
estimation for scalable, likelihood-free inference in
cosmology</article_title>
            <author>Alsing</author>
            <journal_title>Monthly Notices of the Royal Astronomical
Society</journal_title>
            <issue>3</issue>
            <volume>477</volume>
            <doi>10.1093/mnras/sty819</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Alsing, J., Wandelt, B., &amp;
Feeney, S. (2018). Massive optimal data compression and density
estimation for scalable, likelihood-free inference in cosmology. Monthly
Notices of the Royal Astronomical Society, 477(3), 2874–2885.
https://doi.org/10.1093/mnras/sty819</unstructured_citation>
          </citation>
          <citation key="alsing2019nuisance">
            <article_title>Nuisance hardened data compression for fast
likelihood-free inference</article_title>
            <author>Alsing</author>
            <journal_title>Monthly Notices of the Royal Astronomical
Society</journal_title>
            <issue>4</issue>
            <volume>488</volume>
            <doi>10.1093/mnras/stz1900</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Alsing, J., &amp; Wandelt, B. (2019).
Nuisance hardened data compression for fast likelihood-free inference.
Monthly Notices of the Royal Astronomical Society, 488(4), 5093–5103.
https://doi.org/10.1093/mnras/stz1900</unstructured_citation>
          </citation>
          <citation key="gutmann2016bayesian">
            <article_title>Bayesian optimization for likelihood-free
inference of simulator-based statistical models</article_title>
            <author>Gutmann</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Gutmann, M. U., Corander, J., &amp;
others. (2016). Bayesian optimization for likelihood-free inference of
simulator-based statistical models. Journal of Machine Learning
Research.</unstructured_citation>
          </citation>
          <citation key="pham2014note">
            <article_title>A note on approximating ABC-MCMC using
flexible classifiers</article_title>
            <author>Pham</author>
            <journal_title>Stat</journal_title>
            <issue>1</issue>
            <volume>3</volume>
            <doi>10.1002/sta4.56</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Pham, K. C., Nott, D. J., &amp;
Chaudhuri, S. (2014). A note on approximating ABC-MCMC using flexible
classifiers. Stat, 3(1), 218–227.
https://doi.org/10.1002/sta4.56</unstructured_citation>
          </citation>
          <citation key="thomas2016likelihood">
            <article_title>Likelihood-free inference by ratio
estimation</article_title>
            <author>Thomas</author>
            <journal_title>Bayesian Analysis</journal_title>
            <doi>10.1214/20-ba1238</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Thomas, O., Dutta, R., Corander, J.,
Kaski, S., Gutmann, M. U., &amp; others. (2016). Likelihood-free
inference by ratio estimation. Bayesian Analysis.
https://doi.org/10.1214/20-ba1238</unstructured_citation>
          </citation>
          <citation key="gutmann2018likelihood">
            <article_title>Likelihood-free inference via
classification</article_title>
            <author>Gutmann</author>
            <journal_title>Statistics and Computing</journal_title>
            <issue>2</issue>
            <volume>28</volume>
            <doi>10.1007/s11222-017-9738-6</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Gutmann, M. U., Dutta, R., Kaski, S.,
&amp; Corander, J. (2018). Likelihood-free inference via classification.
Statistics and Computing, 28(2), 411–425.
https://doi.org/10.1007/s11222-017-9738-6</unstructured_citation>
          </citation>
          <citation key="Cranmer2015">
            <article_title>Approximating likelihood ratios with
calibrated discriminative classifiers</article_title>
            <author>Cranmer</author>
            <journal_title>arXiv preprint
arXiv:1506.02169</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Cranmer, K., Pavez, J., &amp; Louppe,
G. (2015). Approximating likelihood ratios with calibrated
discriminative classifiers. arXiv Preprint
arXiv:1506.02169.</unstructured_citation>
          </citation>
          <citation key="sisson2018handbook">
            <volume_title>Handbook of approximate bayesian
computation</volume_title>
            <author>Sisson</author>
            <cYear>2018</cYear>
            <unstructured_citation>Sisson, S. A., Fan, Y., &amp;
Beaumont, M. (2018). Handbook of approximate bayesian computation. CRC
Press.</unstructured_citation>
          </citation>
          <citation key="karabatsos2018approximate">
            <article_title>An approximate likelihood perspective on ABC
methods</article_title>
            <author>Karabatsos</author>
            <journal_title>Statistics Surveys</journal_title>
            <volume>12</volume>
            <doi>10.1214/18-ss120</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Karabatsos, G., &amp; Leisen, F.
(2018). An approximate likelihood perspective on ABC methods. Statistics
Surveys, 12, 66–104.
https://doi.org/10.1214/18-ss120</unstructured_citation>
          </citation>
          <citation key="Toni2009-fd">
            <article_title>Approximate bayesian computation scheme for
parameter inference and model selection in dynamical
systems</article_title>
            <author>Toni</author>
            <journal_title>J. R. Soc. Interface</journal_title>
            <issue>31</issue>
            <volume>6</volume>
            <doi>10.1098/rsif.2008.0172</doi>
            <cYear>2009</cYear>
            <unstructured_citation>Toni, T., Welch, D., Strelkowa, N.,
Ipsen, A., &amp; Stumpf, M. P. H. (2009). Approximate bayesian
computation scheme for parameter inference and model selection in
dynamical systems. J. R. Soc. Interface, 6(31), 187–202.
https://doi.org/10.1098/rsif.2008.0172</unstructured_citation>
          </citation>
          <citation key="Beaumont2009-gl">
            <article_title>Adaptive approximate bayesian
computation</article_title>
            <author>Beaumont</author>
            <journal_title>Biometrika</journal_title>
            <issue>4</issue>
            <volume>96</volume>
            <cYear>2009</cYear>
            <unstructured_citation>Beaumont, M. A., Cornuet, J.-M.,
Marin, J.-M., &amp; Robert, C. P. (2009). Adaptive approximate bayesian
computation. Biometrika, 96(4), 983–990.</unstructured_citation>
          </citation>
          <citation key="diggle1984monte">
            <article_title>Monte carlo methods of inference for implicit
statistical models</article_title>
            <author>Diggle</author>
            <journal_title>Journal of the Royal Statistical Society:
Series B (Methodological)</journal_title>
            <issue>2</issue>
            <volume>46</volume>
            <doi>10.1111/j.2517-6161.1984.tb01290.x</doi>
            <cYear>1984</cYear>
            <unstructured_citation>Diggle, P. J., &amp; Gratton, R. J.
(1984). Monte carlo methods of inference for implicit statistical
models. Journal of the Royal Statistical Society: Series B
(Methodological), 46(2), 193–212.
https://doi.org/10.1111/j.2517-6161.1984.tb01290.x</unstructured_citation>
          </citation>
          <citation key="first_abc">
            <article_title>Bayesianly Justifiable and Relevant Frequency
Calculations for the Applied Statistician</article_title>
            <author>Rubin</author>
            <journal_title>The Annals of Statistics</journal_title>
            <issue>4</issue>
            <volume>12</volume>
            <doi>10.1214/aos/1176346785</doi>
            <cYear>1984</cYear>
            <unstructured_citation>Rubin, D. B. (1984). Bayesianly
Justifiable and Relevant Frequency Calculations for the Applied
Statistician. The Annals of Statistics, 12(4), 1151–1172.
https://doi.org/10.1214/aos/1176346785</unstructured_citation>
          </citation>
          <citation key="second_abc">
            <article_title>Inferring Coalescence Times From DNA Sequence
Data</article_title>
            <author>Tavaré</author>
            <journal_title>Genetics</journal_title>
            <issue>2</issue>
            <volume>145</volume>
            <doi>10.1093/genetics/145.2.505</doi>
            <issn>1943-2631</issn>
            <cYear>1997</cYear>
            <unstructured_citation>Tavaré, S., Balding, D. J.,
Griffiths, R. C., &amp; Donnelly, P. (1997). Inferring Coalescence Times
From DNA Sequence Data. Genetics, 145(2), 505–518.
https://doi.org/10.1093/genetics/145.2.505</unstructured_citation>
          </citation>
          <citation key="blum2010non">
            <article_title>Non-linear regression models for approximate
bayesian computation</article_title>
            <author>Blum</author>
            <journal_title>Statistics and computing</journal_title>
            <issue>1</issue>
            <volume>20</volume>
            <doi>10.1007/s11222-009-9116-0</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Blum, M. G., &amp; François, O.
(2010). Non-linear regression models for approximate bayesian
computation. Statistics and Computing, 20(1), 63–73.
https://doi.org/10.1007/s11222-009-9116-0</unstructured_citation>
          </citation>
          <citation key="metropolis">
            <article_title>Equation of state calculations by fast
computing machines</article_title>
            <author>Metropolis</author>
            <journal_title>J. Chem. Phys.</journal_title>
            <issue>6</issue>
            <volume>21</volume>
            <doi>10.2172/4390578</doi>
            <cYear>1953</cYear>
            <unstructured_citation>Metropolis, N., Rosenbluth, A. W.,
Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of
state calculations by fast computing machines. J. Chem. Phys., 21(6),
1087–1092. https://doi.org/10.2172/4390578</unstructured_citation>
          </citation>
          <citation key="hastings">
            <article_title>Monte carlo sampling methods using markov
chains and their applications</article_title>
            <author>Hastings</author>
            <journal_title>Biometrika</journal_title>
            <issue>1</issue>
            <volume>57</volume>
            <doi>10.1093/biomet/57.1.97</doi>
            <cYear>1970</cYear>
            <unstructured_citation>Hastings, W. K. (1970). Monte carlo
sampling methods using markov chains and their applications. Biometrika,
57(1), 97–109.
https://doi.org/10.1093/biomet/57.1.97</unstructured_citation>
          </citation>
          <citation key="gabbard2022bayesian">
            <article_title>Bayesian parameter estimation using
conditional variational autoencoders for gravitational-wave
astronomy</article_title>
            <author>Gabbard</author>
            <journal_title>Nature Physics</journal_title>
            <issue>1</issue>
            <volume>18</volume>
            <doi>10.1038/s41567-021-01425-7</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Gabbard, H., Messenger, C., Heng, I.
S., Tonolini, F., &amp; Murray-Smith, R. (2022). Bayesian parameter
estimation using conditional variational autoencoders for
gravitational-wave astronomy. Nature Physics, 18(1), 112–117.
https://doi.org/10.1038/s41567-021-01425-7</unstructured_citation>
          </citation>
          <citation key="dax2021real">
            <article_title>Real-time gravitational wave science with
neural posterior estimation</article_title>
            <author>Dax</author>
            <journal_title>Physical review letters</journal_title>
            <issue>24</issue>
            <volume>127</volume>
            <doi>10.1103/PhysRevLett.127.241103</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Dax, M., Green, S. R., Gair, J.,
Macke, J. H., Buonanno, A., &amp; Schölkopf, B. (2021). Real-time
gravitational wave science with neural posterior estimation. Physical
Review Letters, 127(24), 241103.
https://doi.org/10.1103/PhysRevLett.127.241103</unstructured_citation>
          </citation>
          <citation key="chua2020learning">
            <article_title>Learning bayesian posteriors with neural
networks for gravitational-wave inference</article_title>
            <author>Chua</author>
            <journal_title>Physical review letters</journal_title>
            <issue>4</issue>
            <volume>124</volume>
            <doi>10.1103/PhysRevLett.124.041102</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Chua, A. J., &amp; Vallisneri, M.
(2020). Learning bayesian posteriors with neural networks for
gravitational-wave inference. Physical Review Letters, 124(4), 041102.
https://doi.org/10.1103/PhysRevLett.124.041102</unstructured_citation>
          </citation>
          <citation key="delaunoy2020lightning">
            <article_title>Lightning-fast gravitational wave parameter
inference through neural amortization</article_title>
            <author>Delaunoy</author>
            <journal_title>Machine Learning and the Physical Sciences:
Workshop at the 34th Conference on Neural Information Processing Systems
(NeurIPS)</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Delaunoy, A., Wehenkel, A., Hinderer,
T., Nissanke, S., Weniger, C., Williamson, A. R., &amp; Louppe, G.
(2020). Lightning-fast gravitational wave parameter inference through
neural amortization. Machine Learning and the Physical Sciences:
Workshop at the 34th Conference on Neural Information Processing Systems
(NeurIPS).</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
