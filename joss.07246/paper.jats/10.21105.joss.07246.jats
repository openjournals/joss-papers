<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7246</article-id>
<article-id pub-id-type="doi">10.21105/joss.07246</article-id>
<title-group>
<article-title>PySLSQP: A transparent Python package for the SLSQP
optimization algorithm modernized with utilities for visualization and
post-processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-7704-2532</contrib-id>
<name>
<surname>Joshy</surname>
<given-names>Anugrah Jo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hwang</surname>
<given-names>John T.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Mechanical and Aerospace Engineering,
University of California San Diego, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-08-06">
<day>6</day>
<month>8</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>103</issue>
<fpage>7246</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>SLSQP</kwd>
<kwd>optimization</kwd>
<kwd>optimizer</kwd>
<kwd>algorithm</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Nonlinear programming (NLP) addresses optimization problems
  involving nonlinear objective and/or constraint functions defined over
  continuous optimization variables. These functions are assumed to be
  smooth, i.e., continuously differentiable. Nonlinear programming has
  applications ranging from aircraft design in engineering to optimizing
  portfolios in finance and training models in machine learning.
  Sequential Quadratic Programming (SQP) is one of the most successful
  classes of algorithms for solving NLP problems. It solves an NLP
  problem by iteratively formulating and solving a sequence of Quadratic
  Programming (QP) subproblems. The Sequential Least SQuares Programming
  algorithm, or SLSQP, has been one of the most widely used SQP
  algorithms since the 1980s.</p>
  <p>We present <monospace>PySLSQP</monospace>, a seamless interface for
  using the SLSQP algorithm from Python that wraps the original Fortran
  code sourced from the SciPy repository and provides a host of new
  features to improve the research utility of the original algorithm.
  <monospace>PySLSQP</monospace> uses a simple yet modern workflow for
  compiling and using Fortran code from Python. This allows even
  beginner developers to easily modify the algorithm in Fortran for
  their specific needs and use, in Python, the wrapper auto-generated by
  the workflow.</p>
  <p>Some of the additional features offered by
  <monospace>PySLSQP</monospace> include auto-generation of unavailable
  derivatives using finite differences, independent scaling of the
  problem variables and functions, access to internal optimization data,
  live-visualization, saving optimization data from each iteration,
  warm/hot restarting of optimization, and various other utilities for
  post-processing.</p>
  <p><monospace>PySLSQP</monospace> solves the general nonlinear
  programming problem:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \begin{array}{rlr}
  \underset{x \in \mathbb{R}^n}{\text{minimize}} & \; \; f(x) & \\
  \text{subject to} & \begin{array}{ll}
                        c_i(x) = 0, &\quad i = 1,...,m_{eq} \\
                        c_i(x) \geq 0, &\quad i = m_{eq}+1,...,m \\
                        l_i \leq x_i \leq u_i, &\quad i = 1,...,n
                      \end{array}
  \end{array}
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:munder><mml:mtext mathvariant="normal">minimize</mml:mtext><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:munder></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mspace width="0.278em"></mml:mspace><mml:mspace width="0.278em"></mml:mspace><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mtext mathvariant="normal">subject to</mml:mtext></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtable><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mspace width="1.0em"></mml:mspace><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚â•</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mspace width="1.0em"></mml:mspace><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>l</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚â§</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>‚â§</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mspace width="1.0em"></mml:mspace><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[x \in \mathbb{R}^n]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is the vector of optimization variables,
  <inline-formula><alternatives>
  <tex-math><![CDATA[f: \mathbb{R}^n \to \mathbb{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>‚Üí</mml:mo><mml:mi>‚Ñù</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is the objective function, <inline-formula><alternatives>
  <tex-math><![CDATA[c: \mathbb{R}^n \to \mathbb{R}^m]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>c</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>‚Üí</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is the vector-valued constraint function, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[l]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>l</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[u]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>u</mml:mi></mml:math></alternatives></inline-formula>
  are the vectors containing the lower and upper bounds for the
  optimization variables, respectively. The first
  <inline-formula><alternatives>
  <tex-math><![CDATA[m_{eq}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  constraints are equalities while the remaining
  <inline-formula><alternatives>
  <tex-math><![CDATA[(m - m_{eq})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>m</mml:mi><mml:mo>‚àí</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  constraints are inequalities.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The original SLSQP algorithm
  (<xref alt="Kraft, 1988" rid="ref-kraft1988software" ref-type="bibr">Kraft,
  1988</xref>,
  <xref alt="1994" rid="ref-kraft1994algorithm" ref-type="bibr">1994</xref>),
  implemented in Fortran by Dieter Kraft, has been incorporated into
  several software packages for optimization across different
  programming languages. However, the algorithm itself has undergone
  only minimal improvements and has not kept pace with advancements in
  programming languages that could enhance its utility. In contrast,
  other SQP algorithms, such as SNOPT
  (<xref alt="Gill et al., 2005" rid="ref-gill2005snopt" ref-type="bibr">Gill
  et al., 2005</xref>), which also began development around the same
  time as SLSQP, have seen continuous improvements. SNOPT has evolved
  significantly through both algorithmic enhancements and feature
  additions, becoming one of the leading algorithms for nonlinear
  programming.</p>
  <p>The SLSQP algorithm available in most modern packages is
  implemented as a black-box function that takes the optimization
  functions and their derivatives and then outputs the optimized
  results. These packages do not provide users with any options for
  tuning the original algorithm or for assessing the progress of an
  ongoing optimization. This lack of transparency becomes a significant
  disadvantage for problems with expensive optimization functions or
  derivatives. Users might have to wait for hours, only to be informed
  at the end of the optimization procedure that the algorithm could not
  converge. Several such experiences with multiple research applications
  in the authors‚Äô lab were the primary motivation behind developing the
  new <monospace>PySLSQP</monospace> package.</p>
  <p>Despite the lack of timely updates to the core algorithm and
  usability improvements, SLSQP continues to be widely used in research
  primarily due to its open-source nature and the availability of
  convenient installation options through packages such as SciPy
  (<xref alt="Virtanen et al., 2020" rid="ref-virtanen2020scipy" ref-type="bibr">Virtanen
  et al., 2020</xref>). Many optimization practitioners use SLSQP for
  solving medium-sized optimization problems with up to a hundred
  optimization variables and constraints. Additionally, SLSQP is more
  successful compared to some of the most advanced algorithms in solving
  certain classes of optimization problems, such as optimal control
  problems with a coarse discretization in time.</p>
  <fig>
    <caption><p>Performance comparison for an optimal control
    problem.</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="starship_20_jax-objective-cb.pdf" />
  </fig>
  <p>Figure 1 above compares the convergence behaviors of
  <monospace>PySLSQP</monospace> and some of the most advanced
  algorithms in nonlinear programming on a coarsely discretized optimal
  control problem. The problem aims to compute the optimal control
  parameters for a spacecraft landing scenario. The total number of
  function evaluations is indicated within parentheses in the legend. We
  see that <monospace>PySLSQP</monospace> is the only algorithm that
  solves the problem within the 200 function evaluation limit. Among the
  algorithms that failed to converge are SNOPT, TrustConstr, and IPOPT.
  SNOPT is a commercial SQP algorithm, while TrustConstr and IPOPT are
  open-source Interior Point (IP) algorithms. Although IPOPT appears to
  have converged in the plot, the solution returned by IPOPT does not
  satisfy the feasibility criteria. This underscores the relevance of
  SLSQP even today among state-of-the-art optimization algorithms. This
  problem is taken from the suite of examples in the modOpt
  (<xref alt="Joshy &amp; Hwang, 2024" rid="ref-joshy2024modopt" ref-type="bibr">Joshy
  &amp; Hwang, 2024</xref>) optimization library.</p>
  <p>There are several optimization libraries in Python that include the
  SLSQP algorithm, such as SciPy
  (<xref alt="Virtanen et al., 2020" rid="ref-virtanen2020scipy" ref-type="bibr">Virtanen
  et al., 2020</xref>), NLopt
  (<xref alt="Johnson, 2024" rid="ref-NLopt" ref-type="bibr">Johnson,
  2024</xref>), and pyOpt
  (<xref alt="Perez et al., 2012" rid="ref-perez2012pyopt" ref-type="bibr">Perez
  et al., 2012</xref>). NLopt and pyOpt require users to compile the
  Fortran code, which greatly deters the majority of users from
  utilizing SLSQP from these libraries. pyOptSparse
  (<xref alt="Wu et al., 2020" rid="ref-wu2020pyoptsparse" ref-type="bibr">Wu
  et al., 2020</xref>) is a fork of the pyOpt package that supports
  sparse constraint Jacobians and includes additional optimization
  utilities for scaling, visualization, and storing optimization
  history. Most SLSQP users access it through SciPy, which offers
  precompiled libraries that can be easily installed from PyPI by
  running <monospace>pip install scipy</monospace>. However, like other
  libraries, the SLSQP implementation in SciPy also operates as a
  black-box providing limited visibility into the progress of
  optimization or access to internal variables during optimization
  iterations. This lack of transparency can be a drawback, particularly
  for users needing more insight into the optimization process.</p>
  <p><monospace>PySLSQP</monospace> is developed to overcome these
  limitations by:</p>
  <list list-type="bullet">
    <list-item>
      <p>providing a precompiled package through PyPI that can be simply
      installed with <monospace>pip install pyslsqp</monospace>,</p>
    </list-item>
    <list-item>
      <p>offering access to internal optimization variables at each
      iteration through a save file, and</p>
    </list-item>
    <list-item>
      <p>informing users about the progress of optimization through a
      live-updated summary file and visualization.</p>
    </list-item>
  </list>
  <p>The Python wrapper for <monospace>PySLSQP</monospace> is generated
  by a simple workflow automated on GitHub, which allows even beginner
  developers to tune the Fortran code for their specific application and
  extend the current codebase. Offering Python-level access to internal
  optimization variables such as optimality and feasibility measures,
  Lagrange multipliers, etc. enables further analysis of an ongoing or
  completed optimization. <monospace>PySLSQP</monospace> also features
  additional utilities for numerical differentiation, scaling, warm/hot
  restarting, and post-processing.</p>
  <p>By addressing the current limitations and providing new
  capabilities, <monospace>PySLSQP</monospace> enhances the transparency
  and usability of the SLSQP algorithm, making it a more powerful and
  user-friendly tool for solving nonlinear programming problems.
  <monospace>PySLSQP</monospace> is now integrated with the modOpt
  (<xref alt="Joshy &amp; Hwang, 2024" rid="ref-joshy2024modopt" ref-type="bibr">Joshy
  &amp; Hwang, 2024</xref>) library of optimizers, through which it has
  successfully solved problems in aircraft design, spacecraft optimal
  control, and swimming robot design.</p>
</sec>
<sec id="software-features">
  <title>Software features</title>
  <sec id="numerical-differentiation">
    <title>Numerical differentiation</title>
    <p>In the absence of user-supplied first-order derivatives of the
    objective or constraint functions, <monospace>PySLSQP</monospace>
    estimates them using first-order finite differencing. Users have the
    option to set the absolute or relative step size for the finite
    differences. However, it is generally more efficient for users to
    provide the exact gradients, if possible, since each finite
    difference estimation requires <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{O}(n)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    objective or constraint evaluations. Moreover, finite difference
    approximations are susceptible to subtractive cancellation
    errors.</p>
  </sec>
  <sec id="problem-scaling">
    <title>Problem scaling</title>
    <p>Scaling of the variables and functions is crucial for the
    convergence of optimization algorithms. Poor scaling often leads to
    unsuccessful or extremely slow optimization.
    <monospace>PySLSQP</monospace> enables users to scale the
    optimization variables, objective, and constraints individually,
    independent of the user-defined optimization functions.
    <monospace>PySLSQP</monospace> automatically scales the variable
    bounds and derivatives according to the user-specified scaling for
    the variables and functions. This allows the user-defined initial
    guess, bounds, functions, and derivatives to remain the same each
    time an optimization is run with a different scaling.</p>
  </sec>
  <sec id="live-visualization">
    <title>Live visualization</title>
    <p>Optimization becomes slow for problems with functions or
    derivatives that are costly to evaluate. In such scenarios, it is
    important for users to be able to monitor the optimization process
    to ensure that it is proceeding smoothly.
    <monospace>PySLSQP</monospace> offers the capability to visualize
    the optimization progress in real-time. This feature allows users to
    track convergence through optimality and feasibility measures, and
    to understand how the optimization variables, objective,
    constraints, Lagrange multipliers, and derivatives evolve during the
    optimization. An example of a visualization generated by
    <monospace>PySLSQP</monospace>, corresponding to the optimal control
    problem discussed earlier, is shown in Figure 2 below.</p>
    <fig>
      <caption><p>Live visualization of the objective and
      feasibility.</p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="slsqp_plot.pdf" />
    </fig>
  </sec>
  <sec id="access-to-internal-optimization-data">
    <title>Access to internal optimization data</title>
    <p>In addition to live visualization, <monospace>PySLSQP</monospace>
    provides real-time access to optimization data through dynamically
    updated summary and save files. <monospace>PySLSQP</monospace>
    generates a summary file that contains a table that is updated at
    the end of every major iteration. This summary table lists the
    values of different scalar variables in the algorithm to keep users
    informed about the current state of optimization.</p>
    <p>Users can specify which variables to save in the save file and
    whether they should be saved for every iteration or only for major
    iterations. The save file is valuable for analyzing optimization
    progress, post-processing, or performing warm/hot restarts. It can
    store all internal optimization variables - including optimization
    variables, objective, constraints, objective gradient, constraint
    Jacobian, optimality, feasibility, Lagrange multipliers, and line
    search step sizes - facilitating advanced analysis of the
    optimization problem. <monospace>PySLSQP</monospace> provides
    various utilities for working with data from save files, including
    functions for loading and visualizing variables.</p>
    <p>To the best of our knowledge, <monospace>PySLSQP</monospace> is
    the only Python interface to the SLSQP algorithm that provides this
    level of access to internal optimization information.</p>
  </sec>
  <sec id="warmhot-starting">
    <title>Warm/Hot starting</title>
    <p>Re-running an optimization that was terminated prematurely can be
    inefficient and wasteful. For example, if a user desires higher
    accuracy than was achieved in a previous run, they would need to
    re-execute the optimization with a smaller accuracy parameter.
    Similarly, if an optimization terminates upon reaching the iteration
    limit before achieving the required accuracy, a rerun with a higher
    limit is necessary to complete the process. Such repeated runs not
    only consume additional computational resources but also extend the
    overall time required to achieve the desired results.</p>
    <p>To address these scenarios, <monospace>PySLSQP</monospace> offers
    two options for users to efficiently restart an optimization using
    data from saved files: warm starting and hot starting. In
    <monospace>PySLSQP</monospace>, <italic>warm starting</italic>
    refers to restarting a previously run optimization using the most
    recent value of the optimization variables
    <inline-formula><alternatives>
    <tex-math><![CDATA[x]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
    from a saved file. During a warm start, the initial guess
    <inline-formula><alternatives>
    <tex-math><![CDATA[x_0]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
    provided by the user is replaced with the last optimization variable
    iterate available in the saved file.</p>
    <p><italic>Hot starting</italic> in <monospace>PySLSQP</monospace>
    involves re-running a previously completed optimization by reusing
    the function (objective and constraints) and derivative values from
    a saved file. This method is particularly advantageous when the
    functions and/or their derivatives are costly to evaluate. A
    significant benefit of hot starting over warm starting is that the
    BFGS Hessians approximated by the SLSQP algorithm in a hot-start
    will follow the same path as in the previous optimization, while
    also saving the cost of function and derivative evaluations. In
    contrast, during a warm start, although the algorithm starts from
    the previous solution <inline-formula><alternatives>
    <tex-math><![CDATA[x^*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>,
    the Hessian is initialized as the identity matrix, which may
    necessitate more iterations to achieve convergence.</p>
  </sec>
  <sec id="ease-of-extension">
    <title>Ease of extension</title>
    <p><monospace>PySLSQP</monospace> is implemented in Python and
    leverages NumPy‚Äôs <italic>f2py</italic> and the
    <italic>Meson</italic> build system for compiling and interfacing
    the underlying Fortran code with Python. The Python setup script
    automates the build process, making it straightforward for
    developers to build, install, test, and use
    <monospace>PySLSQP</monospace> after modifying the Fortran code. The
    package includes GitHub workflows to automatically generate
    precompiled binaries in the cloud for different system architectures
    using PyPA‚Äôs <italic>cibuildwheel</italic> tool. These automated
    workflows ensure that <monospace>PySLSQP</monospace> remains
    accessible to a broad range of users by providing consistent and
    reliable installation across various platforms using Linux, macOS,
    and Windows operating systems. Additionally, this approach allows
    developers to focus on enhancing the core algorithm and features
    without the overhead of managing complex build environments, thus
    fostering an open-source community that can contribute effectively
    to the development and improvement of the SLSQP algorithm.</p>
  </sec>
</sec>
<sec id="a-simple-example">
  <title>A simple example</title>
  <p>In this section, we solve a simple optimization problem to
  illustrate some of the features explained above. In the standard SLSQP
  problem format presented in the <italic>Summary</italic>, the problem
  is </p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\begin{aligned}
  \underset{x \in \mathbb{R}^2}{\text{minimize}} \quad & x_1^2 + x_2^2\\
  \text{subject to} \quad & x_1 + x_2 - 1 = 0, \\
  & 3x_1 + 2x_2 - 1 \geq 0,  \\
  \text{with} \quad & m_{eq} = 1, \; l = [0.4, -\infty]^T, \; \text{and} \; u = [+\infty, 0.6]^T.
  \end{aligned}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:munder><mml:mtext mathvariant="normal">minimize</mml:mtext><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:munder><mml:mspace width="1.0em"></mml:mspace></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:msubsup><mml:mi>x</mml:mi><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mtext mathvariant="normal">subject to</mml:mtext><mml:mspace width="1.0em"></mml:mspace></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mn>3</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn><mml:mo>‚â•</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mtext mathvariant="normal">with</mml:mtext><mml:mspace width="1.0em"></mml:mspace></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="0.278em"></mml:mspace><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0.4</mml:mn><mml:mo>,</mml:mo><mml:mi>‚àí</mml:mi><mml:mi>‚àû</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>,</mml:mo><mml:mspace width="0.278em"></mml:mspace><mml:mtext mathvariant="normal">and</mml:mtext><mml:mspace width="0.278em"></mml:mspace><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>+</mml:mi><mml:mi>‚àû</mml:mi><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
  </p>
  <p>We begin by importing <monospace>numpy</monospace> and defining the
  optimization functions. We will only define the derivatives for the
  constraints and let <monospace>PySLSQP</monospace> approximate the
  derivatives for the objective function. We then define the constants
  for the optimization, which include the variable bounds, number of
  equality constraints, initial guess, and scaling factors.</p>
  <code language="python">import numpy as np

def objective(x):
    return x[0]**2 + x[1]**2

def constraints(x):
    return  np.array([x[0] + x[1] - 1, 3*x[0] + 2*x[1] - 1])

def jacobian(x):
    return np.array([[1, 1], [3, 2]])

# Variable bounds
x_lower = np.array([0.4, -np.inf])
x_upper = np.array([np.inf, 0.6])

# Number of equality constraints
m_eq = 1

# Initial guess
x0 = np.array([2,3])

# Scaling factors
x_s = 10.0
o_s =  2.0
c_s = np.array([1., 0.5])</code>
  <p>Most of the features in <monospace>PySLSQP</monospace> are accessed
  through the <monospace>optimize</monospace> function. We now import
  <monospace>optimize</monospace> and solve the problem by calling it
  with the functions and constants defined above. When calling
  <monospace>optimize</monospace>, we will define the absolute step size
  for the finite difference approximation of the objective gradient.
  Additionally, we instruct <monospace>PySLSQP</monospace> to save the
  optimization variables <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  and the objective value <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  from each major iteration to a file named
  <monospace>save_file.hdf5</monospace>. Lastly, we configure the
  arguments to live-visualize the objective
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  and the variable <inline-formula><alternatives>
  <tex-math><![CDATA[x_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  during the optimization.</p>
  <code language="python">from pyslsqp import optimize

results = optimize(x0, obj=objective, con=constraints, jac=jacobian, 
                   meq=m_eq, xl=x_lower, xu=x_upper, finite_diff_abs_step=1e-6,
                   x_scaler=x_s, obj_scaler=o_s, con_scaler=c_s,
                   save_itr='major', save_vars=['majiter', 'x', 'objective'],
                   save_filename=&quot;save_file.hdf5&quot;,
                   visualize=True, visualize_vars=['objective', 'x[0]'])

# Print the returned results dictionary
print(results)</code>
  <p>Once <monospace>optimize</monospace> is executed, a summary of the
  optimization will be printed to the console. The function also returns
  a dictionary that contains the results of the optimization. By
  default, <monospace>PySLSQP</monospace> writes the summary of major
  iterations to a file named
  <monospace>slsqp_summary.out</monospace>.</p>
  <p>For additional usage guidelines, API reference, and installation
  instructions, please consult the
  <ext-link ext-link-type="uri" xlink:href="https://pyslsqp.readthedocs.io/">documentation</ext-link>.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was supported by NASA under award No.¬†80NSSC23M0217.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-kraft1988software">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kraft</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>A software package for sequential quadratic programming</article-title>
      <source>Forschungsbericht- Deutsche Forschungs- und Versuchsanstalt fur Luft- und Raumfahrt</source>
      <year iso-8601-date="1988">1988</year>
    </element-citation>
  </ref>
  <ref id="ref-kraft1994algorithm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kraft</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>Algorithm 733: TOMP‚ÄìFortran modules for optimal control calculations</article-title>
      <source>ACM Transactions on Mathematical Software (TOMS)</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="1994">1994</year>
      <volume>20</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1145/192115.192124</pub-id>
      <fpage>262</fpage>
      <lpage>281</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gill2005snopt">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gill</surname><given-names>Philip E</given-names></name>
        <name><surname>Murray</surname><given-names>Walter</given-names></name>
        <name><surname>Saunders</surname><given-names>Michael A</given-names></name>
      </person-group>
      <article-title>SNOPT: An SQP algorithm for large-scale constrained optimization</article-title>
      <source>SIAM review</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2005">2005</year>
      <volume>47</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1137/S0036144504446096</pub-id>
      <fpage>99</fpage>
      <lpage>131</lpage>
    </element-citation>
  </ref>
  <ref id="ref-virtanen2020scipy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in Python</article-title>
      <source>Nature methods</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wu2020pyoptsparse">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Neil</given-names></name>
        <name><surname>Kenway</surname><given-names>Gaetan</given-names></name>
        <name><surname>Mader</surname><given-names>Charles A</given-names></name>
        <name><surname>Jasa</surname><given-names>John</given-names></name>
        <name><surname>Martins</surname><given-names>Joaquim RRA</given-names></name>
      </person-group>
      <article-title>pyOptSparse: A Python framework for large-scale constrained nonlinear optimization of sparse systems</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>54</issue>
      <pub-id pub-id-type="doi">10.21105/joss.02564</pub-id>
      <fpage>2564</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-perez2012pyopt">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Perez</surname><given-names>Ruben E</given-names></name>
        <name><surname>Jansen</surname><given-names>Peter W</given-names></name>
        <name><surname>Martins</surname><given-names>Joaquim RRA</given-names></name>
      </person-group>
      <article-title>pyOpt: A Python-based object-oriented framework for nonlinear constrained optimization</article-title>
      <source>Structural and Multidisciplinary Optimization</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>45</volume>
      <pub-id pub-id-type="doi">10.1007/s00158-011-0666-3</pub-id>
      <fpage>101</fpage>
      <lpage>118</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NLopt">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Johnson</surname><given-names>Steven G.</given-names></name>
      </person-group>
      <article-title>The NLopt nonlinear-optimization package</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>http://github.com/stevengj/nlopt</uri>
    </element-citation>
  </ref>
  <ref id="ref-joshy2024modopt">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Joshy</surname><given-names>Anugrah Jo</given-names></name>
        <name><surname>Hwang</surname><given-names>John T</given-names></name>
      </person-group>
      <article-title>modOpt: A modular development environment and library for optimization algorithms</article-title>
      <source>arXiv preprint arXiv:2410.12942</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2410.12942</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
