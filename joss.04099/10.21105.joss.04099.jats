<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4099</article-id>
<article-id pub-id-type="doi">10.21105/joss.04099</article-id>
<title-group>
<article-title>OfflineMOT: A Python package for multiple objects
detection and tracking from bird view stationary drone
videos</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5282-7259</contrib-id>
<name>
<surname>Yousif</surname>
<given-names>Yasin Maan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4665-1194</contrib-id>
<name>
<surname>Mukbil</surname>
<given-names>Awad</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7533-3852</contrib-id>
<name>
<surname>Müller</surname>
<given-names>Jörg P.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Institut für Informatik, Technische Universität Clausthal
38678, Clausthal-Zellerfeld, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-11-28">
<day>28</day>
<month>11</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>74</issue>
<fpage>4099</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Multiple objects tracking</kwd>
<kwd>Traffic trajectories</kwd>
<kwd>Objects detection</kwd>
<kwd>Drone video analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The topic of multiple objects tracking (MOT) is still considered an
  open research area
  (<xref alt="Dendorfer et al., 2020" rid="ref-MOTChallenge20" ref-type="bibr">Dendorfer
  et al., 2020</xref>). Among the many available methods for this
  problem, it is worth mentioning <italic>Deep Sort</italic>
  (<xref alt="Wojke et al., 2017" rid="ref-wojke2017simple" ref-type="bibr">Wojke
  et al., 2017</xref>), where a detection and tracking steps are done in
  real time and for different types of scenes and areas (for example for
  pedestrians movement or for vehicles tracking). Another
  state-of-the-art method is <italic>Tracktor</italic>
  (<xref alt="Bergmann et al., 2019" rid="ref-bergmann2019tracking" ref-type="bibr">Bergmann
  et al., 2019</xref>), where tracking is done by repetitive detections
  on all the frames in the video.</p>
  <p>The importance of the problem comes from its many applications, for
  example self-driving cars software, traffic analysis, or general
  surveillance applications. Unfortunately, due to the variety of scenes
  and contexts, and due to the time constraints that are needed for some
  applications, there is no one general solution capable of working
  perfectly for all cases. For example, between the two cases of a
  moving camera recording side view road traffic, and a drone camera
  recording from above, there are many different challenges that should
  be addressed for each case. Developing one method for both cases will
  make this method less effective for addressing each case problems
  alone.</p>
  <p><monospace>OfflineMOT</monospace>, the Python package introduced
  here, tries to provide a solution to a more restricted problem, namely
  bird’s eye stationary videos without real-time condition. It applies
  mainly three techniques for detecting and tracking on three different
  priority levels (from lowest to highest):</p>
  <list list-type="bullet">
    <list-item>
      <p>The first level is <bold>background subtraction</bold>. It is a
      fast method to find which pixels have changed in the image (the
      foreground), and this is possible because of the stationary
      condition. Otherwise, if the drone’s camera is moving freely, then
      the background will be less learnable. Another problem here is the
      subtle movement of the drone due to wind and control noise. To
      solve this, a program for fixing the view is implemented. It is
      based on matching the background features with every next frame,
      and then transforming the frame if a big movement is detected.</p>
    </list-item>
    <list-item>
      <p>The second level is <bold>multi-objects tracking</bold> methods
      such as <italic>kernelized correlation filters</italic> (KCF)
      (<xref alt="Henriques et al., 2014" rid="ref-henriques2014high" ref-type="bibr">Henriques
      et al., 2014</xref>). This method takes the output of the
      detection and the next frame and it gives the next positions of
      the objects. It can also return the failure or success states of
      tracking.</p>
    </list-item>
    <list-item>
      <p>The third level is the <bold>deep learning-based detection and
      classification</bold> of the objects. Here,
      <italic>Yolo-v4</italic>
      (<xref alt="Alexey Bochkovskiy, 2020" rid="ref-yolov4" ref-type="bibr">Alexey
      Bochkovskiy, 2020</xref>) is used as a model structure where
      training is done separately. The used code to implement, load and
      train Yolo structure is taken from Tianxiaomo
      (<xref alt="2020" rid="ref-Tianxiaomo" ref-type="bibr">2020</xref>).</p>
    </list-item>
  </list>
  <p>Finally, these three parts are implemented separately in the code,
  making it easy to enable and disable any, with many tunable
  parameters. This is done on purpose to facilitate the processing on
  any new video with different settings by changing only a few
  parameters.</p>
  <p>The pseudo code in
  <xref alt="Figure 1" rid="figU003Aworkflow">Figure 1</xref>
  illustrates the main workflow implemented in this project.</p>
  <fig>
    <caption><p>The general workflow of the
    method.<styled-content id="figU003Aworkflow"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/workflow.PNG" xlink:title="" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The specific case for extracting trajectories from top view,
  stationary traffic videos (for pedestrians, cyclists, and vehicles)
  lacks targeted open source solutions in the literature. Therefore, the
  development of this package is directed towards helping researchers in
  the field of traffic analysis or any other fields where trajectories
  of moving objects are needed.</p>
  <p>With the help of this package, the extraction of trajectories from
  a cyclists’ behaviour dataset in TU Clausthal will be done. The
  package has proved its ability of producing very accurate results for
  different scenes and conditions in these videos. The dataset itself
  will be published later.</p>
</sec>
<sec id="parameters-tuning-procedure">
  <title>Parameters Tuning Procedure</title>
  <p>In order to run the program on a new video, optimally all the
  parameters should be tuned for all the tracking and detection modules,
  starting from the basic operations of general settings and background
  subtraction and ending with detection and post processing
  operations.</p>
  <p>All these parameters can be changed, and saved through the command
  line. If a suitable set of parameters are found, then it can be saved
  for later usage to a <monospace>.ini</monospace> file.</p>
  <code language="python">cfg = offlinemot.config.configs() # load previous set by passing its file.
cfg['detect_every_N'] = 5</code>
  <p>In the following table, the most important parameters are listed
  along with how to tune them for a new video.</p>
  <fig>
    <caption><p>Important parameters to tune in
    <monospace>config.py</monospace>
    <styled-content id="tableU003Aparameters"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/table.PNG" xlink:title="" />
  </fig>
</sec>
<sec id="scope">
  <title>Scope</title>
  <p>The scope of the problems that can be handled by this package is
  defined by the following conditions:</p>
  <list list-type="order">
    <list-item>
      <p><italic>The video is stationary</italic></p>
    </list-item>
    <list-item>
      <p><italic>The real time performance is not a
      priority</italic></p>
    </list-item>
    <list-item>
      <p><italic>The view direction of the video is from a bird’s eye
      view</italic></p>
    </list-item>
    <list-item>
      <p><italic>A pretrained detection model for the objects of
      interest is available</italic></p>
    </list-item>
  </list>
  <p>Regarding the last point, the model provided with the package is
  trained on random images of cyclists, cars and pedestrians from bird’s
  eye view. This can be enough if the problem is the same, i.e.,
  tracking traffic entities. Otherwise, this model could be a good
  starting point to train for other kinds of objects if these objects
  are similar and Yolo-v4 is used as a model structure.</p>
  <sec id="failure-cases">
    <title>Failure Cases</title>
    <p>If the video is too noisy, has low resolution, or the training
    dataset detection is very different from the video background and
    the objects, then errors in tracking can happen.</p>
    <p>As an example, the sample video has some problems with one moving
    object, because of the different background and the new scene of the
    video. This can be avoided by retraining the detection part (Yolo
    network) on similar examples. Additionally, a thorough tuning step
    for the parameters with the <monospace>configs</monospace> class
    should be done to eliminate possible errors in the result.</p>
    <sec id="acknowledgment">
      <title>Acknowledgment</title>
      <p>This work was supported by the German Academic Exchange Service
      (DAAD) under the Graduate School Scholarship Programme (GSSP).The
      training of Yolo network and labelling the datasets was done by
      Merlin Korth and Sakif Hossain.</p>
    </sec>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-MOTChallenge20">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dendorfer</surname><given-names>P.</given-names></name>
        <name><surname>Rezatofighi</surname><given-names>H.</given-names></name>
        <name><surname>Milan</surname><given-names>A.</given-names></name>
        <name><surname>Shi</surname><given-names>J.</given-names></name>
        <name><surname>Cremers</surname><given-names>D.</given-names></name>
        <name><surname>Reid</surname><given-names>I.</given-names></name>
        <name><surname>Roth</surname><given-names>S.</given-names></name>
        <name><surname>Schindler</surname><given-names>K.</given-names></name>
        <name><surname>Leal-Taixé</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>MOT20: A benchmark for multi object tracking in crowded scenes</article-title>
      <source>arXiv:2003.09003[cs]</source>
      <year iso-8601-date="2020-03">2020</year><month>03</month>
      <uri>http://arxiv.org/abs/1906.04567</uri>
    </element-citation>
  </ref>
  <ref id="ref-bergmann2019tracking">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bergmann</surname><given-names>Philipp</given-names></name>
        <name><surname>Meinhardt</surname><given-names>Tim</given-names></name>
        <name><surname>Leal-Taixe</surname><given-names>Laura</given-names></name>
      </person-group>
      <article-title>Tracking without bells and whistles</article-title>
      <source>Proceedings of the IEEE/CVF international conference on computer vision</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1109/iccv.2019.00103</pub-id>
      <fpage>941</fpage>
      <lpage>951</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wojke2017simple">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wojke</surname><given-names>Nicolai</given-names></name>
        <name><surname>Bewley</surname><given-names>Alex</given-names></name>
        <name><surname>Paulus</surname><given-names>Dietrich</given-names></name>
      </person-group>
      <article-title>Simple online and realtime tracking with a deep association metric</article-title>
      <source>2017 IEEE international conference on image processing (ICIP)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.1109/icip.2017.8296962</pub-id>
      <fpage>3645</fpage>
      <lpage>3649</lpage>
    </element-citation>
  </ref>
  <ref id="ref-henriques2014high">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Henriques</surname><given-names>João F</given-names></name>
        <name><surname>Caseiro</surname><given-names>Rui</given-names></name>
        <name><surname>Martins</surname><given-names>Pedro</given-names></name>
        <name><surname>Batista</surname><given-names>Jorge</given-names></name>
      </person-group>
      <article-title>High-speed tracking with kernelized correlation filters</article-title>
      <source>IEEE transactions on pattern analysis and machine intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>37</volume>
      <issue>3</issue>
      <fpage>583</fpage>
      <lpage>596</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yolov4">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alexey Bochkovskiy</surname><given-names>Hong-Yuan Mark Liao</given-names><suffix>Chien-Yao Wang</suffix></name>
      </person-group>
      <article-title>YOLOv4: YOLOv4: Optimal speed and accuracy of object detection</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-Tianxiaomo">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tianxiaomo</surname></name>
      </person-group>
      <article-title>Pytorch yolo v4</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/Tianxiaomo/pytorch-YOLOv4</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
