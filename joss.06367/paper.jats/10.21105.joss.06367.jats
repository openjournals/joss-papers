<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6367</article-id>
<article-id pub-id-type="doi">10.21105/joss.06367</article-id>
<title-group>
<article-title>AMLTK: A Modular AutoML Toolkit in Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-4390-7614</contrib-id>
<name>
<surname>Bergman</surname>
<given-names>Edward</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9611-8588</contrib-id>
<name>
<surname>Feurer</surname>
<given-names>Matthias</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="corresp" rid="cor-2"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-8896-2863</contrib-id>
<name>
<surname>Bahram</surname>
<given-names>Aron</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6882-0051</contrib-id>
<name>
<surname>Balef</surname>
<given-names>Amir Rezaei</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-1181-0549</contrib-id>
<name>
<surname>Purucker</surname>
<given-names>Lennart</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-2966-266X</contrib-id>
<name>
<surname>Segel</surname>
<given-names>Sarah</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9675-3175</contrib-id>
<name>
<surname>Lindauer</surname>
<given-names>Marius</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
<xref ref-type="aff" rid="aff-6"/>
<xref ref-type="corresp" rid="cor-3"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2037-3694</contrib-id>
<name>
<surname>Hutter</surname>
<given-names>Frank</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-4"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0309-401X</contrib-id>
<name>
<surname>Eggensperger</surname>
<given-names>Katharina</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="corresp" rid="cor-5"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Freiburg, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>LMU Munich, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Munich Center for Machine Learning</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>University of Tübingen, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Leibniz University Hannover, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>L3S Research Center, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
<corresp id="cor-2">* E-mail: <email></email></corresp>
<corresp id="cor-3">* E-mail: <email></email></corresp>
<corresp id="cor-4">* E-mail: <email></email></corresp>
<corresp id="cor-5">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-12-06">
<day>6</day>
<month>12</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>100</issue>
<fpage>6367</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Machine Learning</kwd>
<kwd>AutoML</kwd>
<kwd>Hyperparameter Optimization</kwd>
<kwd>Modular</kwd>
<kwd>Data Science</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine Learning is a core building block in novel data-driven
  applications. Practitioners face many ambiguous design decisions while
  developing practical machine learning (ML) solutions. Automated
  machine learning (AutoML) facilitates the development of machine
  learning applications by providing efficient methods for optimizing
  hyperparameters, searching for neural architectures, or constructing
  whole ML pipelines
  (<xref alt="Hutter et al., 2019" rid="ref-hutter-book19a" ref-type="bibr">Hutter
  et al., 2019</xref>). Thereby, design decisions such as the choice of
  modelling, pre-processing, and training algorithm are crucial to
  obtaining well-performing solutions. By automatically obtaining ML
  solutions, AutoML aims to lower the barrier to leveraging machine
  learning and reduce the time needed to develop or adapt ML solutions
  for new domains or data.</p>
  <p>Highly performant software packages for automatically building ML
  pipelines given data, so-called AutoML systems, are available and can
  be used off-the-shelf. Typically, AutoML systems evaluate ML models
  sequentially to return a well-performing single best model or multiple
  models combined into an ensemble. Existing AutoML systems are
  typically highly engineered monolithic software developed for specific
  use cases to perform well and robustly under various conditions.</p>
  <p>With the growing amount of data and design decisions for ML, there
  is also a growing need to improve our understanding of the design
  decisions of AutoML systems. Current state-of-the-art systems vary in
  implemented paradigms (stacking
  (<xref alt="Erickson et al., 2020" rid="ref-erickson-arxiv20a" ref-type="bibr">Erickson
  et al., 2020</xref>;
  <xref alt="LeDell &amp; Poirier, 2020" rid="ref-ledell2020" ref-type="bibr">LeDell
  &amp; Poirier, 2020</xref>) vs CASH
  (<xref alt="Thornton et al., 2013" rid="ref-thornton-kdd13a" ref-type="bibr">Thornton
  et al., 2013</xref>), optimizing a pre-defined pipeline structure
  (<xref alt="Baudart et al., 2021" rid="ref-baudart2021" ref-type="bibr">Baudart
  et al., 2021</xref>;
  <xref alt="Thornton et al., 2013" rid="ref-thornton-kdd13a" ref-type="bibr">Thornton
  et al., 2013</xref>) vs evolving open-ended pipelines
  (<xref alt="Olson et al., 2016" rid="ref-olson-gecco16a" ref-type="bibr">Olson
  et al., 2016</xref>)) and also use different methods within one
  paradigm (i.e. Bayesian optimization
  (<xref alt="Feurer et al., 2015" rid="ref-feurer-nips15a" ref-type="bibr">Feurer
  et al., 2015</xref>;
  <xref alt="Thornton et al., 2013" rid="ref-thornton-kdd13a" ref-type="bibr">Thornton
  et al., 2013</xref>) or Genetic Programming
  (<xref alt="Gijsbers &amp; Vanschoren, 2019" rid="ref-gijsbers-joss19a" ref-type="bibr">Gijsbers
  &amp; Vanschoren, 2019</xref>;
  <xref alt="Olson et al., 2016" rid="ref-olson-gecco16a" ref-type="bibr">Olson
  et al., 2016</xref>) as the optimization algorithm, different search
  spaces for the same machine learning algorithm cf.
  (<xref alt="Feurer et al., 2015" rid="ref-feurer-nips15a" ref-type="bibr">Feurer
  et al., 2015</xref>;
  <xref alt="Gijsbers &amp; Vanschoren, 2019" rid="ref-gijsbers-joss19a" ref-type="bibr">Gijsbers
  &amp; Vanschoren, 2019</xref>;
  <xref alt="Olson et al., 2016" rid="ref-olson-gecco16a" ref-type="bibr">Olson
  et al., 2016</xref>;
  <xref alt="Thornton et al., 2013" rid="ref-thornton-kdd13a" ref-type="bibr">Thornton
  et al., 2013</xref>), different post-hoc ensemble methods or even no
  post-hoc ensembling at all cf.
  (<xref alt="Feurer et al., 2015" rid="ref-feurer-nips15a" ref-type="bibr">Feurer
  et al., 2015</xref>;
  <xref alt="Imrie et al., 2022" rid="ref-autoprognosis" ref-type="bibr">Imrie
  et al., 2022</xref>;
  <xref alt="Wang et al., 2021" rid="ref-wang2021flaml" ref-type="bibr">Wang
  et al., 2021</xref>)), raising many research questions and
  opportunities to study improved algorithms and novel applications.</p>
  <p>AMLTK (Automated Machine Learning ToolKit) is a collection of
  components that enable researchers and developers to easily implement
  AutoML systems without the need for common boilerplate code. AMLTK
  addresses this with a modular perspective on AutoML systems, aiming to
  cover various existing AutoML system paradigms in principle. It
  contributes to the field three-fold: (a) Enabling systematic
  comparison of AutoML design decisions with a higher level of
  reproducibility, (b) fast prototyping and evaluation of new AutoML
  methods, and (c) easy adaptation of developed solutions to new
  tasks.</p>
  <p>In addition, it also facilitates easy integration and swapping of
  components from various AutoML tools, for example, an optimizer from
  <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/SMAC3">SMAC</ext-link>
  (<xref alt="Lindauer et al., 2022" rid="ref-lindauer-jmlr22a" ref-type="bibr">Lindauer
  et al., 2022</xref>) or
  <ext-link ext-link-type="uri" xlink:href="https://github.com/optuna/optuna">Optuna</ext-link>
  (<xref alt="Akiba et al., 2019" rid="ref-akiba-kdd19a" ref-type="bibr">Akiba
  et al., 2019</xref>), a search space from
  <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/ConfigSpace">ConfigSpace</ext-link>
  (<xref alt="Lindauer et al., 2019" rid="ref-lindauer-arxiv19a" ref-type="bibr">Lindauer
  et al., 2019</xref>) or Optuna, as well as the integration with
  additional tools such as the visualization and analysis tool
  <ext-link ext-link-type="uri" xlink:href="https://github.com/automl/DeepCAVE">DeepCAVE</ext-link>
  (<xref alt="Sass et al., 2022" rid="ref-sass-realml22a" ref-type="bibr">Sass
  et al., 2022</xref>). These provided integrations are done without the
  need to modify AMLTK’s source code, enabling users to extend the
  framework as their needs require. Overall, AMLTK lowers the barrier to
  engaging with AutoML research and, thus, opens up the opportunity to
  bundle research efforts towards flexible and effective AutoML
  systems.</p>
  <p>AMLTK is designed for AutoML researchers to develop and study novel
  AutoML systems and domain experts to adapt these AutoML systems for
  novel use cases. AMLTK is based on the experience of a subset of the
  authors in developing AutoML systems (Auto-sklearn
  (<xref alt="Feurer et al., 2015" rid="ref-feurer-nips15a" ref-type="bibr">Feurer
  et al., 2015</xref>,
  <xref alt="2022" rid="ref-feurer-jmlr22a" ref-type="bibr">2022</xref>)
  and Auto-PyTorch
  (<xref alt="Zimmer et al., 2021" rid="ref-zimmer-tpami21a" ref-type="bibr">Zimmer
  et al., 2021</xref>)) and their effort to unify their code bases. Last
  but not least, we also believe that this toolkit will help educate
  students and support ML practitioners in engaging with AutoML
  systems.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Current AutoML systems are monolithic and provide little
  opportunity for customization. As a result, researchers often build
  new AutoML systems to implement a new methodology. This results in two
  issues: (1) it creates a barrier to research on AutoML systems, and
  (2) it hinders the fair comparison of new components in AutoML
  systems. Recent examples of open source AutoML systems are AutoGluon
  (<xref alt="Erickson et al., 2020" rid="ref-erickson-arxiv20a" ref-type="bibr">Erickson
  et al., 2020</xref>), GAMA
  (<xref alt="Gijsbers &amp; Vanschoren, 2021" rid="ref-gijsbers-kdd21a" ref-type="bibr">Gijsbers
  &amp; Vanschoren, 2021</xref>,
  <xref alt="2019" rid="ref-gijsbers-joss19a" ref-type="bibr">2019</xref>),
  and Auto-Sklearn
  (<xref alt="Feurer et al., 2015" rid="ref-feurer-nips15a" ref-type="bibr">Feurer
  et al., 2015</xref>,
  <xref alt="2022" rid="ref-feurer-jmlr22a" ref-type="bibr">2022</xref>).</p>
  <p>To give an example for Issue (1), a researcher working on new
  optimization methods for AutoML would need to develop all components
  of an AutoML system in order to evaluate their method because current
  systems do not allow for easy replacement of the optimization method,
  as pointed out by Mohr &amp; Wever
  (<xref alt="2023" rid="ref-mohr-ml23a" ref-type="bibr">2023</xref>).
  Also, a researcher wanting to study a variation of an existing system
  would need to go through an extensive, potentially undocumented
  codebase to find the right place to apply their variation. The tight
  integration of components allows for highly efficient systems but
  poses a high barrier to new research and novel, innovative AutoML
  systems.</p>
  <p>Issue (2) is also a huge problem. A recent benchmark study
  (<xref alt="Gijsbers et al., 2023" rid="ref-gijsbers-arxiv23a" ref-type="bibr">Gijsbers
  et al., 2023</xref>) extensively compared multiple AutoML systems on a
  common set of ML tasks. While such benchmarking efforts are necessary
  to assess the current state of the art, we note that each system uses
  its own implementation of the search space, optimization, evaluation
  and ensembling, making a principled comparison and ablation study
  virtually impossible and leaving potential performance gains by
  combining solutions unnoticed. Instead of comparing different methods,
  researchers are actually comparing the implementations. By providing a
  unified toolkit for AutoML, researchers can focus on comparing the
  changes they have made while leaving all other parts of the AutoML
  system as they were.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Edward Bergman was partially supported by TAILOR, a project funded
  by EU Horizon 2020 research and innovation programme under GA No
  952215. Katharina Eggensperger and Amir Balef acknowledge funding by
  the German Research Foundation under Germany’s Excellence Strategy -
  ECX number 2064/1 - Project number 390727645. Marius Lindauer
  acknowledges support by the Federal Ministry of Education and Research
  (BMBF) under the project AI service center KISSKI (grantno.
  01IS22093C). Lennart Purucker acknowledges funding by the Deutsche
  Forschungsgemeinschaft (DFG, German Research Foundation) – SFB 1597 –
  499552394. Sarah Segel acknowledges funding by the European Union
  (ERC, “ixAutoML”, grant no. 101041029). Frank Hutter acknowledges
  funding by the European Union (ERC Consolidator Grant “DeepLearning
  2.0”, grant no. 101045765) Views and opinions expressed are however
  those of the author(s) only and do not necessarily reflect those of
  the European Union or the European Research Council Executive Agency.
  Neither the European Union nor the granting authority can be held
  responsible for them.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-erickson-arxiv20a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Erickson</surname><given-names>N.</given-names></name>
        <name><surname>Mueller</surname><given-names>J.</given-names></name>
        <name><surname>Shirkov</surname><given-names>A.</given-names></name>
        <name><surname>Zhang</surname><given-names>H.</given-names></name>
        <name><surname>Larroy</surname><given-names>P.</given-names></name>
        <name><surname>Li</surname><given-names>M.</given-names></name>
        <name><surname>Smola</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>AutoGluon-tabular: Robust and accurate AutoML for structured data</article-title>
      <source>arXiv:2003.06505 [stat.ML]</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2003.06505</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-feurer-nips15a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Klein</surname><given-names>A.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Springenberg</surname><given-names>J.</given-names></name>
        <name><surname>Blum</surname><given-names>M.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>Efficient and robust automated machine learning</article-title>
      <source>Advances in Neural Information Processing Systems 28 (NIPS 2015)</source>
      <year iso-8601-date="2015">2015</year>
      <fpage>2962</fpage>
      <lpage>2970</lpage>
    </element-citation>
  </ref>
  <ref id="ref-feurer-jmlr22a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Falkner</surname><given-names>S.</given-names></name>
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>Auto-Sklearn 2.0: Hands-free AutoML via meta-learning</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>261</issue>
      <pub-id pub-id-type="doi">10.48550/arXiv.2007.04074</pub-id>
      <fpage>1</fpage>
      <lpage>61</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gijsbers-arxiv23a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gijsbers</surname><given-names>P.</given-names></name>
        <name><surname>Bueno</surname><given-names>M.</given-names></name>
        <name><surname>Coors</surname><given-names>S.</given-names></name>
        <name><surname>LeDell</surname><given-names>E.</given-names></name>
        <name><surname>Poirier</surname><given-names>S.</given-names></name>
        <name><surname>Thomas</surname><given-names>J.</given-names></name>
        <name><surname>Bischl</surname><given-names>B.</given-names></name>
        <name><surname>Vanschoren</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>AMLB: An AutoML benchmark</article-title>
      <source>arXiv:2207.12560 [stat.ML]</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arxiv.2207.12560</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zimmer-tpami21a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zimmer</surname><given-names>L.</given-names></name>
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>Auto-Pytorch: Multi-fidelity MetaLearning for efficient and robust AutoDL</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>43</volume>
      <pub-id pub-id-type="doi">10.1109/tpami.2021.3067763</pub-id>
      <fpage>3079</fpage>
      <lpage>3090</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hutter-book19a">
    <element-citation publication-type="book">
      <source>Automated machine learning: Methods, systems, challenges</source>
      <person-group person-group-type="editor">
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
        <name><surname>Kotthoff</surname><given-names>L.</given-names></name>
        <name><surname>Vanschoren</surname><given-names>J.</given-names></name>
      </person-group>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-05318-5</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gijsbers-kdd21a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gijsbers</surname><given-names>P.</given-names></name>
        <name><surname>Vanschoren</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>GAMA: A general automated machine learning assistant</article-title>
      <source>Machine learning and knowledge discovery in databases. Applied data science and demo track</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-67670-4_39</pub-id>
      <fpage>560</fpage>
      <lpage>564</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gijsbers-joss19a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gijsbers</surname><given-names>P.</given-names></name>
        <name><surname>Vanschoren</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>GAMA: Genetic automated machine learning assistant</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2019">2019</year>
      <volume>4</volume>
      <pub-id pub-id-type="doi">10.21105/joss.01132</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lindauer-jmlr22a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Biedenkapp</surname><given-names>A.</given-names></name>
        <name><surname>Deng</surname><given-names>D.</given-names></name>
        <name><surname>Benjamins</surname><given-names>C.</given-names></name>
        <name><surname>Ruhkopf</surname><given-names>T.</given-names></name>
        <name><surname>Sass</surname><given-names>R.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>SMAC3: A versatile bayesian optimization package for Hyperparameter Optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>54</issue>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sass-realml22a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sass</surname><given-names>R.</given-names></name>
        <name><surname>Bergman</surname><given-names>E.</given-names></name>
        <name><surname>Biedenkapp</surname><given-names>A.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>DeepCAVE: An interactive analysis tool for automated machine learning</article-title>
      <source>ICML adaptive experimental design and active learning in the real world (ReALML workshop 2022)</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arxiv.2206.03493</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lindauer-arxiv19a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Biedenkapp</surname><given-names>A.</given-names></name>
        <name><surname>Marben</surname><given-names>J.</given-names></name>
        <name><surname>Müller</surname><given-names>P.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>BOAH: A tool suite for multi-fidelity bayesian optimization &amp; analysis of hyperparameters</article-title>
      <source>arXiv:1908.06756 [cs.LG]</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arxiv.1908.06756</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mohr-ml23a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mohr</surname><given-names>F.</given-names></name>
        <name><surname>Wever</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Naive automated machine learning</article-title>
      <source>Machine Learning</source>
      <year iso-8601-date="2023">2023</year>
      <volume>112</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1007/s10994-022-06200-0</pub-id>
      <fpage>1131</fpage>
      <lpage>1170</lpage>
    </element-citation>
  </ref>
  <ref id="ref-akiba-kdd19a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Akiba</surname><given-names>T.</given-names></name>
        <name><surname>Sano</surname><given-names>S.</given-names></name>
        <name><surname>Yanase</surname><given-names>T.</given-names></name>
        <name><surname>Ohta</surname><given-names>T.</given-names></name>
        <name><surname>Koyama</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Optuna: A next-generation Hyperparameter Optimization framework</article-title>
      <source>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD’19)</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id>
      <fpage>2623</fpage>
      <lpage>2631</lpage>
    </element-citation>
  </ref>
  <ref id="ref-thornton-kdd13a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Thornton</surname><given-names>C.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
        <name><surname>Hoos</surname><given-names>H.</given-names></name>
        <name><surname>Leyton-Brown</surname><given-names>K.</given-names></name>
      </person-group>
      <article-title>Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms</article-title>
      <source>Proceedings of the 19th ACM SIGKDD international conference on knowledge discovery &amp; data mining (KDD’13)</source>
      <year iso-8601-date="2013">2013</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-05318-5_4</pub-id>
      <fpage>847</fpage>
      <lpage>855</lpage>
    </element-citation>
  </ref>
  <ref id="ref-olson-gecco16a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Olson</surname><given-names>R.</given-names></name>
        <name><surname>Bartley</surname><given-names>N.</given-names></name>
        <name><surname>Urbanowicz</surname><given-names>R.</given-names></name>
        <name><surname>Moore</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Evaluation of a tree-based pipeline optimization tool for automating data science</article-title>
      <source>Proceedings of the genetic and evolutionary computation conference 2016</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1145/2908812.2908918</pub-id>
      <fpage>485</fpage>
      <lpage>492</lpage>
    </element-citation>
  </ref>
  <ref id="ref-autoprognosis">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Imrie</surname><given-names>F.</given-names></name>
        <name><surname>Cebere</surname><given-names>B.</given-names></name>
        <name><surname>McKinney</surname><given-names>E.</given-names></name>
        <name><surname>Schaar</surname><given-names>M. van der</given-names></name>
      </person-group>
      <article-title>AutoPrognosis 2.0: Democratizing diagnostic and prognostic modeling in healthcare with automated machine learning</article-title>
      <source>arXiv:2210.12090 [cs.LG]</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1371/journal.pdig.0000276</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wang2021flaml">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>C.</given-names></name>
        <name><surname>Wu</surname><given-names>Q.</given-names></name>
        <name><surname>Weimer</surname><given-names>M.</given-names></name>
        <name><surname>Zhu</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>FLAML: A fast and lightweight automl library</article-title>
      <source>Proceedings of Machine Learning and Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>3</volume>
      <fpage>434</fpage>
      <lpage>447</lpage>
    </element-citation>
  </ref>
  <ref id="ref-baudart2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Baudart</surname><given-names>G.</given-names></name>
        <name><surname>Hirzel</surname><given-names>M.</given-names></name>
        <name><surname>Kate</surname><given-names>K.</given-names></name>
        <name><surname>Ram</surname><given-names>P.</given-names></name>
        <name><surname>Shinnar</surname><given-names>A.</given-names></name>
        <name><surname>Tsay</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Pipeline combinators for gradual AutoML</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2021-12">2021</year><month>12</month>
      <uri>https://proceedings.neurips.cc/paper/2021/file/a3b36cb25e2e0b93b5f334ffb4e4064e-Paper.pdf</uri>
      <fpage>19705</fpage>
      <lpage>19718</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ledell2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>LeDell</surname><given-names>E.</given-names></name>
        <name><surname>Poirier</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>H2O AutoML: Scalable automatic machine learning</article-title>
      <source>7th ICML Workshop on Automated Machine Learning (AutoML)</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.automl.org/wp-content/uploads/2020/07/AutoML_2020_paper_61.pdf</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
