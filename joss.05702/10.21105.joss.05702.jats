<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5702</article-id>
<article-id pub-id-type="doi">10.21105/joss.05702</article-id>
<title-group>
<article-title>BayesFlow: Amortized Bayesian Workflows With Neural
Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6702-9559</contrib-id>
<name>
<surname>Radev</surname>
<given-names>Stefan T.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1293-820X</contrib-id>
<name>
<surname>Schmitt</surname>
<given-names>Marvin</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1512-8288</contrib-id>
<name>
<surname>Schumacher</surname>
<given-names>Lukas</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0368-720X</contrib-id>
<name>
<surname>Elsemüller</surname>
<given-names>Lasse</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8371-3417</contrib-id>
<name>
<surname>Pratz</surname>
<given-names>Valentin</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1293-820X</contrib-id>
<name>
<surname>Schälte</surname>
<given-names>Yannik</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6036-1287</contrib-id>
<name>
<surname>Köthe</surname>
<given-names>Ullrich</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5765-8995</contrib-id>
<name>
<surname>Bürkner</surname>
<given-names>Paul-Christian</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Cluster of Excellence STRUCTURES, Heidelberg University,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Cluster of Excellence SimTech, University of Stuttgart,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Institute for Psychology, Heidelberg University,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Visual Learning Lab, Heidelberg University,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Life and Medical Sciences Institute, University of Bonn,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Department of Statistics, TU Dortmund University,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-06-22">
<day>22</day>
<month>6</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>89</issue>
<fpage>5702</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>simulation-based inference</kwd>
<kwd>likelihood-free inference</kwd>
<kwd>Bayesian inference</kwd>
<kwd>amortized Bayesian inference</kwd>
<kwd>Python</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Modern Bayesian inference involves a mixture of computational
  techniques for estimating, validating, and drawing conclusions from
  probabilistic models as part of principled workflows for data analysis
  (<xref alt="Bürkner et al., 2022" rid="ref-burkner_models_2022" ref-type="bibr">Bürkner
  et al., 2022</xref>;
  <xref alt="Gelman et al., 2020" rid="ref-gelman_bayesian_2020" ref-type="bibr">Gelman
  et al., 2020</xref>;
  <xref alt="Schad et al., 2021" rid="ref-schad2021toward" ref-type="bibr">Schad
  et al., 2021</xref>). Typical problems in Bayesian workflows are the
  approximation of intractable posterior distributions for diverse model
  types and the comparison of competing models of the same process in
  terms of their complexity and predictive performance. However, despite
  their theoretical appeal and utility, the practical execution of
  Bayesian workflows is often limited by computational bottlenecks:
  Obtaining even a single posterior may already take a long time, such
  that repeated estimation for the purpose of model validation or
  calibration becomes completely infeasible.</p>
  <p><monospace>BayesFlow</monospace> provides a framework for
  <italic>simulation-based</italic> training of established neural
  network architectures, such as transformers
  (<xref alt="Vaswani et al., 2017" rid="ref-vaswani2017attention" ref-type="bibr">Vaswani
  et al., 2017</xref>) and normalizing flows
  (<xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>), for <italic>amortized</italic> data compression
  and inference. <italic>Amortized Bayesian inference</italic> (ABI), as
  implemented in <monospace>BayesFlow</monospace>, enables users to
  train custom neural networks on model simulations and re-use these
  networks for any subsequent application of the models. Since the
  trained networks can perform inference almost instantaneously
  (typically well below one second), the upfront neural network training
  is quickly amortized. For instance, amortized inference allows us to
  test a model’s ability to recover its parameters
  (<xref alt="Schad et al., 2021" rid="ref-schad2021toward" ref-type="bibr">Schad
  et al., 2021</xref>) or assess its simulation-based calibration
  (<xref alt="Säilynoja et al., 2022" rid="ref-sailynoja2022graphical" ref-type="bibr">Säilynoja
  et al., 2022</xref>;
  <xref alt="Talts et al., 2018" rid="ref-talts2018" ref-type="bibr">Talts
  et al., 2018</xref>) for different data set sizes in a matter of
  seconds, even though this may require the estimation of thousands of
  posterior distributions. <monospace>BayesFlow</monospace> offers a
  user-friendly API, which encapsulates the details of neural network
  architectures and training procedures that are less relevant for the
  practitioner and provides robust default implementations that work
  well across many applications. At the same time,
  <monospace>BayesFlow</monospace> implements a modular software
  architecture, allowing machine learning scientists to modify every
  component of the pipeline for custom applications as well as research
  at the frontier of Bayesian inference.</p>
  <fig>
    <caption><p><monospace>BayesFlow</monospace> defines a formal
    workflow for data generation, neural approximation, and model
    criticism.<styled-content id="figU003Afigure1"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/bayesflow_software_figure1.pdf" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p><monospace>BayesFlow</monospace> embodies functionality that is
  specifically designed for building and validating amortized Bayesian
  workflows with the help of neural networks.
  <xref alt="[fig:figure1]" rid="figU003Afigure1">[fig:figure1]</xref>
  outlines a typical workflow in the context of amortized posterior and
  likelihood estimation. A simulator coupled with a prior defines a
  generative Bayesian model. The generative model may depend on various
  (optional) context variates like varying numbers of observations,
  design matrices, or positional encodings. The generative scope of the
  model and the range of context variables determine the <italic>scope
  of amortization</italic>, that is, over which types of data the neural
  approximator can be applied without re-training. The neural
  approximators interact with model outputs (parameters, data) and
  context variates through a configurator. The configurator is
  responsible for carrying out transformations (e.g., input
  normalization, double-to-float conversion, etc.) that are not part of
  the model but may facilitate neural network training and
  convergence.</p>
  <p><xref alt="[fig:figure1]" rid="figU003Afigure1">[fig:figure1]</xref>
  also illustrates an example configuration of four neural networks: 1)
  a summary network to compress simulation outcomes (individual data
  points, sets, or time series) into informative embeddings; 2) a
  posterior network to learn an amortized approximate posterior; and 3)
  another summary network to compress simulation inputs (parameters)
  into informative embeddings; and 4) a likelihood network to learn an
  amortized approximate likelihood.
  <xref alt="[fig:figure1]" rid="figU003Afigure1">[fig:figure1]</xref>
  depicts the standalone and joint capabilities of the networks when
  applied in isolation or in tandem. The input conditions for the
  posterior and likelihood networks are partitioned by the configurator:
  Complex (“summary”) conditions are processed by the respective summary
  network into embeddings, while very simple (“direct”) conditions can
  bypass the summary network and flow straight into the neural
  approximator.</p>
  <p>Currently, the software features four key capabilities for
  enhancing Bayesian workflows, which have been described in the
  referenced works:</p>
  <list list-type="order">
    <list-item>
      <p><bold>Amortized posterior estimation:</bold> Train a generative
      network to efficiently infer full posteriors (i.e., solve the
      inverse problem) for all existing and future data compatible with
      a simulation model
      (<xref alt="Radev, Mertens, et al., 2020" rid="ref-radev2020bayesflow" ref-type="bibr">Radev,
      Mertens, et al., 2020</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Amortized likelihood estimation:</bold> Train a
      generative network to efficiently emulate a simulation model
      (i.e., solve the forward problem) for all possible parameter
      configurations or interact with external probabilistic programs
      (<xref alt="Boelts et al., 2022" rid="ref-boelts2022flexible" ref-type="bibr">Boelts
      et al., 2022</xref>;
      <xref alt="Radev et al., 2023" rid="ref-radev2023jana" ref-type="bibr">Radev
      et al., 2023</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Amortized model comparison:</bold> Train a neural
      classifier to recognize the “best” model in a set of competing
      candidates
      (<xref alt="Elsemüller et al., 2023" rid="ref-elsemuller2023deep" ref-type="bibr">Elsemüller
      et al., 2023</xref>;
      <xref alt="Radev, D’Alessandro, et al., 2020" rid="ref-radev2020evidential" ref-type="bibr">Radev,
      D’Alessandro, et al., 2020</xref>;
      <xref alt="Schmitt et al., 2022" rid="ref-schmitt2022meta" ref-type="bibr">Schmitt
      et al., 2022</xref>) or combine amortized posterior and likelihood
      estimation to compute Bayesian evidence and out-of-sample
      predictive performance
      (<xref alt="Radev et al., 2023" rid="ref-radev2023jana" ref-type="bibr">Radev
      et al., 2023</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Model misspecification detection:</bold> Ensure that the
      resulting posteriors are faithful approximations of the otherwise
      intractable target posterior, even when simulations do not
      perfectly represent reality
      (<xref alt="Radev et al., 2023" rid="ref-radev2023jana" ref-type="bibr">Radev
      et al., 2023</xref>;
      <xref alt="Schmitt et al., 2023" rid="ref-schmitt2021detecting" ref-type="bibr">Schmitt
      et al., 2023</xref>).</p>
    </list-item>
  </list>
  <p><monospace>BayesFlow</monospace> has been used for amortized
  Bayesian inference in various areas of applied research, such as
  epidemiology
  (<xref alt="Radev et al., 2021" rid="ref-radev2021outbreakflow" ref-type="bibr">Radev
  et al., 2021</xref>), cognitive modeling
  (<xref alt="Krause et al., 2022" rid="ref-von2022mental" ref-type="bibr">Krause
  et al., 2022</xref>;
  <xref alt="Schumacher et al., 2023" rid="ref-schumacher2023" ref-type="bibr">Schumacher
  et al., 2023</xref>;
  <xref alt="Sokratous et al., 2023" rid="ref-sokratous2023ask" ref-type="bibr">Sokratous
  et al., 2023</xref>;
  <xref alt="Wieschen et al., 2020" rid="ref-wieschen2020jumping" ref-type="bibr">Wieschen
  et al., 2020</xref>), computational psychiatry
  (<xref alt="D’Alessandro et al., 2020" rid="ref-d2020bayesian" ref-type="bibr">D’Alessandro
  et al., 2020</xref>), neuroscience
  (<xref alt="Ghaderi-Kangavari et al., 2022" rid="ref-ghaderi2022general" ref-type="bibr">Ghaderi-Kangavari
  et al., 2022</xref>), particle physics
  (<xref alt="Bieringer et al., 2021" rid="ref-bieringer2021measuring" ref-type="bibr">Bieringer
  et al., 2021</xref>), agent-based econometrics models
  (<xref alt="Shiono, 2021" rid="ref-shiono2021estimation" ref-type="bibr">Shiono,
  2021</xref>), seismic imaging
  (<xref alt="Siahkoohi et al., 2023" rid="ref-siahkoohi2023reliable" ref-type="bibr">Siahkoohi
  et al., 2023</xref>), user behavior
  (<xref alt="Moon et al., 2023" rid="ref-moon2023amortized" ref-type="bibr">Moon
  et al., 2023</xref>), structural health monitoring
  (<xref alt="Zeng et al., 2023" rid="ref-zeng2023probabilistic" ref-type="bibr">Zeng
  et al., 2023</xref>), aerospace
  (<xref alt="Tsilifis et al., 2022" rid="ref-tsilifis2022inverse" ref-type="bibr">Tsilifis
  et al., 2022</xref>) and wind turbine design
  (<xref alt="Noever-Castelos et al., 2022" rid="ref-noever2022model" ref-type="bibr">Noever-Castelos
  et al., 2022</xref>), micro-electro-mechanical systems testing
  (<xref alt="Heringhaus et al., 2022" rid="ref-heringhaus2022towards" ref-type="bibr">Heringhaus
  et al., 2022</xref>), and fractional Brownian motion
  (<xref alt="Verdier et al., 2022" rid="ref-verdier2022variational" ref-type="bibr">Verdier
  et al., 2022</xref>).</p>
  <p>The software is built on top of <monospace>TensorFlow</monospace>
  (<xref alt="Abadi et al., 2016" rid="ref-abadi2016tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>) and thereby enables off-the-shelf support for GPU
  and TPU acceleration. Furthermore, it can seamlessly interact with
  TensorFlow Probability
  (<xref alt="Dillon et al., 2017" rid="ref-dillon2017tensorflow" ref-type="bibr">Dillon
  et al., 2017</xref>) for flexible latent distributions and a variety
  of joint priors.</p>
</sec>
<sec id="related-software">
  <title>Related Software</title>
  <p>When a non-amortized inference procedure does not create a
  computational bottleneck, approximate Bayesian computation (ABC) might
  be an appropriate tool. This is the case if a single data set needs to
  be analyzed, if an infrastructure for parallel computing is readily
  available, or if repeated re-fits of a model (e.g., cross-validation)
  are not desired. A variety of mature Python packages for ABC exist,
  such as PyMC
  (<xref alt="Salvatier et al., 2016" rid="ref-Salvatier2016" ref-type="bibr">Salvatier
  et al., 2016</xref>), pyABC
  (<xref alt="Schälte et al., 2022" rid="ref-schaelte2022pyabc" ref-type="bibr">Schälte
  et al., 2022</xref>), ABCpy
  (<xref alt="Dutta et al., 2021" rid="ref-dutta2021abcpy" ref-type="bibr">Dutta
  et al., 2021</xref>), or ELFI
  (<xref alt="Lintusaari et al., 2018" rid="ref-lintusaari2018elfi" ref-type="bibr">Lintusaari
  et al., 2018</xref>). In contrast to these packages,
  <monospace>BayesFlow</monospace> focuses on amortized inference, but
  can also interact with ABC samplers (e.g., use BayesFlow to learn
  informative summary statistics for an ABC analysis).</p>
  <p>When it comes to simulation-based inference with neural networks,
  the <monospace>sbi</monospace> toolkit enables both likelihood and
  posterior estimation using different inference algorithms, such as
  Neural Posterior Estimation
  (<xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>), Sequential Neural Posterior Estimation
  (<xref alt="Greenberg et al., 2019" rid="ref-greenberg2019automatic" ref-type="bibr">Greenberg
  et al., 2019</xref>) and Sequential Neural Likelihood Estimation
  (<xref alt="Papamakarios et al., 2019" rid="ref-papamakarios2019sequential" ref-type="bibr">Papamakarios
  et al., 2019</xref>). <monospace>BayesFlow</monospace> and
  <monospace>sbi</monospace> can be viewed as complementary toolkits,
  where <monospace>sbi</monospace> implements a variety of different
  approximators for standard modeling scenarios, while
  <monospace>BayesFlow</monospace> focuses on amortized workflows with
  user-friendly default settings and optional customization. The
  <monospace>Swyft</monospace> library focuses on Bayesian parameter
  inference in physics and astronomy. <monospace>Swyft</monospace> uses
  a specific type of simulation-based neural inference technique,
  namely, Truncated Marginal Neural Ratio Estimation
  (<xref alt="Miller et al., 2021" rid="ref-miller2021truncated" ref-type="bibr">Miller
  et al., 2021</xref>). This method improves on standard Markov chain
  Monte Carlo (MCMC) methods for ABC by learning the
  likelihood-to-evidence ratio with neural density estimators. Finally,
  the <monospace>Lampe</monospace> library provides implementations for
  a subset of the methods for posterior estimation in the
  <monospace>sbi</monospace> library, aiming to expose all components
  (e.g., network architectures, optimizers) in order to provide a
  customizable interface for creating neural approximators. All of these
  libraries are built on top of <monospace>PyTorch</monospace>.</p>
</sec>
<sec id="availability-development-and-documentation">
  <title>Availability, Development, and Documentation</title>
  <p><monospace>BayesFlow</monospace> is available through PyPI via
  <monospace>pip install bayesflow</monospace>, the development version
  is available via GitHub. GitHub Actions manage continuous integration
  through automated code testing and documentation. The documentation is
  hosted at
  <ext-link ext-link-type="uri" xlink:href="https://bayesflow.org/">www.bayesflow.org</ext-link>.
  Currently, <monospace>BayesFlow</monospace> features seven tutorial
  notebooks. These notebooks showcase different aspects of the software,
  ranging from toy examples to applied modeling scenarios, and
  illustrating both posterior estimation and model comparison
  workflows.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We thank Ulf Mertens, Marco D’Alessandro, René Bucchia, The-Gia Leo
  Nguyen, Jonas Arruda, Lea Zimmermann, and Leonhard Volz for
  contributing to the GitHub repository. STR was funded by the Deutsche
  Forschungsgemeinschaft (DFG, German Research Foundation) under
  Germany’s Excellence Strategy - EXC-2181 - 390900948 (the Heidelberg
  Cluster of Excellence STRUCTURES), MS and PCB were supported by the
  Cyber Valley Research Fund (grant number: CyVy-RF-2021-16) and the DFG
  EXC-2075 - 390740016 (the Stuttgart Cluster of Excellence SimTech). LS
  and LE were supported by a grant from the DFG (GRK 2277) to the
  research training group Statistical Modeling in Psychology (SMiP). YS
  acknowledges support from the Joachim Herz Foundation. UK was
  supported by the Informatics for Life initiative funded by the Klaus
  Tschira Foundation. YS and UK were supported by the EMUNE project
  (“Invertierbare Neuronale Netze für ein verbessertes Verständnis von
  Infektionskrankheiten”, BMBF, 031L0293A-D).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-abadi2016tensorflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>TensorFlow: A system for large-scale machine learning</article-title>
      <source>Osdi</source>
      <publisher-name>Savannah, GA, USA</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>16</volume>
      <issue>2016</issue>
      <fpage>265</fpage>
      <lpage>283</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dutta2021abcpy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dutta</surname><given-names>Ritabrata</given-names></name>
        <name><surname>Schoengens</surname><given-names>Marcel</given-names></name>
        <name><surname>Pacchiardi</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Ummadisingu</surname><given-names>Avinash</given-names></name>
        <name><surname>Widmer</surname><given-names>Nicole</given-names></name>
        <name><surname>Künzli</surname><given-names>Pierre</given-names></name>
        <name><surname>Onnela</surname><given-names>Jukka-Pekka</given-names></name>
        <name><surname>Mira</surname><given-names>Antonietta</given-names></name>
      </person-group>
      <article-title>ABCpy: A high-performance computing perspective to approximate Bayesian computation</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2021">2021</year>
      <volume>100</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v100.i07</pub-id>
      <fpage>1</fpage>
      <lpage>38</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bieringer2021measuring">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bieringer</surname><given-names>Sebastian</given-names></name>
        <name><surname>Butter</surname><given-names>Anja</given-names></name>
        <name><surname>Heimel</surname><given-names>Theo</given-names></name>
        <name><surname>Höche</surname><given-names>Stefan</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Plehn</surname><given-names>Tilman</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
      </person-group>
      <article-title>Measuring QCD splittings with invertible networks</article-title>
      <source>SciPost Physics</source>
      <year iso-8601-date="2021">2021</year>
      <volume>10</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.21468/SciPostPhys.10.6.126</pub-id>
      <fpage>126</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-boelts2022flexible">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boelts</surname><given-names>Jan</given-names></name>
        <name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name>
        <name><surname>Gao</surname><given-names>Richard</given-names></name>
        <name><surname>Macke</surname><given-names>Jakob H</given-names></name>
      </person-group>
      <article-title>Flexible and efficient simulation-based inference for models of decision-making</article-title>
      <source>Elife</source>
      <publisher-name>eLife Sciences Publications Limited</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>11</volume>
      <fpage>e77220</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-burkner_models_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Scholz</surname><given-names>Maximilian</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T.</given-names></name>
      </person-group>
      <article-title>Some models are useful, but how do we know which ones? Towards a unified Bayesian model taxonomy</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-d2020bayesian">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>D’Alessandro</surname><given-names>Marco</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
        <name><surname>Lombardi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>A Bayesian brain model of adaptive behavior: An application to the wisconsin card sorting task</article-title>
      <source>PeerJ</source>
      <publisher-name>PeerJ Inc.</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>8</volume>
      <fpage>e10316</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-dillon2017tensorflow">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Dillon</surname><given-names>Joshua V.</given-names></name>
        <name><surname>Langmore</surname><given-names>Ian</given-names></name>
        <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Srinivas</given-names></name>
        <name><surname>Moore</surname><given-names>Dave</given-names></name>
        <name><surname>Patton</surname><given-names>Brian</given-names></name>
        <name><surname>Alemi</surname><given-names>Alex</given-names></name>
        <name><surname>Hoffman</surname><given-names>Matt</given-names></name>
        <name><surname>Saurous</surname><given-names>Rif A.</given-names></name>
      </person-group>
      <article-title>TensorFlow distributions</article-title>
      <year iso-8601-date="2017">2017</year>
      <uri>https://arxiv.org/abs/1711.10604</uri>
    </element-citation>
  </ref>
  <ref id="ref-elsemuller2023deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Elsemüller</surname><given-names>Lasse</given-names></name>
        <name><surname>Schnuerch</surname><given-names>Martin</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
      </person-group>
      <article-title>A deep learning method for comparing Bayesian hierarchical models</article-title>
      <source>arXiv preprint arXiv:2301.11873</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-gelman_bayesian_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Simpson</surname><given-names>Daniel</given-names></name>
        <name><surname>Margossian</surname><given-names>Charles C</given-names></name>
        <name><surname>Carpenter</surname><given-names>Bob</given-names></name>
        <name><surname>Yao</surname><given-names>Yuling</given-names></name>
        <name><surname>Kennedy</surname><given-names>Lauren</given-names></name>
        <name><surname>Gabry</surname><given-names>Jonah</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Modrák</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>Bayesian workflow</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-ghaderi2022general">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ghaderi-Kangavari</surname><given-names>Amin</given-names></name>
        <name><surname>Rad</surname><given-names>Jamal Amani</given-names></name>
        <name><surname>Nunez</surname><given-names>Michael D</given-names></name>
      </person-group>
      <article-title>A general integrative neurocognitive modeling framework to jointly describe EEG and decision-making on single trials</article-title>
      <publisher-name>PsyArXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1007/s42113-023-00167-4</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-greenberg2019automatic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Greenberg</surname><given-names>David</given-names></name>
        <name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name>
        <name><surname>Macke</surname><given-names>Jakob</given-names></name>
      </person-group>
      <article-title>Automatic posterior transformation for likelihood-free inference</article-title>
      <source>International Conference on Machine Learning</source>
      <year iso-8601-date="2019">2019</year>
      <volume>97</volume>
      <fpage>2404</fpage>
      <lpage>2414</lpage>
    </element-citation>
  </ref>
  <ref id="ref-heringhaus2022towards">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Heringhaus</surname><given-names>Monika E</given-names></name>
        <name><surname>Zhang</surname><given-names>Yi</given-names></name>
        <name><surname>Zimmermann</surname><given-names>André</given-names></name>
        <name><surname>Mikelsons</surname><given-names>Lars</given-names></name>
      </person-group>
      <article-title>Towards reliable parameter extraction in MEMS final module testing using Bayesian inference</article-title>
      <source>Sensors</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>22</volume>
      <issue>14</issue>
      <pub-id pub-id-type="doi">10.3390/s22145408</pub-id>
      <fpage>5408</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lintusaari2018elfi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lintusaari</surname><given-names>Jarno</given-names></name>
        <name><surname>Vuollekoski</surname><given-names>Henri</given-names></name>
        <name><surname>Kangasrääsiö</surname><given-names>Antti</given-names></name>
        <name><surname>Skytén</surname><given-names>Kusti</given-names></name>
        <name><surname>Järvenpää</surname><given-names>Marko</given-names></name>
        <name><surname>Marttinen</surname><given-names>Pekka</given-names></name>
        <name><surname>Gutmann</surname><given-names>Michael U.</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Corander</surname><given-names>Jukka</given-names></name>
        <name><surname>Kaski</surname><given-names>Samuel</given-names></name>
      </person-group>
      <article-title>ELFI: Engine for likelihood-free inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2018">2018</year>
      <volume>19</volume>
      <issue>16</issue>
      <uri>http://jmlr.org/papers/v19/17-374.html</uri>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-miller2021truncated">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Miller</surname><given-names>Benjamin K</given-names></name>
        <name><surname>Cole</surname><given-names>Alex</given-names></name>
        <name><surname>Forré</surname><given-names>Patrick</given-names></name>
        <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        <name><surname>Weniger</surname><given-names>Christoph</given-names></name>
      </person-group>
      <article-title>Truncated marginal neural ratio estimation</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
      <fpage>129</fpage>
      <lpage>143</lpage>
    </element-citation>
  </ref>
  <ref id="ref-moon2023amortized">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Moon</surname><given-names>Hee-Seung</given-names></name>
        <name><surname>Oulasvirta</surname><given-names>Antti</given-names></name>
        <name><surname>Lee</surname><given-names>Byungjoo</given-names></name>
      </person-group>
      <article-title>Amortized inference with user simulations</article-title>
      <source>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</source>
      <year iso-8601-date="2023">2023</year>
      <fpage>1</fpage>
      <lpage>20</lpage>
    </element-citation>
  </ref>
  <ref id="ref-noever2022model">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Noever-Castelos</surname><given-names>Pablo</given-names></name>
        <name><surname>Ardizzone</surname><given-names>Lynton</given-names></name>
        <name><surname>Balzani</surname><given-names>Claudio</given-names></name>
      </person-group>
      <article-title>Model updating of wind turbine blade cross sections with invertible neural networks</article-title>
      <source>Wind Energy</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>25</volume>
      <issue>3</issue>
      <fpage>573</fpage>
      <lpage>599</lpage>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2021normalizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <publisher-name>JMLR.org</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>1</issue>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2019sequential">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Sterratt</surname><given-names>David</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
      </person-group>
      <article-title>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</article-title>
      <source>The 22nd International Conference on Artificial Intelligence and Statistics</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <fpage>837</fpage>
      <lpage>848</lpage>
    </element-citation>
  </ref>
  <ref id="ref-radev2020evidential">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>D’Alessandro</surname><given-names>Marco</given-names></name>
        <name><surname>Mertens</surname><given-names>Ulf K</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
      </person-group>
      <article-title>Amortized Bayesian model comparison with evidential deep learning</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1109/TNNLS.2021.3124052</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-radev2021outbreakflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Graw</surname><given-names>Frederik</given-names></name>
        <name><surname>Chen</surname><given-names>Simiao</given-names></name>
        <name><surname>Mutters</surname><given-names>Nico T</given-names></name>
        <name><surname>Eichel</surname><given-names>Vanessa M</given-names></name>
        <name><surname>Bärnighausen</surname><given-names>Till</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
      </person-group>
      <article-title>OutbreakFlow: Model-based Bayesian inference of disease outbreak dynamics with invertible neural networks and its application to the COVID-19 pandemics in Germany</article-title>
      <source>PLoS computational biology</source>
      <publisher-name>Public Library of Science San Francisco, CA USA</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>17</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1009472</pub-id>
      <fpage>e1009472</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-radev2020bayesflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Mertens</surname><given-names>Ulf K</given-names></name>
        <name><surname>Voss</surname><given-names>A</given-names></name>
        <name><surname>Ardizzone</surname><given-names>L</given-names></name>
        <name><surname>Köthe</surname><given-names>U</given-names></name>
      </person-group>
      <article-title>BayesFlow: Learning complex stochastic models with invertible neural networks</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1109/TNNLS.2020.3042395</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-radev2023jana">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Schmitt</surname><given-names>Marvin</given-names></name>
        <name><surname>Pratz</surname><given-names>Valentin</given-names></name>
        <name><surname>Picchini</surname><given-names>Umberto</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
      </person-group>
      <article-title>JANA: Jointly amortized neural approximation of complex Bayesian models</article-title>
      <source>arXiv preprint arXiv:2302.09125</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-sailynoja2022graphical">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Säilynoja</surname><given-names>Teemu</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
      </person-group>
      <article-title>Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison</article-title>
      <source>Statistics and Computing</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>32</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1007/s11222-022-10090-6</pub-id>
      <fpage>32</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Salvatier2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Salvatier</surname><given-names>John</given-names></name>
        <name><surname>Wiecki</surname><given-names>Thomas V.</given-names></name>
        <name><surname>Fonnesbeck</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>Probabilistic programming in python using PyMC3</article-title>
      <source>PeerJ Computer Science</source>
      <publisher-name>PeerJ</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>2</volume>
      <pub-id pub-id-type="doi">10.7717/peerj-cs.55</pub-id>
      <fpage>e55</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-schad2021toward">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schad</surname><given-names>Daniel J</given-names></name>
        <name><surname>Betancourt</surname><given-names>Michael</given-names></name>
        <name><surname>Vasishth</surname><given-names>Shravan</given-names></name>
      </person-group>
      <article-title>Toward a principled Bayesian workflow in cognitive science.</article-title>
      <source>Psychological methods</source>
      <publisher-name>American Psychological Association</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>26</volume>
      <issue>1</issue>
      <fpage>103</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-schaelte2022pyabc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schälte</surname><given-names>Yannik</given-names></name>
        <name><surname>Klinger</surname><given-names>Emmanuel</given-names></name>
        <name><surname>Alamoudi</surname><given-names>Emad</given-names></name>
        <name><surname>Hasenauer</surname><given-names>Jan</given-names></name>
      </person-group>
      <article-title>pyABC: Efficient and robust easy-to-use approximate Bayesian computation</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>74</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04304</pub-id>
      <fpage>4304</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-schmitt2021detecting">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schmitt</surname><given-names>Marvin</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T.</given-names></name>
      </person-group>
      <article-title>Detecting model misspecification in amortized Bayesian inference with neural networks</article-title>
      <source>45th German Conference on Pattern Recognition (GCPR)</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-schmitt2022meta">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schmitt</surname><given-names>Marvin</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
      </person-group>
      <article-title>Meta-uncertainty in Bayesian model comparison</article-title>
      <source>arXiv preprint arXiv:2210.07278</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-shiono2021estimation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shiono</surname><given-names>Takashi</given-names></name>
      </person-group>
      <article-title>Estimation of agent-based models using Bayesian deep learning approach of BayesFlow</article-title>
      <source>Journal of Economic Dynamics and Control</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>125</volume>
      <pub-id pub-id-type="doi">10.1016/j.jedc.2021.104082</pub-id>
      <fpage>104082</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-siahkoohi2023reliable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Reliable amortized variational inference with physics-based latent distribution correction</article-title>
      <source>Geophysics</source>
      <publisher-name>Society of Exploration Geophysicists</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>88</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1190/geo2022-0472.1</pub-id>
      <fpage>R297</fpage>
      <lpage>R322</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sokratous2023ask">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sokratous</surname><given-names>Konstantina</given-names></name>
        <name><surname>Fitch</surname><given-names>Anderson K</given-names></name>
        <name><surname>Kvam</surname><given-names>Peter D</given-names></name>
      </person-group>
      <article-title>How to ask twenty questions and win: Machine learning tools for assessing preferences from small samples of willingness-to-pay prices</article-title>
      <source>Journal of Choice Modelling</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>48</volume>
      <pub-id pub-id-type="doi">10.1016/j.jocm.2023.100418</pub-id>
      <fpage>100418</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-talts2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Talts</surname><given-names>Sean</given-names></name>
        <name><surname>Betancourt</surname><given-names>Michael</given-names></name>
        <name><surname>Simpson</surname><given-names>Daniel</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Validating Bayesian inference algorithms with simulation-based calibration</article-title>
      <source>arXiv preprint</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-tsilifis2022inverse">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tsilifis</surname><given-names>Panagiotis</given-names></name>
        <name><surname>Ghosh</surname><given-names>Sayan</given-names></name>
        <name><surname>Andreoli</surname><given-names>Valeria</given-names></name>
      </person-group>
      <article-title>Inverse design under uncertainty using conditional normalizing flows</article-title>
      <source>AIAA Scitech 2022 Forum</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.2514/6.2022-0631</pub-id>
      <fpage>0631</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vaswani2017attention">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N</given-names></name>
        <name><surname>Kaiser</surname><given-names>Łukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention is all you need</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
    </element-citation>
  </ref>
  <ref id="ref-verdier2022variational">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Verdier</surname><given-names>Hippolyte</given-names></name>
        <name><surname>Laurent</surname><given-names>François</given-names></name>
        <name><surname>Cassé</surname><given-names>Alhassan</given-names></name>
        <name><surname>Vestergaard</surname><given-names>Christian L</given-names></name>
        <name><surname>Masson</surname><given-names>Jean-Baptiste</given-names></name>
      </person-group>
      <article-title>Variational inference of fractional Brownian motion with linear computational complexity</article-title>
      <source>Physical Review E</source>
      <publisher-name>APS</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>106</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevE.106.055311</pub-id>
      <fpage>055311</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-von2022mental">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Krause</surname><given-names>Mischa von</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>Mental speed is high until age 60 as revealed by analysis of over a million participants</article-title>
      <source>Nature Human Behaviour</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>6</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1038/s41562-021-01282-7</pub-id>
      <fpage>700</fpage>
      <lpage>708</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wieschen2020jumping">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wieschen</surname><given-names>Eva Marie</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan</given-names></name>
      </person-group>
      <article-title>Jumping to conclusion? A Lévy flight model of decision making</article-title>
      <source>The Quantitative Methods for Psychology</source>
      <year iso-8601-date="2020">2020</year>
      <volume>16</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.20982/tqmp.16.2.p120</pub-id>
      <fpage>120</fpage>
      <lpage>132</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zeng2023probabilistic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zeng</surname><given-names>Jice</given-names></name>
        <name><surname>Todd</surname><given-names>Michael D</given-names></name>
        <name><surname>Hu</surname><given-names>Zhen</given-names></name>
      </person-group>
      <article-title>Probabilistic damage detection using a new likelihood-free Bayesian inference method</article-title>
      <source>Journal of Civil Structural Health Monitoring</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>13</volume>
      <issue>2-3</issue>
      <pub-id pub-id-type="doi">10.1007/s13349-022-00638-5</pub-id>
      <fpage>319</fpage>
      <lpage>341</lpage>
    </element-citation>
  </ref>
  <ref id="ref-schumacher2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schumacher</surname><given-names>Lukas</given-names></name>
        <name><surname>Bürkner</surname><given-names>Paul-Christian</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
      </person-group>
      <article-title>Neural superstatistics for Bayesian estimation of dynamic cognitive models</article-title>
      <source>Scientific Reports</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2023-08-23">2023</year><month>08</month><day>23</day>
      <volume>13</volume>
      <issue>1</issue>
      <issn>2045-2322</issn>
      <pub-id pub-id-type="doi">10.1038/s41598-023-40278-3</pub-id>
      <fpage>13778</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
