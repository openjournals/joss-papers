<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20221009T080619-161200c060bee2e587ee0cf89638fa3922b9b500</doi_batch_id>
    <timestamp>20221009080619</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>10</month>
          <year>2022</year>
        </publication_date>
        <journal_volume>
          <volume>7</volume>
        </journal_volume>
        <issue>78</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Volume Segmantics: A Python Package for Semantic
Segmentation of Volumetric Data Using Pre-trained PyTorch Deep Learning
Models</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Oliver N. F.</given_name>
            <surname>King</surname>
            <ORCID>https://orcid.org/0000-0002-6152-7207</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Dimitrios</given_name>
            <surname>Bellos</surname>
            <ORCID>https://orcid.org/0000-0002-8015-3191</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Mark</given_name>
            <surname>Basham</surname>
            <ORCID>https://orcid.org/0000-0002-8438-1415</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>10</month>
          <day>09</day>
          <year>2022</year>
        </publication_date>
        <pages>
          <first_page>4691</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.04691</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7143363</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/4691</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.04691</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.04691</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.04691.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="russakovsky_imagenet_2015">
            <article_title>ImageNet Large Scale Visual Recognition
Challenge</article_title>
            <author>Russakovsky</author>
            <journal_title>International Journal of Computer
Vision</journal_title>
            <issue>3</issue>
            <volume>115</volume>
            <doi>10.1007/s11263-015-0816-y</doi>
            <issn>1573-1405</issn>
            <cYear>2015</cYear>
            <unstructured_citation>Russakovsky, O., Deng, J., Su, H.,
Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,
Bernstein, M., Berg, A. C., &amp; Fei-Fei, L. (2015). ImageNet Large
Scale Visual Recognition Challenge. International Journal of Computer
Vision, 115(3), 211–252.
https://doi.org/10.1007/s11263-015-0816-y</unstructured_citation>
          </citation>
          <citation key="perslev_one_2019">
            <article_title>One Network to Segment Them All: A General,
Lightweight System for Accurate 3D Medical Image
Segmentation</article_title>
            <author>Perslev</author>
            <journal_title>Medical Image Computing and Computer Assisted
Intervention – MICCAI 2019</journal_title>
            <doi>10.1007/978-3-030-32245-8_4</doi>
            <isbn>978-3-030-32245-8</isbn>
            <cYear>2019</cYear>
            <unstructured_citation>Perslev, M., Dam, E. B., Pai, A.,
&amp; Igel, C. (2019). One Network to Segment Them All: A General,
Lightweight System for Accurate 3D Medical Image Segmentation. In D.
Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap,
&amp; A. Khan (Eds.), Medical Image Computing and Computer Assisted
Intervention – MICCAI 2019 (pp. 30–38). Springer International
Publishing.
https://doi.org/10.1007/978-3-030-32245-8_4</unstructured_citation>
          </citation>
          <citation key="buslaev_albumentations_2020">
            <article_title>Albumentations: Fast and Flexible Image
Augmentations</article_title>
            <author>Buslaev</author>
            <journal_title>Information</journal_title>
            <issue>2</issue>
            <volume>11</volume>
            <doi>10.3390/info11020125</doi>
            <issn>2078-2489</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Buslaev, A., Iglovikov, V. I.,
Khvedchenya, E., Parinov, A., Druzhinin, M., &amp; Kalinin, A. A.
(2020). Albumentations: Fast and Flexible Image Augmentations.
Information, 11(2).
https://doi.org/10.3390/info11020125</unstructured_citation>
          </citation>
          <citation key="alvarez-borges_u-net_2022">
            <article_title>U-Net Segmentation Methods for
Variable-Contrast XCT Images of Methane-Bearing Sand Using Small
Training Datasets</article_title>
            <author>Alvarez-Borges</author>
            <doi>10.1002/essoar.10506807.2</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Alvarez-Borges, F. J., King, O. N.
F., Madhusudhan, B. N., Connolley, T., Basham, M., &amp; Ahmed, S. I.
(2022). U-Net Segmentation Methods for Variable-Contrast XCT Images of
Methane-Bearing Sand Using Small Training Datasets. Earth; Space Science
Open Archive.
https://doi.org/10.1002/essoar.10506807.2</unstructured_citation>
          </citation>
          <citation key="tun_massively_2021">
            <article_title>A massively multi-scale approach to
characterizing tissue architecture by synchrotron micro-CT applied to
the human placenta</article_title>
            <author>Tun</author>
            <journal_title>Journal of The Royal Society
Interface</journal_title>
            <issue>179</issue>
            <volume>18</volume>
            <doi>10.1098/rsif.2021.0140</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Tun, W. M., Poologasundarampillai,
G., Bischof, H., Nye, G., King, O. N. F., Basham, M., Tokudome, Y.,
Lewis, R. M., Johnstone, E. D., Brownbill, P., Darrow, M., &amp;
Chernyavsky, I. L. (2021). A massively multi-scale approach to
characterizing tissue architecture by synchrotron micro-CT applied to
the human placenta. Journal of The Royal Society Interface, 18(179),
20210140. https://doi.org/10.1098/rsif.2021.0140</unstructured_citation>
          </citation>
          <citation key="Yakubovskiy:2019">
            <article_title>Segmentation models pytorch</article_title>
            <author>Yakubovskiy</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Yakubovskiy, P. (2020). Segmentation
models pytorch. In GitHub repository. GitHub.
https://github.com/qubvel/segmentation_models.pytorch</unstructured_citation>
          </citation>
          <citation key="pennington_survos_2022">
            <article_title>SuRVoS 2: Accelerating Annotation and
Segmentation for Large Volumetric Bioimage Workflows Across Modalities
and Scales</article_title>
            <author>Pennington</author>
            <journal_title>Frontiers in Cell and Developmental
Biology</journal_title>
            <volume>10</volume>
            <doi>10.3389/fcell.2022.842342</doi>
            <issn>2296-634X</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Pennington, A., King, O. N. F., Tun,
W. M., Ho, E. M. L., Luengo, I., Darrow, M. C., &amp; Basham, M. (2022).
SuRVoS 2: Accelerating Annotation and Segmentation for Large Volumetric
Bioimage Workflows Across Modalities and Scales. Frontiers in Cell and
Developmental Biology, 10.
https://doi.org/10.3389/fcell.2022.842342</unstructured_citation>
          </citation>
          <citation key="survos2:2018">
            <article_title>SuRVoS2</article_title>
            <author>Pennington</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Pennington, A., King, O. N. F.,
Luengo, I., &amp; Basham, M. (2018). SuRVoS2. In GitHub repository.
GitHub.
https://github.com/DiamondLightSource/SuRVoS2</unstructured_citation>
          </citation>
          <citation key="perslev_github_2019">
            <article_title>Multi-planar U-net</article_title>
            <author>Perslev</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Perslev, M., &amp; Igel, C. (2019).
Multi-planar U-net. In GitHub repository. GitHub.
https://github.com/perslev/MultiPlanarUNet</unstructured_citation>
          </citation>
          <citation key="tekawade_github_2020">
            <article_title>CTSegNet</article_title>
            <author>Tekawade</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Tekawade, A., &amp; Igel, C. (2020).
CTSegNet. In GitHub repository. GitHub.
https://github.com/aniketkt/CTSegNet</unstructured_citation>
          </citation>
          <citation key="wolny_github_2019">
            <article_title>Pytorch-3dunet</article_title>
            <author>Wolny</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Wolny, A. (2019). Pytorch-3dunet. In
GitHub repository. GitHub.
https://github.com/wolny/pytorch-3dunet</unstructured_citation>
          </citation>
          <citation key="lee_deepem_2018">
            <article_title>DeepEM</article_title>
            <author>Lee</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Lee, K., &amp; Turner, N. L. (2018).
DeepEM. In GitHub repository. GitHub.
https://github.com/seung-lab/DeepEM</unstructured_citation>
          </citation>
          <citation key="lin2021pytorch">
            <article_title>PyTorch connectomics: A scalable and flexible
segmentation framework for EM connectomics</article_title>
            <author>Lin</author>
            <journal_title>arXiv preprint
arXiv:2112.05754</journal_title>
            <doi>10.48550/arXiv.2112.05754</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Lin, Z., Wei, D., Lichtman, J., &amp;
Pfister, H. (2021). PyTorch connectomics: A scalable and flexible
segmentation framework for EM connectomics. arXiv Preprint
arXiv:2112.05754.
https://doi.org/10.48550/arXiv.2112.05754</unstructured_citation>
          </citation>
          <citation key="urakubo_uni-em_2019">
            <article_title>UNI-EM: An Environment for Deep Neural
Network-Based Automated Segmentation of Neuronal Electron Microscopic
Images</article_title>
            <author>Urakubo</author>
            <journal_title>Scientific Reports</journal_title>
            <issue>1</issue>
            <volume>9</volume>
            <doi>10.1038/s41598-019-55431-0</doi>
            <issn>2045-2322</issn>
            <cYear>2019</cYear>
            <unstructured_citation>Urakubo, H., Bullmann, T., Kubota,
Y., Oba, S., &amp; Ishii, S. (2019). UNI-EM: An Environment for Deep
Neural Network-Based Automated Segmentation of Neuronal Electron
Microscopic Images. Scientific Reports, 9(1), 19413.
https://doi.org/10.1038/s41598-019-55431-0</unstructured_citation>
          </citation>
          <citation key="lin_pth_connec_github_2019">
            <article_title>PyTorch connectomics</article_title>
            <author>Lin</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Lin, Z., Lu, Y., Belhamissi, M.,
Banerjee, A., Lauenburg, L., Swaroop, K. K., Wei, D., &amp; Pfister, H.
(2019). PyTorch connectomics. In GitHub repository. GitHub.
https://github.com/zudi-lin/pytorch_connectomics</unstructured_citation>
          </citation>
          <citation key="wu_neutorch_2021">
            <article_title>Neutorch</article_title>
            <author>Wu</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Wu, J. (2021). Neutorch. In GitHub
repository. GitHub.
https://github.com/flatironinstitute/neutorch</unstructured_citation>
          </citation>
          <citation key="NEURIPS2019_9015">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E.
Fox, &amp; R. Garnett (Eds.), Advances in neural information processing
systems 32 (pp. 8024–8035). Curran Associates, Inc.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
