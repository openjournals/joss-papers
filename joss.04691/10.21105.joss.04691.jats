<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4691</article-id>
<article-id pub-id-type="doi">10.21105/joss.04691</article-id>
<title-group>
<article-title>Volume Segmantics: A Python Package for Semantic
Segmentation of Volumetric Data Using Pre-trained PyTorch Deep Learning
Models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0002-6152-7207</contrib-id>
<name>
<surname>King</surname>
<given-names>Oliver N. F.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8015-3191</contrib-id>
<name>
<surname>Bellos</surname>
<given-names>Dimitrios</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8438-1415</contrib-id>
<name>
<surname>Basham</surname>
<given-names>Mark</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Diamond Light Source Ltd., Harwell Science and Innovation
Campus, Didcot, Oxfordshire, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Rosalind Franklin Institute, Harwell Science and Innovation
Campus, Didcot, Oxfordshire, UK</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-07-20">
<day>20</day>
<month>7</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>78</issue>
<fpage>4691</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>segmentation</kwd>
<kwd>deep learning</kwd>
<kwd>volumetric</kwd>
<kwd>images</kwd>
<kwd>pre-trained</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Segmentation of 3-dimensional (3D, volumetric) images is a widely
  used technique that allows interpretation and quantification of
  experimental data collected using a number of techniques (for example,
  Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Electron
  Tomography (ET)). Although the idea of semantic segmentation is a
  relatively simple one, giving each pixel a label that defines what it
  represents (e.g cranial bone versus brain tissue); due to the
  subjective and laborious nature of the manual labelling task coupled
  with the huge size of the data (multi-GB files containing billions of
  pixels) this process is often a bottleneck in imaging workflows. In
  recent years, deep learning has brought models capable of fast and
  accurate interpretation of image data into the toolbox available to
  scientists. These models are often trained on large image datasets
  that have been annotated at great expense. In many cases however,
  scientists working on novel samples and using new imaging techniques
  do not yet have access to large stores of annotated data. To overcome
  this issue, simple software tools that allow the scientific community
  to create segmentation models using relatively small amounts of
  training data are required. <monospace>Volume Segmantics</monospace>
  is a Python package that provides a command line interface (CLI) as
  well as an Application Programming Interface (API) for training
  2-dimensional (2D) PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-NEURIPS2019_9015" ref-type="bibr">Paszke
  et al., 2019</xref>) deep learning models on small amounts of
  annotated 3D image data. The package also enables applying these
  models to new (often much larger) 3D datasets to speed up the process
  of semantic segmentation.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>Volume Segmantics</monospace> harnesses the availability
  of 2-dimensional encoders which have been pre-trained on huge
  databases such as ImageNet
  (<xref alt="Russakovsky et al., 2015" rid="ref-russakovsky_imagenet_2015" ref-type="bibr">Russakovsky
  et al., 2015</xref>). This provides two main advantages, namely (i) it
  reduces the time and resource taken to train models — only fine-tuning
  is required and (ii) it prevents over-fitting the models when training
  on small datasets. These models of various architectures are included
  from the <monospace>segmentation-models-pytorch</monospace> repository
  (<xref alt="Yakubovskiy, 2020" rid="ref-YakubovskiyU003A2019" ref-type="bibr">Yakubovskiy,
  2020</xref>). In order to increase the accuracy of the models,
  augmentations of the data are made during training, both via ‘slicing’
  the 3D data in planes perpendicular to the three orthogonal axes
  <inline-formula><alternatives>
  <tex-math><![CDATA[((x, y), (x, z), (y, z))]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  and by using the library <monospace>Albumentations</monospace>
  (<xref alt="Buslaev et al., 2020" rid="ref-buslaev_albumentations_2020" ref-type="bibr">Buslaev
  et al., 2020</xref>). Additionally, user configuration for training is
  kept to a minimum by starting with a reliable default set of
  parameters and by automatically choosing the model learning rate. If
  adjustments to model architecture, encoder type, loss function or
  training epochs are required; these can be made by editing a YAML
  file.</p>
  <p>Even though these 2D models are quicker to train and require fewer
  computational resources than their 3D counterparts
  (<xref alt="Alvarez-Borges et al., 2022" rid="ref-alvarez-borges_u-net_2022" ref-type="bibr">Alvarez-Borges
  et al., 2022</xref>), when predicting a segmentation for a volume, the
  lack of 3D context available to these models can lead to striping
  artifacts in the 3D output, especially when viewed in planes other
  than the one used for prediction. To overcome this, a multi-axis
  prediction method is used, and the multiple predictions are merged by
  using maximum probability voting. It is hoped that in the future other
  merging techniques will be included such as fusion models
  (<xref alt="Perslev et al., 2019" rid="ref-perslev_one_2019" ref-type="bibr">Perslev
  et al., 2019</xref>). A schematic of the training and prediction
  processes performed by the <monospace>Volume Segmantics</monospace>
  package can be seen in
  <xref alt="Figure 1" rid="figU003Aschematic">Figure 1</xref>.</p>
  <fig>
    <caption><p>A schematic diagram showing the model training and
    segmentation prediction processes performed by the
    <monospace>Volume Segmantics</monospace>
    package.<styled-content id="figU003Aschematic"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/schematic_hig_res_crop.png" xlink:title="" />
  </fig>
  <sec id="state-of-the-field">
    <title>State of the field</title>
    <p>Currently there are a number of other software implementations
    available for segmentation of 3D data. Some of these also use 2D
    networks and combine prediction outputs to 3D, for example the
    <monospace>Multi-Planar U-Net</monospace> package
    (<xref alt="Perslev &amp; Igel, 2019" rid="ref-perslev_github_2019" ref-type="bibr">Perslev
    &amp; Igel, 2019</xref>) and the <monospace>CTSegNet</monospace>
    package
    (<xref alt="Tekawade &amp; Igel, 2020" rid="ref-tekawade_github_2020" ref-type="bibr">Tekawade
    &amp; Igel, 2020</xref>). However, neither of these packages allows
    the use of pre-trained encoders or multiple model architectures. In
    addition, the general purpose <monospace>pytorch-3dunet</monospace>
    package
    (<xref alt="Wolny, 2019" rid="ref-wolny_github_2019" ref-type="bibr">Wolny,
    2019</xref>) exists to allow training a 3D U-Net on image data,
    again without the time and resource advantages of pre-trained 2D
    networks.</p>
    <p>In the field of connectomics, several packages
    (<xref alt="Lee &amp; Turner, 2018" rid="ref-lee_deepem_2018" ref-type="bibr">Lee
    &amp; Turner, 2018</xref>;
    <xref alt="Lin et al., 2021" rid="ref-lin2021pytorch" ref-type="bibr">Lin
    et al., 2021</xref>;
    <xref alt="Urakubo et al., 2019" rid="ref-urakubo_uni-em_2019" ref-type="bibr">Urakubo
    et al., 2019</xref>;
    <xref alt="Wu, 2021" rid="ref-wu_neutorch_2021" ref-type="bibr">Wu,
    2021</xref>) enable the segmentation of structures within the brain,
    often from electron microscopy data, these could in principle be
    used in a subject and method-agnostic manner similarly to
    <monospace>Volume Segmantics</monospace>. One member of this set,
    the package <monospace>pytorch-connectomics</monospace>,
    (<xref alt="Lin et al., 2019" rid="ref-lin_pth_connec_github_2019" ref-type="bibr">Lin
    et al., 2019</xref>) allows training of 2D and 3D networks for
    segmentation of 3D image data as well as customisable strategies for
    data augmentation and multi-task and semi-supervised learning.
    Despite the versatility of this software, its deliberate focus on
    connectomics which is essential for effectiveness in this complex
    field, means that there are added levels of complexity for the
    generalist user. This specialisation also means that there is only
    one pre-trained 2D model architecture available, and some of the
    configuration and command-line options are context specific.</p>
  </sec>
  <sec id="real-world-usage">
    <title>Real-world usage</title>
    <p>During development of <monospace>Volume Segmantics</monospace>,
    the software was used to fine-tune pre-trained U-Net models on small
    amounts of annotated data in order to investigate the structures
    that interface maternal and fetal blood volumes in human placental
    tissue
    (<xref alt="Tun et al., 2021" rid="ref-tun_massively_2021" ref-type="bibr">Tun
    et al., 2021</xref>). In this study, expert annotation of volumes of
    size <inline-formula><alternatives>
    <tex-math><![CDATA[256^3]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>256</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[384^3]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>384</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
    were sufficient to create two models that gave accurate segmentation
    of two much larger synchrotron X-ray CT (SXCT) datasets
    (<inline-formula><alternatives>
    <tex-math><![CDATA[2520 \times 2520 \times 2120]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>2520</mml:mn><mml:mo>×</mml:mo><mml:mn>2520</mml:mn><mml:mo>×</mml:mo><mml:mn>2120</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    pixels). In a completely different context, SXCT datasets were
    collected on a soil system in which methane bubbles were forming in
    brine amongst sand particles. The utility of a pre-trained 2D U-Net
    was investigated to segment these variable-contrast image volumes in
    comparison to a 3D U-Net with no prior training
    (<xref alt="Alvarez-Borges et al., 2022" rid="ref-alvarez-borges_u-net_2022" ref-type="bibr">Alvarez-Borges
    et al., 2022</xref>). In this case, the training data ranged in size
    from <inline-formula><alternatives>
    <tex-math><![CDATA[384^3]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>384</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
    pixels to <inline-formula><alternatives>
    <tex-math><![CDATA[572^3]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>572</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>.
    As well as requiring less time to train than a 3D U-Net, the
    pre-trained 2D network provided more accurate segmentation
    results.</p>
  </sec>
  <sec id="the-api">
    <title>The API</title>
    <p>The API provided with the package allows segmentation models to
    be trained and used in other contexts. For example,
    <monospace>Volume Segmantics</monospace> has recently been
    integrated into <monospace>SuRVoS2</monospace>
    (<xref alt="Pennington et al., 2018" rid="ref-survos2U003A2018" ref-type="bibr">Pennington
    et al., 2018</xref>,
    <xref alt="2022" rid="ref-pennington_survos_2022" ref-type="bibr">2022</xref>),
    a client-server application with a GUI for annotating volumetric
    data. SuRVoS2 can be used to create the initial small region of
    interest (ROI) labels needed by
    <monospace>Volume Segmantics</monospace>, this is achieved by using
    machine learning models (e.g. random forests) which are trained
    through ‘scribbles’ drawn on the data. It is hoped that scientists
    using our synchrotron facility and beyond will be able to train and
    use their own deep learning models using this interface to the
    library. These models can then be used to segment data during their
    time using the synchrotron and also when back at their home
    institution. In addition, it is hoped that the scientific community
    will use and extend <monospace>Volume Segmantics</monospace> for
    their own purposes.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to acknowledge helpful discussions with Avery
  Pennington, Sharif Ahmed, Fernando Alvarez-Borges and Michele Darrow
  during the development of this project. Additional thanks to Luis
  Perdigao for inspiration and icons for the schematic figure.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-russakovsky_imagenet_2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Russakovsky</surname><given-names>Olga</given-names></name>
        <name><surname>Deng</surname><given-names>Jia</given-names></name>
        <name><surname>Su</surname><given-names>Hao</given-names></name>
        <name><surname>Krause</surname><given-names>Jonathan</given-names></name>
        <name><surname>Satheesh</surname><given-names>Sanjeev</given-names></name>
        <name><surname>Ma</surname><given-names>Sean</given-names></name>
        <name><surname>Huang</surname><given-names>Zhiheng</given-names></name>
        <name><surname>Karpathy</surname><given-names>Andrej</given-names></name>
        <name><surname>Khosla</surname><given-names>Aditya</given-names></name>
        <name><surname>Bernstein</surname><given-names>Michael</given-names></name>
        <name><surname>Berg</surname><given-names>Alexander C.</given-names></name>
        <name><surname>Fei-Fei</surname><given-names>Li</given-names></name>
      </person-group>
      <article-title>ImageNet Large Scale Visual Recognition Challenge</article-title>
      <source>International Journal of Computer Vision</source>
      <year iso-8601-date="2015-12">2015</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-05-21">2020</year><month>05</month><day>21</day></date-in-citation>
      <volume>115</volume>
      <issue>3</issue>
      <issn>1573-1405</issn>
      <uri>https://doi.org/10.1007/s11263-015-0816-y</uri>
      <pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id>
      <fpage>211</fpage>
      <lpage>252</lpage>
    </element-citation>
  </ref>
  <ref id="ref-perslev_one_2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Perslev</surname><given-names>Mathias</given-names></name>
        <name><surname>Dam</surname><given-names>Erik Bjørnager</given-names></name>
        <name><surname>Pai</surname><given-names>Akshay</given-names></name>
        <name><surname>Igel</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>One Network to Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation</article-title>
      <source>Medical Image Computing and Computer Assisted Intervention – MICCAI 2019</source>
      <person-group person-group-type="editor">
        <name><surname>Shen</surname><given-names>Dinggang</given-names></name>
        <name><surname>Liu</surname><given-names>Tianming</given-names></name>
        <name><surname>Peters</surname><given-names>Terry M.</given-names></name>
        <name><surname>Staib</surname><given-names>Lawrence H.</given-names></name>
        <name><surname>Essert</surname><given-names>Caroline</given-names></name>
        <name><surname>Zhou</surname><given-names>Sean</given-names></name>
        <name><surname>Yap</surname><given-names>Pew-Thian</given-names></name>
        <name><surname>Khan</surname><given-names>Ali</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <isbn>978-3-030-32245-8</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-030-32245-8_4</pub-id>
      <fpage>30</fpage>
      <lpage>38</lpage>
    </element-citation>
  </ref>
  <ref id="ref-buslaev_albumentations_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Buslaev</surname><given-names>Alexander</given-names></name>
        <name><surname>Iglovikov</surname><given-names>Vladimir I.</given-names></name>
        <name><surname>Khvedchenya</surname><given-names>Eugene</given-names></name>
        <name><surname>Parinov</surname><given-names>Alex</given-names></name>
        <name><surname>Druzhinin</surname><given-names>Mikhail</given-names></name>
        <name><surname>Kalinin</surname><given-names>Alexandr A.</given-names></name>
      </person-group>
      <article-title>Albumentations: Fast and Flexible Image Augmentations</article-title>
      <source>Information</source>
      <year iso-8601-date="2020">2020</year>
      <volume>11</volume>
      <issue>2</issue>
      <issn>2078-2489</issn>
      <uri>https://www.mdpi.com/2078-2489/11/2/125</uri>
      <pub-id pub-id-type="doi">10.3390/info11020125</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-alvarez-borges_u-net_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Alvarez-Borges</surname><given-names>Fernando Jesus</given-names></name>
        <name><surname>King</surname><given-names>Oliver N. F.</given-names></name>
        <name><surname>Madhusudhan</surname><given-names>B. N.</given-names></name>
        <name><surname>Connolley</surname><given-names>Thomas</given-names></name>
        <name><surname>Basham</surname><given-names>Mark</given-names></name>
        <name><surname>Ahmed</surname><given-names>Sharif I.</given-names></name>
      </person-group>
      <article-title>U-Net Segmentation Methods for Variable-Contrast XCT Images of Methane-Bearing Sand Using Small Training Datasets</article-title>
      <publisher-name>Earth; Space Science Open Archive</publisher-name>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <uri>http://www.essoar.org/doi/10.1002/essoar.10506807.2</uri>
      <pub-id pub-id-type="doi">10.1002/essoar.10506807.2</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tun_massively_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tun</surname><given-names>W. M.</given-names></name>
        <name><surname>Poologasundarampillai</surname><given-names>G.</given-names></name>
        <name><surname>Bischof</surname><given-names>H.</given-names></name>
        <name><surname>Nye</surname><given-names>G.</given-names></name>
        <name><surname>King</surname><given-names>O. N. F.</given-names></name>
        <name><surname>Basham</surname><given-names>M.</given-names></name>
        <name><surname>Tokudome</surname><given-names>Y.</given-names></name>
        <name><surname>Lewis</surname><given-names>R. M.</given-names></name>
        <name><surname>Johnstone</surname><given-names>E. D.</given-names></name>
        <name><surname>Brownbill</surname><given-names>P.</given-names></name>
        <name><surname>Darrow</surname><given-names>M.</given-names></name>
        <name><surname>Chernyavsky</surname><given-names>I. L.</given-names></name>
      </person-group>
      <article-title>A massively multi-scale approach to characterizing tissue architecture by synchrotron micro-CT applied to the human placenta</article-title>
      <source>Journal of The Royal Society Interface</source>
      <year iso-8601-date="2021-06">2021</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-21">2022</year><month>07</month><day>21</day></date-in-citation>
      <volume>18</volume>
      <issue>179</issue>
      <uri>https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0140</uri>
      <pub-id pub-id-type="doi">10.1098/rsif.2021.0140</pub-id>
      <fpage>20210140</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-YakubovskiyU003A2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yakubovskiy</surname><given-names>Pavel</given-names></name>
      </person-group>
      <article-title>Segmentation models pytorch</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/qubvel/segmentation_models.pytorch</uri>
    </element-citation>
  </ref>
  <ref id="ref-pennington_survos_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pennington</surname><given-names>Avery</given-names></name>
        <name><surname>King</surname><given-names>Oliver N. F.</given-names></name>
        <name><surname>Tun</surname><given-names>Win Min</given-names></name>
        <name><surname>Ho</surname><given-names>Elaine M. L.</given-names></name>
        <name><surname>Luengo</surname><given-names>Imanol</given-names></name>
        <name><surname>Darrow</surname><given-names>Michele C.</given-names></name>
        <name><surname>Basham</surname><given-names>Mark</given-names></name>
      </person-group>
      <article-title>SuRVoS 2: Accelerating Annotation and Segmentation for Large Volumetric Bioimage Workflows Across Modalities and Scales</article-title>
      <source>Frontiers in Cell and Developmental Biology</source>
      <year iso-8601-date="2022">2022</year>
      <volume>10</volume>
      <issn>2296-634X</issn>
      <uri>https://www.frontiersin.org/articles/10.3389/fcell.2022.842342</uri>
      <pub-id pub-id-type="doi">10.3389/fcell.2022.842342</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-survos2U003A2018">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Pennington</surname><given-names>Avery</given-names></name>
        <name><surname>King</surname><given-names>Oliver N. F.</given-names></name>
        <name><surname>Luengo</surname><given-names>Imanol</given-names></name>
        <name><surname>Basham</surname><given-names>Mark</given-names></name>
      </person-group>
      <article-title>SuRVoS2</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/DiamondLightSource/SuRVoS2</uri>
    </element-citation>
  </ref>
  <ref id="ref-perslev_github_2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Perslev</surname><given-names>Mathias</given-names></name>
        <name><surname>Igel</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>Multi-planar U-net</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/perslev/MultiPlanarUNet</uri>
    </element-citation>
  </ref>
  <ref id="ref-tekawade_github_2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tekawade</surname><given-names>Aniket</given-names></name>
        <name><surname>Igel</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>CTSegNet</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/aniketkt/CTSegNet</uri>
    </element-citation>
  </ref>
  <ref id="ref-wolny_github_2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wolny</surname><given-names>Adrian</given-names></name>
      </person-group>
      <article-title>Pytorch-3dunet</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/wolny/pytorch-3dunet</uri>
    </element-citation>
  </ref>
  <ref id="ref-lee_deepem_2018">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Kisuk</given-names></name>
        <name><surname>Turner</surname><given-names>Nicholas L.</given-names></name>
      </person-group>
      <article-title>DeepEM</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/seung-lab/DeepEM</uri>
    </element-citation>
  </ref>
  <ref id="ref-lin2021pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lin</surname><given-names>Zudi</given-names></name>
        <name><surname>Wei</surname><given-names>Donglai</given-names></name>
        <name><surname>Lichtman</surname><given-names>Jeff</given-names></name>
        <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>
      </person-group>
      <article-title>PyTorch connectomics: A scalable and flexible segmentation framework for EM connectomics</article-title>
      <source>arXiv preprint arXiv:2112.05754</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2112.05754</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-urakubo_uni-em_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Urakubo</surname><given-names>Hidetoshi</given-names></name>
        <name><surname>Bullmann</surname><given-names>Torsten</given-names></name>
        <name><surname>Kubota</surname><given-names>Yoshiyuki</given-names></name>
        <name><surname>Oba</surname><given-names>Shigeyuki</given-names></name>
        <name><surname>Ishii</surname><given-names>Shin</given-names></name>
      </person-group>
      <article-title>UNI-EM: An Environment for Deep Neural Network-Based Automated Segmentation of Neuronal Electron Microscopic Images</article-title>
      <source>Scientific Reports</source>
      <year iso-8601-date="2019-12">2019</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-09-14">2022</year><month>09</month><day>14</day></date-in-citation>
      <volume>9</volume>
      <issue>1</issue>
      <issn>2045-2322</issn>
      <uri>https://www.nature.com/articles/s41598-019-55431-0</uri>
      <pub-id pub-id-type="doi">10.1038/s41598-019-55431-0</pub-id>
      <fpage>19413</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lin_pth_connec_github_2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lin</surname><given-names>Zudi</given-names></name>
        <name><surname>Lu</surname><given-names>Yuhao</given-names></name>
        <name><surname>Belhamissi</surname><given-names>Mourad</given-names></name>
        <name><surname>Banerjee</surname><given-names>Atmadeep</given-names></name>
        <name><surname>Lauenburg</surname><given-names>Leander</given-names></name>
        <name><surname>Swaroop</surname><given-names>K. Krishna</given-names></name>
        <name><surname>Wei</surname><given-names>Donglai</given-names></name>
        <name><surname>Pfister</surname><given-names>Hanspeter</given-names></name>
      </person-group>
      <article-title>PyTorch connectomics</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/zudi-lin/pytorch_connectomics</uri>
    </element-citation>
  </ref>
  <ref id="ref-wu_neutorch_2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Jingpeng</given-names></name>
      </person-group>
      <article-title>Neutorch</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/flatironinstitute/neutorch</uri>
    </element-citation>
  </ref>
  <ref id="ref-NEURIPS2019_9015">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
