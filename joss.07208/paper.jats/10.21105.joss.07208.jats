<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7208</article-id>
<article-id pub-id-type="doi">10.21105/joss.07208</article-id>
<title-group>
<article-title>AMaze: a benchmark generator for sighted maze-navigating
agents</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-6033-3555</contrib-id>
<name>
<surname>Godin-Dubois</surname>
<given-names>Kevin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4942-3488</contrib-id>
<name>
<surname>Miras</surname>
<given-names>Karine</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4138-7024</contrib-id>
<name>
<surname>Kononova</surname>
<given-names>Anna V.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Vrije Universiteit Amsterdam, The Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Leiden University, The Netherlands</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-06-21">
<day>21</day>
<month>6</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>115</issue>
<fpage>7208</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Reinforcement Learning</kwd>
<kwd>NeurEvolution</kwd>
<kwd>Benchmark</kwd>
<kwd>Vision</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The need to provide fair comparisons between agents, especially in
  the field of Reinforcement Learning, has led to a plethora of
  benchmarks. While these are devoted to tailor-made problems, they
  offer with very little degrees of freedom for the experimenter. AMaze
  is instead a benchmark <italic>generator</italic> capable of producing
  human-intelligible environments of arbitrarily high complexity. By
  using visual cues in a maze-navigation task, the library empowers
  researchers across a large range of fields.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>AMaze is a pure-Python package with an emphasis on the easy and
  intuitive generation, evaluation and analysis of mazes. Its primary
  goal is to provide a way to quickly generate mazes of targeted
  difficulty, e.g., to test a Reinforcement Learning algorithm. By
  modeling loosely embodied robots with three distinct input/output
  spaces, AMaze makes it possible to prototype agent-centric scenarios
  of decision making, pattern recognition and general behavior through
  exposition to a wide array of contexts.</p>
  <fig>
    <caption><p>A sample maze from the AMaze library. In the API, every
    maze can be converted to and from a human-readable string where each
    underscore-separated component describes one of its facets. The
    <italic>seed</italic> seeds the random number generator used for the
    paths and stochastic placement of <italic>lures</italic> and
    <italic>traps</italic>. These have a specific probability, shape
    and/or value and may be specified multiple times to increase the
    complexity, as described in the
    documentation<xref ref-type="fn" rid="fn1">1</xref><styled-content id="figU003Amaze"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="../docs/latex/maze/light-wide.png" />
  </fig>
</sec>
<sec id="features">
  <title>Features</title>
  <p>Users of AMaze have two main components to take into consideration:
  mazes and agents. These are introduced below with more details
  available in the
  <ext-link ext-link-type="uri" xlink:href="https://amaze.readthedocs.io/en/latest/">documentation</ext-link>.</p>
  <sec id="mazes">
    <title>Mazes</title>
    <p>Mazes can be described by human-readable string as illustrated in
    <xref alt="[fig:maze]" rid="figU003Amaze">[fig:maze]</xref>, where
    every component is optional. The <italic>seed</italic> is used in
    the random number generator responsible for: a) the depth-first
    search that creates the paths and b) the stochastic placement of the
    <italic>lures</italic> and <italic>traps</italic>. As will be
    detailed below, agents only see a single cell at a time making
    intersections impossible to handle without additional information.
    <italic>Clues</italic> provide such an information by helpfully
    pointing towards the correct direction. However, users may
    additionally specify the presence of <italic>traps</italic>, at a
    given frequency, to replace a clue at an intersection. Traps always
    point towards the wrong direction thereby forcing agents to
    discriminate between the two. Furthermore, there is a lighter class
    of negative sign, namely <italic>lures</italic>, which occur outside
    of intersection and unhelpfully point towards an obviously bad
    direction (e.g. a wall).</p>
    <p>Mazes can broadly be grouped into classes according to the
    features they exhibit. The most <italic>trivial</italic> cases
    correspond to mazes with a single path (enforced by removing
    intersections). When intersections are labeled with appropriate
    clues, mazes are considered as <italic>simple</italic>.
    Additionally, exhibiting either lures or traps form the
    corresponding classes while the more general case with all types of
    signs is labeled as <italic>complex</italic>. To accurately compare
    between different types of mazes across multiple categories, the
    library provides, for any given maze <inline-formula><alternatives>
    <tex-math><![CDATA[M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>,
    two dedicated metrics, the surprisingness
    <inline-formula><alternatives>
    <tex-math><![CDATA[S_M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    and deceptiveness <inline-formula><alternatives>
    <tex-math><![CDATA[D_M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    defined as follows:</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[S_M = - \sum\limits_{i \in I_M} p(i) * log_2(p(i))]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
    <disp-formula><alternatives>
    <tex-math><![CDATA[D_M = \sum\limits_{c \in \text{cells}(M)}
               \sum\limits_{\substack{s \in \text{traps}(M)\\s[0:3] = c}}
                - p(s|c) log_2(p(s|c))]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>∈</mml:mo><mml:mtext mathvariant="normal">cells</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mtext mathvariant="normal">traps</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center" style="text-align: center"><mml:mrow><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:munder><mml:mo>−</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>which, informally, account for the likelihood of encountering
    different states (walls, signs) and different
    <italic>variations</italic> of a given cell (same walls, different
    signs). Through these metrics, experimenters can make an informed
    decision about the level of complexity of the mazes they use. As
    illustrated by the distributions of <inline-formula><alternatives>
    <tex-math><![CDATA[S_M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[D_M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
    sampled from 500’000 mazes across all five classes
    (<xref alt="[fig:complexity]" rid="figU003Acomplexity">[fig:complexity]</xref>),
    the space of all possible mazes is both diverse and arbitrarily
    complex.</p>
    <fig>
      <caption><p>Distribution of Surprisingness
      <inline-formula><alternatives>
      <tex-math><![CDATA[S_M]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>S</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      versus Deceptiveness <inline-formula><alternatives>
      <tex-math><![CDATA[D_M]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>D</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      across 500’000 unique mazes from all five different classes.
      Outlier mazes are depicted in the borders to illustrate the
      underlying Surprisingness (right column) or lack thereof (left
      column).<styled-content id="figU003Acomplexity"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="../docs/latex/complexity/light.png" />
    </fig>
    <fig>
      <caption><p>Discrete (left) and continuous (right) inputs for the
      examples shown in
      <xref alt="[fig:maze]" rid="figU003Amaze">[fig:maze]</xref>. The
      former is solely used for the fully discrete case while the latter
      covers both hybrid and fully continuous
      cases.<styled-content id="figU003Ainputs"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="../docs/latex/agents/light-1-3.png" />
    </fig>
  </sec>
  <sec id="agents">
    <title>Agents</title>
    <p>Agents in AMaze are loosely embodied robots that wander around
    mazes perceiving only local information (the cell they are in) and a
    one-item memory (the direction they come from, if any). To
    accommodate various use cases, these agents come in three different
    forms: fully discrete, fully continuous and hybrid. In the former
    case, an agent has access to something akin to a pre-processed
    input, as in
    <xref alt="[fig:inputs]" rid="figU003Ainputs">[fig:inputs]</xref>,
    where the first four fields describes the wall configuration and the
    remainder provide information about signs, if any. These can be
    distinguished through their luminosity as agents only perceive
    grayscale values. These observations are used to deduce the correct
    action out of the four cardinal directions.</p>
    <p>In the hybrid case, actions are identical while observations are
    coarse-grained images, of configurable size (e.g., 11x11 in
    <xref alt="[fig:inputs]" rid="figU003Ainputs">[fig:inputs]</xref>),
    where walls are indicated by pixels on the perimeter. The temporal
    information of the previous direction is still provided, as a single
    white pixel centered on the appropriate side. More importantly, the
    center of the image is used to display an arbitrary shape as a sign
    (clue, lure or trap). Finally, the fully continuous case is
    characterized by having the robot control its acceleration. Thus,
    the agent must also infer and take into consideration its position
    and inertia.</p>
  </sec>
</sec>
<sec id="comparison-to-existing-literature">
  <title>Comparison to existing literature</title>
  <p>AMaze differs from existing benchmarks on two important
  aspects:</p>
  <list list-type="bullet">
    <list-item>
      <p><italic>Computational efficiency</italic> when compared to
      alternative vision-based tasks</p>
    </list-item>
    <list-item>
      <p><italic>Extensive control</italic> over the environment and
      <italic>intuitive understanding</italic> of an agent’s
      behavior</p>
    </list-item>
  </list>
  <p>The former relates to the underlying LUT-based generation of visual
  information which alleviates the need for expensive rendering
  techniques. Through having only array pointers moving around, AMaze
  was designed to have fast-running simulations while still being
  directly usable with traditional architectures such as CNNs. On the
  latter point, the API allows precise tuning of many of a maze’s
  characteristics, in addition to random exploration. Additionally, as
  an agent behavior is a 2D trajectory in a maze, it is very
  straightforward for a human observer to interpret its behavior and
  determine what went right or wrong, and when
  (<xref alt="Godin-Dubois et al., 2025" rid="ref-GodinDubois2025" ref-type="bibr">Godin-Dubois
  et al., 2025</xref>).</p>
  <p>To illustrate the initial statements, we compare AMaze to a sample
  of benchmark suites
  (<xref alt="[tab:comparison]" rid="tabU003Acomparison">[tab:comparison]</xref>).
  This includes
  <ext-link ext-link-type="uri" xlink:href="https://gymnasium.farama.org/">gymnasium</ext-link>
  (<xref alt="Towers et al., 2023" rid="ref-Towers2023" ref-type="bibr">Towers
  et al., 2023</xref>), an ubiquitous benchmark suite in the Python
  ecosystem; Lab2D
  (<xref alt="Beattie et al., 2020" rid="ref-Beattie2020" ref-type="bibr">Beattie
  et al., 2020</xref>), a grid-world environment with both text and
  script parametrization; and Maze Explorer
  (<xref alt="Harries et al., 2019" rid="ref-Harries2019" ref-type="bibr">Harries
  et al., 2019</xref>), a customizable 3D maze platform based on the
  DOOM video-game. Indeed, while mazes are commonly used as evaluation
  environments in Machine Learning
  (<xref alt="Lehman &amp; Stanley, 2008" rid="ref-Lehman2008" ref-type="bibr">Lehman
  &amp; Stanley, 2008</xref>;
  <xref alt="Miconi et al., 2018" rid="ref-Miconi2018" ref-type="bibr">Miconi
  et al., 2018</xref>) they are often ad-hock solutions, deeply tied to
  a specific framework as in Beattie et al.
  (<xref alt="2016" rid="ref-Beattie2016" ref-type="bibr">2016</xref>).</p>
  <p>The test uses 81 variations of AMaze with different input image
  sizes (11x11, 15x15, 21x21), maze sizes (5, 10, 20), lure frequencies
  (0, 0.5, 1), and observation and action spaces (discrete, hybrid and
  continuous). This diversity of environment types was generated to give
  sufficient data for a fair comparison while also showcasing the ease
  with which AMaze can create feature-specific sets of mazes e.g. for
  benchmarking purposes. In the figure, <inline-formula><alternatives>
  <tex-math><![CDATA[N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
  is the number of unique environments used/provided by the library and
  Time is measured on 1000 time steps averaged over 10 replicates on an
  i7-1185G7 (3GHz). Discrete inputs are enumerable and finite while
  Continuous uses decimal values. Images can fall in either categories,
  but are characterized by a high number of inputs.</p>
  <fig>
    <caption><p>Comparison of AMaze with gymnasium’s environments suite.
    Inputs, Outputs and amount of human Control are taken from the
    documentation while Time is measured on 1000 timesteps averaged over
    10 replicates. AMaze is more computationally efficient than all but
    the simplest environments while also being the highly
    parametrizable.<styled-content id="tabU003Acomparison"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="../docs/latex/benchmarking/gym_pretty_table.pdf" />
  </fig>
  <p>Control describes how a human experimenter can specify, or at least
  influence, environmental features to suit their needs. Thus None
  implies fixed environments (most common) while various libraries use
  different methods to allow for customization such as the Lua scripting
  language (Lab2D), built-in Modes (ALE) or hand-made maps (Toy Text,
  Frozen Lake only). Extensive control requires a streamlined way to
  generate feature-specific custom environments with dense visual
  information.</p>
  <p>In terms of computational speed, while taking more time than
  Classical Control tasks
  (<xref alt="Barto et al., 1983" rid="ref-Barto1983" ref-type="bibr">Barto
  et al., 1983</xref>) or Toy Text environments
  (<xref alt="Sutton &amp; Barto, 2018" rid="ref-Sutton2018" ref-type="bibr">Sutton
  &amp; Barto, 2018</xref>), AMaze is demonstrably faster than those
  based on 2D
  (<ext-link ext-link-type="uri" xlink:href="https://box2d.org/">Box2d</ext-link>)
  or 3D
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/google-deepmind/mujoco">MuJoCo</ext-link>,
  Todorov et al.
  (<xref alt="2012" rid="ref-Todorov2012" ref-type="bibr">2012</xref>))
  simulators or the Arcade Learning Environment
  (<xref alt="Bellemare et al., 2013" rid="ref-Bellemare2013" ref-type="bibr">Bellemare
  et al., 2013</xref>).</p>
  <p>Given the broad range of generated environments, this comparison
  demonstrates how competitive the library is compared to existing
  alternatives with respect to its execution speed and
  customizability.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This research was funded by the Hybrid Intelligence Center, a
  10-year programme funded by the Dutch Ministry of Education, Culture
  and Science through the Netherlands Organisation for Scientific
  Research,
  <ext-link ext-link-type="uri" xlink:href="https://hybrid-intelligence-centre.nl">https://hybrid-intelligence-centre.nl</ext-link>,
  grant number 024.004.022.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Barto1983">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barto</surname><given-names>Andrew G.</given-names></name>
        <name><surname>Sutton</surname><given-names>Richard S.</given-names></name>
        <name><surname>Anderson</surname><given-names>Charles W.</given-names></name>
      </person-group>
      <article-title>Neuronlike adaptive elements that can solve difficult learning control problems</article-title>
      <source>IEEE Transactions on Systems, Man, and Cybernetics</source>
      <year iso-8601-date="1983-09">1983</year><month>09</month>
      <volume>SMC-13</volume>
      <issue>5</issue>
      <issn>0018-9472</issn>
      <uri>http://ieeexplore.ieee.org/document/6313077/</uri>
      <pub-id pub-id-type="doi">10.1109/TSMC.1983.6313077</pub-id>
      <fpage>834</fpage>
      <lpage>846</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Beattie2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Beattie</surname><given-names>Charles</given-names></name>
        <name><surname>Leibo</surname><given-names>Joel Z.</given-names></name>
        <name><surname>Teplyashin</surname><given-names>Denis</given-names></name>
        <name><surname>Ward</surname><given-names>Tom</given-names></name>
        <name><surname>Wainwright</surname><given-names>Marcus</given-names></name>
        <name><surname>Küttler</surname><given-names>Heinrich</given-names></name>
        <name><surname>Lefrancq</surname><given-names>Andrew</given-names></name>
        <name><surname>Green</surname><given-names>Simon</given-names></name>
        <name><surname>Valdés</surname><given-names>Víctor</given-names></name>
        <name><surname>Sadik</surname><given-names>Amir</given-names></name>
        <name><surname>Schrittwieser</surname><given-names>Julian</given-names></name>
        <name><surname>Anderson</surname><given-names>Keith</given-names></name>
        <name><surname>York</surname><given-names>Sarah</given-names></name>
        <name><surname>Cant</surname><given-names>Max</given-names></name>
        <name><surname>Cain</surname><given-names>Adam</given-names></name>
        <name><surname>Bolton</surname><given-names>Adrian</given-names></name>
        <name><surname>Gaffney</surname><given-names>Stephen</given-names></name>
        <name><surname>King</surname><given-names>Helen</given-names></name>
        <name><surname>Hassabis</surname><given-names>Demis</given-names></name>
        <name><surname>Legg</surname><given-names>Shane</given-names></name>
        <name><surname>Petersen</surname><given-names>Stig</given-names></name>
      </person-group>
      <article-title>DeepMind Lab</article-title>
      <year iso-8601-date="2016-12">2016</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-01-26">2024</year><month>01</month><day>26</day></date-in-citation>
      <uri>https://arxiv.org/abs/1612.03801v2</uri>
    </element-citation>
  </ref>
  <ref id="ref-Beattie2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Beattie</surname><given-names>Charles</given-names></name>
        <name><surname>Köppe</surname><given-names>Thomas</given-names></name>
        <name><surname>Duéñez-Guzmán</surname><given-names>Edgar A</given-names></name>
        <name><surname>Leibo</surname><given-names>Joel Z</given-names></name>
      </person-group>
      <article-title>DeepMind Lab2D</article-title>
      <year iso-8601-date="2020">2020</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-01-26">2024</year><month>01</month><day>26</day></date-in-citation>
      <uri>https://github.com/deepmind/lab2d</uri>
    </element-citation>
  </ref>
  <ref id="ref-Bellemare2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bellemare</surname><given-names>Marc G.</given-names></name>
        <name><surname>Naddaf</surname><given-names>Yavar</given-names></name>
        <name><surname>Veness</surname><given-names>Joel</given-names></name>
        <name><surname>Bowling</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>The Arcade Learning Environment: An Evaluation Platform for General Agents</article-title>
      <source>Journal of Artificial Intelligence Research</source>
      <year iso-8601-date="2013-06">2013</year><month>06</month>
      <volume>47</volume>
      <isbn>9781577357384</isbn>
      <issn>1076-9757</issn>
      <uri>https://jair.org/index.php/jair/article/view/10819</uri>
      <pub-id pub-id-type="doi">10.1613/jair.3912</pub-id>
      <fpage>253</fpage>
      <lpage>279</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Harries2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Harries</surname><given-names>Luke</given-names></name>
        <name><surname>Lee</surname><given-names>Sebastian</given-names></name>
        <name><surname>Rzepecki</surname><given-names>Jaroslaw</given-names></name>
        <name><surname>Hofmann</surname><given-names>Katja</given-names></name>
        <name><surname>Devlin</surname><given-names>Sam</given-names></name>
      </person-group>
      <article-title>MazeExplorer: A Customisable 3D Benchmark for Assessing Generalisation in Reinforcement Learning</article-title>
      <source>2019 IEEE Conference on Games (CoG)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2019-08">2019</year><month>08</month>
      <volume>2019-Augus</volume>
      <isbn>978-1-72811-884-0</isbn>
      <uri>https://ieeexplore.ieee.org/document/8848048/</uri>
      <pub-id pub-id-type="doi">10.1109/CIG.2019.8848048</pub-id>
      <fpage>1</fpage>
      <lpage>4</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Lehman2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lehman</surname><given-names>Joel</given-names></name>
        <name><surname>Stanley</surname><given-names>Kenneth O</given-names></name>
      </person-group>
      <article-title>Exploiting Open-Endedness to Solve Problems Through the Search for Novelty</article-title>
      <source>Artificial Life XI</source>
      <year iso-8601-date="2008">2008</year>
      <issue>Alife Xi</issue>
      <fpage>329</fpage>
      <lpage>336</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Miconi2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Miconi</surname><given-names>Thomas</given-names></name>
        <name><surname>Clune</surname><given-names>Jeff</given-names></name>
        <name><surname>Stanley</surname><given-names>Kenneth O.</given-names></name>
      </person-group>
      <article-title>Differentiable plasticity: Training plastic neural networks with backpropagation</article-title>
      <source>35th International Conference on Machine Learning, ICML 2018</source>
      <year iso-8601-date="2018">2018</year>
      <volume>8</volume>
      <isbn>9781510867963</isbn>
      <uri>https://proceedings.mlr.press/v80/miconi18a.html</uri>
      <fpage>5728</fpage>
      <lpage>5739</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Sutton2018">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Sutton</surname><given-names>Richard S</given-names></name>
        <name><surname>Barto</surname><given-names>Andrew G</given-names></name>
      </person-group>
      <source>Reinforcement learning: An introduction</source>
      <publisher-name>MIT press</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <isbn>9780262039246</isbn>
    </element-citation>
  </ref>
  <ref id="ref-Todorov2012">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Todorov</surname><given-names>Emanuel</given-names></name>
        <name><surname>Erez</surname><given-names>Tom</given-names></name>
        <name><surname>Tassa</surname><given-names>Yuval</given-names></name>
      </person-group>
      <article-title>MuJoCo: A physics engine for model-based control</article-title>
      <source>2012 IEEE/RSJ international conference on intelligent robots and systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2012-10">2012</year><month>10</month>
      <isbn>978-1-4673-1736-8</isbn>
      <uri>http://ieeexplore.ieee.org/document/6386109/</uri>
      <pub-id pub-id-type="doi">10.1109/IROS.2012.6386109</pub-id>
      <fpage>5026</fpage>
      <lpage>5033</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Towers2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Towers</surname><given-names>Mark</given-names></name>
        <name><surname>Terry</surname><given-names>Jordan K.</given-names></name>
        <name><surname>Kwiatkowski</surname><given-names>Ariel</given-names></name>
        <name><surname>Balis</surname><given-names>John U.</given-names></name>
        <name><surname>Cola</surname><given-names>Gianluca de</given-names></name>
        <name><surname>Deleu</surname><given-names>Tristan</given-names></name>
        <name><surname>Goulão</surname><given-names>Manuel</given-names></name>
        <name><surname>Kallinteris</surname><given-names>Andreas</given-names></name>
        <name><surname>KG</surname><given-names>Arjun</given-names></name>
        <name><surname>Krimmel</surname><given-names>Markus</given-names></name>
        <name><surname>Perez-Vicente</surname><given-names>Rodrigo</given-names></name>
        <name><surname>Pierré</surname><given-names>Andrea</given-names></name>
        <name><surname>Schulhoff</surname><given-names>Sander</given-names></name>
        <name><surname>Tai</surname><given-names>Jun Jet</given-names></name>
        <name><surname>Shen</surname><given-names>Andrew Tan Jin</given-names></name>
        <name><surname>Younis</surname><given-names>Omar G.</given-names></name>
      </person-group>
      <article-title>Gymnasium</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://zenodo.org/record/8127025</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.8269265</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-GodinDubois2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Godin-Dubois</surname><given-names>Kevin</given-names></name>
        <name><surname>Miras</surname><given-names>Karine</given-names></name>
        <string-name>Anna V Kononova</string-name>
      </person-group>
      <article-title>AMaze: An intuitive benchmark generator for fast prototyping of generalizable agents</article-title>
      <source>Frontiers in Artificial Intelligence</source>
      <year iso-8601-date="2025">2025</year>
      <volume>Volume 8 - 2025</volume>
      <issn>2624-8212</issn>
      <pub-id pub-id-type="doi">10.3389/frai.2025.1511712</pub-id>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://amaze.readthedocs.io/en/latest/">https://amaze.readthedocs.io/en/latest/</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
