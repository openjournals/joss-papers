<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6684</article-id>
<article-id pub-id-type="doi">10.21105/joss.06684</article-id>
<title-group>
<article-title>cuallee: A Python package for data quality checks across
multiple DataFrame APIs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1937-8006</contrib-id>
<name>
<surname>Vazquez</surname>
<given-names>Herminio</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8249-7182</contrib-id>
<name>
<surname>Grosboillot</surname>
<given-names>Virginie</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Researcher, Mexico</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Swiss Federal Institute of Technology (ETH)</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-12-11">
<day>11</day>
<month>12</month>
<year>2022</year>
</pub-date>
<volume>9</volume>
<issue>98</issue>
<fpage>6684</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>data quality</kwd>
<kwd>data checks</kwd>
<kwd>data unit tests</kwd>
<kwd>data pipelines</kwd>
<kwd>data validation</kwd>
<kwd>data observability</kwd>
<kwd>data lake</kwd>
<kwd>pyspark</kwd>
<kwd>duckdb</kwd>
<kwd>pandas</kwd>
<kwd>snowpark</kwd>
<kwd>polars</kwd>
<kwd>big data</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In today’s world, where vast amounts of data are generated and
  collected daily, and where data heavily influence business, political,
  and societal decisions, it is crucial to evaluate the quality of the
  data used for analysis, decision-making, and reporting. This involves
  understanding how reliable and trustworthy the data are. To address
  this need, we have created <monospace>cuallee</monospace>, a Python
  package for assessing data quality. <monospace>cuallee</monospace> is
  designed to be dataframe-agnostic, offering an intuitive and
  user-friendly API for describing checks across the most popular
  dataframe implementations such as PySpark, Pandas, Snowpark, Polars,
  DuckDB, and BigQuery. Currently, <monospace>cuallee</monospace> offers
  over 50 checks to help users evaluate the quality of their data.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>For data engineers and data scientists, maintaining a consistent
  workflow involves operating in hybrid environments, where they develop
  locally before transitioning data pipelines and analyses to
  cloud-based environments. Whilst working in local environments
  typically allows them to fit data sets in memory, moving workloads to
  cloud environments involve operating with full scale data that
  requires a different computing framework
  (<xref alt="Schelter et al., 2018" rid="ref-10.14778U002F3229863.3229867" ref-type="bibr">Schelter
  et al., 2018</xref>), i.e. distributed computing, parallelization, and
  horizontal scaling. <monospace>cuallee</monospace> accomodates the
  testing activities required by this shift in computing frameworks, in
  both local and remote environments, without the need to rewrite test
  scenarios or employ different testing approaches for assessing various
  quality dimensions of the data
  (<xref alt="Fadlallah et al., 2023b" rid="ref-10.1145U002F3603707" ref-type="bibr">Fadlallah
  et al., 2023b</xref>).</p>
  <p>An additional argument is related to the rapid evolution of the
  data ecosystem
  (<xref alt="Fadlallah et al., 2023a" rid="ref-10.1145U002F3603706" ref-type="bibr">Fadlallah
  et al., 2023a</xref>). Organizations and data teams are constantly
  seeking ways to improve, whether through cost-effective solutions or
  by integrating new capabilities into their data operations. However,
  this pursuit presents new challenges when migrating workloads from one
  technology to another. As information technology and data strategies
  become more resilient against vendor lock-ins, they turn to
  technologies that enable seamless operation across platforms, avoiding
  the chaos of fully re-implementing data products. In essence, with
  <monospace>cuallee</monospace> no data testing strategy needs to be
  rewritten or reformulated due to platform changes.</p>
  <p>One last argument in favor of using a quality tool such as
  <monospace>cuallee</monospace> is the need to integrate quality
  procedures into the early stages of data product development. Whether
  in industry or academia, there is often a tendency to prioritize
  functional aspects over quality, leading to less time being dedicated
  to quality activities. By providing a clear, easy-to-use, and
  adaptable programming interface for data quality, teams can
  incorporate quality into their development process, promoting a
  proactive approach of building quality in rather than relying solely
  on testing to ensure quality.</p>
</sec>
<sec id="data-quality-frameworks">
  <title>Data Quality Frameworks</title>
  <p>Data platforms have diversified from file systems and relational
  databases, to full ecosystems including the concept of data lakes
  (<xref alt="Dumke et al., 2020" rid="ref-10.3389U002Ffdata.2022.945720" ref-type="bibr">Dumke
  et al., 2020</xref>). Modern platforms host a variety of data formats,
  beyond traditional tabular data, including semi-structured like
  <monospace>JSON</monospace>
  (<xref alt="Pezoa et al., 2016" rid="ref-10.1145U002F2872427.2883029" ref-type="bibr">Pezoa
  et al., 2016</xref>) or unstructured like audio or images.</p>
  <p>Operating with modern data platforms, requires a versatile data
  processing framework capable to handle structured and unstructured
  data, supports data operations in various programming languages,
  fulfills the imperative and declarative form to data operations from
  practitioners and does it reliably for any size of data. Apache Spark
  (<xref alt="Armbrust et al., 2015" rid="ref-10.1145U002F2723372.2742797" ref-type="bibr">Armbrust
  et al., 2015</xref>) represents an exemplar framework due to the wide
  range of data processing capabilities —batch processing, real-time
  streaming, machine learning, and graph processing—within a unified
  framework commended and adopted
  (<xref alt="O’Reilly Media, 2023" rid="ref-oreilly2023technology" ref-type="bibr">O’Reilly
  Media, 2023</xref>) by the data industry.</p>
  <p><monospace>cuallee</monospace> is powered by native data engines,
  including Apache Spark, and offers a robust structure that can be
  extended to new engines with fully open-source implementation
  guidelines and rigorous testing. <monospace>pydeequ</monospace>
  (<xref alt="Schelter et al., 2018" rid="ref-10.14778U002F3229863.3229867" ref-type="bibr">Schelter
  et al., 2018</xref>) is a pioneer in large-scale data quality
  frameworks and is fully open-source. However, its adoption is limited
  due to the smaller community of developers proficient in the
  <monospace>scala</monospace> programming language.</p>
  <p>On the other hand, <monospace>great-expectations</monospace>
  (<xref alt="Gong et al., n.d." rid="ref-Gong_Great_Expectations" ref-type="bibr">Gong
  et al., n.d.</xref>) and <monospace>soda</monospace>
  (<xref alt="Soda Core, n.d." rid="ref-soda_core" ref-type="bibr"><italic>Soda
  Core</italic>, n.d.</xref>) additionaly to an open-source platform
  also offer commercial options that require registration and issuing of
  keys for cloud reporting capabilities.</p>
  <p><monospace>cuallee</monospace> provides a fully open-source data
  quality framework designed for both academia and industry
  practitioners, offering unparalleled performance compared to the
  aforementioned alternatives.</p>
  <sec id="performance-benchmark">
    <title>Performance Benchmark</title>
    <p>A reproducible performance benchmark is available in the code
    repository
    (<xref alt="Vazquez, 2024" rid="ref-cuallee_performance_tests" ref-type="bibr">Vazquez,
    2024</xref>). It consists of <monospace>38</monospace> checks over
    an open sourced data set
    (<xref alt="New York City Taxi and Limousine Commission, 2024" rid="ref-nyc_tlc_trip_record_data" ref-type="bibr">New
    York City Taxi and Limousine Commission, 2024</xref>) made of
    <monospace>19.8 million rows</monospace>. The validation performs
    <monospace>19</monospace> checks for <bold>completeness</bold> and
    <monospace>19</monospace> checks for <bold>uniqueness</bold> for
    each column of the dataset.</p>
    <p>The following table
    (<xref alt="Table 1" rid="tabU003Aperformance">Table 1</xref>)
    provides a summary of the performance comparison:</p>
    <table-wrap>
      <caption>
        <p>Performance comparison on popular data quality frameworks
        <styled-content id="tabU003Aperformance"></styled-content></p>
      </caption>
      <table>
        <thead>
          <tr>
            <th>Framework</th>
            <th>Definitions</th>
            <th>Time</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><monospace>great_expectations==0.18.13</monospace></td>
            <td><monospace>python</monospace></td>
            <td><monospace>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 66s</monospace></td>
          </tr>
          <tr>
            <td><monospace>soda==1.4.10</monospace></td>
            <td><monospace>yaml</monospace></td>
            <td><monospace>▇▇▇▇▇▇▇▇▇▇▇▇▇ 43s</monospace></td>
          </tr>
          <tr>
            <td><monospace>pydeequ==1.3.0</monospace></td>
            <td><monospace>python</monospace></td>
            <td><monospace>▇▇▇ 11s</monospace></td>
          </tr>
          <tr>
            <td><monospace>cuallee==0.10.3</monospace></td>
            <td><monospace>python</monospace></td>
            <td><monospace>▇▇ 7s</monospace></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
</sec>
<sec id="methods">
  <title>Methods</title>
  <p><monospace>cuallee</monospace> employs a heuristic-based approach
  to define quality rules for each dataset. This prevents the
  inadvertent duplication of quality predicates, thus reducing the
  likelihood of human error in defining rules with identical predicates.
  Several studies have been conducted on the efficiency of these rules,
  including auto-validation and auto-definition using profilers
  (<xref alt="Tu et al., 2023" rid="ref-10.1145U002F3580305.3599776" ref-type="bibr">Tu
  et al., 2023</xref>).</p>
</sec>
<sec id="checks">
  <title>Checks</title>
  <p>In <monospace>cuallee</monospace>, checks serve as the fundamental
  concept. These checks
  (<xref alt="Table 2" rid="tabU003Achecks">Table 2</xref>) are
  implemented by <bold>rules</bold>, which specify <italic>quality
  predicates</italic>. These predicates, when aggregated, form the
  criteria used to evaluate the quality of a dataset. Efforts to
  establish a universal quality metric
  (<xref alt="Pleimling et al., 2022" rid="ref-10.1145U002F3529190.3529222" ref-type="bibr">Pleimling
  et al., 2022</xref>) typically involve using statistics and combining
  dimensions to derive a single reference value that encapsulates
  overall quality attributes.</p>
  <table-wrap>
    <caption>
      <p>List and description of the currently available checks
      <styled-content id="tabU003Achecks"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="32%" />
        <col width="50%" />
        <col width="18%" />
      </colgroup>
      <thead>
        <tr>
          <th>Check</th>
          <th>Description</th>
          <th>DataType</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><monospace>is_complete</monospace></td>
          <td>Zero <monospace>nulls</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_unique</monospace></td>
          <td>Zero <monospace>duplicates</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_primary_key</monospace></td>
          <td>Zero <monospace>duplicates</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>are_complete</monospace></td>
          <td>Zero <monospace>nulls</monospace> on group of columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>are_unique</monospace></td>
          <td>Composite primary key check</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_composite_key</monospace></td>
          <td>Zero duplicates on multiple columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_greater_than</monospace></td>
          <td><monospace>col &gt; x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_positive</monospace></td>
          <td><monospace>col &gt; 0</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_negative</monospace></td>
          <td><monospace>col &lt; 0</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_greater_or_equal_than</monospace></td>
          <td><monospace>col &gt;= x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_less_than</monospace></td>
          <td><monospace>col &lt; x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_less_or_equal_than</monospace></td>
          <td><monospace>col &lt;= x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_equal_than</monospace></td>
          <td><monospace>col == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_contained_in</monospace></td>
          <td><monospace>col in [a, b, c, ...]</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in</monospace></td>
          <td>Alias of <monospace>is_contained_in</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>not_contained_in</monospace></td>
          <td><monospace>col not in [a, b, c, ...]</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>not_in</monospace></td>
          <td>Alias of <monospace>not_contained_in</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>is_between</monospace></td>
          <td><monospace>a &lt;= col &lt;= b</monospace></td>
          <td><italic>numeric, date</italic></td>
        </tr>
        <tr>
          <td><monospace>has_pattern</monospace></td>
          <td>Matching a pattern defined as a
          <monospace>regex</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>is_legit</monospace></td>
          <td>String not null &amp; not empty
          <monospace>^\S$</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>has_min</monospace></td>
          <td><monospace>min(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_max</monospace></td>
          <td><monospace>max(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_std</monospace></td>
          <td><monospace>σ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_mean</monospace></td>
          <td><monospace>μ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_sum</monospace></td>
          <td><monospace>Σ(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_percentile</monospace></td>
          <td><monospace>%(col) == x</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_cardinality</monospace></td>
          <td><monospace>count(distinct(col)) == x</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_max_by</monospace></td>
          <td>A utilitary predicate for
          <monospace>max(col_a) == x for max(col_b)</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_min_by</monospace></td>
          <td>A utilitary predicate for
          <monospace>min(col_a) == x for min(col_b)</monospace></td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>has_correlation</monospace></td>
          <td>Finds correlation between <monospace>0..1</monospace> on
          <monospace>corr(col_a, col_b)</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>has_entropy</monospace></td>
          <td>Calculates the entropy of a column
          <monospace>entropy(col) == x</monospace> for classification
          problems</td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_inside_iqr</monospace></td>
          <td>Verifies column values reside inside limits of
          interquartile range
          <monospace>Q1 &lt;= col &lt;= Q3</monospace> used on
          anomalies.</td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in_millions</monospace></td>
          <td><monospace>col &gt;= 1e6</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_in_billions</monospace></td>
          <td><monospace>col &gt;= 1e9</monospace></td>
          <td><italic>numeric</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_1</monospace></td>
          <td>For date fields confirms 1 day ago
          <monospace>t-1</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_2</monospace></td>
          <td>For date fields confirms 2 days ago
          <monospace>t-2</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_3</monospace></td>
          <td>For date fields confirms 3 days ago
          <monospace>t-3</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_t_minus_n</monospace></td>
          <td>For date fields confirms n days ago
          <monospace>t-n</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_today</monospace></td>
          <td>For date fields confirms day is current date
          <monospace>t-0</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_yesterday</monospace></td>
          <td>For date fields confirms 1 day ago
          <monospace>t-1</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_weekday</monospace></td>
          <td>For date fields confirms day is between
          <monospace>Mon-Fri</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_weekend</monospace></td>
          <td>For date fields confirms day is between
          <monospace>Sat-Sun</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_monday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Mon</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_tuesday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Tue</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_wednesday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Wed</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_thursday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Thu</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_friday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Fri</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_saturday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Sat</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_sunday</monospace></td>
          <td>For date fields confirms day is
          <monospace>Sun</monospace></td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>is_on_schedule</monospace></td>
          <td>For date fields confirms time windows
          i.e. <monospace>9:00 - 17:00</monospace></td>
          <td><italic>timestamp</italic></td>
        </tr>
        <tr>
          <td><monospace>is_daily</monospace></td>
          <td>Can verify daily continuity on date fields by default.
          <monospace>[2,3,4,5,6]</monospace> which represents
          <monospace>Mon-Fri</monospace> in PySpark. However new
          schedules can be used for custom date continuity</td>
          <td><italic>date</italic></td>
        </tr>
        <tr>
          <td><monospace>has_workflow</monospace></td>
          <td>Adjacency matrix validation on
          <monospace>3-column</monospace> graph, based on
          <monospace>group</monospace>, <monospace>event</monospace>,
          <monospace>order</monospace> columns.</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>satisfies</monospace></td>
          <td>An open <monospace>SQL expression</monospace> builder to
          construct custom checks</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>validate</monospace></td>
          <td>The ultimate transformation of a check with a
          <monospace>dataframe</monospace> input for validation</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>iso.iso_4217</monospace></td>
          <td>currency compliant <monospace>ccy</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>iso.iso_3166</monospace></td>
          <td>country compliant <monospace>country</monospace></td>
          <td><italic>string</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.completeness</monospace></td>
          <td>Zero <monospace>nulls</monospace> all columns</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.percentage_fill</monospace></td>
          <td><monospace>% rows</monospace> not empty</td>
          <td><italic>agnostic</italic></td>
        </tr>
        <tr>
          <td><monospace>Control.percentage_empty</monospace></td>
          <td><monospace>% rows</monospace> empty</td>
          <td><italic>agnostic</italic></td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-10.1145U002F3603707">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fadlallah</surname><given-names>Hadi</given-names></name>
        <name><surname>Kilany</surname><given-names>Rima</given-names></name>
        <name><surname>Dhayne</surname><given-names>Houssein</given-names></name>
        <name><surname>El Haddad</surname><given-names>Rami</given-names></name>
        <name><surname>Haque</surname><given-names>Rafiqul</given-names></name>
        <name><surname>Taher</surname><given-names>Yehia</given-names></name>
        <name><surname>Jaber</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>Context-aware big data quality assessment: A scoping review</article-title>
      <source>Journal of Data and Information Quality</source>
      <year iso-8601-date="2023-06">2023</year><month>06</month>
      <volume>15</volume>
      <pub-id pub-id-type="doi">10.1145/3603707</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3603706">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fadlallah</surname><given-names>Hadi</given-names></name>
        <name><surname>Kilany</surname><given-names>Rima</given-names></name>
        <name><surname>Dhayne</surname><given-names>Houssein</given-names></name>
        <name><surname>El Haddad</surname><given-names>Rami</given-names></name>
        <name><surname>Haque</surname><given-names>Rafiqul</given-names></name>
        <name><surname>Taher</surname><given-names>Yehia</given-names></name>
        <name><surname>Jaber</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>BIGQA: Declarative big data quality assessment</article-title>
      <source>Journal of Data and Information Quality</source>
      <year iso-8601-date="2023-06">2023</year><month>06</month>
      <volume>15</volume>
      <pub-id pub-id-type="doi">10.1145/3603706</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3580305.3599776">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tu</surname><given-names>Dezhan</given-names></name>
        <name><surname>He</surname><given-names>Yeye</given-names></name>
        <name><surname>Cui</surname><given-names>Weiwei</given-names></name>
        <name><surname>Ge</surname><given-names>Song</given-names></name>
        <name><surname>Zhang</surname><given-names>Haidong</given-names></name>
        <name><surname>Han</surname><given-names>Shi</given-names></name>
        <name><surname>Zhang</surname><given-names>Dongmei</given-names></name>
        <name><surname>Chaudhuri</surname><given-names>Surajit</given-names></name>
      </person-group>
      <article-title>Auto-validate by-history: Auto-program data quality constraints to validate recurring data pipelines</article-title>
      <source>Proceedings of the 29th ACM SIGKDD conference on knowledge discovery and data mining</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <isbn>9798400701030</isbn>
      <uri>https://doi.org/10.1145/3580305.3599776</uri>
      <pub-id pub-id-type="doi">10.1145/3580305.3599776</pub-id>
      <fpage>4991</fpage>
      <lpage>5003</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.14778U002F3229863.3229867">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schelter</surname><given-names>Sebastian</given-names></name>
        <name><surname>Lange</surname><given-names>Dustin</given-names></name>
        <name><surname>Schmidt</surname><given-names>Philipp</given-names></name>
        <name><surname>Celikel</surname><given-names>Meltem</given-names></name>
        <name><surname>Biessmann</surname><given-names>Felix</given-names></name>
        <name><surname>Grafberger</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>Automating large-scale data quality verification</article-title>
      <source>Proc. VLDB Endow.</source>
      <publisher-name>VLDB Endowment</publisher-name>
      <year iso-8601-date="2018-08">2018</year><month>08</month>
      <volume>11</volume>
      <issue>12</issue>
      <issn>2150-8097</issn>
      <uri>https://doi.org/10.14778/3229863.3229867</uri>
      <pub-id pub-id-type="doi">10.14778/3229863.3229867</pub-id>
      <fpage>1781</fpage>
      <lpage>1794</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3529190.3529222">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pleimling</surname><given-names>Xavier</given-names></name>
        <name><surname>Shah</surname><given-names>Vedant</given-names></name>
        <name><surname>Lourentzou</surname><given-names>Ismini</given-names></name>
      </person-group>
      <article-title>[Data] quality lies in the eyes of the beholder</article-title>
      <source>Proceedings of the 15th international conference on PErvasive technologies related to assistive environments</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>9781450396318</isbn>
      <uri>https://doi.org/10.1145/3529190.3529222</uri>
      <pub-id pub-id-type="doi">10.1145/3529190.3529222</pub-id>
      <fpage>118</fpage>
      <lpage>124</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F2723372.2742797">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Armbrust</surname><given-names>Michael</given-names></name>
        <name><surname>Xin</surname><given-names>Reynold S.</given-names></name>
        <name><surname>Lian</surname><given-names>Cheng</given-names></name>
        <name><surname>Huai</surname><given-names>Yin</given-names></name>
        <name><surname>Liu</surname><given-names>Davies</given-names></name>
        <name><surname>Bradley</surname><given-names>Joseph K.</given-names></name>
        <name><surname>Meng</surname><given-names>Xiangrui</given-names></name>
        <name><surname>Kaftan</surname><given-names>Tomer</given-names></name>
        <name><surname>Franklin</surname><given-names>Michael J.</given-names></name>
        <name><surname>Ghodsi</surname><given-names>Ali</given-names></name>
        <name><surname>Zaharia</surname><given-names>Matei</given-names></name>
      </person-group>
      <article-title>Spark SQL: Relational data processing in Spark</article-title>
      <source>Proceedings of the 2015 ACM SIGMOD international conference on management of data</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>9781450327589</isbn>
      <uri>https://doi.org/10.1145/2723372.2742797</uri>
      <pub-id pub-id-type="doi">10.1145/2723372.2742797</pub-id>
      <fpage>1383</fpage>
      <lpage>1394</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F2872427.2883029">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pezoa</surname><given-names>Felipe</given-names></name>
        <name><surname>Reutter</surname><given-names>Juan</given-names></name>
        <name><surname>Suarez</surname><given-names>Fernando</given-names></name>
        <name><surname>Ugarte</surname><given-names>Martin</given-names></name>
        <name><surname>Vrgoč</surname><given-names>Domagoj</given-names></name>
      </person-group>
      <article-title>Foundations of JSON schema</article-title>
      <year iso-8601-date="2016-04">2016</year><month>04</month>
      <pub-id pub-id-type="doi">10.1145/2872427.2883029</pub-id>
      <fpage>263</fpage>
      <lpage>273</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.3389U002Ffdata.2022.945720">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dumke</surname><given-names>André R.</given-names></name>
        <name><surname>Parchmann</surname><given-names>Andreas</given-names></name>
        <name><surname>Schmid</surname><given-names>Stefan</given-names></name>
        <name><surname>Hauswirth</surname><given-names>Manfred</given-names></name>
      </person-group>
      <article-title>Toward data lakes as central building blocks for data management and analysis</article-title>
      <source>Frontiers in Big Data</source>
      <publisher-name>Frontiers</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>3</volume>
      <pub-id pub-id-type="doi">10.3389/fdata.2022.945720</pub-id>
      <fpage>564115</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-oreilly2023technology">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>O’Reilly Media</string-name>
      </person-group>
      <article-title>Technology trends for 2023</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.oreilly.com/radar/technology-trends-for-2023/</uri>
    </element-citation>
  </ref>
  <ref id="ref-cuallee_performance_tests">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Vazquez</surname><given-names>Herminio</given-names></name>
      </person-group>
      <article-title>Cuallee: Performance tests</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/canimus/cuallee/tree/main/test/performance</uri>
    </element-citation>
  </ref>
  <ref id="ref-nyc_tlc_trip_record_data">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>New York City Taxi and Limousine Commission</string-name>
      </person-group>
      <article-title>TLC trip record data</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page</uri>
    </element-citation>
  </ref>
  <ref id="ref-Gong_Great_Expectations">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Gong</surname><given-names>Abe</given-names></name>
        <name><surname>Campbell</surname><given-names>James</given-names></name>
        <string-name>Great Expectations</string-name>
      </person-group>
      <article-title>Great Expectations</article-title>
      <uri>https://github.com/great-expectations/great_expectations</uri>
    </element-citation>
  </ref>
  <ref id="ref-soda_core">
    <element-citation publication-type="software">
      <article-title>Soda core</article-title>
      <uri>https://github.com/sodadata/soda-core</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
