<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7210</article-id>
<article-id pub-id-type="doi">10.21105/joss.07210</article-id>
<title-group>
<article-title>Annotate-Lab: Simplifying Image
Annotation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4345-1050</contrib-id>
<name>
<surname>Kunwar</surname>
<given-names>Suman</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Faculty of Computer Science, Selinus University of Sciences
and Literature, Ragusa, Italy</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-06-12">
<day>12</day>
<month>6</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>103</issue>
<fpage>7210</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Image Annotation</kwd>
<kwd>Open-Source Tools</kwd>
<kwd>Machine Learning</kwd>
<kwd>Computer Vision</kwd>
<kwd>Annotation Software</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Annotate-Lab is an open-source image annotation tool designed to
  enhance the efficiency and accuracy of the annotation process through
  a client-server architecture. The application consists of a
  React-based client that offers an intuitive user interface for
  performing annotations, and a Flask-based server that manages the
  storage and generation of annotated images and configuration settings.
  This robust combination makes Annotate-Lab a versatile solution for
  various fields requiring precise image labeling, such as machine
  learning, medical imaging, and autonomous driving.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Image annotation is a critical task in various fields, including
  machine learning, computer vision
  (<xref alt="Ojha et al., 2017" rid="ref-ojha_image_2017" ref-type="bibr">Ojha
  et al., 2017</xref>), medical imaging
  (<xref alt="Aljabri et al., 2022" rid="ref-aljabri_towards_2022" ref-type="bibr">Aljabri
  et al., 2022</xref>), and autonomous driving
  (<xref alt="Liu et al., 2024" rid="ref-liu_survey_2024" ref-type="bibr">Liu
  et al., 2024</xref>). Annotating images involves labeling specific
  elements within an image, a process crucial for training algorithms to
  recognize and interpret visual data. Given the growing reliance on
  annotated datasets for accurate model training, there’s a significant
  need for efficient, user-friendly, and customizable annotation tools.
  Traditional methods often involve manual processes that are
  time-consuming and prone to errors, highlighting the necessity for
  automated and semi-automated solutions that enhance accuracy and
  efficiency.</p>
</sec>
<sec id="related-software">
  <title>Related Software</title>
  <p>Several image annotation tools are available, each catering to
  different aspects of the annotation process. Some of the notable ones
  include:</p>
  <list list-type="order">
    <list-item>
      <p><bold><ext-link ext-link-type="uri" xlink:href="https://github.com/HumanSignal/label-studio">Label
      Studio</ext-link></bold>: A graphical image annotation tool that
      is easy to use and supports various annotation formats. It’s
      popular for creating bounding boxes for object detection
      tasks.</p>
    </list-item>
    <list-item>
      <p><bold><ext-link ext-link-type="uri" xlink:href="https://www.robots.ox.ac.uk/~vgg/software/via/">VGG
      Image Annotator (VIA)</ext-link></bold>: A lightweight and
      standalone application for image, audio, and video annotation
      (<xref alt="Dutta &amp; Zisserman, 2019" rid="ref-dutta_via_2019" ref-type="bibr">Dutta
      &amp; Zisserman, 2019</xref>). It is browser-based and does not
      require installation.</p>
    </list-item>
    <list-item>
      <p><bold><ext-link ext-link-type="uri" xlink:href="https://github.com/jsbroks/coco-annotator">COCO
      Annotator</ext-link></bold>: An open-source web-based image
      annotation tool that supports the COCO format. It provides
      functionalities for bounding boxes, segmentation masks, and
      keypoints.</p>
    </list-item>
    <list-item>
      <p><bold><ext-link ext-link-type="uri" xlink:href="https://www.superannotate.com/">SuperAnnotate</ext-link></bold>:
      A more advanced and collaborative platform that offers tools for
      annotating images and videos, focusing on high-quality output for
      complex tasks.</p>
    </list-item>
    <list-item>
      <p><bold><ext-link ext-link-type="uri" xlink:href="https://github.com/cvat-ai/cvat">CVAT
      (Computer Vision Annotation Tool)</ext-link></bold>: An
      open-source tool designed for the annotation of images and videos,
      providing a robust set of features for different types of
      annotations such as bounding boxes, polylines, and segmentation
      (<xref alt="CVAT.ai Corporation, 2024" rid="ref-cvat_ai_corporation_computer_2024" ref-type="bibr">CVAT.ai
      Corporation, 2024</xref>).</p>
    </list-item>
  </list>
  <p>Compared to these tools, Annotate-Lab stands out for its
  open-source, community-driven nature and its client-server
  architecture. This setup enhances customizability, flexibility, and
  scalability by separating the React-based client from the Flask-based
  server.</p>
</sec>
<sec id="application-overview">
  <title>Application Overview</title>
  <p><bold>Annotate-Lab</bold> is an open-source image annotation
  application designed to streamline the annotation process through a
  client-server architecture. The application comprises two main
  components:</p>
  <list list-type="order">
    <list-item>
      <p><bold>Client (React Application)</bold>:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Functionality</bold>: The client component provides
          the user interface where users interact with the application
          to perform annotations. The interface is designed to be
          intuitive, allowing users to efficiently label images with
          various annotation tools.</p>
        </list-item>
        <list-item>
          <p><bold>Features</bold>: It includes tools for drawing
          bounding boxes, segmentation masks, and other annotation
          types. Users can also configure settings to tailor the
          annotation process to specific needs.</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><bold>Server (Flask Application)</bold>:</p>
      <list list-type="bullet">
        <list-item>
          <p><bold>Functionality</bold>: The server component handles
          the backend processes, including persisting annotated changes
          and generating masked and annotated images. It ensures that
          all annotations are stored securely and can be retrieved or
          modified as needed.</p>
        </list-item>
        <list-item>
          <p><bold>Features</bold>: The server also manages
          configuration settings, enabling customization of the
          annotation process to suit different project requirements. It
          supports the generation of various output formats necessary
          for training machine learning models.</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p><bold>Annotate-Lab</bold> aims to provide a comprehensive solution
  for image annotation, combining ease of use with powerful features to
  support a wide range of applications. Its open-source nature allows
  for community contributions and customizability, making it a versatile
  tool for both academic and commercial purposes.</p>
</sec>
<sec id="example-annotating-a-bicycle">
  <title>Example: Annotating a Bicycle</title>
  <p>This example guides users through the process of annotating a
  bicycle and reviewing the results. The process begins with reviewing
  task details in the “Task Info” tab to understand the objectives for
  segmenting the annotated image. In the “Configuration” tab, users set
  up annotation tools and label names to prepare for annotation.
  Following setup, users upload images to be annotated in the “Image”
  tab to initiate the annotation process.
  <xref alt="[fig:configuration_setup]" rid="figU003Aconfiguration_setup">[fig:configuration_setup]</xref>
  shows the tabs.</p>
  <fig>
    <caption><p>Setup Configuration
    <styled-content id="figU003Aconfiguration_setup"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./setup.png" />
  </fig>
  <p>During annotation, users select tools such as the circle tool to
  highlight specific areas of interest on the bicycle, labeling each
  region and adding contextual comments as needed.
  <xref alt="[fig:annotation_example]" rid="figU003Aannotation_example">[fig:annotation_example]</xref>
  shows the annotation of tyre in bicycle.</p>
  <fig>
    <caption><p>Annotating Bicycle
    <styled-content id="figU003Aannotation_example"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./annotation.png" />
  </fig>
  <p>Once annotations are complete, users save their work to preserve
  changes. Finally, users can download configuration settings, annotated
  images, or masked images using the download button based on their
  requirements. The annoted and masked images are shown in
  <xref alt="[fig:masked_output]" rid="figU003Amasked_output">[fig:masked_output]</xref>.</p>
  <fig>
    <caption><p>Downloaded annotated image mask and annotated image
    <styled-content id="figU003Amasked_output"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./download_outputs.png" />
  </fig>
  <p>The downloaded configurations provides the regions information
  along with co-ordinates, class and comments. The downloaded
  configuration of above annotation is shown below.</p>
  <code language="json">{
   &quot;configuration&quot;:[
      {
         &quot;image-name&quot;:&quot;bianchi.jpeg&quot;,
         &quot;regions&quot;:[
            {
               &quot;region-id&quot;:&quot;06503829808876826&quot;,
               &quot;image-src&quot;:&quot;http://127.0.0.1:5000/uploads/bianchi.jpeg&quot;,
               &quot;class&quot;:&quot;Tyre&quot;,
               &quot;comment&quot;:&quot;Back Tyre&quot;,
               &quot;tags&quot;:&quot;&quot;,
               &quot;rx&quot;:[
                  0.05432613622740995
               ],
               &quot;ry&quot;:[
                  0.350004519235816
               ],
               &quot;rw&quot;:[
                  0.3601621878393051
               ],
               &quot;rh&quot;:[
                  0.5781758957654723
               ]
            }
         ],
         &quot;color-map&quot;:{
            &quot;Tyre&quot;:[
               244,
               67,
               54
            ]
         }
      }
   ]
}</code>
  <p>This tool also supports the
  <ext-link ext-link-type="uri" xlink:href="https://docs.ultralytics.com/datasets/detect/#ultralytics-yolo-format">YOLO
  format</ext-link>. A dataset of ripe and unripe tomatoes has been
  created and can be found on
  <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/sumn2u/riped-and-unriped-tomato-dataset">Kaggle</ext-link>.
  <xref alt="[fig:annotated_tomatoes]" rid="figU003Aannotated_tomatoes">[fig:annotated_tomatoes]</xref>
  shows the original and annotated tomatoes from that dataset. Besides
  this, it also selects the bounding box using the Segment Anything
  Model (SAM)
  (<xref alt="Kirillov et al., 2023" rid="ref-kirillov_segment_2023" ref-type="bibr">Kirillov
  et al., 2023</xref>).</p>
  <fig>
    <caption><p>Annotated Tomatoes
    <styled-content id="figU003Aannotated_tomatoes"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./annotated_tomatoes.png" />
  </fig>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Annotate-Lab stands out as a robust and user-friendly open-source
  solution for image annotation. By leveraging a client-server
  architecture, it effectively separates the user interface from backend
  processes, ensuring a smooth and efficient annotation workflow. The
  React-based client provides an intuitive interface for performing
  annotations, while the Flask-based server handles data persistence,
  configuration, and the generation of annotated images. This
  comprehensive approach makes Annotate-Lab a valuable tool for various
  applications, including machine learning, computer vision, and medical
  imaging, among others. Its open-source nature also encourages
  community contributions and customization, enhancing its potential for
  widespread adoption.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the developers and contributors of Annotate-Lab for their
  dedication and hard work. We also acknowledge idapgroup’s
  <ext-link ext-link-type="uri" xlink:href="https://github.com/idapgroup/react-image-annotate"><monospace>react-image-annotate</monospace></ext-link>
  package and Ghahramanpour’s
  <ext-link ext-link-type="uri" xlink:href="https://github.com/gnamiro/image_annotator/tree/master"><monospace>image_annotator</monospace></ext-link>
  project, whose work has been instrumental in the development of this
  application.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-dutta_via_2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dutta</surname><given-names>Abhishek</given-names></name>
        <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>The via annotation software for images, audio and video</article-title>
      <source>Proceedings of the 27th ACM International Conference on Multimedia</source>
      <publisher-name>ACM</publisher-name>
      <publisher-loc>Nice France</publisher-loc>
      <year iso-8601-date="2019-10">2019</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-06-13">2024</year><month>06</month><day>13</day></date-in-citation>
      <isbn>9781450368896</isbn>
      <uri>https://dl.acm.org/doi/10.1145/3343031.3350535</uri>
      <pub-id pub-id-type="doi">10.1145/3343031.3350535</pub-id>
      <fpage>2276</fpage>
      <lpage>2279</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cvat_ai_corporation_computer_2024">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>CVAT.ai Corporation</string-name>
      </person-group>
      <article-title>Computer vision annotation tool(Cvat)</article-title>
      <publisher-name>CVAT.ai Corporation</publisher-name>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-06-19">2024</year><month>06</month><day>19</day></date-in-citation>
      <uri>https://zenodo.org/doi/10.5281/zenodo.11543564</uri>
      <pub-id pub-id-type="doi">10.5281/ZENODO.11543564</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ojha_image_2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ojha</surname><given-names>Utkarsh</given-names></name>
        <name><surname>Adhikari</surname><given-names>Utsav</given-names></name>
        <name><surname>Singh</surname><given-names>Dushyant Kumar</given-names></name>
      </person-group>
      <article-title>Image annotation using deep learning: A review</article-title>
      <source>2017 International Conference on Intelligent Computing and Control (I2C2)</source>
      <year iso-8601-date="2017-06">2017</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-06-13">2024</year><month>06</month><day>13</day></date-in-citation>
      <uri>https://ieeexplore.ieee.org/document/8321819</uri>
      <pub-id pub-id-type="doi">10.1109/I2C2.2017.8321819</pub-id>
      <fpage>1</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
  <ref id="ref-aljabri_towards_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Aljabri</surname><given-names>Manar</given-names></name>
        <name><surname>AlAmir</surname><given-names>Manal</given-names></name>
        <name><surname>AlGhamdi</surname><given-names>Manal</given-names></name>
        <name><surname>Abdel-Mottaleb</surname><given-names>Mohamed</given-names></name>
        <name><surname>Collado-Mesa</surname><given-names>Fernando</given-names></name>
      </person-group>
      <article-title>Towards a better understanding of annotation tools for medical imaging: A survey</article-title>
      <source>Multimedia Tools and Applications</source>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-06-13">2024</year><month>06</month><day>13</day></date-in-citation>
      <volume>81</volume>
      <issue>18</issue>
      <issn>1380-7501</issn>
      <uri>https://link.springer.com/10.1007/s11042-022-12100-1</uri>
      <pub-id pub-id-type="doi">10.1007/s11042-022-12100-1</pub-id>
      <fpage>25877</fpage>
      <lpage>25911</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liu_survey_2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Mingyu</given-names></name>
        <name><surname>Yurtsever</surname><given-names>Ekim</given-names></name>
        <name><surname>Fossaert</surname><given-names>Jonathan</given-names></name>
        <name><surname>Zhou</surname><given-names>Xingcheng</given-names></name>
        <name><surname>Zimmer</surname><given-names>Walter</given-names></name>
        <name><surname>Cui</surname><given-names>Yuning</given-names></name>
        <name><surname>Zagar</surname><given-names>Bare Luka</given-names></name>
        <name><surname>Knoll</surname><given-names>Alois C.</given-names></name>
      </person-group>
      <article-title>A survey on autonomous driving datasets: Statistics, annotation quality, and a future outlook</article-title>
      <year iso-8601-date="2024">2024</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-06-13">2024</year><month>06</month><day>13</day></date-in-citation>
      <uri>https://arxiv.org/abs/2401.01454</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2401.01454</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kirillov_segment_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kirillov</surname><given-names>Alexander</given-names></name>
        <name><surname>Mintun</surname><given-names>Eric</given-names></name>
        <name><surname>Ravi</surname><given-names>Nikhila</given-names></name>
        <name><surname>Mao</surname><given-names>Hanzi</given-names></name>
        <name><surname>Rolland</surname><given-names>Chloe</given-names></name>
        <name><surname>Gustafson</surname><given-names>Laura</given-names></name>
        <name><surname>Xiao</surname><given-names>Tete</given-names></name>
        <name><surname>Whitehead</surname><given-names>Spencer</given-names></name>
        <name><surname>Berg</surname><given-names>Alexander C.</given-names></name>
        <name><surname>Lo</surname><given-names>Wan-Yen</given-names></name>
        <name><surname>Dollár</surname><given-names>Piotr</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
      </person-group>
      <article-title>Segment anything</article-title>
      <year iso-8601-date="2023">2023</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-08-01">2024</year><month>08</month><day>01</day></date-in-citation>
      <uri>https://arxiv.org/abs/2304.02643</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2304.02643</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
