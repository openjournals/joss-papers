<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5867</article-id>
<article-id pub-id-type="doi">10.21105/joss.05867</article-id>
<title-group>
<article-title>sparse-lm: Sparse linear regression models in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6453-9545</contrib-id>
<name>
<surname>Barroso-Luque</surname>
<given-names>Luis</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1169-1690</contrib-id>
<name>
<surname>Xie</surname>
<given-names>Fengyu</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Materials Sciences Division, Lawrence Berkeley National
Laboratory, Berkeley CA, 94720, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Materials Science and Engineering, University
of California Berkeley, Berkeley CA, 94720, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-07-10">
<day>10</day>
<month>7</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>92</issue>
<fpage>5867</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>scikit-learn</kwd>
<kwd>cvxpy</kwd>
<kwd>linear regression</kwd>
<kwd>regularization</kwd>
<kwd>structured sparsity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Sparse linear regression models are a powerful tool for capturing
  linear relationships in high dimensional spaces. Sparse models have
  only a small number of nonzero parameters ‚Äîeven if the number of
  covariates used in estimation is large‚Äîas a result they can be easier
  to fit and interpret compared to dense models
  (<xref alt="Hastie et al., 2015" rid="ref-HastieU003A2015" ref-type="bibr">Hastie
  et al., 2015</xref>). Regression objectives resulting in sparse linear
  models such as the Lasso
  (<xref alt="Tibshirani, 2018" rid="ref-TibshiraniU003A1996" ref-type="bibr">Tibshirani,
  2018</xref>;
  <xref alt="Zou, 2006" rid="ref-ZouU003A2006" ref-type="bibr">Zou,
  2006</xref>) and Best Subset Selection
  (<xref alt="Hocking &amp; Leslie, 1967" rid="ref-HockingU003A1967" ref-type="bibr">Hocking
  &amp; Leslie, 1967</xref>) have been widely used in a variety of
  fields. However, many regression problems involve covariates that have
  a natural underlying structure, such as group or hierarchical
  relationships, that can be further leveraged to obtain improved model
  performance and interpretability. Such structured regression problems
  occur in a wide range of fields including genomics
  (<xref alt="Chen &amp; Wang, 2021" rid="ref-ChenU003A2021" ref-type="bibr">Chen
  &amp; Wang, 2021</xref>), bioinformatics
  (<xref alt="Ma et al., 2007" rid="ref-MaU003A2007" ref-type="bibr">Ma
  et al., 2007</xref>), medicine
  (<xref alt="Kim et al., 2012" rid="ref-KimU003A2012" ref-type="bibr">Kim
  et al., 2012</xref>), econometrics
  (<xref alt="Athey &amp; Imbens, 2017" rid="ref-AtheyU003A2017" ref-type="bibr">Athey
  &amp; Imbens, 2017</xref>), chemistry
  (<xref alt="Gu et al., 2018" rid="ref-GuU003A2018" ref-type="bibr">Gu
  et al., 2018</xref>), and materials science
  (<xref alt="Leong &amp; Tan, 2019" rid="ref-LeongU003A2019" ref-type="bibr">Leong
  &amp; Tan, 2019</xref>). Several generalizations of the Lasso
  (<xref alt="Friedman et al., 2010" rid="ref-FriedmanU003A2010" ref-type="bibr">Friedman
  et al., 2010</xref>;
  <xref alt="Simon et al., 2013" rid="ref-SimonU003A2013" ref-type="bibr">Simon
  et al., 2013</xref>;
  <xref alt="Wang &amp; Tian, 2019" rid="ref-WangU003A2019" ref-type="bibr">Wang
  &amp; Tian, 2019</xref>;
  <xref alt="Yuan &amp; Lin, 2006" rid="ref-YuanU003A2006" ref-type="bibr">Yuan
  &amp; Lin, 2006</xref>) and Best Subset Selection
  (<xref alt="Bertsimas et al., 2016" rid="ref-BertsimasU003A2016-a" ref-type="bibr">Bertsimas
  et al., 2016</xref>;
  <xref alt="Bertsimas &amp; King, 2016" rid="ref-BertsimasU003A2016-b" ref-type="bibr">Bertsimas
  &amp; King, 2016</xref>) have been developed to effectively exploit
  additional structure in linear regression. The
  <monospace>sparse-lm</monospace> Python package provides a flexible,
  comprehensive, and user-friendly implementation of (structured) sparse
  linear regression models, which allows researchers to easily
  experiment and choose the best regression model for their specific
  problem.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The <monospace>sparse-lm</monospace> Python package implements a
  variety of sparse linear regression models based on convex objectives
  (generalizations of the Lasso) and mixed integer quadratic programming
  objectives (generalizations of Best Subset Selection) that support a
  flexible range of ways to introduce structured sparsity. The linear
  models in <monospace>sparse-lm</monospace> are implemented to be
  compatible with <monospace>scikit-learn</monospace>
  (<xref alt="Buitinck et al., 2013" rid="ref-BuitinckU003A2013" ref-type="bibr">Buitinck
  et al., 2013</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-PedregosaU003A2011" ref-type="bibr">Pedregosa
  et al., 2011</xref>), in order to enable interoperability with the
  wide range of tools and workflows available. The regression
  optimization problems in <monospace>sparse-lm</monospace> are
  implemented and solved using <monospace>cvxpy</monospace>
  (<xref alt="Diamond &amp; Boyd, 2016" rid="ref-DiamondU003A2016" ref-type="bibr">Diamond
  &amp; Boyd, 2016</xref>), which allows users to choose from a variety
  of well-established open-source and proprietary solvers. In
  particular, for regression problems with mixed integer programming
  objectives, access to state-of-the-art proprietary solvers enables
  solving larger problems that would otherwise be unsolvable within
  reasonable time limits.</p>
  <p>A handful of pre-existing Python libraries implement a subset of
  sparse linear regression models that are also
  <monospace>scikit-learn</monospace> compatible.
  <monospace>celer</monospace>
  (<xref alt="Massias et al., 2018" rid="ref-MassiasU003A2018" ref-type="bibr">Massias
  et al., 2018</xref>) and <monospace>groupyr</monospace>
  (<xref alt="Richie-Halford et al., 2021" rid="ref-Richie-HalfordU003A2021" ref-type="bibr">Richie-Halford
  et al., 2021</xref>) include efficient implementations of the Lasso
  and Group Lasso. <monospace>group-lasso</monospace>
  (<xref alt="Moe, 2020" rid="ref-MoeU003A2020" ref-type="bibr">Moe,
  2020</xref>) is another <monospace>scikit-learn</monospace> compatible
  implementation of the Group Lasso. <monospace>skglm</monospace>
  (<xref alt="Bertrand et al., 2022" rid="ref-BertrandU003A2022" ref-type="bibr">Bertrand
  et al., 2022</xref>) includes several implementations of sparse linear
  models based on regularization using combinations of
  <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mi>p</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[p\in\{1/2,2/3,1,2\}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mo>‚àà</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>)
  norms and pseudo-norms. And <monospace>abess</monospace>
  (<xref alt="Zhu et al., 2022" rid="ref-ZhuU003A2022" ref-type="bibr">Zhu
  et al., 2022</xref>) includes an implementation of Best Subset
  Selection and <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  pseudo-norm regularization.</p>
  <p>The aforementioned packages include highly performant versions of
  the specific models they implement. However, none of these packages
  implement the full range of sparse linear models available in
  <monospace>sparse-lm</monospace>, nor do they support the flexibility
  to modify the optimization objective and choose among many open-source
  and commercially available solvers. <monospace>sparse-lm</monospace>
  satisfies the need for a flexible and comprehensive library that
  enables easy experimentation and comparisons of different sparse
  linear regression algorithms within a single package.</p>
</sec>
<sec id="background">
  <title>Background</title>
  <fig>
    <caption><p>Schematic of a linear model with grouped covariates with
    hierarchical relations. Groups of covariates are represented with
    different colors and hierarchical relationships are represented with
    arrows (i.e.¬†group 3 depends on group 1). The figure was inspired by
    Ref.
    (<xref alt="Richie-Halford et al., 2021" rid="ref-Richie-HalfordU003A2021" ref-type="bibr">Richie-Halford
    et al., 2021</xref>).</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/linear-model.pdf" />
  </fig>
  <p>Structured sparsity can be introduced into regression problems in
  one of two ways: convex group regularization or mixed integer
  quadratic programming with linear constraints. The first way to obtain
  structured sparsity is by using regularization based on
  generalizations of the Lasso, such as the Group Lasso and the Sparse
  Group Lasso
  (<xref alt="Friedman et al., 2010" rid="ref-FriedmanU003A2010" ref-type="bibr">Friedman
  et al., 2010</xref>;
  <xref alt="Simon et al., 2013" rid="ref-SimonU003A2013" ref-type="bibr">Simon
  et al., 2013</xref>;
  <xref alt="Wang &amp; Tian, 2019" rid="ref-WangU003A2019" ref-type="bibr">Wang
  &amp; Tian, 2019</xref>;
  <xref alt="Yuan &amp; Lin, 2006" rid="ref-YuanU003A2006" ref-type="bibr">Yuan
  &amp; Lin, 2006</xref>). The Sparse Group Lasso regression problem can
  be expressed as follows,</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\beta^* = \underset{\beta}{\text{argmin}}\;||\mathbf{X}
      \beta - \mathbf{y}||^2_2 + (1-\alpha)\lambda\sum_{\mathbf{g}\in G}\sqrt{|\mathbf{g}
      }| ||\beta_{\mathbf{g}}||_2 + \alpha\lambda||\beta||_1]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>Œ≤</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mtext mathvariant="normal">argmin</mml:mtext><mml:mi>Œ≤</mml:mi></mml:munder><mml:mspace width="0.278em"></mml:mspace><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mi>ùêó</mml:mi><mml:mi>Œ≤</mml:mi><mml:mo>‚àí</mml:mo><mml:mi>ùê≤</mml:mi><mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:mi>Œ±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>Œª</mml:mi><mml:munder><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>ùê†</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>G</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:mrow><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>ùê†</mml:mi></mml:mrow></mml:msqrt><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msub><mml:mi>Œ≤</mml:mi><mml:mi>ùê†</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msub><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mi>Œ±</mml:mi><mml:mi>Œª</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>Œ≤</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:msub><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  is the design matrix, <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula>
  is the response vector, and <inline-formula><alternatives>
  <tex-math><![CDATA[\beta]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ≤</mml:mi></mml:math></alternatives></inline-formula>
  are the regression coefficients. <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{g}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê†</mml:mi></mml:math></alternatives></inline-formula>
  are groups of covariate indices, <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  is the set of all such groups being considered, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[\beta_{\mathbf{g}}\in\mathbb{R}^{|\mathbf{g}|}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>Œ≤</mml:mi><mml:mi>ùê†</mml:mi></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>ùê†</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  are the covariate coefficients in group <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{g}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê†</mml:mi></mml:math></alternatives></inline-formula>.
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda \in \mathbb{R}_{+}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œª</mml:mi><mml:mo>‚àà</mml:mo><mml:msub><mml:mi>‚Ñù</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha\in[0,1]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  are regularization hyperparameters. The parameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha\in[0,1]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  controls the relative weight between the single covariate
  <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  regularization and the group regularization term. When
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  the regression problem reduces to the Group Lasso objective, and when
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha=1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  the problem reduces to the Lasso objective.</p>
  <p>The (Sparse) Group Lasso can be directly used to obtain a grouped
  sparsity pattern. Hierarchical sparsity patterns can be obtained by
  extending the Group Lasso to allow overlapping groups, which is
  referred to as the Overlap Group Lasso
  (<xref alt="Hastie et al., 2015" rid="ref-HastieU003A2015" ref-type="bibr">Hastie
  et al., 2015</xref>).</p>
  <p>The second method to obtain structured sparsity is by introducing
  linear constraints into the regression objective. Introducing linear
  constraints is straight-forward in mixed integer quadratic programming
  (MIQP) formulations of the Best Subset Selection
  (<xref alt="Bertsimas et al., 2016" rid="ref-BertsimasU003A2016-a" ref-type="bibr">Bertsimas
  et al., 2016</xref>;
  <xref alt="Bertsimas &amp; King, 2016" rid="ref-BertsimasU003A2016-b" ref-type="bibr">Bertsimas
  &amp; King, 2016</xref>). The general MIQP formulation of Best Subset
  Selection with grouped covariates and hierarchical constraints can be
  expressed as follows,</p>
  <p><disp-formula><tex-math><![CDATA[\begin{aligned}
      \beta^* = \underset{\beta}{\text{argmin}}\;
      & \beta^{\top}\left(\mathbf{X}^{\top}\mathbf{X} +
      \lambda\mathbf{I}\right)\beta - 2\mathbf{y}^{\top}\mathbf{X}\beta\\
      &\begin{array}{r@{~}l@{}l@{\quad}l}
      \text{subject to} \quad &z_{\mathbf{g}} \in \{0,1\} \\
      &-Mz_{\mathbf{g}}\mathbf{1} \leq \beta_{\mathbf{g}} \leq Mz_{\mathbf{g}}\mathbf{1} \\
      &\sum_{\mathbf{g}\in G} z_{\mathbf{g}} \le k \\
      &z_\mathbf{g} \le z_\mathbf{h}
      \end{array} \nonumber
  \end{aligned}]]></tex-math></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[z_\mathbf{g}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>z</mml:mi><mml:mi>ùê†</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  are binary slack variables that indicate whether the covariates in
  each group <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{g}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê†</mml:mi></mml:math></alternatives></inline-formula>
  are included in the model. The first set of inequality constraints
  ensure that coefficients <inline-formula><alternatives>
  <tex-math><![CDATA[\beta_{\mathbf{g}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œ≤</mml:mi><mml:mi>ùê†</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  are nonzero if and only if their corresponding slack variable
  <inline-formula><alternatives>
  <tex-math><![CDATA[z_{\mathbf{g}} = 1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>ùê†</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  <inline-formula><alternatives>
  <tex-math><![CDATA[M]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>
  is a fixed parameter that can be estimated from the data
  (<xref alt="Bertsimas et al., 2016" rid="ref-BertsimasU003A2016-a" ref-type="bibr">Bertsimas
  et al., 2016</xref>). The second inequality constraint introduces
  general sparsity by ensuring that at most
  <inline-formula><alternatives>
  <tex-math><![CDATA[k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
  coefficients are nonzero. If <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  includes only singleton groups of covariates then the MIQP formulation
  is equivalent to the Best Subset Selection problem, otherwise it is a
  generalization that enables group-level sparsity structure. The last
  inequality constraint can be used to introduce hierarchical structure
  into the model. Finally, we have also included an
  <inline-formula><alternatives>
  <tex-math><![CDATA[\ell_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  regularization term controlled by the hyperparameter
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>,
  which is useful when dealing with poorly conditioned design
  matrices.</p>
  <p>The user-friendly implementation of statistical regression models
  with structured sparsity parametrized via Group Lasso or Best Subset
  Selection based objectives in <monospace>sparse-lm</monospace>, along
  with the flexibility to choose from a variety of established solvers,
  enables researchers to quickly iterate, experiment and benchmark
  performance when choosing the best regression model for their specific
  problem. <monospace>sparse-lm</monospace> has already been used to
  build linear models with structured sparsity in a handful of material
  science studies
  (<xref alt="Barroso-Luque et al., 2022" rid="ref-Barroso-LuqueU003A2022" ref-type="bibr">Barroso-Luque
  et al., 2022</xref>;
  <xref alt="Xie et al., 2023" rid="ref-XieU003A2023" ref-type="bibr">Xie
  et al., 2023</xref>;
  <xref alt="Zhong et al., 2022" rid="ref-ZhongU003A2022" ref-type="bibr">Zhong
  et al., 2022</xref>,
  <xref alt="2023" rid="ref-ZhongU003A2023" ref-type="bibr">2023</xref>).</p>
</sec>
<sec id="usage">
  <title>Usage</title>
  <p>Since the linear regression models in
  <monospace>sparse-lm</monospace> are implemented to be compatible with
  <monospace>scikit-learn</monospace>
  (<xref alt="Buitinck et al., 2013" rid="ref-BuitinckU003A2013" ref-type="bibr">Buitinck
  et al., 2013</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-PedregosaU003A2011" ref-type="bibr">Pedregosa
  et al., 2011</xref>), they can be used independently or as part of a
  workflow‚Äîsuch as in a hyperparameter selection class or a pipeline‚Äî in
  similar fashion to any of the available models in the
  <monospace>sklearn.linear_model</monospace> module.</p>
  <sec id="implemented-regression-models">
    <title>Implemented regression models</title>
    <p>The table below shows the regression models that are implemented
    in <monospace>sparse-lm</monospace> as well as available
    implementations in other Python packages. A checkmark
    (<inline-formula><alternatives>
    <tex-math><![CDATA[\checkmark]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>)
    indicates that the model selected is implemented in the package
    located in the corresponding column.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="28%" />
          <col width="13%" />
          <col width="10%" />
          <col width="12%" />
          <col width="12%" />
          <col width="13%" />
          <col width="13%" />
        </colgroup>
        <thead>
          <tr>
            <th align="center">Model</th>
            <th align="center"><monospace>sparse-lm</monospace></th>
            <th align="center"><monospace>celer</monospace></th>
            <th align="center"><monospace>groupyr</monospace></th>
            <th align="center"><monospace>group-lasso</monospace></th>
            <th align="center"><monospace>skglm</monospace></th>
            <th align="center"><monospace>abess</monospace></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center">(Adaptive) Lasso</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center">Ô∏è</td>
          </tr>
          <tr>
            <td align="center">(Adaptive) Group Lasso</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">Ô∏è</td>
          </tr>
          <tr>
            <td align="center">(Adaptive) Sparse Group Lasso</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">Ô∏è</td>
          </tr>
          <tr>
            <td align="center">(Adaptive) Ridged Group Lasso</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center"></td>
          </tr>
          <tr>
            <td align="center">Best Subset Selection</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center">Ô∏è<inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="center">Ridged Best Subset Selection</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center">Ô∏è</td>
          </tr>
          <tr>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\ell_0]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
            pseudo-norm</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center">Ô∏è</td>
          </tr>
          <tr>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\ell_0\ell_2]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
            mixed-norm</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula>Ô∏è</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
          </tr>
          <tr>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\ell_{1/2}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
            psuedo-norm</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center"></td>
          </tr>
          <tr>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\ell_{2/3}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>/</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
            psuedo-norm</td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>‚úì</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center"></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>Note that only <monospace>sparse-lm</monospace> includes adaptive
    versions of Lasso based estimators. However, some of the third party
    packages, notably <monospace>skglm</monospace> and
    <monospace>abess</monospace>, include additional penalties and
    regression objectives that are not implemented in
    <monospace>sparse-lm</monospace>.</p>
  </sec>
  <sec id="implemented-model-selection-and-composition-tools">
    <title>Implemented model selection and composition tools</title>
    <p>In addition to the regression models in the table above, a few
    model selection and composition models are also implemented. These
    models are listed below:</p>
    <list list-type="bullet">
      <list-item>
        <p>One standard deviation rule grid search cross-validation</p>
      </list-item>
      <list-item>
        <p>Line search cross-validation</p>
      </list-item>
      <list-item>
        <p>Stepwise composite estimator</p>
      </list-item>
    </list>
    <p>The package can be downloaded through the
    <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/sparse-lm/">Python
    Package Index</ext-link>. Documentation, including an API reference
    and examples, can be found in the
    <ext-link ext-link-type="uri" xlink:href="https://cedergrouphub.github.io/sparse-lm">online
    documentation</ext-link>.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The first author (L.B.L.) is the lead developer of
  <monospace>sparse-lm</monospace>, and the lead and corresponding
  author. The second author (F.X.) is a main contributor to the package.
  Both authors drafted, reviewed and edited the manuscript.</p>
  <p>L.B.L. and F.X. would like acknowledge the contributions from
  Peichen Zhong, Ronald L. Kam, and Tina Chen to the development of
  <monospace>sparse-lm</monospace>. L.B.L gratefully acknowledges
  support from the National Science Foundation Graduate Research
  Fellowship under Grant No.¬†DGE 1752814.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-HastieU003A2015">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        <name><surname>Wainwright</surname><given-names>Martin</given-names></name>
      </person-group>
      <source>Statistical learning with sparsity: The lasso and generalizations</source>
      <publisher-name>Chapman &amp; Hall/CRC</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <isbn>1498712169</isbn>
    </element-citation>
  </ref>
  <ref id="ref-TibshiraniU003A1996">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
      </person-group>
      <article-title>Regression Shrinkage and Selection Via the Lasso</article-title>
      <source>Journal of the Royal Statistical Society: Series B (Methodological)</source>
      <year iso-8601-date="2018-12">2018</year><month>12</month>
      <volume>58</volume>
      <issue>1</issue>
      <issn>0035-9246</issn>
      <uri>https://doi.org/10.1111/j.2517-6161.1996.tb02080.x</uri>
      <pub-id pub-id-type="doi">10.1111/j.2517-6161.1996.tb02080.x</pub-id>
      <fpage>267</fpage>
      <lpage>288</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZouU003A2006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zou</surname><given-names>Hui</given-names></name>
      </person-group>
      <article-title>The Adaptive Lasso and Its Oracle Properties</article-title>
      <source>Journal of the American Statistical Association</source>
      <year iso-8601-date="2006-12">2006</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-07-10">2023</year><month>07</month><day>10</day></date-in-citation>
      <volume>101</volume>
      <issue>476</issue>
      <issn>0162-1459</issn>
      <uri>https://doi.org/10.1198/016214506000000735</uri>
      <pub-id pub-id-type="doi">10.1198/016214506000000735</pub-id>
      <fpage>1418</fpage>
      <lpage>1429</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HockingU003A1967">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hocking</surname><given-names>R. R.</given-names></name>
        <name><surname>Leslie</surname><given-names>R. N.</given-names></name>
      </person-group>
      <article-title>Selection of the best subset in regression analysis</article-title>
      <source>Technometrics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1967">1967</year>
      <volume>9</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1080/00401706.1967.10490502</pub-id>
      <fpage>531</fpage>
      <lpage>540</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BertsimasU003A2016-a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bertsimas</surname><given-names>Dimitris</given-names></name>
        <name><surname>King</surname><given-names>Angela</given-names></name>
        <name><surname>Mazumder</surname><given-names>Rahul</given-names></name>
      </person-group>
      <article-title>Best subset selection via a modern optimization lens</article-title>
      <source>The Annals of Statistics</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>44</volume>
      <issue>2</issue>
      <uri>https://doi.org/10.1214/15-AOS1388</uri>
      <pub-id pub-id-type="doi">10.1214/15-AOS1388</pub-id>
      <fpage>813 </fpage>
      <lpage> 852</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BertsimasU003A2016-b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bertsimas</surname><given-names>Dimitris</given-names></name>
        <name><surname>King</surname><given-names>Angela</given-names></name>
      </person-group>
      <article-title>OR Forum‚ÄîAn Algorithmic Approach to Linear Regression</article-title>
      <source>Operations Research</source>
      <year iso-8601-date="2016-02">2016</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-06-06">2022</year><month>06</month><day>06</day></date-in-citation>
      <volume>64</volume>
      <issue>1</issue>
      <issn>0030-364X</issn>
      <uri>https://pubsonline.informs.org/doi/abs/10.1287/opre.2015.1436</uri>
      <pub-id pub-id-type="doi">10.1287/opre.2015.1436</pub-id>
      <fpage>2</fpage>
      <lpage>16</lpage>
    </element-citation>
  </ref>
  <ref id="ref-YuanU003A2006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yuan</surname><given-names>Ming</given-names></name>
        <name><surname>Lin</surname><given-names>Yi</given-names></name>
      </person-group>
      <article-title>Model selection and estimation in regression with grouped variables</article-title>
      <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
      <year iso-8601-date="2006">2006</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-10-05">2021</year><month>10</month><day>05</day></date-in-citation>
      <volume>68</volume>
      <issue>1</issue>
      <issn>1467-9868</issn>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x</uri>
      <pub-id pub-id-type="doi">10.1111/j.1467-9868.2005.00532.x</pub-id>
      <fpage>49</fpage>
      <lpage>67</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FriedmanU003A2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friedman</surname><given-names>J.</given-names></name>
        <name><surname>Hastie</surname><given-names>T.</given-names></name>
        <name><surname>Tibshirani</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>A note on the group lasso and a sparse group lasso</article-title>
      <source>arXiv:1001.0736 [math, stat]</source>
      <year iso-8601-date="2010-01">2010</year><month>01</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-05">2022</year><month>04</month><day>05</day></date-in-citation>
      <uri>http://arxiv.org/abs/1001.0736</uri>
    </element-citation>
  </ref>
  <ref id="ref-SimonU003A2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Simon</surname><given-names>Noah</given-names></name>
        <name><surname>Friedman</surname><given-names>Jerome</given-names></name>
        <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
      </person-group>
      <article-title>A Sparse-Group Lasso</article-title>
      <source>Journal of Computational and Graphical Statistics</source>
      <year iso-8601-date="2013-04">2013</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-10-05">2021</year><month>10</month><day>05</day></date-in-citation>
      <volume>22</volume>
      <issue>2</issue>
      <issn>1061-8600</issn>
      <uri>https://doi.org/10.1080/10618600.2012.681250</uri>
      <pub-id pub-id-type="doi">10.1080/10618600.2012.681250</pub-id>
      <fpage>231</fpage>
      <lpage>245</lpage>
    </element-citation>
  </ref>
  <ref id="ref-WangU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Mingqiu</given-names></name>
        <name><surname>Tian</surname><given-names>Guo-Liang</given-names></name>
      </person-group>
      <article-title>Adaptive group Lasso for high-dimensional generalized linear models</article-title>
      <source>Statistical Papers</source>
      <year iso-8601-date="2019-10">2019</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-12">2022</year><month>04</month><day>12</day></date-in-citation>
      <volume>60</volume>
      <issue>5</issue>
      <issn>1613-9798</issn>
      <uri>https://doi.org/10.1007/s00362-017-0882-z</uri>
      <pub-id pub-id-type="doi">10.1007/s00362-017-0882-z</pub-id>
      <fpage>1469</fpage>
      <lpage>1486</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PedregosaU003A2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BuitinckU003A2013">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Buitinck</surname><given-names>Lars</given-names></name>
        <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>Mueller</surname><given-names>Andreas</given-names></name>
        <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
        <name><surname>Niculae</surname><given-names>Vlad</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Grobler</surname><given-names>Jaques</given-names></name>
        <name><surname>Layton</surname><given-names>Robert</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Joly</surname><given-names>Arnaud</given-names></name>
        <name><surname>Holt</surname><given-names>Brian</given-names></name>
        <name><surname>Varoquaux</surname><given-names>Ga√´l</given-names></name>
      </person-group>
      <article-title>API design for machine learning software: Experiences from the scikit-learn project</article-title>
      <source>ECML PKDD workshop: Languages for data mining and machine learning</source>
      <year iso-8601-date="2013">2013</year>
      <fpage>108</fpage>
      <lpage>122</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DiamondU003A2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Diamond</surname><given-names>Steven</given-names></name>
        <name><surname>Boyd</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>CVXPY: A Python-embedded modeling language for convex optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2016">2016</year>
      <volume>17</volume>
      <issue>83</issue>
      <fpage>1</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
  <ref id="ref-MassiasU003A2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Massias</surname><given-names>Mathurin</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Salmon</surname><given-names>Joseph</given-names></name>
      </person-group>
      <article-title>Celer: A fast solver for the lasso with dual extrapolation</article-title>
      <source>Proceedings of the 35th international conference on machine learning</source>
      <year iso-8601-date="2018">2018</year>
      <volume>80</volume>
      <fpage>3321</fpage>
      <lpage>3330</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Richie-HalfordU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Richie-Halford</surname><given-names>Adam</given-names></name>
        <name><surname>Narayan</surname><given-names>Manjari</given-names></name>
        <name><surname>Simon</surname><given-names>Noah</given-names></name>
        <name><surname>Yeatman</surname><given-names>Jason</given-names></name>
        <name><surname>Rokem</surname><given-names>Ariel</given-names></name>
      </person-group>
      <article-title>Groupyr: Sparse group lasso in Python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>58</issue>
      <uri>https://doi.org/10.21105/joss.03024</uri>
      <pub-id pub-id-type="doi">10.21105/joss.03024</pub-id>
      <fpage>3024</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-MoeU003A2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Moe</surname><given-names>Yngve Mardal</given-names></name>
      </person-group>
      <article-title>Group lasso</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/yngvem/group-lasso; GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-BertrandU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bertrand</surname><given-names>Quentin</given-names></name>
        <name><surname>Klopfenstein</surname><given-names>Quentin</given-names></name>
        <name><surname>Bannier</surname><given-names>Pierre-Antoine</given-names></name>
        <name><surname>Gidel</surname><given-names>Gauthier</given-names></name>
        <name><surname>Massias</surname><given-names>Mathurin</given-names></name>
      </person-group>
      <article-title>Beyond L1: Faster and Better Sparse Models with skglm</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-07-10">2023</year><month>07</month><day>10</day></date-in-citation>
      <volume>35</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2022/hash/fe5c31e525e9a26a1426ab0b589f42fe-Abstract-Conference.html</uri>
      <fpage>38950</fpage>
      <lpage>38965</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZhuU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhu</surname><given-names>Jin</given-names></name>
        <name><surname>Wang</surname><given-names>Xueqin</given-names></name>
        <name><surname>Hu</surname><given-names>Liyuan</given-names></name>
        <name><surname>Huang</surname><given-names>Junhao</given-names></name>
        <name><surname>Jiang</surname><given-names>Kangkang</given-names></name>
        <name><surname>Zhang</surname><given-names>Yanhang</given-names></name>
        <name><surname>Lin</surname><given-names>Shiyun</given-names></name>
        <name><surname>Zhu</surname><given-names>Junxian</given-names></name>
      </person-group>
      <article-title>Abess: A fast best-subset selection library in Python and R</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>202</issue>
      <uri>http://jmlr.org/papers/v23/21-1060.html</uri>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AtheyU003A2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Athey</surname><given-names>Susan</given-names></name>
        <name><surname>Imbens</surname><given-names>Guido W.</given-names></name>
      </person-group>
      <article-title>The State of Applied Econometrics: Causality and Policy Evaluation</article-title>
      <source>Journal of Economic Perspectives</source>
      <year iso-8601-date="2017">2017</year>
      <volume>31</volume>
      <issue>2</issue>
      <issn>0895-3309</issn>
      <pub-id pub-id-type="doi">10.1257/jep.31.2.3</pub-id>
      <fpage>3</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ChenU003A2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Shunjie</given-names></name>
        <name><surname>Wang</surname><given-names>Pei</given-names></name>
      </person-group>
      <article-title>Gene Selection from Biological Data via Group Lasso for Logistic Regression Model: Effects of Different Clustering Algorithms</article-title>
      <year iso-8601-date="2021">2021</year>
      <issn>1934-1768</issn>
      <pub-id pub-id-type="doi">10.23919/CCC52363.2021.9549471</pub-id>
      <fpage>6374</fpage>
      <lpage>6379</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KimU003A2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kim</surname><given-names>Jinseog</given-names></name>
        <name><surname>Sohn</surname><given-names>Insuk</given-names></name>
        <name><surname>Jung</surname><given-names>Sin-Ho</given-names></name>
        <name><surname>Kim</surname><given-names>Sujong</given-names></name>
        <name><surname>Park</surname><given-names>Changyi</given-names></name>
      </person-group>
      <article-title>Analysis of Survival Data with Group Lasso</article-title>
      <source>Communications in Statistics - Simulation and Computation</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>41</volume>
      <issue>9</issue>
      <issn>0361-0918</issn>
      <pub-id pub-id-type="doi">10.1080/03610918.2011.611311</pub-id>
      <fpage>1593</fpage>
      <lpage>1605</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GuU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gu</surname><given-names>Geun Ho</given-names></name>
        <name><surname>Plechac</surname><given-names>Petr</given-names></name>
        <name><surname>Vlachos</surname><given-names>Dionisios G.</given-names></name>
      </person-group>
      <article-title>Thermochemistry of gas-phase and surface species via LASSO-assisted subgraph selection</article-title>
      <source>Reaction Chemistry &amp; Engineering</source>
      <publisher-name>The Royal Society of Chemistry</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>4</issue>
      <issn>2058-9883</issn>
      <pub-id pub-id-type="doi">10.1039/C7RE00210F</pub-id>
      <fpage>454</fpage>
      <lpage>466</lpage>
    </element-citation>
  </ref>
  <ref id="ref-MaU003A2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ma</surname><given-names>Shuangge</given-names></name>
        <name><surname>Song</surname><given-names>Xiao</given-names></name>
        <name><surname>Huang</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Supervised group Lasso with applications to microarray data analysis</article-title>
      <source>BMC Bioinformatics</source>
      <year iso-8601-date="2007">2007</year>
      <volume>8</volume>
      <issue>1</issue>
      <issn>1471-2105</issn>
      <pub-id pub-id-type="doi">10.1186/1471-2105-8-60</pub-id>
      <fpage>60</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-LeongU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Leong</surname><given-names>Zhidong</given-names></name>
        <name><surname>Tan</surname><given-names>Teck Leong</given-names></name>
      </person-group>
      <article-title>Robust cluster expansion of multicomponent systems using structured sparsity</article-title>
      <source>Physical Review B</source>
      <year iso-8601-date="2019-10-28">2019</year><month>10</month><day>28</day>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-04-29">2020</year><month>04</month><day>29</day></date-in-citation>
      <volume>100</volume>
      <issue>13</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevB.100.134108</pub-id>
      <fpage>134108</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-XieU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Xie</surname><given-names>Fengyu</given-names></name>
        <name><surname>Zhong</surname><given-names>Peichen</given-names></name>
        <name><surname>Barroso-Luque</surname><given-names>Luis</given-names></name>
        <name><surname>Ouyang</surname><given-names>Bin</given-names></name>
        <name><surname>Ceder</surname><given-names>Gerbrand</given-names></name>
      </person-group>
      <article-title>Semigrand-canonical Monte-Carlo simulation methods for charge-decorated cluster expansions</article-title>
      <source>Computational Materials Science</source>
      <year iso-8601-date="2023">2023</year>
      <volume>218</volume>
      <issn>0927-0256</issn>
      <pub-id pub-id-type="doi">10.1016/j.commatsci.2022.112000</pub-id>
      <fpage>112000</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZhongU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhong</surname><given-names>Peichen</given-names></name>
        <name><surname>Chen</surname><given-names>Tina</given-names></name>
        <name><surname>Barroso-Luque</surname><given-names>Luis</given-names></name>
        <name><surname>Xie</surname><given-names>Fengyu</given-names></name>
        <name><surname>Ceder</surname><given-names>Gerbrand</given-names></name>
      </person-group>
      <article-title>An L0 L2-norm regularized regression model for construction of robust cluster expansions in multicomponent systems</article-title>
      <source>Physical Review B</source>
      <publisher-name>American Physical Society</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>106</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevB.106.024203</pub-id>
      <fpage>024203</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZhongU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhong</surname><given-names>Peichen</given-names></name>
        <name><surname>Xie</surname><given-names>Fengyu</given-names></name>
        <name><surname>Barroso-Luque</surname><given-names>Luis</given-names></name>
        <name><surname>Huang</surname><given-names>Liliang</given-names></name>
        <name><surname>Ceder</surname><given-names>Gerbrand</given-names></name>
      </person-group>
      <article-title>Modeling Intercalation Chemistry with Multiredox Reactions by Sparse Lattice Models in Disordered Rocksalt Cathodes</article-title>
      <source>PRX Energy</source>
      <publisher-name>American Physical Society</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>2</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1103/PRXEnergy.2.043005</pub-id>
      <fpage>043005</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Barroso-LuqueU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barroso-Luque</surname><given-names>Luis</given-names></name>
        <name><surname>Zhong</surname><given-names>Peichen</given-names></name>
        <name><surname>Yang</surname><given-names>Julia H.</given-names></name>
        <name><surname>Xie</surname><given-names>Fengyu</given-names></name>
        <name><surname>Chen</surname><given-names>Tina</given-names></name>
        <name><surname>Ouyang</surname><given-names>Bin</given-names></name>
        <name><surname>Ceder</surname><given-names>Gerbrand</given-names></name>
      </person-group>
      <article-title>Cluster expansions of multicomponent ionic materials: Formalism and methodology</article-title>
      <source>Physical Review B</source>
      <publisher-name>American Physical Society</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>106</volume>
      <issue>14</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevB.106.144202</pub-id>
      <fpage>144202</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
