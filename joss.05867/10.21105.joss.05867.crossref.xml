<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20231221T113241-e9a3b42aed74893ff4e46791d1927da5ac8435ec</doi_batch_id>
    <timestamp>20231221113241</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>12</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>92</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>sparse-lm: Sparse linear regression models in
Python</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Luis</given_name>
            <surname>Barroso-Luque</surname>
            <ORCID>https://orcid.org/0000-0002-6453-9545</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Fengyu</given_name>
            <surname>Xie</surname>
            <ORCID>https://orcid.org/0000-0002-1169-1690</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>12</month>
          <day>21</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5867</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05867</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.10246640</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5867</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05867</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05867</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05867.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="Hastie:2015">
            <volume_title>Statistical learning with sparsity: The lasso
and generalizations</volume_title>
            <author>Hastie</author>
            <isbn>1498712169</isbn>
            <cYear>2015</cYear>
            <unstructured_citation>Hastie, T., Tibshirani, R., &amp;
Wainwright, M. (2015). Statistical learning with sparsity: The lasso and
generalizations. Chapman &amp; Hall/CRC.
ISBN: 1498712169</unstructured_citation>
          </citation>
          <citation key="Tibshirani:1996">
            <article_title>Regression Shrinkage and Selection Via the
Lasso</article_title>
            <author>Tibshirani</author>
            <journal_title>Journal of the Royal Statistical Society:
Series B (Methodological)</journal_title>
            <issue>1</issue>
            <volume>58</volume>
            <doi>10.1111/j.2517-6161.1996.tb02080.x</doi>
            <issn>0035-9246</issn>
            <cYear>2018</cYear>
            <unstructured_citation>Tibshirani, R. (2018). Regression
Shrinkage and Selection Via the Lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1), 267–288.
https://doi.org/10.1111/j.2517-6161.1996.tb02080.x</unstructured_citation>
          </citation>
          <citation key="Zou:2006">
            <article_title>The Adaptive Lasso and Its Oracle
Properties</article_title>
            <author>Zou</author>
            <journal_title>Journal of the American Statistical
Association</journal_title>
            <issue>476</issue>
            <volume>101</volume>
            <doi>10.1198/016214506000000735</doi>
            <issn>0162-1459</issn>
            <cYear>2006</cYear>
            <unstructured_citation>Zou, H. (2006). The Adaptive Lasso
and Its Oracle Properties. Journal of the American Statistical
Association, 101(476), 1418–1429.
https://doi.org/10.1198/016214506000000735</unstructured_citation>
          </citation>
          <citation key="Hocking:1967">
            <article_title>Selection of the best subset in regression
analysis</article_title>
            <author>Hocking</author>
            <journal_title>Technometrics</journal_title>
            <issue>4</issue>
            <volume>9</volume>
            <doi>10.1080/00401706.1967.10490502</doi>
            <cYear>1967</cYear>
            <unstructured_citation>Hocking, R. R., &amp; Leslie, R. N.
(1967). Selection of the best subset in regression analysis.
Technometrics, 9(4), 531–540.
https://doi.org/10.1080/00401706.1967.10490502</unstructured_citation>
          </citation>
          <citation key="Bertsimas:2016-a">
            <article_title>Best subset selection via a modern
optimization lens</article_title>
            <author>Bertsimas</author>
            <journal_title>The Annals of Statistics</journal_title>
            <issue>2</issue>
            <volume>44</volume>
            <doi>10.1214/15-AOS1388</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Bertsimas, D., King, A., &amp;
Mazumder, R. (2016). Best subset selection via a modern optimization
lens. The Annals of Statistics, 44(2), 813–852.
https://doi.org/10.1214/15-AOS1388</unstructured_citation>
          </citation>
          <citation key="Bertsimas:2016-b">
            <article_title>OR Forum—An Algorithmic Approach to Linear
Regression</article_title>
            <author>Bertsimas</author>
            <journal_title>Operations Research</journal_title>
            <issue>1</issue>
            <volume>64</volume>
            <doi>10.1287/opre.2015.1436</doi>
            <issn>0030-364X</issn>
            <cYear>2016</cYear>
            <unstructured_citation>Bertsimas, D., &amp; King, A. (2016).
OR Forum—An Algorithmic Approach to Linear Regression. Operations
Research, 64(1), 2–16.
https://doi.org/10.1287/opre.2015.1436</unstructured_citation>
          </citation>
          <citation key="Yuan:2006">
            <article_title>Model selection and estimation in regression
with grouped variables</article_title>
            <author>Yuan</author>
            <journal_title>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</journal_title>
            <issue>1</issue>
            <volume>68</volume>
            <doi>10.1111/j.1467-9868.2005.00532.x</doi>
            <issn>1467-9868</issn>
            <cYear>2006</cYear>
            <unstructured_citation>Yuan, M., &amp; Lin, Y. (2006). Model
selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology),
68(1), 49–67.
https://doi.org/10.1111/j.1467-9868.2005.00532.x</unstructured_citation>
          </citation>
          <citation key="Friedman:2010">
            <article_title>A note on the group lasso and a sparse group
lasso</article_title>
            <author>Friedman</author>
            <journal_title>arXiv:1001.0736 [math, stat]</journal_title>
            <cYear>2010</cYear>
            <unstructured_citation>Friedman, J., Hastie, T., &amp;
Tibshirani, R. (2010). A note on the group lasso and a sparse group
lasso. arXiv:1001.0736 [Math, Stat].
http://arxiv.org/abs/1001.0736</unstructured_citation>
          </citation>
          <citation key="Simon:2013">
            <article_title>A Sparse-Group Lasso</article_title>
            <author>Simon</author>
            <journal_title>Journal of Computational and Graphical
Statistics</journal_title>
            <issue>2</issue>
            <volume>22</volume>
            <doi>10.1080/10618600.2012.681250</doi>
            <issn>1061-8600</issn>
            <cYear>2013</cYear>
            <unstructured_citation>Simon, N., Friedman, J., Hastie, T.,
&amp; Tibshirani, R. (2013). A Sparse-Group Lasso. Journal of
Computational and Graphical Statistics, 22(2), 231–245.
https://doi.org/10.1080/10618600.2012.681250</unstructured_citation>
          </citation>
          <citation key="Wang:2019">
            <article_title>Adaptive group Lasso for high-dimensional
generalized linear models</article_title>
            <author>Wang</author>
            <journal_title>Statistical Papers</journal_title>
            <issue>5</issue>
            <volume>60</volume>
            <doi>10.1007/s00362-017-0882-z</doi>
            <issn>1613-9798</issn>
            <cYear>2019</cYear>
            <unstructured_citation>Wang, M., &amp; Tian, G.-L. (2019).
Adaptive group Lasso for high-dimensional generalized linear models.
Statistical Papers, 60(5), 1469–1486.
https://doi.org/10.1007/s00362-017-0882-z</unstructured_citation>
          </citation>
          <citation key="Pedregosa:2011">
            <article_title>Scikit-learn: Machine learning in
Python</article_title>
            <author>Pedregosa</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <volume>12</volume>
            <cYear>2011</cYear>
            <unstructured_citation>Pedregosa, F., Varoquaux, G.,
Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A.,
Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011).
Scikit-learn: Machine learning in Python. Journal of Machine Learning
Research, 12, 2825–2830.</unstructured_citation>
          </citation>
          <citation key="Buitinck:2013">
            <article_title>API design for machine learning software:
Experiences from the scikit-learn project</article_title>
            <author>Buitinck</author>
            <journal_title>ECML PKDD workshop: Languages for data mining
and machine learning</journal_title>
            <cYear>2013</cYear>
            <unstructured_citation>Buitinck, L., Louppe, G., Blondel,
M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer,
P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A.,
Holt, B., &amp; Varoquaux, G. (2013). API design for machine learning
software: Experiences from the scikit-learn project. ECML PKDD Workshop:
Languages for Data Mining and Machine Learning,
108–122.</unstructured_citation>
          </citation>
          <citation key="Diamond:2016">
            <article_title>CVXPY: A Python-embedded modeling language
for convex optimization</article_title>
            <author>Diamond</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <issue>83</issue>
            <volume>17</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Diamond, S., &amp; Boyd, S. (2016).
CVXPY: A Python-embedded modeling language for convex optimization.
Journal of Machine Learning Research, 17(83),
1–5.</unstructured_citation>
          </citation>
          <citation key="Massias:2018">
            <article_title>Celer: A fast solver for the lasso with dual
extrapolation</article_title>
            <author>Massias</author>
            <journal_title>Proceedings of the 35th international
conference on machine learning</journal_title>
            <volume>80</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Massias, M., Gramfort, A., &amp;
Salmon, J. (2018). Celer: A fast solver for the lasso with dual
extrapolation. Proceedings of the 35th International Conference on
Machine Learning, 80, 3321–3330.</unstructured_citation>
          </citation>
          <citation key="Richie-Halford:2021">
            <article_title>Groupyr: Sparse group lasso in
Python</article_title>
            <author>Richie-Halford</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>58</issue>
            <volume>6</volume>
            <doi>10.21105/joss.03024</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Richie-Halford, A., Narayan, M.,
Simon, N., Yeatman, J., &amp; Rokem, A. (2021). Groupyr: Sparse group
lasso in Python. Journal of Open Source Software, 6(58), 3024.
https://doi.org/10.21105/joss.03024</unstructured_citation>
          </citation>
          <citation key="Moe:2020">
            <article_title>Group lasso</article_title>
            <author>Moe</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Moe, Y. M. (2020). Group lasso. In
GitHub repository. https://github.com/yngvem/group-lasso;
GitHub.</unstructured_citation>
          </citation>
          <citation key="Bertrand:2022">
            <article_title>Beyond L1: Faster and Better Sparse Models
with skglm</article_title>
            <author>Bertrand</author>
            <journal_title>Advances in Neural Information Processing
Systems</journal_title>
            <volume>35</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Bertrand, Q., Klopfenstein, Q.,
Bannier, P.-A., Gidel, G., &amp; Massias, M. (2022). Beyond L1: Faster
and Better Sparse Models with skglm. Advances in Neural Information
Processing Systems, 35, 38950–38965.
https://proceedings.neurips.cc/paper_files/paper/2022/hash/fe5c31e525e9a26a1426ab0b589f42fe-Abstract-Conference.html</unstructured_citation>
          </citation>
          <citation key="Zhu:2022">
            <article_title>Abess: A fast best-subset selection library
in Python and R</article_title>
            <author>Zhu</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <issue>202</issue>
            <volume>23</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Zhu, J., Wang, X., Hu, L., Huang, J.,
Jiang, K., Zhang, Y., Lin, S., &amp; Zhu, J. (2022). Abess: A fast
best-subset selection library in Python and R. Journal of Machine
Learning Research, 23(202), 1–7.
http://jmlr.org/papers/v23/21-1060.html</unstructured_citation>
          </citation>
          <citation key="Athey:2017">
            <article_title>The State of Applied Econometrics: Causality
and Policy Evaluation</article_title>
            <author>Athey</author>
            <journal_title>Journal of Economic
Perspectives</journal_title>
            <issue>2</issue>
            <volume>31</volume>
            <doi>10.1257/jep.31.2.3</doi>
            <issn>0895-3309</issn>
            <cYear>2017</cYear>
            <unstructured_citation>Athey, S., &amp; Imbens, G. W.
(2017). The State of Applied Econometrics: Causality and Policy
Evaluation. Journal of Economic Perspectives, 31(2), 3–32.
https://doi.org/10.1257/jep.31.2.3</unstructured_citation>
          </citation>
          <citation key="Chen:2021">
            <article_title>Gene Selection from Biological Data via Group
Lasso for Logistic Regression Model: Effects of Different Clustering
Algorithms</article_title>
            <author>Chen</author>
            <doi>10.23919/CCC52363.2021.9549471</doi>
            <issn>1934-1768</issn>
            <cYear>2021</cYear>
            <unstructured_citation>Chen, S., &amp; Wang, P. (2021). Gene
Selection from Biological Data via Group Lasso for Logistic Regression
Model: Effects of Different Clustering Algorithms. 6374–6379.
https://doi.org/10.23919/CCC52363.2021.9549471</unstructured_citation>
          </citation>
          <citation key="Kim:2012">
            <article_title>Analysis of Survival Data with Group
Lasso</article_title>
            <author>Kim</author>
            <journal_title>Communications in Statistics - Simulation and
Computation</journal_title>
            <issue>9</issue>
            <volume>41</volume>
            <doi>10.1080/03610918.2011.611311</doi>
            <issn>0361-0918</issn>
            <cYear>2012</cYear>
            <unstructured_citation>Kim, J., Sohn, I., Jung, S.-H., Kim,
S., &amp; Park, C. (2012). Analysis of Survival Data with Group Lasso.
Communications in Statistics - Simulation and Computation, 41(9),
1593–1605.
https://doi.org/10.1080/03610918.2011.611311</unstructured_citation>
          </citation>
          <citation key="Gu:2018">
            <article_title>Thermochemistry of gas-phase and surface
species via LASSO-assisted subgraph selection</article_title>
            <author>Gu</author>
            <journal_title>Reaction Chemistry &amp;
Engineering</journal_title>
            <issue>4</issue>
            <volume>3</volume>
            <doi>10.1039/C7RE00210F</doi>
            <issn>2058-9883</issn>
            <cYear>2018</cYear>
            <unstructured_citation>Gu, G. H., Plechac, P., &amp;
Vlachos, D. G. (2018). Thermochemistry of gas-phase and surface species
via LASSO-assisted subgraph selection. Reaction Chemistry &amp;
Engineering, 3(4), 454–466.
https://doi.org/10.1039/C7RE00210F</unstructured_citation>
          </citation>
          <citation key="Ma:2007">
            <article_title>Supervised group Lasso with applications to
microarray data analysis</article_title>
            <author>Ma</author>
            <journal_title>BMC Bioinformatics</journal_title>
            <issue>1</issue>
            <volume>8</volume>
            <doi>10.1186/1471-2105-8-60</doi>
            <issn>1471-2105</issn>
            <cYear>2007</cYear>
            <unstructured_citation>Ma, S., Song, X., &amp; Huang, J.
(2007). Supervised group Lasso with applications to microarray data
analysis. BMC Bioinformatics, 8(1), 60.
https://doi.org/10.1186/1471-2105-8-60</unstructured_citation>
          </citation>
          <citation key="Leong:2019">
            <article_title>Robust cluster expansion of multicomponent
systems using structured sparsity</article_title>
            <author>Leong</author>
            <journal_title>Physical Review B</journal_title>
            <issue>13</issue>
            <volume>100</volume>
            <doi>10.1103/PhysRevB.100.134108</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Leong, Z., &amp; Tan, T. L. (2019).
Robust cluster expansion of multicomponent systems using structured
sparsity. Physical Review B, 100(13), 134108.
https://doi.org/10.1103/PhysRevB.100.134108</unstructured_citation>
          </citation>
          <citation key="Xie:2023">
            <article_title>Semigrand-canonical Monte-Carlo simulation
methods for charge-decorated cluster expansions</article_title>
            <author>Xie</author>
            <journal_title>Computational Materials
Science</journal_title>
            <volume>218</volume>
            <doi>10.1016/j.commatsci.2022.112000</doi>
            <issn>0927-0256</issn>
            <cYear>2023</cYear>
            <unstructured_citation>Xie, F., Zhong, P., Barroso-Luque,
L., Ouyang, B., &amp; Ceder, G. (2023). Semigrand-canonical Monte-Carlo
simulation methods for charge-decorated cluster expansions.
Computational Materials Science, 218, 112000.
https://doi.org/10.1016/j.commatsci.2022.112000</unstructured_citation>
          </citation>
          <citation key="Zhong:2022">
            <article_title>An L0 L2-norm regularized regression model
for construction of robust cluster expansions in multicomponent
systems</article_title>
            <author>Zhong</author>
            <journal_title>Physical Review B</journal_title>
            <issue>2</issue>
            <volume>106</volume>
            <doi>10.1103/PhysRevB.106.024203</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Zhong, P., Chen, T., Barroso-Luque,
L., Xie, F., &amp; Ceder, G. (2022). An L0 L2-norm regularized
regression model for construction of robust cluster expansions in
multicomponent systems. Physical Review B, 106(2), 024203.
https://doi.org/10.1103/PhysRevB.106.024203</unstructured_citation>
          </citation>
          <citation key="Zhong:2023">
            <article_title>Modeling Intercalation Chemistry with
Multiredox Reactions by Sparse Lattice Models in Disordered Rocksalt
Cathodes</article_title>
            <author>Zhong</author>
            <journal_title>PRX Energy</journal_title>
            <issue>4</issue>
            <volume>2</volume>
            <doi>10.1103/PRXEnergy.2.043005</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Zhong, P., Xie, F., Barroso-Luque,
L., Huang, L., &amp; Ceder, G. (2023). Modeling Intercalation Chemistry
with Multiredox Reactions by Sparse Lattice Models in Disordered
Rocksalt Cathodes. PRX Energy, 2(4), 043005.
https://doi.org/10.1103/PRXEnergy.2.043005</unstructured_citation>
          </citation>
          <citation key="Barroso-Luque:2022">
            <article_title>Cluster expansions of multicomponent ionic
materials: Formalism and methodology</article_title>
            <author>Barroso-Luque</author>
            <journal_title>Physical Review B</journal_title>
            <issue>14</issue>
            <volume>106</volume>
            <doi>10.1103/PhysRevB.106.144202</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Barroso-Luque, L., Zhong, P., Yang,
J. H., Xie, F., Chen, T., Ouyang, B., &amp; Ceder, G. (2022). Cluster
expansions of multicomponent ionic materials: Formalism and methodology.
Physical Review B, 106(14), 144202.
https://doi.org/10.1103/PhysRevB.106.144202</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
