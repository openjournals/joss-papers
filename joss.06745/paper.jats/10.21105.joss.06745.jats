<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6745</article-id>
<article-id pub-id-type="doi">10.21105/joss.06745</article-id>
<title-group>
<article-title>nimbleHMC: An R package for Hamiltonian Monte Carlo
sampling in nimble</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1453-1908</contrib-id>
<name>
<surname>Turek</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8329-6796</contrib-id>
<name>
<surname>de Valpine</surname>
<given-names>Perry</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Paciorek</surname>
<given-names>Christopher J.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Lafayette College, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of California, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-07-05">
<day>5</day>
<month>7</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6745</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>hierarchical model</kwd>
<kwd>Markov chain Monte Carlo</kwd>
<kwd>Hamiltonian Monte Carlo</kwd>
<kwd>nimble</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Markov chain Monte Carlo (MCMC) algorithms are widely used for
  fitting hierarchical models to data. MCMC is the predominant tool used
  in Bayesian analyses to generate samples from the posterior
  distribution of model parameters conditional on observed data. MCMC is
  not a single algorithm, but rather a framework in which various
  sampling methods (samplers) are assigned to operate on subsets of
  unobserved parameters. There exists a vast set of valid samplers to
  draw upon, which differ in complexity, autocorrelation of samples
  produced, and applicability.</p>
  <p>Hamiltonian Monte Carlo [HMC; Radford M. Neal
  (<xref alt="2011" rid="ref-neal2011mcmc" ref-type="bibr">2011</xref>)]
  sampling is one such technique, applicable to continuous-valued
  parameters, which uses gradients to generate large transitions in
  parameter space. The resulting samples have low autocorrelation, and
  therefore have high information content, relative for example to an
  equal-length sequence of highly autocorrelated samples. The No-U-Turn
  (NUTS) variety of HMC sampling [HMC-NUTS; Hoffman &amp; Gelman
  (<xref alt="2014" rid="ref-hoffman2014no" ref-type="bibr">2014</xref>)]
  greatly increases the usability of HMC by introducing a recursive tree
  of numerical integration steps that makes it unnecessary to
  pre-specify a fixed number of steps. Hoffman &amp; Gelman
  (<xref alt="2014" rid="ref-hoffman2014no" ref-type="bibr">2014</xref>)
  also introduce a self-tuning scheme for the step size, resulting in a
  fully automated HMC sampler with no need for manual tuning.</p>
  <p>Many software packages offer implementations of MCMC, such as
  <monospace>nimble</monospace>
  (<xref alt="de Valpine et al., 2017" rid="ref-de2017programming" ref-type="bibr">de
  Valpine et al., 2017</xref>), <monospace>WinBUGS</monospace>
  (<xref alt="Lunn et al., 2000" rid="ref-lunn2000winbugs" ref-type="bibr">Lunn
  et al., 2000</xref>), <monospace>JAGS</monospace>
  (<xref alt="Plummer, 2003" rid="ref-plummer2003jags" ref-type="bibr">Plummer,
  2003</xref>), <monospace>PyMC</monospace>
  (<xref alt="Fonnesbeck et al., 2015" rid="ref-fonnesbeck2015pymc" ref-type="bibr">Fonnesbeck
  et al., 2015</xref>), <monospace>NumPyro</monospace>
  (<xref alt="Phan et al., 2019" rid="ref-phan2019composable" ref-type="bibr">Phan
  et al., 2019</xref>), <monospace>TensorFlow Probability</monospace>
  (<xref alt="Pang et al., 2020" rid="ref-pang2020deep" ref-type="bibr">Pang
  et al., 2020</xref>), and <monospace>Stan</monospace>
  (<xref alt="Carpenter et al., 2017" rid="ref-carpenter2017stan" ref-type="bibr">Carpenter
  et al., 2017</xref>), among others. These packages differ, however, in
  their approaches to sampler assignments. As sampling techniques vary
  in computation and quality of the samples, the effectiveness of the
  MCMC algorithms will vary depending on the software and model.</p>
  <p>A key design feature of <monospace>nimble</monospace>’s MCMC is to
  allow easy customization of sampler assignments from a high-level
  interface. Users may assign any valid samplers to each parameter or
  group of parameters, selecting from samplers provided with
  <monospace>nimble</monospace> or samplers they have written in
  <monospace>nimble</monospace>’s algorithm programming system. Samplers
  provided with <monospace>nimble</monospace> include random walk
  Metropolis-Hastings sampling
  (<xref alt="Robert &amp; Casella, 1999" rid="ref-robert1999metropolis" ref-type="bibr">Robert
  &amp; Casella, 1999</xref>), slice sampling
  (<xref alt="Radford M. Neal, 2003" rid="ref-neal2003slice" ref-type="bibr">Radford
  M. Neal, 2003</xref>), elliptical slice sampling
  (<xref alt="Murray et al., 2010" rid="ref-murray2010elliptical" ref-type="bibr">Murray
  et al., 2010</xref>), automated factor slice sampling
  (<xref alt="Tibbits et al., 2014" rid="ref-tibbits2014automated" ref-type="bibr">Tibbits
  et al., 2014</xref>), conjugate sampling
  (<xref alt="George et al., 1993" rid="ref-george1993conjugate" ref-type="bibr">George
  et al., 1993</xref>), and others.</p>
  <p>The <monospace>nimbleHMC</monospace> package provides
  implementations of two versions of HMC-NUTS sampling for use within
  <monospace>nimble</monospace>, both written in
  <monospace>nimble</monospace>’s algorithm programming system within R.
  Specifically, <monospace>nimbleHMC</monospace> provides the original
  (“classic”) HMC-NUTS algorithm as developed in Hoffman &amp; Gelman
  (<xref alt="2014" rid="ref-hoffman2014no" ref-type="bibr">2014</xref>),
  and a modern version of HMC-NUTS sampling matching the HMC sampler
  available in version 2.32.2 of <monospace>Stan</monospace>
  (<xref alt="Stan Development Team, 2023" rid="ref-stan2023stan" ref-type="bibr">Stan
  Development Team, 2023</xref>). The samplers provided in
  <monospace>nimbleHMC</monospace> can be assigned to any
  continuous-valued parameters, and may be used in combination with
  other samplers provided with <monospace>nimble</monospace>.</p>
</sec>
<sec id="example">
  <title>Example</title>
  <p>The following example demonstrates fitting a hierarchical model to
  data using <monospace>nimbleHMC</monospace>. We use the European
  Dipper <italic>(Cinclus cinclus)</italic> dataset drawn from
  ecological capture-recapture (<italic>e.g.</italic>,
  <xref alt="Lebreton et al., 1992" rid="ref-lebreton1992modeling" ref-type="bibr">Lebreton
  et al., 1992</xref>;
  <xref alt="Turek et al., 2016" rid="ref-turek2016efficient" ref-type="bibr">Turek
  et al., 2016</xref>). Modelling includes both continuous parameters to
  undergo HMC sampling and discrete parameters that cannot be sampled
  via HMC.</p>
  <p>Individual birds are captured, tagged, and potentially recaptured
  on subsequent sighting occasions. Data is a
  <inline-formula><alternatives>
  <tex-math><![CDATA[255 \times 7]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>255</mml:mn><mml:mo>×</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  binary-valued array of capture histories of 255 uniquely tagged birds
  over 7 years. Model parameters are detection probability
  (<inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>),
  and annual survival rates on non-flood years
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\phi_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>)
  and flood years (<inline-formula><alternatives>
  <tex-math><![CDATA[\phi_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>).
  Data is provided in the R package <monospace>mra</monospace>
  (<xref alt="McDonald, 2018" rid="ref-mcdonald2018mra" ref-type="bibr">McDonald,
  2018</xref>), and individuals which are first sighted on the final
  (7<inline-formula><alternatives>
  <tex-math><![CDATA[^{th}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>)
  sighting occasion do not contribute to inference, and are removed from
  the sighting histories.</p>
  <preformat>library(mra) 
data(dipper.data) 
dipper &lt;- dipper.data[,1:7]
y &lt;- dipper[apply(dipper, 1, which.max) &lt; 7, ]</preformat>
  <p>We specify the hierarchical model using uniform priors on the
  interval <inline-formula><alternatives>
  <tex-math><![CDATA[[0,1]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  for all parameters. Binary-valued latent states
  <inline-formula><alternatives>
  <tex-math><![CDATA[x_{i,t}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  represent the true alive (1) or dead (0) state of individual
  <inline-formula><alternatives>
  <tex-math><![CDATA[i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>
  on year <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.
  Doing so allows the survival process to be modelled as
  <inline-formula><alternatives>
  <tex-math><![CDATA[x_{i,t+1}~\sim~\text{Bernoulli}(\phi_{f_t} \cdot
  x_{i,t})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.222em"></mml:mspace><mml:mo>∼</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mtext mathvariant="normal">Bernoulli</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[f_t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>f</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  indicates the flood/non-flood history of year
  <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.
  The model structure conditions on the first observation of each
  individual, where <inline-formula><alternatives>
  <tex-math><![CDATA[\text{first}_i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mtext mathvariant="normal">first</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the first observation period of individual
  <inline-formula><alternatives>
  <tex-math><![CDATA[i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>i</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[x_{i,\text{first}_i}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mtext mathvariant="normal">first</mml:mtext><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  is assigned the value one. Observations are modelled as
  <inline-formula><alternatives>
  <tex-math><![CDATA[y_{i,t}~\sim~\text{Bernoulli}(p \cdot x_{i,t})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.222em"></mml:mspace><mml:mo>∼</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mtext mathvariant="normal">Bernoulli</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <preformat>library(nimbleHMC) 

code &lt;- nimbleCode({
    phi[1] ~ dunif(0, 1)
    phi[2] ~ dunif(0, 1)
    p ~ dunif(0, 1)
    for(i in 1:N) {
        x[i,first[i]] &lt;- 1
        for(t in (first[i]+1):T) {
            x[i,t] ~ dbern(phi[f[t]] * x[i,t-1])
            y[i,t] ~ dbern(p * x[i,t])
        }
    }
})</preformat>
  <p>A <monospace>nimble</monospace> model object is now built. The
  argument <monospace>buildDerivs = TRUE</monospace> results in
  under-the-hood support for obtaining derivatives from model
  calculations, as necessary for derivative-based HMC sampling.</p>
  <preformat>Rmodel &lt;- nimbleModel(
    code,
    constants = list(N = nrow(y), T = ncol(y), f = c(1,2,2,1,1,1,1),
                     first = apply(y, 1, which.max)),
    data = list(y = y),
    inits = list(phi = c(0.5, 0.5), p = 0.5, x = array(1, dim(y))),
    buildDerivs = TRUE)</preformat>
  <p>Next we create an MCMC configuration object, which specifies the
  sampling algorithm to be applied to each parameter. By default,
  <monospace>configureMCMC</monospace> uses
  <monospace>nimble</monospace>’s default sampler assignments of
  adaptive random walk Metropolis-Hastings [<monospace>RW</monospace>
  sampler; Robert &amp; Casella
  (<xref alt="1999" rid="ref-robert1999metropolis" ref-type="bibr">1999</xref>)]
  for each parameter, and a <monospace>binary</monospace> Gibbs sampler
  for each <inline-formula><alternatives>
  <tex-math><![CDATA[x_{i,t}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
  latent state.</p>
  <preformat>conf &lt;- configureMCMC(Rmodel)

## RW sampler (3)
##   - phi[]  (2 elements)
##   - p
## binary sampler (848)
##   - x[]  (848 elements)</preformat>
  <p>Now we customize the MCMC configuration object to use HMC sampling
  for the model parameters. <monospace>replaceSamplers</monospace>
  replaces the samplers operating on <inline-formula><alternatives>
  <tex-math><![CDATA[\phi_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\phi_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>
  with the modern HMC-NUTS sampler (called the
  <monospace>NUTS</monospace> sampler) provided in
  <monospace>nimbleHMC</monospace>. The classic version of the HMC-NUTS
  sampler could be assigned by specifying
  <monospace>type = &quot;NUTS_classic&quot;</monospace>.</p>
  <preformat>conf$replaceSamplers(target = c(&quot;phi&quot;, &quot;p&quot;), type = &quot;NUTS&quot;)
conf$printSamplers(byType = TRUE)

## NUTS sampler (1)
##   - phi, p 
## binary sampler (848)
##   - x[]  (848 elements)</preformat>
  <p>Alternatively, the convenience function
  <monospace>configureHMC</monospace> could be used to create an
  identical MCMC configuration, applying HMC-NUTS sampling to
  <inline-formula><alternatives>
  <tex-math><![CDATA[\phi_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\phi_2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ϕ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>,
  and default binary samplers for discrete parameters.</p>
  <p>Now we build and compile the MCMC algorithm.</p>
  <preformat>Rmcmc &lt;- buildMCMC(conf)
Cmodel &lt;- compileNimble(Rmodel)
Cmcmc &lt;- compileNimble(Rmcmc, project = Rmodel)</preformat>
  <p>We execute the MCMC for 20,000 iterations, and discard the initial
  10,000 samples as burn-in.</p>
  <preformat>set.seed(0)
samples &lt;- runMCMC(Cmcmc, niter = 20000, nburnin = 10000)</preformat>
  <p>Finally, posterior summary statistics are calculated for the model
  parameters.</p>
  <preformat>samplesSummary(samples, round = 2)

##        Mean Median St.Dev. 95%CI_low 95%CI_upp
## p      0.90   0.90    0.03      0.84      0.95
## phi[1] 0.58   0.58    0.03      0.52      0.63
## phi[2] 0.50   0.50    0.06      0.39      0.61</preformat>
  <p>Traceplots and posterior density plots are generated using the
  <monospace>samplesPlot</monospace> function from the
  <monospace>basicMCMCplots</monospace> package.</p>
  <preformat>basicMCMCplots::samplesPlot(samples, legend.location = &quot;topleft&quot;)</preformat>
  <graphic mimetype="application" mime-subtype="pdf" xlink:href="samplesPlot.pdf" />
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>HMC is recognized as a state-of-the-art MCMC sampling algorithm. As
  testimony to this, software packages such as
  <monospace>Stan</monospace> exclusively employ HMC sampling.
  Consequently, such software cannot operate on models containing
  discrete parameters (upon which HMC cannot operate), or would require
  marginalization of the likelihood to remove these discrete dimensions
  from the sampling problem. Models with discrete parameters arise in a
  range of statistical motifs including hidden Markov models, finite
  mixture models, and generally in the presence of unobserved
  categorical data
  (<xref alt="Bartolucci et al., 2022" rid="ref-bartolucci2022discrete" ref-type="bibr">Bartolucci
  et al., 2022</xref>). In contrast, other mainstream MCMC packages
  (<monospace>WinBUGS</monospace>, <monospace>OpenBUGS</monospace> and
  <monospace>JAGS</monospace>) can sample discrete parameters, but
  provide no facilities for HMC sampling. This leaves the use case of
  HMC sampling of hierarchical models that also contain discrete
  parameters.</p>
  <p><monospace>nimbleHMC</monospace> accomplishes this, by providing
  two HMC samplers that operate inside <monospace>nimble</monospace>’s
  MCMC engine. The base <monospace>nimble</monospace> package provides a
  variety of MCMC sampling algorithms, as well as the ability to
  customize MCMC sampler assignments. <monospace>nimbleHMC</monospace>
  augments the set of sampling algorithms provided in
  <monospace>nimble</monospace> with two options for HMC sampling, which
  can be used alongside any other samplers. The example presented here
  demonstrates precisely that: HMC sampling operating alongside discrete
  samplers, which is not possible without the use of
  <monospace>nimbleHMC</monospace>.</p>
  <p>Which combination of samplers will optimize MCMC efficiency for any
  particular problem is an open question. One metric of comparison is
  the effective sample size of the samples generated per unit runtime of
  the algorithm, which quantifies how quickly an MCMC algorithm
  generates information about parameter posteriors. This metric is
  studied in Turek et al.
  (<xref alt="2017" rid="ref-turek2017automated" ref-type="bibr">2017</xref>)
  and Ponisio et al.
  (<xref alt="2020" rid="ref-ponisio2020one" ref-type="bibr">2020</xref>),
  with the conclusion that the best sampling strategy is
  problem-specific rather than universal. For that reason, the ability
  to mix-and-match samplers from a large pool of candidates is important
  from both practical and theoretical standpoints. Indeed, packages such
  as <monospace>compareMCMCs</monospace>
  (<xref alt="de Valpine et al., 2022" rid="ref-de2022comparemcmcs" ref-type="bibr">de
  Valpine et al., 2022</xref>) exist specifically to compare the
  relative performance of MCMC algorithms. The addition of HMC sampling
  provided by <monospace>nimbleHMC</monospace> supports new practical
  combinations for applied MCMC, as well as facilitates a deeper study
  of Bayesian modelling.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-neal2011mcmc">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Neal</surname><given-names>Radford M.</given-names></name>
      </person-group>
      <article-title>Handbook of markov chain monte carlo</article-title>
      <publisher-name>Chapman; Hall/CRC</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <fpage>113</fpage>
      <lpage>162</lpage>
    </element-citation>
  </ref>
  <ref id="ref-de2022comparemcmcs">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>de Valpine</surname><given-names>Perry</given-names></name>
        <name><surname>Paganin</surname><given-names>Sally</given-names></name>
        <name><surname>Turek</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>compareMCMCs: An R package for studying MCMC efficiency</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>69</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03844</pub-id>
      <fpage>3844</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-de2017programming">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>de Valpine</surname><given-names>Perry</given-names></name>
        <name><surname>Turek</surname><given-names>Daniel</given-names></name>
        <name><surname>Paciorek</surname><given-names>Christopher J</given-names></name>
        <name><surname>Anderson-Bergman</surname><given-names>Clifford</given-names></name>
        <name><surname>Lang</surname><given-names>Duncan Temple</given-names></name>
        <name><surname>Bodik</surname><given-names>Rastislav</given-names></name>
      </person-group>
      <article-title>Programming with models: Writing statistical algorithms for general model structures with NIMBLE</article-title>
      <source>Journal of Computational and Graphical Statistics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>26</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1080/10618600.2016.1172487</pub-id>
      <fpage>403</fpage>
      <lpage>413</lpage>
    </element-citation>
  </ref>
  <ref id="ref-plummer2003jags">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Plummer</surname><given-names>Martyn</given-names></name>
      </person-group>
      <article-title>JAGS: A program for analysis of bayesian graphical models using gibbs sampling</article-title>
      <source>Proceedings of the 3rd international workshop on distributed statistical computing</source>
      <publisher-name>Vienna, Austria.</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <volume>124</volume>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fonnesbeck2015pymc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fonnesbeck</surname><given-names>Chris</given-names></name>
        <name><surname>Patil</surname><given-names>Anand</given-names></name>
        <name><surname>Huard</surname><given-names>David</given-names></name>
        <name><surname>Salvatier</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>PyMC: Bayesian stochastic modelling in python</article-title>
      <source>Astrophysics Source Code Library</source>
      <year iso-8601-date="2015">2015</year>
      <fpage>ascl</fpage>
      <lpage>1506</lpage>
    </element-citation>
  </ref>
  <ref id="ref-carpenter2017stan">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carpenter</surname><given-names>Bob</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
        <name><surname>Hoffman</surname><given-names>Matthew D</given-names></name>
        <name><surname>Lee</surname><given-names>Daniel</given-names></name>
        <name><surname>Goodrich</surname><given-names>Ben</given-names></name>
        <name><surname>Betancourt</surname><given-names>Michael</given-names></name>
        <name><surname>Brubaker</surname><given-names>Marcus</given-names></name>
        <name><surname>Guo</surname><given-names>Jiqiang</given-names></name>
        <name><surname>Li</surname><given-names>Peter</given-names></name>
        <name><surname>Riddell</surname><given-names>Allen</given-names></name>
      </person-group>
      <article-title>Stan: A probabilistic programming language</article-title>
      <source>Journal of statistical software</source>
      <publisher-name>Columbia Univ., New York, NY (United States); Harvard Univ., Cambridge, MA</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>76</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-stan2023stan">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Stan Development Team</string-name>
      </person-group>
      <article-title>Stan modeling language users guide and reference manual, version 2.32.2</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://mc-stan.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-robert1999metropolis">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Robert</surname><given-names>Christian P</given-names></name>
        <name><surname>Casella</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>The metropolis—hastings algorithm</article-title>
      <source>Monte carlo statistical methods</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="1999">1999</year>
      <fpage>231</fpage>
      <lpage>283</lpage>
    </element-citation>
  </ref>
  <ref id="ref-neal2003slice">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Neal</surname><given-names>Radford M</given-names></name>
      </person-group>
      <article-title>Slice sampling</article-title>
      <source>The annals of statistics</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <volume>31</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1214/aos/1056562461</pub-id>
      <fpage>705</fpage>
      <lpage>767</lpage>
    </element-citation>
  </ref>
  <ref id="ref-george1993conjugate">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>George</surname><given-names>Edward I</given-names></name>
        <name><surname>Makov</surname><given-names>UE</given-names></name>
        <name><surname>Smith</surname><given-names>AFM</given-names></name>
      </person-group>
      <article-title>Conjugate likelihood distributions</article-title>
      <source>Scandinavian Journal of Statistics</source>
      <publisher-name>JSTOR</publisher-name>
      <year iso-8601-date="1993">1993</year>
      <fpage>147</fpage>
      <lpage>156</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hoffman2014no">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matthew D</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo.</article-title>
      <source>J. Mach. Learn. Res.</source>
      <year iso-8601-date="2014">2014</year>
      <volume>15</volume>
      <issue>1</issue>
      <fpage>1593</fpage>
      <lpage>1623</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bartolucci2022discrete">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bartolucci</surname><given-names>Francesco</given-names></name>
        <name><surname>Pandolfi</surname><given-names>Silvia</given-names></name>
        <name><surname>Pennoni</surname><given-names>Fulvia</given-names></name>
      </person-group>
      <article-title>Discrete latent variable models</article-title>
      <source>Annual Review of Statistics and Its Application</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.1146/annurev-statistics-040220-091910</pub-id>
      <fpage>425</fpage>
      <lpage>452</lpage>
    </element-citation>
  </ref>
  <ref id="ref-turek2017automated">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Turek</surname><given-names>Daniel</given-names></name>
        <name><surname>de Valpine</surname><given-names>Perry</given-names></name>
        <name><surname>Paciorek</surname><given-names>Christopher J</given-names></name>
        <name><surname>Anderson-Bergman</surname><given-names>Clifford</given-names></name>
      </person-group>
      <article-title>Automated parameter blocking for efficient markov chain monte carlo sampling</article-title>
      <source>Bayesian Analysis</source>
      <publisher-name>International Society for Bayesian Analysis</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>12</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1214/16-BA1008</pub-id>
      <fpage>465</fpage>
      <lpage>490</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ponisio2020one">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ponisio</surname><given-names>Lauren C</given-names></name>
        <name><surname>de Valpine</surname><given-names>Perry</given-names></name>
        <name><surname>Michaud</surname><given-names>Nicholas</given-names></name>
        <name><surname>Turek</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>One size does not fit all: Customizing MCMC methods for hierarchical models using NIMBLE</article-title>
      <source>Ecology and evolution</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>10</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1002/ece3.6053</pub-id>
      <fpage>2385</fpage>
      <lpage>2416</lpage>
    </element-citation>
  </ref>
  <ref id="ref-murray2010elliptical">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
        <name><surname>Adams</surname><given-names>Ryan</given-names></name>
        <name><surname>MacKay</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Elliptical slice sampling</article-title>
      <source>Proceedings of the thirteenth international conference on artificial intelligence and statistics</source>
      <publisher-name>JMLR Workshop; Conference Proceedings</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <fpage>541</fpage>
      <lpage>548</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tibbits2014automated">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tibbits</surname><given-names>Matthew M</given-names></name>
        <name><surname>Groendyke</surname><given-names>Chris</given-names></name>
        <name><surname>Haran</surname><given-names>Murali</given-names></name>
        <name><surname>Liechty</surname><given-names>John C</given-names></name>
      </person-group>
      <article-title>Automated factor slice sampling</article-title>
      <source>Journal of Computational and Graphical Statistics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>23</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1080/10618600.2013.791193</pub-id>
      <fpage>543</fpage>
      <lpage>563</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mcdonald2018mra">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>McDonald</surname><given-names>Trent</given-names></name>
      </person-group>
      <source>Mra: Mark-recapture analysis</source>
      <year iso-8601-date="2018">2018</year>
      <uri>https://CRAN.R-project.org/package=mra</uri>
    </element-citation>
  </ref>
  <ref id="ref-lebreton1992modeling">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lebreton</surname><given-names>Jean-Dominique</given-names></name>
        <name><surname>Burnham</surname><given-names>Kenneth P</given-names></name>
        <name><surname>Clobert</surname><given-names>Jean</given-names></name>
        <name><surname>Anderson</surname><given-names>David R</given-names></name>
      </person-group>
      <article-title>Modeling survival and testing biological hypotheses using marked animals: A unified approach with case studies</article-title>
      <source>Ecological monographs</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1992">1992</year>
      <volume>62</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.2307/2937171</pub-id>
      <fpage>67</fpage>
      <lpage>118</lpage>
    </element-citation>
  </ref>
  <ref id="ref-turek2016efficient">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Turek</surname><given-names>Daniel</given-names></name>
        <name><surname>de Valpine</surname><given-names>Perry</given-names></name>
        <name><surname>Paciorek</surname><given-names>Christopher J</given-names></name>
      </person-group>
      <article-title>Efficient markov chain monte carlo sampling for hierarchical hidden markov models</article-title>
      <source>Environmental and ecological statistics</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>23</volume>
      <pub-id pub-id-type="doi">10.1007/s10651-016-0353-z</pub-id>
      <fpage>549</fpage>
      <lpage>564</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lunn2000winbugs">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lunn</surname><given-names>David J</given-names></name>
        <name><surname>Thomas</surname><given-names>Andrew</given-names></name>
        <name><surname>Best</surname><given-names>Nicky</given-names></name>
        <name><surname>Spiegelhalter</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>WinBUGS-a bayesian modelling framework: Concepts, structure, and extensibility</article-title>
      <source>Statistics and computing</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2000">2000</year>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.1023/A:1008929526011</pub-id>
      <fpage>325</fpage>
      <lpage>337</lpage>
    </element-citation>
  </ref>
  <ref id="ref-phan2019composable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Phan</surname><given-names>Du</given-names></name>
        <name><surname>Pradhan</surname><given-names>Neeraj</given-names></name>
        <name><surname>Jankowiak</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>Composable effects for flexible and accelerated probabilistic programming in NumPyro</article-title>
      <source>arXiv preprint arXiv:1912.11554</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.11554</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pang2020deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pang</surname><given-names>Bo</given-names></name>
        <name><surname>Nijkamp</surname><given-names>Erik</given-names></name>
        <name><surname>Wu</surname><given-names>Ying Nian</given-names></name>
      </person-group>
      <article-title>Deep learning with tensorflow: A review</article-title>
      <source>Journal of Educational and Behavioral Statistics</source>
      <publisher-name>SAGE Publications Sage CA: Los Angeles, CA</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>45</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.3102/1076998619872761</pub-id>
      <fpage>227</fpage>
      <lpage>248</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
