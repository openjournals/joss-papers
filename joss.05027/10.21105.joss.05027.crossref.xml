<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230624T034914-111ec7bb41c8ab7ad012add0dc40b505ddf7a325</doi_batch_id>
    <timestamp>20230624034914</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>06</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>86</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>pytorch-widedeep: A flexible package for multimodal
deep learning</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Javier Rodriguez</given_name>
            <surname>Zaurin</surname>
            <ORCID>https://orcid.org/0000-0002-1082-1107</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Pavol</given_name>
            <surname>Mulinka</surname>
            <ORCID>https://orcid.org/0000-0002-9394-8794</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>06</month>
          <day>24</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5027</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05027</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7908172</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5027</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05027</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05027</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05027.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="tensorflow2015-whitepaper">
            <article_title>TensorFlow: Large-scale machine learning on
heterogeneous systems</article_title>
            <author>Abadi</author>
            <cYear>2015</cYear>
            <unstructured_citation>Abadi, M., Agarwal, A., Barham, P.,
Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J.,
Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M., … Zheng, X. (2015).
TensorFlow: Large-scale machine learning on heterogeneous systems.
https://www.tensorflow.org/</unstructured_citation>
          </citation>
          <citation key="NEURIPS2019_9015">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In Advances in neural information processing systems 32 (pp.
8024–8035). Curran Associates, Inc.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
          <citation key="joseph2021pytorch">
            <article_title>PyTorch tabular: A framework for deep
learning with tabular data</article_title>
            <author>Joseph</author>
            <cYear>2021</cYear>
            <unstructured_citation>Joseph, M. (2021). PyTorch tabular: A
framework for deep learning with tabular data.
https://arxiv.org/abs/2104.13638</unstructured_citation>
          </citation>
          <citation key="erickson2020autogluon">
            <article_title>AutoGluon-tabular: Robust and accurate AutoML
for structured data</article_title>
            <author>Erickson</author>
            <doi>10.48550/ARXIV.2003.06505</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Erickson, N., Mueller, J., Shirkov,
A., Zhang, H., Larroy, P., Li, M., &amp; Smola, A. (2020).
AutoGluon-tabular: Robust and accurate AutoML for structured data.
arXiv. https://doi.org/10.48550/ARXIV.2003.06505</unstructured_citation>
          </citation>
          <citation key="singh2020mmf">
            <article_title>MMF: A multimodal framework for vision and
language research</article_title>
            <author>Singh</author>
            <cYear>2020</cYear>
            <unstructured_citation>Singh, A., Goswami, V., Natarajan,
V., Jiang, Y., Chen, X., Shah, M., Rohrbach, M., Batra, D., &amp;
Parikh, D. (2020). MMF: A multimodal framework for vision and language
research.
https://github.com/facebookresearch/mmf.</unstructured_citation>
          </citation>
          <citation key="cheng2016wide">
            <article_title>Wide &amp; deep learning for recommender
systems</article_title>
            <author>Cheng</author>
            <journal_title>Proceedings of the 1st workshop on deep
learning for recommender systems</journal_title>
            <doi>10.48550/arXiv.1606.07792</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Cheng, H.-T., Koc, L., Harmsen, J.,
Shaked, T., Chandra, T., Aradhye, H., Anderson, G., Corrado, G., Chai,
W., Ispir, M., &amp; others. (2016). Wide &amp; deep learning for
recommender systems. Proceedings of the 1st Workshop on Deep Learning
for Recommender Systems, 7–10.
https://doi.org/10.48550/arXiv.1606.07792</unstructured_citation>
          </citation>
          <citation key="arik2021tabnet">
            <article_title>Tabnet: Attentive interpretable tabular
learning</article_title>
            <author>Arik</author>
            <journal_title>Proceedings of the AAAI conference on
artificial intelligence</journal_title>
            <volume>35</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Arik, S. Ö., &amp; Pfister, T.
(2021). Tabnet: Attentive interpretable tabular learning. Proceedings of
the AAAI Conference on Artificial Intelligence, 35,
6679–6687.</unstructured_citation>
          </citation>
          <citation key="yang2016hierarchical">
            <article_title>Hierarchical attention networks for document
classification</article_title>
            <author>Yang</author>
            <doi>10.18653/v1/N16-1174</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Yang, Z., Yang, D., Dyer, C., He, X.,
Smola, A., &amp; Hovy, E. (2016). Hierarchical attention networks for
document classification. 1480–1489.
https://doi.org/10.18653/v1/N16-1174</unstructured_citation>
          </citation>
          <citation key="huang2020tabtransformer">
            <article_title>Tabtransformer: Tabular data modeling using
contextual embeddings</article_title>
            <author>Huang</author>
            <journal_title>arXiv preprint
arXiv:2012.06678</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Huang, X., Khetan, A., Cvitkovic, M.,
&amp; Karnin, Z. (2020). Tabtransformer: Tabular data modeling using
contextual embeddings. arXiv Preprint
arXiv:2012.06678.</unstructured_citation>
          </citation>
          <citation key="somepalli2021saint">
            <article_title>Saint: Improved neural networks for tabular
data via row attention and contrastive pre-training</article_title>
            <author>Somepalli</author>
            <journal_title>arXiv preprint
arXiv:2106.01342</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Somepalli, G., Goldblum, M.,
Schwarzschild, A., Bruss, C. B., &amp; Goldstein, T. (2021). Saint:
Improved neural networks for tabular data via row attention and
contrastive pre-training. arXiv Preprint
arXiv:2106.01342.</unstructured_citation>
          </citation>
          <citation key="gorishniy2021revisiting">
            <article_title>Revisiting deep learning models for tabular
data</article_title>
            <author>Gorishniy</author>
            <journal_title>Advances in Neural Information Processing
Systems</journal_title>
            <volume>34</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Gorishniy, Y., Rubachev, I.,
Khrulkov, V., &amp; Babenko, A. (2021). Revisiting deep learning models
for tabular data. Advances in Neural Information Processing Systems, 34,
18932–18943.</unstructured_citation>
          </citation>
          <citation key="wu2021fastformer">
            <article_title>Fastformer: Additive attention can be all you
need</article_title>
            <author>Wu</author>
            <journal_title>arXiv preprint
arXiv:2108.09084</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Wu, C., Wu, F., Qi, T., Huang, Y.,
&amp; Xie, X. (2021). Fastformer: Additive attention can be all you
need. arXiv Preprint arXiv:2108.09084.</unstructured_citation>
          </citation>
          <citation key="jaegle2021perceiver">
            <article_title>Perceiver: General perception with iterative
attention</article_title>
            <author>Jaegle</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Jaegle, A., Gimeno, F., Brock, A.,
Vinyals, O., Zisserman, A., &amp; Carreira, J. (2021). Perceiver:
General perception with iterative attention. International Conference on
Machine Learning, 4651–4664.</unstructured_citation>
          </citation>
          <citation key="blundell2015weight">
            <article_title>Weight uncertainty in neural
network</article_title>
            <author>Blundell</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Blundell, C., Cornebise, J.,
Kavukcuoglu, K., &amp; Wierstra, D. (2015). Weight uncertainty in neural
network. International Conference on Machine Learning,
1613–1622.</unstructured_citation>
          </citation>
          <citation key="he2016deep">
            <article_title>Deep residual learning for image
recognition</article_title>
            <author>He</author>
            <journal_title>Proceedings of the IEEE conference on
computer vision and pattern recognition</journal_title>
            <doi>10.1109/cvpr.2016.90</doi>
            <cYear>2016</cYear>
            <unstructured_citation>He, K., Zhang, X., Ren, S., &amp;
Sun, J. (2016). Deep residual learning for image recognition.
Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 770–778.
https://doi.org/10.1109/cvpr.2016.90</unstructured_citation>
          </citation>
          <citation key="zhang2018shufflenet">
            <article_title>Shufflenet: An extremely efficient
convolutional neural network for mobile devices</article_title>
            <author>Zhang</author>
            <journal_title>Proceedings of the IEEE conference on
computer vision and pattern recognition</journal_title>
            <doi>10.1109/CVPR.2018.00716</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Zhang, X., Zhou, X., Lin, M., &amp;
Sun, J. (2018). Shufflenet: An extremely efficient convolutional neural
network for mobile devices. Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, 6848–6856.
https://doi.org/10.1109/CVPR.2018.00716</unstructured_citation>
          </citation>
          <citation key="xie2017aggregated">
            <article_title>Aggregated residual transformations for deep
neural networks</article_title>
            <author>Xie</author>
            <journal_title>Proceedings of the IEEE conference on
computer vision and pattern recognition</journal_title>
            <doi>10.1109/CVPR.2017.634</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Xie, S., Girshick, R., Dollár, P.,
Tu, Z., &amp; He, K. (2017). Aggregated residual transformations for
deep neural networks. Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, 1492–1500.
https://doi.org/10.1109/CVPR.2017.634</unstructured_citation>
          </citation>
          <citation key="zagoruyko2016wide">
            <article_title>Wide residual networks</article_title>
            <author>Zagoruyko</author>
            <journal_title>arXiv preprint
arXiv:1605.07146</journal_title>
            <doi>10.5244/c.30.87</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Zagoruyko, S., &amp; Komodakis, N.
(2016). Wide residual networks. arXiv Preprint arXiv:1605.07146.
https://doi.org/10.5244/c.30.87</unstructured_citation>
          </citation>
          <citation key="xu2022regnet">
            <article_title>RegNet: Self-regulated network for image
classification</article_title>
            <author>Xu</author>
            <journal_title>IEEE Transactions on Neural Networks and
Learning Systems</journal_title>
            <doi>10.1109/TNNLS.2022.3158966</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Xu, J., Pan, Y., Pan, X., Hoi, S.,
Yi, Z., &amp; Xu, Z. (2022). RegNet: Self-regulated network for image
classification. IEEE Transactions on Neural Networks and Learning
Systems.
https://doi.org/10.1109/TNNLS.2022.3158966</unstructured_citation>
          </citation>
          <citation key="huang2017densely">
            <article_title>Densely connected convolutional
networks</article_title>
            <author>Huang</author>
            <journal_title>Proceedings of the IEEE conference on
computer vision and pattern recognition</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Huang, G., Liu, Z., Van Der Maaten,
L., &amp; Weinberger, K. Q. (2017). Densely connected convolutional
networks. Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, 4700–4708.</unstructured_citation>
          </citation>
          <citation key="howard2017mobilenets">
            <article_title>Mobilenets: Efficient convolutional neural
networks for mobile vision applications</article_title>
            <author>Howard</author>
            <journal_title>arXiv preprint
arXiv:1704.04861</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Howard, A. G., Zhu, M., Chen, B.,
Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., &amp; Adam, H.
(2017). Mobilenets: Efficient convolutional neural networks for mobile
vision applications. arXiv Preprint
arXiv:1704.04861.</unstructured_citation>
          </citation>
          <citation key="tan2019mnasnet">
            <article_title>Mnasnet: Platform-aware neural architecture
search for mobile</article_title>
            <author>Tan</author>
            <journal_title>Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</journal_title>
            <doi>10.1109/CVPR.2019.00293</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Tan, M., Chen, B., Pang, R.,
Vasudevan, V., Sandler, M., Howard, A., &amp; Le, Q. V. (2019). Mnasnet:
Platform-aware neural architecture search for mobile. Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2820–2828.
https://doi.org/10.1109/CVPR.2019.00293</unstructured_citation>
          </citation>
          <citation key="tan2019efficientnet">
            <article_title>Efficientnet: Rethinking model scaling for
convolutional neural networks</article_title>
            <author>Tan</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Tan, M., &amp; Le, Q. (2019).
Efficientnet: Rethinking model scaling for convolutional neural
networks. International Conference on Machine Learning,
6105–6114.</unstructured_citation>
          </citation>
          <citation key="iandola2016squeezenet">
            <article_title>SqueezeNet: AlexNet-level accuracy with 50x
fewer parameters and&lt; 0.5 MB model size</article_title>
            <author>Iandola</author>
            <journal_title>arXiv preprint
arXiv:1602.07360</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Iandola, F. N., Han, S., Moskewicz,
M. W., Ashraf, K., Dally, W. J., &amp; Keutzer, K. (2016). SqueezeNet:
AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model
size. arXiv Preprint arXiv:1602.07360.</unstructured_citation>
          </citation>
          <citation key="wolf2019huggingface">
            <article_title>Huggingface’s transformers: State-of-the-art
natural language processing</article_title>
            <author>Wolf</author>
            <journal_title>arXiv preprint
arXiv:1910.03771</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Wolf, T., Debut, L., Sanh, V.,
Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,
Funtowicz, M., &amp; others. (2019). Huggingface’s transformers:
State-of-the-art natural language processing. arXiv Preprint
arXiv:1910.03771.</unstructured_citation>
          </citation>
          <citation key="yang2021delving">
            <article_title>Delving into deep imbalanced
regression</article_title>
            <author>Yang</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Yang, Y., Zha, K., Chen, Y., Wang,
H., &amp; Katabi, D. (2021). Delving into deep imbalanced regression.
International Conference on Machine Learning,
11842–11851.</unstructured_citation>
          </citation>
          <citation key="wang2019deep">
            <article_title>A deep probabilistic model for customer
lifetime value prediction</article_title>
            <author>Wang</author>
            <journal_title>arXiv preprint
arXiv:1912.07753</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Wang, X., Liu, T., &amp; Miao, J.
(2019). A deep probabilistic model for customer lifetime value
prediction. arXiv Preprint arXiv:1912.07753.</unstructured_citation>
          </citation>
          <citation key="torchvision2016">
            <article_title>TorchVision: PyTorch’s computer vision
library</article_title>
            <author>maintainers</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>maintainers, T., &amp; contributors.
(2016). TorchVision: PyTorch’s computer vision library. In GitHub
repository. https://github.com/pytorch/vision;
GitHub.</unstructured_citation>
          </citation>
          <citation key="torch_sample">
            <article_title>TorchSample: Lightweight pytorch functions
for neural network featuremap sampling</article_title>
            <author>maintainers</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>maintainers, T., &amp; contributors.
(2017). TorchSample: Lightweight pytorch functions for neural network
featuremap sampling. In GitHub repository.
https://github.com/ncullen93/torchsample;
GitHub.</unstructured_citation>
          </citation>
          <citation key="chollet2015keras">
            <article_title>Keras</article_title>
            <author>Chollet</author>
            <cYear>2015</cYear>
            <unstructured_citation>Chollet, F., &amp; others. (2015).
Keras. https://keras.io.</unstructured_citation>
          </citation>
          <citation key="info11020108">
            <article_title>Fastai: A layered API for deep
learning</article_title>
            <author>Howard</author>
            <journal_title>Information</journal_title>
            <issue>2</issue>
            <volume>11</volume>
            <doi>10.3390/info11020108</doi>
            <issn>2078-2489</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Howard, J., &amp; Gugger, S. (2020).
Fastai: A layered API for deep learning. Information, 11(2).
https://doi.org/10.3390/info11020108</unstructured_citation>
          </citation>
          <citation key="adrian2017deep">
            <volume_title>Deep learning for computer vision with
python</volume_title>
            <author>Adrian</author>
            <cYear>2017</cYear>
            <unstructured_citation>Adrian, R. (2017). Deep learning for
computer vision with python. PyImageSearch.com.</unstructured_citation>
          </citation>
          <citation key="garg2022multimodality">
            <article_title>Multimodality for NLP-centered applications:
Resources, advances and frontiers</article_title>
            <author>Garg</author>
            <journal_title>Proceedings of the thirteenth language
resources and evaluation conference</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Garg, M., Wazarkar, S., Singh, M.,
&amp; Bojar, O. (2022). Multimodality for NLP-centered applications:
Resources, advances and frontiers. Proceedings of the Thirteenth
Language Resources and Evaluation Conference,
6837–6847.</unstructured_citation>
          </citation>
          <citation key="wang2017deep">
            <article_title>Deep &amp; cross network for ad click
predictions</article_title>
            <author>Wang</author>
            <journal_title>Proceedings of the ADKDD’17</journal_title>
            <doi>10.48550/arXiv.1708.05123</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Wang, R., Fu, B., Fu, G., &amp; Wang,
M. (2017). Deep &amp; cross network for ad click predictions. In
Proceedings of the ADKDD’17 (pp. 1–7).
https://doi.org/10.48550/arXiv.1708.05123</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
