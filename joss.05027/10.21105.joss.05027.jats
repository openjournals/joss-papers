<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5027</article-id>
<article-id pub-id-type="doi">10.21105/joss.05027</article-id>
<title-group>
<article-title>pytorch-widedeep: A flexible package for multimodal deep
learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1082-1107</contrib-id>
<name>
<surname>Zaurin</surname>
<given-names>Javier Rodriguez</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9394-8794</contrib-id>
<name>
<surname>Mulinka</surname>
<given-names>Pavol</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Researcher, Spain</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Centre Tecnologic de Telecomunicacions de Catalunya
(CTTC/CERCA), Catalunya, Spain</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-11-13">
<day>13</day>
<month>11</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>5027</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Pytorch</kwd>
<kwd>Deep learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In recent years datasets have grown in size and diversity,
  combining different data types. Multimodal machine learning projects
  involving tabular data, images and/or text are gaining popularity
  (e.g. Garg et al.
  (<xref alt="2022" rid="ref-garg2022multimodality" ref-type="bibr">2022</xref>)).
  Traditional approaches involved independent feature generation from
  every data type and their combination in the later stage before
  passing them to an algorithm for classification or regression.</p>
  <p>However, with the advent of “<italic>easy-to-use</italic>” Deep
  Learning (DL) frameworks such as Tensorflow
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>) or PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-NEURIPS2019_9015" ref-type="bibr">Paszke
  et al., 2019</xref>), and the subsequent advances in the fields of
  Computer Vision, Natural Language Processing or Deep Learning for
  Tabular data, it is now possible to use state-of-the-art DL models and
  combine all datasets early in the process. This has two main
  advantages: (i) we can partially or entirely skip the feature
  engineering step, and (ii) the representations of each data type are
  learned jointly. This means that such representations contain
  information: (i) related to the target if the problem is supervised;
  and (ii) how the different data types relate to each other.</p>
  <p>Furthermore, the flexibility inherent to DL approaches allows the
  usage of techniques primarily designed only for text and/or images to
  tabular data, e.g., transfer learning or self-supervised
  pre-training.</p>
  <p>With that in mind, we introduce
  <monospace>pytorch-widedeep</monospace>, a flexible package for
  multimodal deep learning designed to facilitate the combination of
  tabular data with text and images.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>There is a small number of packages available to use DL for tabular
  data alone (e.g., pytorch-tabular
  (<xref alt="Joseph, 2021" rid="ref-joseph2021pytorch" ref-type="bibr">Joseph,
  2021</xref>), pytorch-tabnet or autogluon-tabular
  (<xref alt="Erickson et al., 2020" rid="ref-erickson2020autogluon" ref-type="bibr">Erickson
  et al., 2020</xref>)) or that focus mainly on combining text and
  images (e.g., MMF
  (<xref alt="Singh et al., 2020" rid="ref-singh2020mmf" ref-type="bibr">Singh
  et al., 2020</xref>)). With that in mind, our goal is to provide a
  modular, flexible, and “<italic>easy-to-use</italic>” framework that
  allows the combination of a wide variety of models for all data
  types.</p>
  <p><monospace>pytorch-widedeep</monospace> is based on Google’s Wide
  and Deep Algorithm
  (<xref alt="Cheng et al., 2016" rid="ref-cheng2016wide" ref-type="bibr">Cheng
  et al., 2016</xref>), hence its name. The original algorithm is
  heavily adjusted for multimodal datasets and intended to facilitate
  the combination of text and images with corresponding tabular data. As
  opposed to Google’s <italic>“Wide and Deep”</italic> and <italic>“Deep
  and Cross”</italic>
  (<xref alt="R. Wang et al., 2017" rid="ref-wang2017deep" ref-type="bibr">R.
  Wang et al., 2017</xref>) architecture implementations in
  Keras/Tensorflow, we use the wide/cross and deep model design as an
  initial building block of PyTorch deep learning models to provide the
  basis for a plethora of state-of-the-art models and architecture
  implementations that can be seamlessly assembled with just a few lines
  of code. Additionally, the individual components do not necessarily
  have to be a part of the final architecture. The main components of
  those architectures are shown in
  <xref alt="[fig:widedeep_arch]" rid="figU003Awidedeep_arch">[fig:widedeep_arch]</xref>.</p>
  <fig>
    <caption><p>Main components of the pytorch-widedeep architecture.
    The blue and green boxes in the figure represent the main data types
    and their corresponding model components, namely
    <monospace>wide</monospace>, <monospace>deeptabular</monospace>,
    <monospace>deeptext</monospace> and
    <monospace>deepimage</monospace>. The yellow boxes represent
    <italic>so-called</italic> fully-connected (FC) heads, simply MLPs
    that one can optionally add on top of the main components. These are
    referred to in the figure as <monospace>TextHead</monospace> and
    <monospace>ImageHead</monospace>. The dashed-line rectangles
    indicate that the outputs from the components inside are
    concatenated if a final FC head (referred to as
    <monospace>DeepHead</monospace> in the figure) is used. The
    faded-green <monospace>deeptabular</monospace> box aims to indicate
    that the output of the deeptabular component will be concatenated
    directly with the output of the <monospace>deeptext</monospace> or
    <monospace>deepimage</monospace> components or with the FC heads if
    these are used. Finally, the arrows indicate the connections, which
    of course, depend on the final architecture that the user chooses to
    build.
    <styled-content id="figU003Awidedeep_arch"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/widedeep_arch.png" />
  </fig>
  <p>Following the notation of
  (<xref alt="Cheng et al., 2016" rid="ref-cheng2016wide" ref-type="bibr">Cheng
  et al., 2016</xref>), the expression for the architecture without a
  <monospace>deephead</monospace> component can be formulated as:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[pred = \sigma(W_{wide}^{T}[x,\phi(x)] + \sum_{i \in \mathcal{I}} W_{i}^{T}a_{i}^{l_f} + b)]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:mi>ℐ</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>W</mml:mi><mml:mi>i</mml:mi><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:msubsup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>Where <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{I} = \{deeptabular, deeptext, deepimage \}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ℐ</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
  is the sigmoid function, <inline-formula><alternatives>
  <tex-math><![CDATA[W]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>W</mml:mi></mml:math></alternatives></inline-formula>
  are the weight matrices applied to the wide model and to the final
  activations of the deep models, <inline-formula><alternatives>
  <tex-math><![CDATA[a]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>a</mml:mi></mml:math></alternatives></inline-formula>
  are these final activations, <inline-formula><alternatives>
  <tex-math><![CDATA[\phi(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  are the cross-product transformations of the original features
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[b]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
  is the bias term.</p>
  <p>If there is a <monospace>deephead</monospace> component, the
  previous expression turns into:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[pred = \sigma(W_{wide}^{T}[x,\phi(x)] + W_{deephead}^{T}a_{deephead}^{l_f} + b)]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>w</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup><mml:msubsup><mml:mi>a</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:msubsup><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>At this stage, it is worth mentioning that the library has been
  built with a special emphasis on flexibility. That is, we want users
  to easily run as many different models as possible and/or use their
  custom components if they prefer. With that in mind, each and every
  data type component in the figure above can be used independently and
  in isolation. For example, if the user wants to use a ResNet model to
  perform classification in an image-only dataset, that is perfectly
  possible using this library. In addition, following some minor
  adjustments described in the documentation, the user can use any
  custom model for each data type – mainly, a custom model is a standard
  PyTorch model class that must have a property or attribute called
  <monospace>output_dim</monospace>. This way, the
  <monospace>WideDeep</monospace> collector class knows the size of the
  incoming activations and is able to construct the multimodal model.
  Examples of how to use custom components can be found in the
  repository and documentation.</p>
</sec>
<sec id="the-model-hub">
  <title>The Model Hub</title>
  <p>This section will briefly introduce the current model components
  available for each data type in the library. Remember that the library
  is constantly under development, and models are constantly added to
  the “model-hub”.</p>
  <sec id="the-wide-component">
    <title>The <monospace>wide</monospace> component</title>
    <p>This is a linear model for tabular data where the non-linearities
    are captured via cross-product transformations. This is the simplest
    of all components, and we consider it very useful as a benchmark
    when used on its own.</p>
  </sec>
  <sec id="the-deeptabular-component">
    <title>The <monospace>deeptabular</monospace> component</title>
    <p>Currently, <monospace>pytorch-widedeep</monospace> offers the
    following models for the so-called
    <monospace>deeptabular</monospace> component:(i) TabMlp, (ii)
    TabResnet, (iii) TabNet
    (<xref alt="Arik &amp; Pfister, 2021" rid="ref-arik2021tabnet" ref-type="bibr">Arik
    &amp; Pfister, 2021</xref>), (iv) ContextAttentionMLP
    (<xref alt="Z. Yang et al., 2016" rid="ref-yang2016hierarchical" ref-type="bibr">Z.
    Yang et al., 2016</xref>), (v) SelfAttentionMLP
    (<xref alt="X. Huang et al., 2020" rid="ref-huang2020tabtransformer" ref-type="bibr">X.
    Huang et al., 2020</xref>), (vi) TabTransformer
    (<xref alt="X. Huang et al., 2020" rid="ref-huang2020tabtransformer" ref-type="bibr">X.
    Huang et al., 2020</xref>), (vii) SAINT
    (<xref alt="Somepalli et al., 2021" rid="ref-somepalli2021saint" ref-type="bibr">Somepalli
    et al., 2021</xref>), (viii) FT-Transformer
    (<xref alt="Gorishniy et al., 2021" rid="ref-gorishniy2021revisiting" ref-type="bibr">Gorishniy
    et al., 2021</xref>), (ix) TabFastFormer: our adaptation of the
    FastFormer
    (<xref alt="Wu et al., 2021" rid="ref-wu2021fastformer" ref-type="bibr">Wu
    et al., 2021</xref>), (x) TabPerceiver: our adaptation of the
    Perceiver
    (<xref alt="Jaegle et al., 2021" rid="ref-jaegle2021perceiver" ref-type="bibr">Jaegle
    et al., 2021</xref>), (xi) BayesianWide and (xii) BayesianTabMlp
    (both based on Blundell et al.
    (<xref alt="2015" rid="ref-blundell2015weight" ref-type="bibr">2015</xref>)).</p>
  </sec>
  <sec id="the-deepimage-component">
    <title>The <monospace>deepimage</monospace> component</title>
    <p>The image-related component is fully integrated with the newest
    version of torchvision
    (<xref alt="TorchVision maintainers &amp; contributors, 2016" rid="ref-torchvision2016" ref-type="bibr">TorchVision
    maintainers &amp; contributors, 2016</xref>) (0.13 at the time of
    writing). This version has Multi-Weight Support. Therefore, a
    variety of model variants are available to use with pre-trained
    weights obtained with different datasets. Currently, the model
    variants supported by <monospace>pytorch-widedeep</monospace> are
    (i) Resnet
    (<xref alt="He et al., 2016" rid="ref-he2016deep" ref-type="bibr">He
    et al., 2016</xref>), (ii) Shufflenet
    (<xref alt="Zhang et al., 2018" rid="ref-zhang2018shufflenet" ref-type="bibr">Zhang
    et al., 2018</xref>), (iii) Resnext
    (<xref alt="Xie et al., 2017" rid="ref-xie2017aggregated" ref-type="bibr">Xie
    et al., 2017</xref>), (iv) Wide Resnet
    (<xref alt="Zagoruyko &amp; Komodakis, 2016" rid="ref-zagoruyko2016wide" ref-type="bibr">Zagoruyko
    &amp; Komodakis, 2016</xref>), (v) Regnet
    (<xref alt="Xu et al., 2022" rid="ref-xu2022regnet" ref-type="bibr">Xu
    et al., 2022</xref>), (vi) Densenet
    (<xref alt="G. Huang et al., 2017" rid="ref-huang2017densely" ref-type="bibr">G.
    Huang et al., 2017</xref>), (vii) Mobilenet
    (<xref alt="A. G. Howard et al., 2017" rid="ref-howard2017mobilenets" ref-type="bibr">A.
    G. Howard et al., 2017</xref>), (viii) MNasnet
    (<xref alt="Tan et al., 2019" rid="ref-tan2019mnasnet" ref-type="bibr">Tan
    et al., 2019</xref>), (ix) Efficientnet
    (<xref alt="Tan &amp; Le, 2019" rid="ref-tan2019efficientnet" ref-type="bibr">Tan
    &amp; Le, 2019</xref>) and (x) Squeezenet
    (<xref alt="Iandola et al., 2016" rid="ref-iandola2016squeezenet" ref-type="bibr">Iandola
    et al., 2016</xref>).</p>
  </sec>
  <sec id="the-deeptext-component">
    <title>The <monospace>deeptext</monospace> component</title>
    <p>Currently, <monospace>pytorch-widedeep</monospace> offers the
    following models for the <monospace>deeptext</monospace> component:
    (i) BasicRNN, (ii) AttentiveRNN and (iii) StackedAttentiveRNN. The
    library will be integrated with the Huggingface transformers library
    (<xref alt="Wolf et al., 2019" rid="ref-wolf2019huggingface" ref-type="bibr">Wolf
    et al., 2019</xref>) in the near future. However, it is worth
    mentioning that although transformer-based models are not natively
    supported by our library, these can be used easily with
    <monospace>pytorch-widedeep</monospace> as a custom model (please,
    see the documentation for details).</p>
  </sec>
</sec>
<sec id="forms-of-model-training">
  <title>Forms of model training:</title>
  <p>Training single or multi-mode models in
  <monospace>pytorch-widedeep</monospace> is handled by the different
  training classes. Currently, <monospace>pytorch-widedeep</monospace>
  offers the following training options: (i) “<italic>Standard</italic>”
  Supervised training, (ii) Supervised Bayesian training, and (iii)
  Self-supervised pre-training.</p>
</sec>
<sec id="contribution">
  <title>Contribution</title>
  <p><monospace>pytorch-widedeep</monospace> is being developed and used
  by many active community members. Anyone can join the discussion on
  Slack.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge the work of other researchers, engineers, and
  programmers from the following projects and libraries:</p>
  <list list-type="bullet">
    <list-item>
      <p>the <monospace>Callbacks</monospace> and
      <monospace>Initializers</monospace> structure and code is inspired
      by the torchsample library
      (<xref alt="TorchSample maintainers &amp; contributors, 2017" rid="ref-torch_sample" ref-type="bibr">TorchSample
      maintainers &amp; contributors, 2017</xref>), which in itself
      partially inspired by Keras
      (<xref alt="Chollet &amp; others, 2015" rid="ref-chollet2015keras" ref-type="bibr">Chollet
      &amp; others, 2015</xref>)</p>
    </list-item>
    <list-item>
      <p>the <monospace>TextProcessor</monospace> class in this library
      uses the fastai
      (<xref alt="J. Howard &amp; Gugger, 2020" rid="ref-info11020108" ref-type="bibr">J.
      Howard &amp; Gugger, 2020</xref>) <monospace>Tokenizer</monospace>
      and <monospace>Vocab</monospace>; the code at
      <monospace>utils.fastai_transforms</monospace> is a minor
      adaptation of their code, so it functions within this library; to
      our experience, their <monospace>Tokenizer</monospace> is the best
      in class</p>
    </list-item>
    <list-item>
      <p>the <monospace>ImageProcessor</monospace> class in this library
      uses code from the fantastic Deep Learning for Computer Vision
      (DL4CV)
      (<xref alt="Adrian, 2017" rid="ref-adrian2017deep" ref-type="bibr">Adrian,
      2017</xref>) book by Adrian Rosebrock</p>
    </list-item>
    <list-item>
      <p>we adjusted and integrated ideas of Label and Feature
      Distribution Smoothing
      (<xref alt="Y. Yang et al., 2021" rid="ref-yang2021delving" ref-type="bibr">Y.
      Yang et al., 2021</xref>)</p>
    </list-item>
    <list-item>
      <p>we adjusted and integrated ZILNloss code written in
      Tensorflow/Keras
      (<xref alt="X. Wang et al., 2019" rid="ref-wang2019deep" ref-type="bibr">X.
      Wang et al., 2019</xref>)</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-tensorflow2015-whitepaper">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Mané</surname><given-names>Dandelion</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://www.tensorflow.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-NEURIPS2019_9015">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-joseph2021pytorch">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Joseph</surname><given-names>Manu</given-names></name>
      </person-group>
      <article-title>PyTorch tabular: A framework for deep learning with tabular data</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2104.13638</uri>
    </element-citation>
  </ref>
  <ref id="ref-erickson2020autogluon">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Erickson</surname><given-names>Nick</given-names></name>
        <name><surname>Mueller</surname><given-names>Jonas</given-names></name>
        <name><surname>Shirkov</surname><given-names>Alexander</given-names></name>
        <name><surname>Zhang</surname><given-names>Hang</given-names></name>
        <name><surname>Larroy</surname><given-names>Pedro</given-names></name>
        <name><surname>Li</surname><given-names>Mu</given-names></name>
        <name><surname>Smola</surname><given-names>Alexander</given-names></name>
      </person-group>
      <article-title>AutoGluon-tabular: Robust and accurate AutoML for structured data</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2003.06505</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2003.06505</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-singh2020mmf">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Singh</surname><given-names>Amanpreet</given-names></name>
        <name><surname>Goswami</surname><given-names>Vedanuj</given-names></name>
        <name><surname>Natarajan</surname><given-names>Vivek</given-names></name>
        <name><surname>Jiang</surname><given-names>Yu</given-names></name>
        <name><surname>Chen</surname><given-names>Xinlei</given-names></name>
        <name><surname>Shah</surname><given-names>Meet</given-names></name>
        <name><surname>Rohrbach</surname><given-names>Marcus</given-names></name>
        <name><surname>Batra</surname><given-names>Dhruv</given-names></name>
        <name><surname>Parikh</surname><given-names>Devi</given-names></name>
      </person-group>
      <article-title>MMF: A multimodal framework for vision and language research</article-title>
      <publisher-name>https://github.com/facebookresearch/mmf</publisher-name>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-cheng2016wide">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Cheng</surname><given-names>Heng-Tze</given-names></name>
        <name><surname>Koc</surname><given-names>Levent</given-names></name>
        <name><surname>Harmsen</surname><given-names>Jeremiah</given-names></name>
        <name><surname>Shaked</surname><given-names>Tal</given-names></name>
        <name><surname>Chandra</surname><given-names>Tushar</given-names></name>
        <name><surname>Aradhye</surname><given-names>Hrishi</given-names></name>
        <name><surname>Anderson</surname><given-names>Glen</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg</given-names></name>
        <name><surname>Chai</surname><given-names>Wei</given-names></name>
        <name><surname>Ispir</surname><given-names>Mustafa</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Wide &amp; deep learning for recommender systems</article-title>
      <source>Proceedings of the 1st workshop on deep learning for recommender systems</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1606.07792</pub-id>
      <fpage>7</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-arik2021tabnet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Arik</surname><given-names>Sercan Ö</given-names></name>
        <name><surname>Pfister</surname><given-names>Tomas</given-names></name>
      </person-group>
      <article-title>Tabnet: Attentive interpretable tabular learning</article-title>
      <source>Proceedings of the AAAI conference on artificial intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>35</volume>
      <fpage>6679</fpage>
      <lpage>6687</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yang2016hierarchical">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Zichao</given-names></name>
        <name><surname>Yang</surname><given-names>Diyi</given-names></name>
        <name><surname>Dyer</surname><given-names>Chris</given-names></name>
        <name><surname>He</surname><given-names>Xiaodong</given-names></name>
        <name><surname>Smola</surname><given-names>Alex</given-names></name>
        <name><surname>Hovy</surname><given-names>Eduard</given-names></name>
      </person-group>
      <article-title>Hierarchical attention networks for document classification</article-title>
      <year iso-8601-date="2016-01">2016</year><month>01</month>
      <pub-id pub-id-type="doi">10.18653/v1/N16-1174</pub-id>
      <fpage>1480</fpage>
      <lpage>1489</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huang2020tabtransformer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Xin</given-names></name>
        <name><surname>Khetan</surname><given-names>Ashish</given-names></name>
        <name><surname>Cvitkovic</surname><given-names>Milan</given-names></name>
        <name><surname>Karnin</surname><given-names>Zohar</given-names></name>
      </person-group>
      <article-title>Tabtransformer: Tabular data modeling using contextual embeddings</article-title>
      <source>arXiv preprint arXiv:2012.06678</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-somepalli2021saint">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Somepalli</surname><given-names>Gowthami</given-names></name>
        <name><surname>Goldblum</surname><given-names>Micah</given-names></name>
        <name><surname>Schwarzschild</surname><given-names>Avi</given-names></name>
        <name><surname>Bruss</surname><given-names>C Bayan</given-names></name>
        <name><surname>Goldstein</surname><given-names>Tom</given-names></name>
      </person-group>
      <article-title>Saint: Improved neural networks for tabular data via row attention and contrastive pre-training</article-title>
      <source>arXiv preprint arXiv:2106.01342</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-gorishniy2021revisiting">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gorishniy</surname><given-names>Yury</given-names></name>
        <name><surname>Rubachev</surname><given-names>Ivan</given-names></name>
        <name><surname>Khrulkov</surname><given-names>Valentin</given-names></name>
        <name><surname>Babenko</surname><given-names>Artem</given-names></name>
      </person-group>
      <article-title>Revisiting deep learning models for tabular data</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
      <fpage>18932</fpage>
      <lpage>18943</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wu2021fastformer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Chuhan</given-names></name>
        <name><surname>Wu</surname><given-names>Fangzhao</given-names></name>
        <name><surname>Qi</surname><given-names>Tao</given-names></name>
        <name><surname>Huang</surname><given-names>Yongfeng</given-names></name>
        <name><surname>Xie</surname><given-names>Xing</given-names></name>
      </person-group>
      <article-title>Fastformer: Additive attention can be all you need</article-title>
      <source>arXiv preprint arXiv:2108.09084</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-jaegle2021perceiver">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Jaegle</surname><given-names>Andrew</given-names></name>
        <name><surname>Gimeno</surname><given-names>Felix</given-names></name>
        <name><surname>Brock</surname><given-names>Andy</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
        <name><surname>Carreira</surname><given-names>Joao</given-names></name>
      </person-group>
      <article-title>Perceiver: General perception with iterative attention</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <fpage>4651</fpage>
      <lpage>4664</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blundell2015weight">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Blundell</surname><given-names>Charles</given-names></name>
        <name><surname>Cornebise</surname><given-names>Julien</given-names></name>
        <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
        <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
      </person-group>
      <article-title>Weight uncertainty in neural network</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <fpage>1613</fpage>
      <lpage>1622</lpage>
    </element-citation>
  </ref>
  <ref id="ref-he2016deep">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>Kaiming</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Ren</surname><given-names>Shaoqing</given-names></name>
        <name><surname>Sun</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Deep residual learning for image recognition</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/cvpr.2016.90</pub-id>
      <fpage>770</fpage>
      <lpage>778</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhang2018shufflenet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Zhou</surname><given-names>Xinyu</given-names></name>
        <name><surname>Lin</surname><given-names>Mengxiao</given-names></name>
        <name><surname>Sun</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Shufflenet: An extremely efficient convolutional neural network for mobile devices</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2018.00716</pub-id>
      <fpage>6848</fpage>
      <lpage>6856</lpage>
    </element-citation>
  </ref>
  <ref id="ref-xie2017aggregated">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Xie</surname><given-names>Saining</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
        <name><surname>Dollár</surname><given-names>Piotr</given-names></name>
        <name><surname>Tu</surname><given-names>Zhuowen</given-names></name>
        <name><surname>He</surname><given-names>Kaiming</given-names></name>
      </person-group>
      <article-title>Aggregated residual transformations for deep neural networks</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2017.634</pub-id>
      <fpage>1492</fpage>
      <lpage>1500</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zagoruyko2016wide">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zagoruyko</surname><given-names>Sergey</given-names></name>
        <name><surname>Komodakis</surname><given-names>Nikos</given-names></name>
      </person-group>
      <article-title>Wide residual networks</article-title>
      <source>arXiv preprint arXiv:1605.07146</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.5244/c.30.87</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-xu2022regnet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Xu</surname><given-names>Jing</given-names></name>
        <name><surname>Pan</surname><given-names>Yu</given-names></name>
        <name><surname>Pan</surname><given-names>Xinglin</given-names></name>
        <name><surname>Hoi</surname><given-names>Steven</given-names></name>
        <name><surname>Yi</surname><given-names>Zhang</given-names></name>
        <name><surname>Xu</surname><given-names>Zenglin</given-names></name>
      </person-group>
      <article-title>RegNet: Self-regulated network for image classification</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/TNNLS.2022.3158966</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-huang2017densely">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Gao</given-names></name>
        <name><surname>Liu</surname><given-names>Zhuang</given-names></name>
        <name><surname>Van Der Maaten</surname><given-names>Laurens</given-names></name>
        <name><surname>Weinberger</surname><given-names>Kilian Q</given-names></name>
      </person-group>
      <article-title>Densely connected convolutional networks</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2017">2017</year>
      <fpage>4700</fpage>
      <lpage>4708</lpage>
    </element-citation>
  </ref>
  <ref id="ref-howard2017mobilenets">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howard</surname><given-names>Andrew G</given-names></name>
        <name><surname>Zhu</surname><given-names>Menglong</given-names></name>
        <name><surname>Chen</surname><given-names>Bo</given-names></name>
        <name><surname>Kalenichenko</surname><given-names>Dmitry</given-names></name>
        <name><surname>Wang</surname><given-names>Weijun</given-names></name>
        <name><surname>Weyand</surname><given-names>Tobias</given-names></name>
        <name><surname>Andreetto</surname><given-names>Marco</given-names></name>
        <name><surname>Adam</surname><given-names>Hartwig</given-names></name>
      </person-group>
      <article-title>Mobilenets: Efficient convolutional neural networks for mobile vision applications</article-title>
      <source>arXiv preprint arXiv:1704.04861</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-tan2019mnasnet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tan</surname><given-names>Mingxing</given-names></name>
        <name><surname>Chen</surname><given-names>Bo</given-names></name>
        <name><surname>Pang</surname><given-names>Ruoming</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Sandler</surname><given-names>Mark</given-names></name>
        <name><surname>Howard</surname><given-names>Andrew</given-names></name>
        <name><surname>Le</surname><given-names>Quoc V</given-names></name>
      </person-group>
      <article-title>Mnasnet: Platform-aware neural architecture search for mobile</article-title>
      <source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2019.00293</pub-id>
      <fpage>2820</fpage>
      <lpage>2828</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tan2019efficientnet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tan</surname><given-names>Mingxing</given-names></name>
        <name><surname>Le</surname><given-names>Quoc</given-names></name>
      </person-group>
      <article-title>Efficientnet: Rethinking model scaling for convolutional neural networks</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <fpage>6105</fpage>
      <lpage>6114</lpage>
    </element-citation>
  </ref>
  <ref id="ref-iandola2016squeezenet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Iandola</surname><given-names>Forrest N</given-names></name>
        <name><surname>Han</surname><given-names>Song</given-names></name>
        <name><surname>Moskewicz</surname><given-names>Matthew W</given-names></name>
        <name><surname>Ashraf</surname><given-names>Khalid</given-names></name>
        <name><surname>Dally</surname><given-names>William J</given-names></name>
        <name><surname>Keutzer</surname><given-names>Kurt</given-names></name>
      </person-group>
      <article-title>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5 MB model size</article-title>
      <source>arXiv preprint arXiv:1602.07360</source>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-wolf2019huggingface">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>Rémi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Huggingface’s transformers: State-of-the-art natural language processing</article-title>
      <source>arXiv preprint arXiv:1910.03771</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-yang2021delving">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Yuzhe</given-names></name>
        <name><surname>Zha</surname><given-names>Kaiwen</given-names></name>
        <name><surname>Chen</surname><given-names>Yingcong</given-names></name>
        <name><surname>Wang</surname><given-names>Hao</given-names></name>
        <name><surname>Katabi</surname><given-names>Dina</given-names></name>
      </person-group>
      <article-title>Delving into deep imbalanced regression</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <fpage>11842</fpage>
      <lpage>11851</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2019deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Xiaojing</given-names></name>
        <name><surname>Liu</surname><given-names>Tianqi</given-names></name>
        <name><surname>Miao</surname><given-names>Jingang</given-names></name>
      </person-group>
      <article-title>A deep probabilistic model for customer lifetime value prediction</article-title>
      <source>arXiv preprint arXiv:1912.07753</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-torchvision2016">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>maintainers</surname><given-names>TorchVision</given-names></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>TorchVision: PyTorch’s computer vision library</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/pytorch/vision; GitHub</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-torch_sample">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>maintainers</surname><given-names>TorchSample</given-names></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>TorchSample: Lightweight pytorch functions for neural network featuremap sampling</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/ncullen93/torchsample; GitHub</publisher-name>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-chollet2015keras">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Keras</article-title>
      <publisher-name>https://keras.io</publisher-name>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-info11020108">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howard</surname><given-names>Jeremy</given-names></name>
        <name><surname>Gugger</surname><given-names>Sylvain</given-names></name>
      </person-group>
      <article-title>Fastai: A layered API for deep learning</article-title>
      <source>Information</source>
      <year iso-8601-date="2020">2020</year>
      <volume>11</volume>
      <issue>2</issue>
      <issn>2078-2489</issn>
      <uri>https://www.mdpi.com/2078-2489/11/2/108</uri>
      <pub-id pub-id-type="doi">10.3390/info11020108</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-adrian2017deep">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Adrian</surname><given-names>Rosebrock</given-names></name>
      </person-group>
      <source>Deep learning for computer vision with python</source>
      <publisher-name>PyImageSearch.com</publisher-name>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-garg2022multimodality">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Garg</surname><given-names>Muskan</given-names></name>
        <name><surname>Wazarkar</surname><given-names>Seema</given-names></name>
        <name><surname>Singh</surname><given-names>Muskaan</given-names></name>
        <name><surname>Bojar</surname><given-names>Ondřej</given-names></name>
      </person-group>
      <article-title>Multimodality for NLP-centered applications: Resources, advances and frontiers</article-title>
      <source>Proceedings of the thirteenth language resources and evaluation conference</source>
      <year iso-8601-date="2022">2022</year>
      <fpage>6837</fpage>
      <lpage>6847</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2017deep">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Ruoxi</given-names></name>
        <name><surname>Fu</surname><given-names>Bin</given-names></name>
        <name><surname>Fu</surname><given-names>Gang</given-names></name>
        <name><surname>Wang</surname><given-names>Mingliang</given-names></name>
      </person-group>
      <article-title>Deep &amp; cross network for ad click predictions</article-title>
      <source>Proceedings of the ADKDD’17</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1708.05123</pub-id>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
