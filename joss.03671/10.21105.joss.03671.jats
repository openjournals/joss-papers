<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3671</article-id>
<article-id pub-id-type="doi">10.21105/joss.03671</article-id>
<title-group>
<article-title>AdaptiveResonance.jl: A Julia Implementation of Adaptive
Resonance Theory (ART) Algorithms</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-2442-8901</contrib-id>
<name>
<surname>Petrenko</surname>
<given-names>Sasha</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9726-9051</contrib-id>
<name>
<surname>II</surname>
<given-names>Donald C. Wunsch</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Missouri University of Science and Technology</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-06-01">
<day>1</day>
<month>6</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>73</issue>
<fpage>3671</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Julia</kwd>
<kwd>ART</kwd>
<kwd>Adaptive Resonance Theory</kwd>
<kwd>Machine Learning</kwd>
<kwd>Clustering</kwd>
<kwd>Neural Networks</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>AdaptiveResonance.jl is a Julia package for machine learning with
  adaptive resonance theory (ART) algorithms, written in the numerical
  computing language Julia. ART is a neurocognitive theory of how
  competitive cellular networks can learn distributed patterns without
  supervision through recurrent field connections, eliciting the
  mechanisms of perception, expectation, and recognition
  (<xref alt="Grossberg, 1980" rid="ref-Grossberg1980" ref-type="bibr">Grossberg,
  1980</xref>,
  <xref alt="2013" rid="ref-Grossberg2013" ref-type="bibr">2013</xref>).
  Engineering algorithms based upon ART are principally in the class of
  competitive, incremental, single-layer, neurogenesis clustering
  algorithms, but they have been adapted for multimodal learning in
  diverse applications.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>There exist many variations of algorithms built upon ART
  (<xref alt="Brito da Silva et al., 2019" rid="ref-DaSilva2019" ref-type="bibr">Brito
  da Silva et al., 2019</xref>). Each variation is related by utilizing
  recurrent connections of fields, driven by learning through match and
  mismatch of distributed patterns, and though they all differ in the
  details of their implementations, their algorithmic and programmatic
  requirements are often very similar. Despite the relevance and
  successes of this class of algorithms in the literature, there does
  not exist to date a unified repository of their implementations in
  Julia. Therefore, the purpose of this package is to create a unified
  framework and repository of ART algorithms in Julia.</p>
  <sec id="target-audience">
    <title>Target Audience</title>
    <p>This package is principally intended as a resource for
    researchers in machine learning and adaptive resonance theory for
    testing and developing new ART algorithms. However, implementing
    these algorithms in the Julia language brings all of the benefits of
    the Julia itself, such as the speed of a low-level language such as
    C while having the transparency of a high-level language such as
    MATLAB. Being implemented in Julia allows the package to be
    understood and expanded upon by research scientists while also being
    able to be used in resource-demanding production environments.</p>
  </sec>
  <sec id="comparison-to-existing-implementations">
    <title>Comparison to Existing Implementations</title>
    <p>There exist a myriad of open implementations of ART algorithms
    that are the result of reproducibility efforts in the ART
    literature. The Boston University Department of Cognitive and Neural
    Systems (CNS) Technology Laboratory software repository contains one
    of the largest collections of algorithms and utilities related to
    ART, principally implemented in the MATLAB and C++ programming
    languages, from demonstrations of the learning laws of ART to
    implementations of ART and ARTMAP modules
    (<xref alt="Boston University Cognitive and Neural Systems Technology Lab Software Repository, 2009 [Online]" rid="ref-CNS_Software" ref-type="bibr"><italic>Boston
    University Cognitive and Neural Systems Technology Lab Software
    Repository</italic>, 2009 [Online]</xref>). However, this repository
    serves as a codebase for the reproducibility of the software
    associated CNS papers rather than as a single unified framework for
    ART implementations.</p>
    <p>The Missouri University of Science and Technology Applied
    Computational Intelligence Laboratory (ACIL) hosts a myriad of
    individual ART algorithm implementations on its public GitHub group
    repository page
    (<xref alt="Missouri University of Science and Technology Applied Computational Intelligence Laboratory GitHub Software Repository, 2022 [Online]" rid="ref-ACIL_GitHub" ref-type="bibr"><italic>Missouri
    University of Science and Technology Applied Computational
    Intelligence Laboratory GitHub Software Repository</italic>, 2022
    [Online]</xref>), chiefly implemented in the MATLAB and Python
    programming languages. Though these ART implementations are designed
    for open use, they also principally serve the reproducibility of
    their associated ACIL papers.</p>
    <p>The ACIL group GitHub page additionally contains the NuART-Py
    library, which organizes a suite of clustering and biclustering ART
    algorithms as a distributable package in the Python language
    (<xref alt="Elnabarawy, 2019 [Online]" rid="ref-NuART-Py" ref-type="bibr">Elnabarawy,
    2019 [Online]</xref>). A similar package exists in the Java
    programming language in a separate repository containing only
    fundamental ART algorithms
    (<xref alt="Chen, 2018 [Online]" rid="ref-JavaART" ref-type="bibr">Chen,
    2018 [Online]</xref>), and a new package in the R statistical
    programming language has only begun development at the time of this
    writing
    (<xref alt="Steinmeister &amp; Wunsch, 2021" rid="ref-R_FuzzyART" ref-type="bibr">Steinmeister
    &amp; Wunsch, 2021</xref>).</p>
    <p>Though each of these ART software projects (and the very many and
    disparate implementations of individual algorithms in the
    literature) combined may implement the majority of ART algorithms
    relevant to modern research and engineering, together they lack
    cohesion in programming language and usage. When considering ease of
    use and barrier to entry, many of these projects may be difficult to
    utilize for those less versed in the ART literature who might still
    significantly benefit from their use and understanding.</p>
    <p>Lastly, many ART implementations exist in the MATLAB programming
    language due to its popularity amongst the research scientists that
    have been the theory’s primary clientele, which is at the detriment
    to those without private MATLAB licenses in research and industry.
    The Julia programming language is selected for this open-source ART
    package implementation due to its syntactic ease of use and speed of
    development without compromising computational efficiency due to the
    language’s just-in-time compilation.</p>
  </sec>
</sec>
<sec id="adaptive-resonance-theory">
  <title>Adaptive Resonance Theory</title>
  <p>ART is originally a theory of how competitive fields of neurons
  interact to form stable representations without supervision, and ART
  algorithms draw from this theory as a biological inspiration for their
  design. It is not strictly necessary to have an understanding of the
  theory to understand the use of the algorithms, but they share a
  common nomenclature that makes knowledge of the former useful for the
  latter.</p>
  <sec id="theory">
    <title>Theory</title>
    <p>Adaptive resonance theory is a collection of neurological studies
    from the neuron level to the network level
    (<xref alt="Hestenes, 1987" rid="ref-ARTHestenes1987" ref-type="bibr">Hestenes,
    1987</xref>). ART begins with a set of neural field differential
    equations and procedurally tackles problems such as why sigmoidal
    neural activations are used, the conditions of stability for
    competitive neural networks
    (<xref alt="Cohen &amp; Grossberg, 1983" rid="ref-Cohen1983a" ref-type="bibr">Cohen
    &amp; Grossberg, 1983</xref>), how the mammalian visual system works
    (<xref alt="Grossberg &amp; Huang, 2009" rid="ref-Grossberg2009" ref-type="bibr">Grossberg
    &amp; Huang, 2009</xref>), and the hard problem of consciousness
    linking resonant states to conscious experiences
    (<xref alt="Grossberg, 2017" rid="ref-Grossberg2017" ref-type="bibr">Grossberg,
    2017</xref>,
    <xref alt="2021" rid="ref-grossberg2021conscious" ref-type="bibr">2021</xref>).</p>
  </sec>
  <sec id="algorithms">
    <title>Algorithms</title>
    <p>ART algorithms are generally characterized in behavior by the
    following:</p>
    <list list-type="order">
      <list-item>
        <p>They are inherently <italic>unsupervised</italic> learning
        algorithms at their core, but they have been adapted to
        supervised and reinforcement learning paradigms with frameworks
        such as ARTMAP
        (<xref alt="Carpenter et al., 1991" rid="ref-Carpenter1991" ref-type="bibr">Carpenter
        et al., 1991</xref>,
        <xref alt="1992" rid="ref-Carpenter1992" ref-type="bibr">1992</xref>)
        and FALCON
        (<xref alt="Tan et al., 2019" rid="ref-Tan2019" ref-type="bibr">Tan
        et al., 2019</xref>), respectively.</p>
      </list-item>
      <list-item>
        <p>They are <italic>incremental</italic> learning algorithms,
        adjusting their weights or creating new ones at every sample
        presentation.</p>
      </list-item>
      <list-item>
        <p>They are <italic>neurogenesis</italic> neural networks,
        representing their learning by the modification of existing
        prototype weights or instantiating new ones entirely.</p>
      </list-item>
      <list-item>
        <p>They belong to the class of <italic>competitive</italic>
        neural networks, which compute their outputs with more complex
        dynamics than feedforward activation.</p>
      </list-item>
    </list>
    <p>Because of the breadth of the original theory and the variety of
    possible applications, ART-based algorithms are diverse in their
    nomenclature and implementation details. Nevertheless, they are
    generally structured as follows:</p>
    <list list-type="order">
      <list-item>
        <p>ART models typically have two layers/fields denoted F1 and
        F2.</p>
      </list-item>
      <list-item>
        <p>The F1 field is the feature representation field. Most often,
        it is simply the input feature sample itself (after some
        requisite feature preprocessing, depending on the model).</p>
      </list-item>
      <list-item>
        <p>The F2 field is the category representation field. With some
        exceptions, each node in the F2 field represents its own
        category. This is most easily interpreted as a weight vector
        representing a prototype for a class or centroid of a
        cluster.</p>
      </list-item>
      <list-item>
        <p>An activation function is used to find the order of
        categories “most activated” for a given sample in F1.</p>
      </list-item>
      <list-item>
        <p>In order of highest activation, a match function is used to
        compute the agreement between the sample and the categories.</p>
      </list-item>
      <list-item>
        <p>If the match function for a category evaluates to a value
        above a threshold known as the vigilance parameter
        (<inline-formula><alternatives>
        <tex-math><![CDATA[\rho]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ρ</mml:mi></mml:math></alternatives></inline-formula>),
        the weights of that category may be updated according to a
        learning rule.</p>
      </list-item>
      <list-item>
        <p>If there is a complete mismatch across all categories, then a
        new category is created according to an instantiation rule.</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <p>In creating a unified framework for ART modules in Julia, the
  development of this package faces the challenges of organizing and
  categorizing the designs and objectives of many different ART
  algorithms, which necessitates the formalization of the distinctions
  between training versus inference, batch versus incremental learning,
  supervised versus unsupervised learning modes, and match versus
  mismatch.</p>
  <sec id="training-vs-inference">
    <title>Training vs Inference</title>
    <p>All modules in the package have states that are tracked and
    updated during learning, and so they have their own module
    constructors with options that are themselves also stateful. The two
    most simple operations available on these ART modules are
    <monospace>train!</monospace> and <monospace>classify</monospace>
    for training and inference, respectively. This package utilizes the
    Julia convention of appending an exclamation point to the end of
    functions that modify their parameters. During training, ART modules
    are allowed to mutate their internal parameters, whereas during
    inference, they report their categorization of the data without
    allowing parameters to change.</p>
  </sec>
  <sec id="batch-vs-incremental-learning">
    <title>Batch vs Incremental Learning</title>
    <p>ART modules are generally incremental learning algorithms,
    meaning that they update their parameters or conduct inference on
    one data sample at a time rather than in large batches. If many
    samples are presented at once, batch learning is still done by
    incrementally learning upon all provided samples. This package,
    however, accommodates batch learning without the need to implement
    multiple methods by utilizing Julia’s multiple dispatch system,
    correctly inferring which function to use by the dimensionality of
    the input samples. As done in many other machine learning methods, a
    single sample is denoted by a vector of features, while a set of
    samples is a matrix of many features.</p>
  </sec>
  <sec id="supervised-vs-unsupervised-learning">
    <title>Supervised vs Unsupervised Learning</title>
    <p>Though ART modules are generally multimodal machine learning
    algorithms in that they may be designed to learn with or without
    prescribed labels (i.e., supervised or unsupervised), algorithms in
    the ARTMAP family are expressly supervised. To accommodate this
    distinction, this package organizes algorithms that are by default
    unsupervised but that can accept optional labels as ART modules
    while distinguishing explicitly supervised modules as ARTMAP
    modules. This distinction is enforced programmatically by making
    labels an optional argument in <monospace>train!</monospace>
    declarations upon ART modules and a required positional argument in
    <monospace>train!</monospace> declarations upon ARTMAP modules.</p>
  </sec>
  <sec id="match-vs-mismatch">
    <title>Match vs Mismatch</title>
    <p>A match function is used in ART and ARTMAP modules to evaluate if
    a given sample sufficiently aligns with a particular category for
    the weight to be mutated during learning or for the category to be
    reported during inference. Mismatch occurs when this match function
    evaluates to below the vigilance threshold for all internal
    categories. Complete mismatch during learning triggers the creation
    of a new category. Mismatch during inference, on the other hand,
    results in the module reporting an unknown category signal, which is
    the default behavior for all modules by precedent in the
    literature.</p>
    <p>It is sometimes desirable to report a next-best category in the
    case of complete mismatch, which is referred to as the best matching
    unit. All modules are equipped with an option to report the best
    matching unit in the case of mismatch.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This package is developed and maintained with sponsorship by the
  Applied Computational Intelligence Laboratory (ACIL) of the Missouri
  University of Science and Technology. This project is supported by
  grants from the Army Research Labs Night Vision Electronic Sensors
  Directorate (NVESD), the DARPA Lifelong Learning Machines (L2M)
  program, Teledyne Technologies, and the National Science Foundation.
  The material, findings, and conclusions here do not necessarily
  reflect the views of these entities.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-CNS_Software">
    <element-citation>
      <article-title>Boston university cognitive and neural systems technology lab software repository</article-title>
      <uri>http://techlab.bu.edu/resources/software/C51/index.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-ACIL_GitHub">
    <element-citation>
      <article-title>Missouri university of science and technology applied computational intelligence laboratory GitHub software repository</article-title>
      <uri>https://github.com/ACIL-Group</uri>
    </element-citation>
  </ref>
  <ref id="ref-NuART-Py">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Elnabarawy</surname><given-names>Islam</given-names></name>
      </person-group>
      <article-title>NuART-py: A python library of adaptive theory neural networks</article-title>
      <uri>https://github.com/ACIL-Group/NuART-Py</uri>
    </element-citation>
  </ref>
  <ref id="ref-JavaART">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Xianshun</given-names></name>
      </person-group>
      <article-title>Java-adaptive-resonance-theory</article-title>
      <uri>https://github.com/chen0040/java-adaptive-resonance-theory</uri>
    </element-citation>
  </ref>
  <ref id="ref-R_FuzzyART">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Steinmeister</surname><given-names>Louis</given-names></name>
        <name><surname>Wunsch</surname><given-names>Donald C</given-names></name>
      </person-group>
      <article-title>FuzzyART: An r package for ART-based clustering FuzzyART: An r package for ART-based clustering.</article-title>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.13140/RG.2.2.11823.25761</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Grossberg2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>Adaptive resonance theory: How a brain learns to consciously attend, learn, and recognize a changing world</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier Ltd</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>37</volume>
      <issn>08936080</issn>
      <uri>http://dx.doi.org/10.1016/j.neunet.2012.09.017</uri>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2012.09.017</pub-id>
      <pub-id pub-id-type="pmid">23149242</pub-id>
      <fpage>1</fpage>
      <lpage>47</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Grossberg1980">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>How does a brain build a cognitive code?</article-title>
      <source>Psychological Review</source>
      <year iso-8601-date="1980-01">1980</year><month>01</month>
      <volume>87</volume>
      <issue>1</issue>
      <issn>0033295X</issn>
      <pub-id pub-id-type="doi">10.1037/0033-295X.87.1.1</pub-id>
      <fpage>1</fpage>
      <lpage>51</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DaSilva2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brito da Silva</surname><given-names>Leonardo Enzo</given-names></name>
        <name><surname>Elnabarawy</surname><given-names>Islam</given-names></name>
        <name><surname>Wunsch</surname><given-names>Donald C.</given-names></name>
      </person-group>
      <article-title>A survey of adaptive resonance theory neural network models for engineering applications</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier Ltd</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>120</volume>
      <issn>0893-6080</issn>
      <uri>https://doi.org/10.1016/j.neunet.2019.09.012 http://arxiv.org/abs/1905.11437</uri>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2019.09.012</pub-id>
      <fpage>167</fpage>
      <lpage>203</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Carpenter1991">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Carpenter</surname><given-names>Gail A.</given-names></name>
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
        <name><surname>Reynolds</surname><given-names>John H.</given-names></name>
      </person-group>
      <article-title>ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network</article-title>
      <source>IEEE conference on neural networks for ocean engineering</source>
      <year iso-8601-date="1991">1991</year>
      <isbn>0780302052</isbn>
      <issn>08936080</issn>
      <pub-id pub-id-type="doi">10.1016/0893-6080(91)90012-T</pub-id>
      <fpage>341</fpage>
      <lpage>342</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Carpenter1992">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carpenter</surname><given-names>Gail A.</given-names></name>
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
        <name><surname>Markuzon</surname><given-names>Natalya</given-names></name>
        <name><surname>Reynolds</surname><given-names>John H.</given-names></name>
        <name><surname>Rosen</surname><given-names>David B.</given-names></name>
      </person-group>
      <article-title>Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps</article-title>
      <source>IEEE Transactions on Neural Networks</source>
      <year iso-8601-date="1992">1992</year>
      <volume>3</volume>
      <issue>5</issue>
      <issn>19410093</issn>
      <pub-id pub-id-type="doi">10.1109/72.159059</pub-id>
      <fpage>698</fpage>
      <lpage>713</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ARTHestenes1987">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hestenes</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>How the brain works: The next great scientific revolution</article-title>
      <source>Maximum-entropy and bayesian spectral analysis and estimation problems</source>
      <publisher-name>Springer Netherlands</publisher-name>
      <year iso-8601-date="1987">1987</year>
      <pub-id pub-id-type="doi">10.1007/978-94-009-3961-5_11</pub-id>
      <fpage>173</fpage>
      <lpage>205</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Grossberg2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>Towards solving the hard problem of consciousness: The varieties of brain resonances and the conscious experiences that they support</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier Ltd</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>87</volume>
      <issn>18792782</issn>
      <uri>http://dx.doi.org/10.1016/j.neunet.2016.11.003</uri>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2016.11.003</pub-id>
      <pub-id pub-id-type="pmid">28088645</pub-id>
      <fpage>38</fpage>
      <lpage>95</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cohen1983a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cohen</surname><given-names>Michael A.</given-names></name>
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>Absolute stability of global pattern formation and parallel memory storage by competitive neural networks</article-title>
      <source>IEEE Transactions on Systems, Man and Cybernetics</source>
      <year iso-8601-date="1983">1983</year>
      <volume>SMC-13</volume>
      <issue>5</issue>
      <issn>21682909</issn>
      <pub-id pub-id-type="doi">10.1109/TSMC.1983.6313075</pub-id>
      <fpage>815</fpage>
      <lpage>826</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Grossberg2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
        <name><surname>Huang</surname><given-names>Tsung Ren</given-names></name>
      </person-group>
      <article-title>ARTSCENE: A neural system for natural scene classification</article-title>
      <source>Journal of Vision</source>
      <year iso-8601-date="2009">2009</year>
      <volume>9</volume>
      <issue>4</issue>
      <issn>15347362</issn>
      <pub-id pub-id-type="doi">10.1167/9.4.6</pub-id>
      <pub-id pub-id-type="pmid">19757915</pub-id>
      <fpage>1</fpage>
      <lpage>19</lpage>
    </element-citation>
  </ref>
  <ref id="ref-grossberg2021conscious">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Grossberg</surname><given-names>Stephen</given-names></name>
      </person-group>
      <source>Conscious mind, resonant brain: How each brain makes a mind</source>
      <publisher-name>OUP Premium Oxford University Press</publisher-name>
      <publisher-loc>Oxford, England</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <isbn>978-0190070557</isbn>
    </element-citation>
  </ref>
  <ref id="ref-Tan2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tan</surname><given-names>Ah-Hwee Hwee</given-names></name>
        <name><surname>Subagdja</surname><given-names>Budhitama</given-names></name>
        <name><surname>Wang</surname><given-names>Di</given-names></name>
        <name><surname>Meng</surname><given-names>Lei</given-names></name>
      </person-group>
      <article-title>Self-organizing neural networks for universal learning and multimodal memory encoding</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier Ltd</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>120</volume>
      <issn>08936080</issn>
      <uri>https://doi.org/10.1016/j.neunet.2019.08.020</uri>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2019.08.020</pub-id>
      <fpage>58</fpage>
      <lpage>73</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
