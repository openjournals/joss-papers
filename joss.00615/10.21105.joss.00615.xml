<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Optim: A mathematical optimization package for Julia</title>
    <authors>
      <author>
        <name>Patrick K Mogensen</name>
        <orcid>0000-0002-4910-1932</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
      <author>
        <name>Asbjørn N Riseth</name>
        <orcid>0000-0002-5861-7885</orcid>
        <affiliation>
          <orgname>
            2
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>Optimization</tag>
      <tag>Julia</tag>
    </tags>
    <date>7 March 2018</date>
    <paper_doi>10.21105/joss.00615</paper_doi>
    <software_repository>https://github.com/JuliaNLSolvers/Optim.jl</software_repository>
    <software_archive></software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00615/10.21105.joss.00615.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p><a href="https://github.com/JuliaNLSolvers/Optim.jl/">Optim</a> provides a range of optimization capabilities written in the Julia programming language <span class="citation" data-cites="bezanson2017julia">(Bezanson et al. 2017)</span>. Our aim is to enable researchers, users, and other Julia packages to solve optimization problems without writing such algorithms themselves. The package supports optimization on manifolds, functions of complex numbers, and input types such as arbitrary precision vectors and matrices. We have implemented routines for derivative free, first-order, and second-order optimization methods. The user can provide derivatives themselves, or request that they are calculated using automatic differentiation or finite difference methods. The main focus of the package has currently been on unconstrained optimization, however, box-constrained optimization is supported, and a more comprehensive support for constraints is underway.</p>
    <p>Similar to Optim, the C library <a href="http://ab-initio.mit.edu/nlopt">NLopt</a> <span class="citation" data-cites="johnson2018nlopt">(Johnson 2008)</span> contains a collection of nonlinear optimization routines. In Python, <a href="https://docs.scipy.org/doc/scipy/reference/optimize.html">scipy.optimize</a> supports many of the same algorithms as Optim does, and <a href="https://pymanopt.github.io/">Pymanopt</a> <span class="citation" data-cites="townsend2016pymanopt">(Townsend, Niklas, and Weichwald 2016)</span> is a toolbox for manifold optimization. Within the Julia community, the packages <a href="https://github.com/robertfeldt/BlackBoxOptim.jl">BlackBoxOptim.jl</a> and <a href="https://github.com/JuliaSmoothOptimizers/Optimize.jl">Optimize.jl</a> provide optimization capabilities focusing on derivative-free and large-scale smooth problems respectively. The packages <a href="https://github.com/JuliaOpt/Convex.jl">Convex.jl</a> and <a href="https://github.com/JuliaOpt/JuMP.jl">JuMP.jl</a> <span class="citation" data-cites="dunning2017jump">(Dunning, Huchette, and Lubin 2017)</span> define modelling languages for which users can formulate optimization problems. In contrast to the previously mentioned optimization codes, Convex and JuMP work as abstraction layers between the user and solvers from a other packages.</p>
    <h2 id="optimization-routines">Optimization routines</h2>
    <p>As of version 0.14, the following optimization routines are available.</p>
    <ul>
    <li>Second-order methods
    <ul>
    <li>Newton</li>
    <li>Newton with trust region</li>
    <li>Hessian-vector with trust region</li>
    </ul></li>
    <li>First-order methods
    <ul>
    <li>BFGS</li>
    <li>L-BFGS (with linear preconditioning)</li>
    <li>Conjugate gradient (with linear preconditioning)</li>
    <li>Gradient descent (with linear preconditioning)</li>
    </ul></li>
    <li>Acceleration methods
    <ul>
    <li>Nonlinear GMRES</li>
    <li>Objective acceleration</li>
    </ul></li>
    <li>Derivative-free methods
    <ul>
    <li>Nelder–Mead</li>
    <li>Simulated annealing</li>
    <li>Particle swarm</li>
    </ul></li>
    <li>Interval bound univariate methods
    <ul>
    <li>Brent’s method</li>
    <li>Golden-section search</li>
    </ul></li>
    </ul>
    <p>The derivative based methods use line searches to assist convergence. Multiple line search algorithms are available, including interpolating backtracking and methods that aim to satisfy the Wolfe conditions.</p>
    <h1 id="usage-in-research-and-industry">Usage in research and industry</h1>
    <p>The optimization routines in this package have been used in both industrial and academic contexts. For example, parts of the internal work in the company Ternary Intelligence Inc. <span class="citation" data-cites="ternary2017">(Paramonov 2017)</span> rely on the package. Notably, an upcoming book on optimization <span class="citation" data-cites="mykel2018optimization">(Kochenderfer and Wheeler Forthcoming, 2018)</span> uses Optim for its examples. Optim has been used for a wide range of applications in academic research, including optimal control <span class="citation" data-cites="riseth2017comparison riseth2017dynamic">(Riseth, Dewynne, and Farmer 2017; Riseth 2017a)</span>, parameter estimation <span class="citation" data-cites="riseth2017operator rackauckas2017differentialequations dony2018parametric">(Riseth and Taylor-King 2017; Rackauckas and Nie 2017; and Dony, He, and Stumpf 2018)</span>, quantum physics <span class="citation" data-cites="damle2018variational">(Damle, Levitt, and Lin 2018)</span>, crystalline modelling <span class="citation" data-cites="chen2017qm braun2017effect">(Chen and Ortner 2017; Braun, Buze, and Ortner 2017)</span>, and the large-scale astronomical cataloguing project Celeste <span class="citation" data-cites="regier2015celeste regier2016celeste">(Regier et al. 2015; Regier et al. 2016)</span>. A new acceleration scheme for optimization <span class="citation" data-cites="riseth2017objective">(Riseth 2017b)</span>, and a preconditioning scheme for geometry optimisation <span class="citation" data-cites="packwood2016universal">(Packwood et al. 2016)</span> have also been tested within the Optim framework.</p>
    <h1 id="acknowledgements">Acknowledgements</h1>
    <p>John Myles White initiated the development of the Optim code base in 2012. We owe much to him and Timothy Holy for creating a solid package for optimization that the rest of the Julia community could further improve upon. We would also like to thank everyone who has contributed with code and discussions to help improve the package. In particular, Antoine Levitt, Christoph Ortner, and Chris Rackauckas have been helpful in providing suggestions and code contributions towards more modularity and greater support for non-trivial inputs and decision spaces.</p>
    <h1 id="funding">Funding</h1>
    <p>Asbjørn Riseth is partially supported by the EPSRC research grant EP/L015803/1.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-bezanson2017julia">
    <p>Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B Shah. 2017. “Julia: A Fresh Approach to Numerical Computing.” <em>SIAM Review</em> 59 (1). SIAM:65–98. <a href="https://doi.org/10.1137/141000671" class="uri">https://doi.org/10.1137/141000671</a>.</p>
    </div>
    <div id="ref-braun2017effect">
    <p>Braun, Julian, Maciej Buze, and Christoph Ortner. 2017. “The Effect of Crystal Symmetries on the Locality of Screw Dislocation Cores.” <em>arXiv Preprint arXiv:1710.07708</em>.</p>
    </div>
    <div id="ref-chen2017qm">
    <p>Chen, Huajie, and Christoph Ortner. 2017. “QM/Mm Methods for Crystalline Defects. Part 2: Consistent Energy and Force-Mixing.” <em>Multiscale Modeling &amp; Simulation</em> 15 (1). SIAM:184–214. <a href="https://doi.org/10.1137/15M1041250" class="uri">https://doi.org/10.1137/15M1041250</a>.</p>
    </div>
    <div id="ref-damle2018variational">
    <p>Damle, A., A. Levitt, and L. Lin. 2018. “Variational formulation for Wannier functions with entangled band structure.” <em>ArXiv E-Prints</em>, January.</p>
    </div>
    <div id="ref-dony2018parametric">
    <p>Dony, Leander, Fei He, and Michael Stumpf. 2018. “Parametric and Non-Parametric Gradient Matching for Network Inference.” <em>bioRxiv</em>. Cold Spring Harbor Laboratory. <a href="https://doi.org/10.1101/254003" class="uri">https://doi.org/10.1101/254003</a>.</p>
    </div>
    <div id="ref-dunning2017jump">
    <p>Dunning, Iain, Joey Huchette, and Miles Lubin. 2017. “JuMP: A Modeling Language for Mathematical Optimization.” <em>SIAM Review</em> 59 (2). SIAM:295–320. <a href="https://doi.org/10.1137/15M1020575" class="uri">https://doi.org/10.1137/15M1020575</a>.</p>
    </div>
    <div id="ref-johnson2018nlopt">
    <p>Johnson, Steven G. 2008. “The Nlopt Nonlinear-Optimization Package.” <a href="http://ab-initio.mit.edu/nlopt" class="uri">http://ab-initio.mit.edu/nlopt</a>.</p>
    </div>
    <div id="ref-mykel2018optimization">
    <p>Kochenderfer, Mykel J., and Tim A. Wheeler. Forthcoming, 2018. <em>Algorithms for Optimization</em>. MIT Press.</p>
    </div>
    <div id="ref-packwood2016universal">
    <p>Packwood, David, James Kermode, Letif Mones, Noam Bernstein, John Woolley, Nicholas Gould, Christoph Ortner, and Gábor Csányi. 2016. “A Universal Preconditioner for Simulating Condensed Phase Materials.” <em>The Journal of Chemical Physics</em> 144 (16). AIP Publishing:164109. <a href="https://doi.org/10.1063/1.4947024" class="uri">https://doi.org/10.1063/1.4947024</a>.</p>
    </div>
    <div id="ref-ternary2017">
    <p>Paramonov, Pavel. 2017. Private communication, Chief Science and Technology Officer; Ternary Intelligence Inc.</p>
    </div>
    <div id="ref-rackauckas2017differentialequations">
    <p>Rackauckas, Christopher, and Qing Nie. 2017. “Differentialequations.jl – a Performant and Feature-Rich Ecosystem for Solving Differential Equations in Julia.” <em>Journal of Open Research Software</em> 5 (1). Ubiquity Press. <a href="https://doi.org/10.5334/jors.151" class="uri">https://doi.org/10.5334/jors.151</a>.</p>
    </div>
    <div id="ref-regier2016celeste">
    <p>Regier, J., K. Pamnany, R. Giordano, R. Thomas, D. Schlegel, J. McAuliffe, and Prabhat. 2016. “Learning an Astronomical Catalog of the Visible Universe through Scalable Bayesian Inference.” <em>ArXiv E-Prints</em>, November.</p>
    </div>
    <div id="ref-regier2015celeste">
    <p>Regier, Jeffrey, Andrew Miller, Jon McAuliffe, Ryan Adams, Matt Hoffman, Dustin Lang, David Schlegel, and Mr Prabhat. 2015. “Celeste: Variational Inference for a Generative Model of Astronomical Images.” In <em>International Conference on Machine Learning</em>, 2095–2103.</p>
    </div>
    <div id="ref-riseth2017dynamic">
    <p>Riseth, A. N. 2017a. “Dynamic pricing in retail with diffusion process demand.” <em>ArXiv E-Prints</em>, September.</p>
    </div>
    <div id="ref-riseth2017objective">
    <p>———. 2017b. “Objective acceleration for unconstrained optimization.” <em>ArXiv E-Prints</em>, October.</p>
    </div>
    <div id="ref-riseth2017operator">
    <p>Riseth, A. N., and J. P. Taylor-King. 2017. “Operator Fitting for Parameter Estimation of Stochastic Differential Equations.” <em>ArXiv E-Prints</em>, September.</p>
    </div>
    <div id="ref-riseth2017comparison">
    <p>Riseth, A. N., J. N. Dewynne, and C. L. Farmer. 2017. “A comparison of control strategies applied to a pricing problem in retail.” <em>ArXiv E-Prints</em>, October.</p>
    </div>
    <div id="ref-townsend2016pymanopt">
    <p>Townsend, James, Koep Niklas, and Sebastian Weichwald. 2016. “Pymanopt: A Python Toolbox for Optimization on Manifolds Using Automatic Differentiation.” <em>Journal of Machine Learning Research</em> 17 (137):1–5. <a href="http://jmlr.org/papers/v17/16-177.html" class="uri">http://jmlr.org/papers/v17/16-177.html</a>.</p>
    </div>
    </div>
  </body>
</article>
