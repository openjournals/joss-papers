<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20240405T201059-ef8e7f823fcb9feb7f7ef7b485702f34cf3b59c9</doi_batch_id>
    <timestamp>20240405201059</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>04</month>
          <year>2024</year>
        </publication_date>
        <journal_volume>
          <volume>9</volume>
        </journal_volume>
        <issue>96</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>LINFA: a Python library for variational inference with
normalizing flow and annealing</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Yu</given_name>
            <surname>Wang</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Emma R.</given_name>
            <surname>Cobian</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Jubilee</given_name>
            <surname>Lee</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Fang</given_name>
            <surname>Liu</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Jonathan D.</given_name>
            <surname>Hauenstein</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Daniele E.</given_name>
            <surname>Schiavazzi</surname>
          </person_name>
        </contributors>
        <publication_date>
          <month>04</month>
          <day>05</day>
          <year>2024</year>
        </publication_date>
        <pages>
          <first_page>6309</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.06309</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.10883597</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/6309</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.06309</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.06309</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.06309.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="geman1984stochastic">
            <article_title>Stochastic relaxation, Gibbs distributions,
and the Bayesian restoration of images</article_title>
            <author>Geman</author>
            <journal_title>IEEE Transactions on pattern analysis and
machine intelligence</journal_title>
            <issue>6</issue>
            <doi>10.1109/TPAMI.1984.4767596</doi>
            <cYear>1984</cYear>
            <unstructured_citation>Geman, S., &amp; Geman, D. (1984).
Stochastic relaxation, Gibbs distributions, and the Bayesian restoration
of images. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 6, 721–741.
https://doi.org/10.1109/TPAMI.1984.4767596</unstructured_citation>
          </citation>
          <citation key="metropolis1953equation">
            <article_title>Equation of state calculations by fast
computing machines</article_title>
            <author>Metropolis</author>
            <journal_title>The journal of chemical
physics</journal_title>
            <issue>6</issue>
            <volume>21</volume>
            <doi>10.1063/1.1699114</doi>
            <cYear>1953</cYear>
            <unstructured_citation>Metropolis, N., Rosenbluth, A. W.,
Rosenbluth, M. N., Teller, A. H., &amp; Teller, E. (1953). Equation of
state calculations by fast computing machines. The Journal of Chemical
Physics, 21(6), 1087–1092.
https://doi.org/10.1063/1.1699114</unstructured_citation>
          </citation>
          <citation key="hastings1970monte">
            <article_title>Monte Carlo sampling methods using Markov
chains and their applications</article_title>
            <author>Hastings</author>
            <doi>10.1093/biomet/57.1.97</doi>
            <cYear>1970</cYear>
            <unstructured_citation>Hastings, W. K. (1970). Monte Carlo
sampling methods using Markov chains and their applications.
https://doi.org/10.1093/biomet/57.1.97</unstructured_citation>
          </citation>
          <citation key="gelfand1990sampling">
            <article_title>Sampling-based approaches to calculating
marginal densities</article_title>
            <author>Gelfand</author>
            <journal_title>Journal of the American statistical
association</journal_title>
            <issue>410</issue>
            <volume>85</volume>
            <doi>10.1080/01621459.1990.10476213</doi>
            <cYear>1990</cYear>
            <unstructured_citation>Gelfand, A. E., &amp; Smith, A. F.
(1990). Sampling-based approaches to calculating marginal densities.
Journal of the American Statistical Association, 85(410), 398–409.
https://doi.org/10.1080/01621459.1990.10476213</unstructured_citation>
          </citation>
          <citation key="wainwright2008graphical">
            <article_title>Graphical models, exponential families, and
variational inference</article_title>
            <author>Wainwright</author>
            <journal_title>Foundations and Trends in Machine
Learning</journal_title>
            <issue>1–2</issue>
            <volume>1</volume>
            <doi>10.1561/2200000001</doi>
            <cYear>2008</cYear>
            <unstructured_citation>Wainwright, M. J., Jordan, M. I.,
&amp; others. (2008). Graphical models, exponential families, and
variational inference. Foundations and Trends in Machine Learning,
1(1–2), 1–305.
https://doi.org/10.1561/2200000001</unstructured_citation>
          </citation>
          <citation key="villani2009optimal">
            <volume_title>Optimal transport: Old and new</volume_title>
            <author>Villani</author>
            <volume>338</volume>
            <doi>10.1007/978-3-540-71050-9</doi>
            <cYear>2009</cYear>
            <unstructured_citation>Villani, C., &amp; others. (2009).
Optimal transport: Old and new (Vol. 338). Springer.
https://doi.org/10.1007/978-3-540-71050-9</unstructured_citation>
          </citation>
          <citation key="kobyzev2020normalizing">
            <article_title>Normalizing flows: An introduction and review
of current methods</article_title>
            <author>Kobyzev</author>
            <journal_title>IEEE transactions on pattern analysis and
machine intelligence</journal_title>
            <issue>11</issue>
            <volume>43</volume>
            <doi>10.1109/TPAMI.2020.2992934</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Kobyzev, I., Prince, S. J., &amp;
Brubaker, M. A. (2020). Normalizing flows: An introduction and review of
current methods. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 43(11), 3964–3979.
https://doi.org/10.1109/TPAMI.2020.2992934</unstructured_citation>
          </citation>
          <citation key="papamakarios2021normalizing">
            <article_title>Normalizing flows for probabilistic modeling
and inference</article_title>
            <author>Papamakarios</author>
            <journal_title>The Journal of Machine Learning
Research</journal_title>
            <issue>1</issue>
            <volume>22</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Papamakarios, G., Nalisnick, E.,
Rezende, D. J., Mohamed, S., &amp; Lakshminarayanan, B. (2021).
Normalizing flows for probabilistic modeling and inference. The Journal
of Machine Learning Research, 22(1), 2617–2680.</unstructured_citation>
          </citation>
          <citation key="rezende2015variational">
            <article_title>Variational inference with normalizing
flows</article_title>
            <author>Rezende</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Rezende, D., &amp; Mohamed, S.
(2015). Variational inference with normalizing flows. International
Conference on Machine Learning, 1530–1538.</unstructured_citation>
          </citation>
          <citation key="wang2022variational">
            <article_title>Variational inference with NoFAS: Normalizing
flow with adaptive surrogate for computationally expensive
models</article_title>
            <author>Wang</author>
            <journal_title>Journal of Computational
Physics</journal_title>
            <volume>467</volume>
            <doi>10.1016/j.jcp.2022.111454</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Wang, Y., Liu, F., &amp; Schiavazzi,
D. E. (2022). Variational inference with NoFAS: Normalizing flow with
adaptive surrogate for computationally expensive models. Journal of
Computational Physics, 467, 111454.
https://doi.org/10.1016/j.jcp.2022.111454</unstructured_citation>
          </citation>
          <citation key="cobian2023adaann">
            <article_title>AdaAnn: Adaptive annealing scheduler for
probability density approximation</article_title>
            <author>Cobian</author>
            <journal_title>International Journal for Uncertainty
Quantification</journal_title>
            <volume>13</volume>
            <doi>10.1615/Int.J.UncertaintyQuantification.2022043110</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Cobian, E. R., Hauenstein, J. D.,
Liu, F., &amp; Schiavazzi, D. E. (2023). AdaAnn: Adaptive annealing
scheduler for probability density approximation. International Journal
for Uncertainty Quantification, 13.
https://doi.org/10.1615/Int.J.UncertaintyQuantification.2022043110</unstructured_citation>
          </citation>
          <citation key="dinh2016density">
            <article_title>Density estimation using real
NVP</article_title>
            <author>Dinh</author>
            <journal_title>arXiv preprint
arXiv:1605.08803</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Dinh, L., Sohl-Dickstein, J., &amp;
Bengio, S. (2016). Density estimation using real NVP. arXiv Preprint
arXiv:1605.08803.</unstructured_citation>
          </citation>
          <citation key="kingma2018glow">
            <article_title>Glow: Generative flow with invertible 1x1
convolutions</article_title>
            <author>Kingma</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>31</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Kingma, D. P., &amp; Dhariwal, P.
(2018). Glow: Generative flow with invertible 1x1 convolutions. Advances
in Neural Information Processing Systems, 31.</unstructured_citation>
          </citation>
          <citation key="papamakarios2018masked">
            <article_title>Masked autoregressive flow for density
estimation</article_title>
            <author>Papamakarios</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>30</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Papamakarios, G., Pavlakou, T., &amp;
Murray, I. (2017). Masked autoregressive flow for density estimation. In
I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S.
Vishwanathan, &amp; R. Garnett (Eds.), Advances in neural information
processing systems (Vol. 30). Curran Associates, Inc.
https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf</unstructured_citation>
          </citation>
          <citation key="kingma2016improved">
            <article_title>Improved variational inference with inverse
autoregressive flow</article_title>
            <author>Kingma</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>29</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Kingma, D. P., Salimans, T.,
Jozefowicz, R., Chen, X., Sutskever, I., &amp; Welling, M. (2016).
Improved variational inference with inverse autoregressive flow.
Advances in Neural Information Processing Systems, 29,
4743–4751.</unstructured_citation>
          </citation>
          <citation key="germain2015made">
            <article_title>MADE: Masked autoencoder for distribution
estimation</article_title>
            <author>Germain</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Germain, M., Gregor, K., Murray, I.,
&amp; Larochelle, H. (2015). MADE: Masked autoencoder for distribution
estimation. International Conference on Machine Learning,
881–889.</unstructured_citation>
          </citation>
          <citation key="ioffe2015batch">
            <article_title>Batch normalization: Accelerating deep
network training by reducing internal covariate shift</article_title>
            <author>Ioffe</author>
            <journal_title>International conference on machine
learning</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Ioffe, S., &amp; Szegedy, C. (2015).
Batch normalization: Accelerating deep network training by reducing
internal covariate shift. International Conference on Machine Learning,
448–456.</unstructured_citation>
          </citation>
          <citation key="su2023differentially">
            <article_title>Differentially private normalizing flows for
density estimation, data synthesis, and variational inference with
application to electronic health records</article_title>
            <author>Su</author>
            <journal_title>arXiv preprint
arXiv:2302.05787</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Su, B., Wang, Y., Schiavazzi, D. E.,
&amp; Liu, F. (2023). Differentially private normalizing flows for
density estimation, data synthesis, and variational inference with
application to electronic health records. arXiv Preprint
arXiv:2302.05787.</unstructured_citation>
          </citation>
          <citation key="friedman1991multivariate">
            <article_title>Multivariate adaptive regression
splines</article_title>
            <author>Friedman</author>
            <journal_title>The annals of statistics</journal_title>
            <issue>1</issue>
            <volume>19</volume>
            <doi>10.1214/aos/1176347963</doi>
            <cYear>1991</cYear>
            <unstructured_citation>Friedman, J. H. (1991). Multivariate
adaptive regression splines. The Annals of Statistics, 19(1), 1–67.
https://doi.org/10.1214/aos/1176347963</unstructured_citation>
          </citation>
          <citation key="gramacy2007tgp">
            <article_title>Tgp: An R package for Bayesian nonstationary,
semiparametric nonlinear regression and design by treed Gaussian process
models</article_title>
            <author>Gramacy</author>
            <journal_title>Journal of Statistical
Software</journal_title>
            <volume>19</volume>
            <doi>10.18637/jss.v019.i09</doi>
            <cYear>2007</cYear>
            <unstructured_citation>Gramacy, R. B. (2007). Tgp: An R
package for Bayesian nonstationary, semiparametric nonlinear regression
and design by treed Gaussian process models. Journal of Statistical
Software, 19, 1–46.
https://doi.org/10.18637/jss.v019.i09</unstructured_citation>
          </citation>
          <citation key="sobol2003theorems">
            <article_title>Theorems and examples on high dimensional
model representation</article_title>
            <author>Sobol’</author>
            <journal_title>Reliability Engineering and System
Safety</journal_title>
            <issue>2</issue>
            <volume>79</volume>
            <doi>10.1016/S0951-8320(02)00229-6</doi>
            <cYear>2003</cYear>
            <unstructured_citation>Sobol’, I. M. (2003). Theorems and
examples on high dimensional model representation. Reliability
Engineering and System Safety, 79(2), 187–193.
https://doi.org/10.1016/S0951-8320(02)00229-6</unstructured_citation>
          </citation>
          <citation key="brennan2020greedy">
            <article_title>Greedy inference with structure-exploiting
lazy maps</article_title>
            <author>Brennan</author>
            <journal_title>Advances in Neural Information Processing
Systems</journal_title>
            <volume>33</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Brennan, M., Bigoni, D., Zahm, O.,
Spantini, A., &amp; Marzouk, Y. (2020). Greedy inference with
structure-exploiting lazy maps. Advances in Neural Information
Processing Systems, 33, 8330–8342.</unstructured_citation>
          </citation>
          <citation key="siahkoohi2021preconditioned">
            <article_title>Preconditioned training of normalizing flows
for variational inference in inverse problems</article_title>
            <author>Siahkoohi</author>
            <journal_title>Third symposium on advances in approximate
bayesian inference</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Siahkoohi, A., Rizzuti, G.,
Louboutin, M., Witte, P., &amp; Herrmann, F. (2021). Preconditioned
training of normalizing flows for variational inference in inverse
problems. Third Symposium on Advances in Approximate Bayesian Inference.
https://openreview.net/forum?id=P9m1sMaNQ8T</unstructured_citation>
          </citation>
          <citation key="el2012bayesian">
            <article_title>Bayesian inference with optimal
maps</article_title>
            <author>El Moselhy</author>
            <journal_title>Journal of Computational
Physics</journal_title>
            <issue>23</issue>
            <volume>231</volume>
            <doi>10.1016/j.jcp.2012.07.022</doi>
            <cYear>2012</cYear>
            <unstructured_citation>El Moselhy, T. A., &amp; Marzouk, Y.
M. (2012). Bayesian inference with optimal maps. Journal of
Computational Physics, 231(23), 7815–7850.
https://doi.org/10.1016/j.jcp.2012.07.022</unstructured_citation>
          </citation>
          <citation key="sobol1967distribution">
            <article_title>On the distribution of points in a cube and
the approximate evaluation of integrals</article_title>
            <author>Sobol’</author>
            <journal_title>Zhurnal Vychislitel’noi Matematiki i
Matematicheskoi Fiziki</journal_title>
            <issue>4</issue>
            <volume>7</volume>
            <doi>10.1016/0041-5553(67)90144-9</doi>
            <cYear>1967</cYear>
            <unstructured_citation>Sobol’, I. M. (1967). On the
distribution of points in a cube and the approximate evaluation of
integrals. Zhurnal Vychislitel’noi Matematiki i Matematicheskoi Fiziki,
7(4), 784–802.
https://doi.org/10.1016/0041-5553(67)90144-9</unstructured_citation>
          </citation>
          <citation key="abril2023pymc">
            <article_title>PyMC: A modern, and comprehensive
probabilistic programming framework in Python</article_title>
            <author>Abril-Pla</author>
            <journal_title>PeerJ Computer Science</journal_title>
            <volume>9</volume>
            <doi>10.7717/peerj-cs.1516</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Abril-Pla, O., Andreani, V., Carroll,
C., Dong, L., Fonnesbeck, C. J., Kochurov, M., Kumar, R., Lao, J.,
Luhmann, C. C., Martin, O. A., &amp; others. (2023). PyMC: A modern, and
comprehensive probabilistic programming framework in Python. PeerJ
Computer Science, 9, e1516.
https://doi.org/10.7717/peerj-cs.1516</unstructured_citation>
          </citation>
          <citation key="luttinen2016bayespy">
            <article_title>Bayespy: Variational Bayesian inference in
Python</article_title>
            <author>Luttinen</author>
            <journal_title>The Journal of Machine Learning
Research</journal_title>
            <issue>1</issue>
            <volume>17</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Luttinen, J. (2016). Bayespy:
Variational Bayesian inference in Python. The Journal of Machine
Learning Research, 17(1), 1419–1424.</unstructured_citation>
          </citation>
          <citation key="bingham2019pyro">
            <article_title>Pyro: Deep universal probabilistic
programming</article_title>
            <author>Bingham</author>
            <journal_title>Journal of machine learning
research</journal_title>
            <issue>28</issue>
            <volume>20</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Bingham, E., Chen, J. P., Jankowiak,
M., Obermeyer, F., Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P.,
Horsfall, P., &amp; Goodman, N. D. (2019). Pyro: Deep universal
probabilistic programming. Journal of Machine Learning Research, 20(28),
1–6.</unstructured_citation>
          </citation>
          <citation key="huggins2023pyvbmc">
            <article_title>PyVBMC: Efficient Bayesian inference in
Python</article_title>
            <author>Huggins</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>86</issue>
            <volume>8</volume>
            <doi>10.21105/joss.05428</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Huggins, B., Li, C., Tobaben, M.,
Aarnos, M. J., &amp; Acerbi, L. (2023). PyVBMC: Efficient Bayesian
inference in Python. Journal of Open Source Software, 8(86), 5428.
https://doi.org/10.21105/joss.05428</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
