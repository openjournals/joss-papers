<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7489</article-id>
<article-id pub-id-type="doi">10.21105/joss.07489</article-id>
<title-group>
<article-title>ReadmeReady: Free and Customizable Code Documentation
with LLMs - A Fine-Tuning Approach</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0004-6179-389X</contrib-id>
<name>
<surname>Chakrabarty</surname>
<given-names>Sayak</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5781-3032</contrib-id>
<name>
<surname>Pal</surname>
<given-names>Souradip</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Northwestern University</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Purdue University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-11-14">
<day>14</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>108</issue>
<fpage>7489</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>machine learning</kwd>
<kwd>large language models</kwd>
<kwd>retrieval augmented generation</kwd>
<kwd>fine-tuning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Automated documentation of programming source code is a challenging
  task with significant practical and scientific implications for the
  developer community. ReadmeReady is a large language model (LLM)-based
  application that developers can use as a support tool to generate
  basic documentation for any publicly available or custom repository.
  Over the last decade, several research have been done on generating
  documentation for source code using neural network architectures. With
  the recent advancements in LLM technology, some open-source
  applications have been developed to address this problem. However,
  these applications typically rely on the OpenAI APIs, which incur
  substantial financial costs, particularly for large repositories.
  Moreover, none of these open-source applications offer a fine-tuned
  model or features to enable users to fine-tune custom LLMs.
  Additionally, finding suitable data for fine-tuning is often
  challenging. Our application addresses these issues.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>The integration of natural and programming languages is a research
  area that addresses tasks such as automatic documentation of source
  code, code generation from natural language descriptions, and
  searching for code using natural language queries. These tasks are
  highly practical, as they can significantly enhance programmer
  efficiency, and they are scientifically intriguing due to their
  complexity and the proposed relationships between natural language,
  computation, and reasoning
  (<xref alt="Chomsky, 1956" rid="ref-chomsky1956three" ref-type="bibr">Chomsky,
  1956</xref>;
  <xref alt="Graves et al., 2014" rid="ref-graves2014neural" ref-type="bibr">Graves
  et al., 2014</xref>;
  <xref alt="Miller, 2003" rid="ref-miller2003cognitive" ref-type="bibr">Miller,
  2003</xref>).</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the Field</title>
  <p>Recently, large language models (LLMs) have become increasingly
  significant, demonstrating human-like abilities across various fields
  (<xref alt="Brown et al., 2020" rid="ref-brown2020language" ref-type="bibr">Brown
  et al., 2020</xref>;
  <xref alt="Ouyang et al., 2022" rid="ref-ouyang2022training" ref-type="bibr">Ouyang
  et al., 2022</xref>;
  <xref alt="Radford et al., 2019" rid="ref-radford2019language" ref-type="bibr">Radford
  et al., 2019</xref>). LLMs typically employ transformer architecture
  variants and are trained on massive data volumes to detect patterns
  (<xref alt="Vaswani et al., 2017" rid="ref-vaswani2017attention" ref-type="bibr">Vaswani
  et al., 2017</xref>).</p>
  <p>We present an LLM-based application that developers can use as a
  support tool to generate basic documentation for any code repository.
  Some open-source applications have been developed to address this
  issue, to name a few:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>AutoDoc-ChatGPT</bold>
      (<xref alt="Awekrx, 2023" rid="ref-autodoc-chatgpt" ref-type="bibr">Awekrx,
      2023</xref>)</p>
    </list-item>
    <list-item>
      <p><bold>AutoDoc</bold>
      (<xref alt="Labs, 2023" rid="ref-context-labs-autodoc" ref-type="bibr">Labs,
      2023</xref>)</p>
    </list-item>
    <list-item>
      <p><bold>Auto-GitHub-Docs-Generator</bold>
      (<xref alt="Microsoft, 2023" rid="ref-microsoft-auto-github-docs-generator" ref-type="bibr">Microsoft,
      2023</xref>)</p>
    </list-item>
  </list>
  <p>However, these applications suffer from two major issues. Firstly,
  all of them are built on top of the OpenAI APIs, requiring users to
  have an OpenAI API key and incurring a cost with each API request.
  Generating documentation for a large repository could result in costs
  reaching hundreds of dollars. Our application allows users to choose
  among OpenAI’s GPT, Meta’s Llama2, and Google’s Gemma models. Notably,
  apart from the first, the other models are open-source and incur no
  charges, allowing documentation to be generated for free.</p>
  <p>Secondly, none of the existing open-source applications provide a
  fine-tuned model or an option for users to fine-tune. Our application
  offers a fine-tuning option using QLoRA
  (<xref alt="Dettmers et al., 2023" rid="ref-dettmers2023qlora" ref-type="bibr">Dettmers
  et al., 2023</xref>), which can be trained on the user’s own dataset.
  It is important to note that using this feature requires access to
  powerful GPU clusters. Some existing applications provide a
  command-line tool for interacting with the entire repository, allowing
  users to ask specific questions about the repository but not
  generating a README file.</p>
</sec>
<sec id="methodology">
  <title>Methodology</title>
  <p>The application prompts the user to enter the project’s name,
  GitHub URL, and select the desired model from the following
  options:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>gpt-3.5-turbo</monospace>
      (<xref alt="OpenAI, 2023a" rid="ref-gpt-3.5-turbo" ref-type="bibr">OpenAI,
      2023a</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>gpt-4</monospace>
      (<xref alt="OpenAI, 2023b" rid="ref-gpt-4" ref-type="bibr">OpenAI,
      2023b</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>gpt-4-32k</monospace>
      (<xref alt="OpenAI, 2023c" rid="ref-gpt-4-32k" ref-type="bibr">OpenAI,
      2023c</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>TheBloke/Llama-2-7B-Chat-GPTQ</monospace>
      (quantized)
      (<xref alt="TheBloke, 2023b" rid="ref-llama-2-7b-chat-gptq" ref-type="bibr">TheBloke,
      2023b</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>TheBloke/CodeLlama-7B-Instruct-GPTQ</monospace>
      (quantized)
      (<xref alt="TheBloke, 2023a" rid="ref-code-llama-7b-instruct-gptq" ref-type="bibr">TheBloke,
      2023a</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>meta-llama/Llama-2-7b-chat-hf</monospace>
      (<xref alt="Meta, 2023b" rid="ref-llama-2-7b-chat-hf" ref-type="bibr">Meta,
      2023b</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>meta-llama/CodeLlama-7b-Instruct-hf</monospace>
      (<xref alt="Meta, 2023a" rid="ref-code-llama-7b-instruct-hf" ref-type="bibr">Meta,
      2023a</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>google/gemma-2b-it</monospace>
      (<xref alt="Google, 2023b" rid="ref-gemma-2b-it" ref-type="bibr">Google,
      2023b</xref>)</p>
    </list-item>
    <list-item>
      <p><monospace>google/codegemma-2b-it</monospace>
      (<xref alt="Google, 2023a" rid="ref-codegemma-2b-it" ref-type="bibr">Google,
      2023a</xref>)</p>
    </list-item>
  </list>
  <p>For our experimentation and tests, we used 1 × NVIDIA Tesla V100
  with 16GB of GPU memory which is ideal for running the
  application.</p>
  <p><bold>Document Retrieval:</bold> Our application indexes the
  codebase through a depth-first traversal of all repository contents
  and utilizes an LLM to generate documentation. All files are converted
  into text, tokenized, and then chunked, with each chunk containing
  1000 tokens. The application employs the
  <monospace>sentence-transformers/all-mpnet-base-v2</monospace>
  (<xref alt="HuggingFace, 2023" rid="ref-sentence-transformers-all-mpnet-base-v2" ref-type="bibr">HuggingFace,
  2023</xref>) sentence encoder to convert each chunk into a
  768-dimensional embedding vector, which is stored in an in-memory
  vector store. When a query is provided, it is converted into a similar
  vector using the same sentence encoder. The neighbor nearest to the
  query embedding vector is searched using KNN (k=4) from the vector
  store, utilizing cosine similarity as the distance metric. For the KNN
  search, we use the HNSWLib library, which implements an approximate
  nearest-neighbor search based on hierarchical navigable small-world
  graphs
  (<xref alt="Malkov &amp; Yashunin, 2018" rid="ref-malkov2018efficient" ref-type="bibr">Malkov
  &amp; Yashunin, 2018</xref>). This methodology provides the relevant
  sections of the source code, aiding in answering the prompted
  question. The entire methodology for Retrieval Augmented Generation
  (RAG) and fine-tuning is illustrated in
  <xref alt="[fig:rag_workflow]" rid="figU003Arag_workflow">[fig:rag_workflow]</xref>.</p>
  <fig>
    <caption><p>Input to Output Workflow showing the Retrieval and
    Generator modules. The retrieval module uses HNSW algorithm to
    create a context for the prompt to the Language model for text
    generation.<styled-content id="figU003Arag_workflow"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="rag_workflow.jpg" />
  </fig>
  <p><bold>Prompt Configuration:</bold> Prompt engineering is
  accomplished using the Langchain API. For our purpose, a prompt
  template has been used. This template includes placeholders for
  questions, which users can edit and modify as needed. This flexibility
  allows the README to be generated according to the user’s specific
  requirements. Our default README structure includes sections on
  description, requirements, installation, usage, contributing methods,
  and licensing, which align with standard documentation practices. The
  temperature for text generation is kept at the default value of 0.2.
  The current prompts are developer-focused and assume that the
  repository is code-centric.</p>
  <sec id="fine-tuning">
    <title>Fine Tuning</title>
    <p>In our work, we fine-tune only one model,
    <monospace>TheBloke/Llama-2-7B-Chat-GPTQ</monospace>
    (<xref alt="TheBloke, 2023b" rid="ref-llama-2-7b-chat-gptq" ref-type="bibr">TheBloke,
    2023b</xref>), which is a 4-bit quantized model with 1.13 billion
    parameters. It supports a maximum sequence length of 4096 tokens and
    requires 3.9 GB of memory.</p>
  </sec>
  <sec id="data-collection">
    <title>Data Collection</title>
    <p>We limit our scope to Python-based repositories; however, this
    approach is easily adaptable to multiple programming languages. A
    CSV file was created with three features: questions, context, and
    answers. Questions were derived from README file headings and
    subheadings, identified by markdown signatures
    <monospace>#</monospace> or <monospace>##</monospace>. Answers
    correspond to the text under these headings.</p>
    <p>The entire source code from the repositories is concatenated into
    a single string and separated into document chunks of 1000 tokens
    employing LangChain’s text-splitter. Using the
    <monospace>sentence-transformers/all-mpnet-base-v2</monospace>
    (<xref alt="HuggingFace, 2023" rid="ref-sentence-transformers-all-mpnet-base-v2" ref-type="bibr">HuggingFace,
    2023</xref>) sentence encoder, these chunks were converted into
    768-dimensional vectors. Each question is then converted into a
    768-dimensional vector and subjected to a KNN
    <inline-formula><alternatives>
    <tex-math><![CDATA[(k=4)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    search using HNSW
    (<xref alt="Malkov &amp; Yashunin, 2018" rid="ref-malkov2018efficient" ref-type="bibr">Malkov
    &amp; Yashunin, 2018</xref>) to find the closest match from the
    entire set of document embeddings, stored as the context.</p>
    <p><bold>Data Preprocessing:</bold> Following the creation of the
    CSV file, we pre-process the data using regex patterns to clean the
    text. Since the context only captures source code, this eliminates
    the possibility of using offensive content. Regex is used to remove
    hashtags, email addresses, usernames, image URLs, and other
    personally identifiable information. Note that only repositories
    written entirely in English are used, with other languages filtered
    out. Prompt engineering in our source code ensures that the prompts
    are designed to avoid generating any personally identifiable data or
    offensive content.</p>
  </sec>
</sec>
<sec id="experiments">
  <title>Experiments</title>
  <p>We conducted the fine-tuning experiment on a small dataset
  consisting of randomly selected 190 README files, which may not
  address our default documentation questions. For each README, we
  examine its sections and subsections, frame relevant questions, and
  use the answers generated by our tool for training. For evaluation, we
  selected the rest of the 10 repositories and compared the original
  answers with the autogenerated documentation using BLEU and BERT
  scores to assess our model’s performance.</p>
  <sec id="before-fine-tuning">
    <title>Before Fine-tuning</title>
    <p>We conducted a series of experiments utilizing the
    <monospace>TheBloke/Llama-2-7B-Chat-GPTQ</monospace> model
    (<xref alt="TheBloke, 2023b" rid="ref-llama-2-7b-chat-gptq" ref-type="bibr">TheBloke,
    2023b</xref>) to demonstrate the functionality and efficacy of our
    proposed pipeline. The accompanying codebase is designed to be
    flexible, allowing the user to easily switch between different large
    language models (LLMs) by simply modifying the configuration file.
    Given the characteristics of LLMs, models with a greater number of
    parameters are generally expected to deliver enhanced
    performance.</p>
  </sec>
  <sec id="after-fine-tuning">
    <title>After Fine-tuning</title>
    <p>We utilized the PEFT library from Hugging Face, which supports
    several Parameter Efficient Fine-Tuning (PEFT) methods. This
    approach is cost-effective for fine-tuning large language models
    (LLMs), particularly on lightweight hardware. The training
    configuration and hyperparameters are detailed in Table 1 and Table
    2 respectively.</p>
    <sec id="table-1-qlora-configuration">
      <title>Table 1: QLoRA Configuration</title>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Parameter</th>
              <th>Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><monospace>r</monospace></td>
              <td>2</td>
            </tr>
            <tr>
              <td><monospace>lora_alpha</monospace></td>
              <td>32</td>
            </tr>
            <tr>
              <td><monospace>lora_dropout</monospace></td>
              <td>0.05</td>
            </tr>
            <tr>
              <td><monospace>bias</monospace></td>
              <td>None</td>
            </tr>
            <tr>
              <td><monospace>task_type</monospace></td>
              <td>CAUSAL_LM</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
    <sec id="table-2-training-hyper-parameters">
      <title>Table 2: Training Hyper-parameters</title>
      <table-wrap>
        <table>
          <thead>
            <tr>
              <th>Parameter</th>
              <th>Value</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><monospace>per_device_train_batch_size</monospace></td>
              <td>1</td>
            </tr>
            <tr>
              <td><monospace>gradient_accumulation_steps</monospace></td>
              <td>1</td>
            </tr>
            <tr>
              <td><monospace>num_train_epochs</monospace></td>
              <td>3</td>
            </tr>
            <tr>
              <td><monospace>learning_rate</monospace></td>
              <td>1e-4</td>
            </tr>
            <tr>
              <td><monospace>fp16</monospace></td>
              <td>True</td>
            </tr>
            <tr>
              <td><monospace>optim</monospace></td>
              <td>paged_adamw_8bit</td>
            </tr>
            <tr>
              <td><monospace>lr_scheduler_type</monospace></td>
              <td>cosine</td>
            </tr>
            <tr>
              <td><monospace>warmup_ratio</monospace></td>
              <td>0.01</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </sec>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-chomsky1956three">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chomsky</surname><given-names>Noam</given-names></name>
      </person-group>
      <article-title>Three models for the description of language</article-title>
      <source>IRE Transactions on information theory</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="1956">1956</year>
      <volume>2</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/TIT.1956.1056813</pub-id>
      <fpage>113</fpage>
      <lpage>124</lpage>
    </element-citation>
  </ref>
  <ref id="ref-miller2003cognitive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Miller</surname><given-names>George A</given-names></name>
      </person-group>
      <article-title>The cognitive revolution: a historical perspective</article-title>
      <source>Trends in cognitive sciences</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <volume>7</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1016/S1364-6613(03)00029-9</pub-id>
      <fpage>141</fpage>
      <lpage>144</lpage>
    </element-citation>
  </ref>
  <ref id="ref-graves2014neural">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Graves</surname><given-names>Alex</given-names></name>
        <name><surname>Wayne</surname><given-names>Greg</given-names></name>
        <name><surname>Danihelka</surname><given-names>Ivo</given-names></name>
      </person-group>
      <article-title>Neural Turing Machines</article-title>
      <source>arXiv preprint arXiv:1410.5401</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1410.5401</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-radford2019language">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radford</surname><given-names>Alec</given-names></name>
        <name><surname>Wu</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Child</surname><given-names>Rewon</given-names></name>
        <name><surname>Luan</surname><given-names>David</given-names></name>
        <name><surname>Amodei</surname><given-names>Dario</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Language Models are Unsupervised Multitask Learners</article-title>
      <source>OpenAI blog</source>
      <year iso-8601-date="2019">2019</year>
      <volume>1</volume>
      <issue>8</issue>
      <fpage>9</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-brown2020language">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Tom</given-names></name>
        <name><surname>Mann</surname><given-names>Benjamin</given-names></name>
        <name><surname>Ryder</surname><given-names>Nick</given-names></name>
        <name><surname>Subbiah</surname><given-names>Melanie</given-names></name>
        <name><surname>Kaplan</surname><given-names>Jared D</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
        <name><surname>Neelakantan</surname><given-names>Arvind</given-names></name>
        <name><surname>Shyam</surname><given-names>Pranav</given-names></name>
        <name><surname>Sastry</surname><given-names>Girish</given-names></name>
        <name><surname>Askell</surname><given-names>Amanda</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Language Models are Few-Shot Learners</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2005.14165</pub-id>
      <fpage>1877</fpage>
      <lpage>1901</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ouyang2022training">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ouyang</surname><given-names>Long</given-names></name>
        <name><surname>Wu</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Jiang</surname><given-names>Xu</given-names></name>
        <name><surname>Almeida</surname><given-names>Diogo</given-names></name>
        <name><surname>Wainwright</surname><given-names>Carroll</given-names></name>
        <name><surname>Mishkin</surname><given-names>Pamela</given-names></name>
        <name><surname>Zhang</surname><given-names>Chong</given-names></name>
        <name><surname>Agarwal</surname><given-names>Sandhini</given-names></name>
        <name><surname>Slama</surname><given-names>Katarina</given-names></name>
        <name><surname>Ray</surname><given-names>Alex</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Training language models to follow instructions with human feedback</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>35</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2203.02155</pub-id>
      <fpage>27730</fpage>
      <lpage>27744</lpage>
    </element-citation>
  </ref>
  <ref id="ref-vaswani2017attention">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N</given-names></name>
        <name><surname>Kaiser</surname><given-names>Łukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention is All You Need</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gpt-3.5-turbo">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>OpenAI</surname></name>
      </person-group>
      <article-title>Gpt-3.5-turbo</article-title>
      <publisher-name>https://github.com/openai/gpt-3.5-turbo</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-gpt-4">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>OpenAI</surname></name>
      </person-group>
      <article-title>Gpt-4</article-title>
      <publisher-name>https://github.com/openai/gpt-4</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-gpt-4-32k">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>OpenAI</surname></name>
      </person-group>
      <article-title>Gpt-4-32k</article-title>
      <publisher-name>https://github.com/openai/gpt-4-32k</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-llama-2-7b-chat-gptq">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>TheBloke</surname></name>
      </person-group>
      <article-title>Llama-2-7B-chat-GPTQ</article-title>
      <publisher-name>https://github.com/TheBloke/Llama-2-7B-Chat-GPTQ</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-code-llama-7b-instruct-gptq">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>TheBloke</surname></name>
      </person-group>
      <article-title>CodeLlama-7B-instruct-GPTQ</article-title>
      <publisher-name>https://github.com/TheBloke/CodeLlama-7B-Instruct-GPTQ</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-llama-2-7b-chat-hf">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Meta</surname></name>
      </person-group>
      <article-title>Llama-2-7b-chat-hf</article-title>
      <publisher-name>https://github.com/meta-llama/Llama-2-7b-chat-hf</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-code-llama-7b-instruct-hf">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Meta</surname></name>
      </person-group>
      <article-title>CodeLlama-7b-instruct-hf</article-title>
      <publisher-name>https://github.com/meta-llama/CodeLlama-7b-Instruct-hf</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-gemma-2b-it">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Google</surname></name>
      </person-group>
      <article-title>Gemma-2b-it</article-title>
      <publisher-name>https://github.com/google/gemma-2b-it</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-codegemma-2b-it">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Google</surname></name>
      </person-group>
      <article-title>Codegemma-2b-it</article-title>
      <publisher-name>https://github.com/google/codegemma-2b-it</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-autodoc-chatgpt">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Awekrx</surname></name>
      </person-group>
      <article-title>AutoDoc-ChatGPT</article-title>
      <publisher-name>https://github.com/awekrx/AutoDoc-ChatGPT</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-context-labs-autodoc">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Labs</surname><given-names>Context</given-names></name>
      </person-group>
      <article-title>AutoDoc</article-title>
      <publisher-name>https://github.com/context-labs/autodoc</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-microsoft-auto-github-docs-generator">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Microsoft</surname></name>
      </person-group>
      <article-title>Auto-GitHub-docs-generator</article-title>
      <publisher-name>https://github.com/microsoft/auto-github-docs-generator</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-sentence-transformers-all-mpnet-base-v2">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>HuggingFace</surname></name>
      </person-group>
      <article-title>Sentence transformers: All-mpnet-base-v2</article-title>
      <publisher-name>https://huggingface.co/sentence-transformers/all-mpnet-base-v2</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-malkov2018efficient">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Malkov</surname><given-names>Yu A</given-names></name>
        <name><surname>Yashunin</surname><given-names>Dmitry A</given-names></name>
      </person-group>
      <article-title>Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs</article-title>
      <source>IEEE transactions on pattern analysis and machine intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>42</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2018.2889473</pub-id>
      <fpage>824</fpage>
      <lpage>836</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dettmers2023qlora">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dettmers</surname><given-names>Tim</given-names></name>
        <name><surname>Pagnoni</surname><given-names>Artidoro</given-names></name>
        <name><surname>Holtzman</surname><given-names>Ari</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
      </person-group>
      <article-title>QLoRA: Efficient Finetuning of Quantized LLMs</article-title>
      <source>arXiv preprint arXiv:2305.14314</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2305.14314</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
