<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6611</article-id>
<article-id pub-id-type="doi">10.21105/joss.06611</article-id>
<title-group>
<article-title>CBX: Python and Julia Packages for Consensus-Based
Interacting Particle Methods</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8018-3799</contrib-id>
<name>
<surname>Bailo</surname>
<given-names>Rafael</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9856-2818</contrib-id>
<name>
<surname>Barbaro</surname>
<given-names>Alethea</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8731-367X</contrib-id>
<name>
<surname>Gomes</surname>
<given-names>Susana N.</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2206-4334</contrib-id>
<name>
<surname>Riedl</surname>
<given-names>Konstantin</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8440-2928</contrib-id>
<name>
<surname>Roith</surname>
<given-names>Tim</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6283-7154</contrib-id>
<name>
<surname>Totzeck</surname>
<given-names>Claudia</given-names>
</name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7629-7184</contrib-id>
<name>
<surname>Vaes</surname>
<given-names>Urbain</given-names>
</name>
<xref ref-type="aff" rid="aff-8"/>
<xref ref-type="aff" rid="aff-9"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Mathematical Institute, University of Oxford, United
Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Delft University of Technology, The
Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Mathematics Institute, University of Warwick, United
Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Technical University of Munich, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Munich Center for Machine Learning, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Helmholtz Imaging, Deutsches Elektronen-Synchrotron DESY,
Notkestr. 85, 22607 Hamburg, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>University of Wuppertal, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>MATHERIALS team, Inria Paris, France</institution>
</institution-wrap>
</aff>
<aff id="aff-9">
<institution-wrap>
<institution>École des Ponts ParisTech, Marne-la-Vallée,
France</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>9</volume>
<issue>98</issue>
<fpage>6611</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Julia</kwd>
<kwd>Optimisation</kwd>
<kwd>Sampling</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We introduce
  <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/CBXpy/">CBXPy</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/ConsensusBasedX.jl/">ConsensusBasedX.jl</ext-link>,
  Python and Julia implementations of consensus-based interacting
  particle systems (CBX), which generalise consensus-based optimization
  methods (CBO) for global, derivative-free optimisation. The
  <italic>raison d’être</italic> of our libraries is twofold: on the one
  hand, to offer high-performance implementations of CBX methods that
  the community can use directly, while on the other, providing a
  general interface that can accommodate and be extended to further
  variations of the CBX family. Python and Julia were selected as the
  leading high-level languages in terms of usage and performance, as
  well as for their popularity among the scientific computing community.
  Both libraries have been developed with a common
  <italic>ethos</italic>, ensuring a similar API and core functionality,
  while leveraging the strengths of each language and writing idiomatic
  code.</p>
</sec>
<sec id="mathematical-background">
  <title>Mathematical background</title>
  <p>Consensus-based optimisation (CBO) is an approach to solve, for a
  given (continuous) <italic>objective function</italic>
  <inline-formula><alternatives>
  <tex-math><![CDATA[f:\mathbb{R}^d \rightarrow \mathbb{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  the <italic>global minimisation problem</italic></p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  x^* = \operatorname*{argmin}_{x\in\mathbb{R}^d} f(x),
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mo>argmin</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>i.e., the task of finding the point <inline-formula><alternatives>
  <tex-math><![CDATA[x^*]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  attains its lowest value. Such problems arise in a variety of
  disciplines including engineering, where
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  might represent a vector of design parameters for a structure and
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  a function related to its cost and structural integrity, or machine
  learning, where <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  could comprise the parameters of a neural network and
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  the empirical loss, which measures the discrepancy of the neural
  network prediction with the observed data.</p>
  <p>In some cases, so-called <italic>gradient-based methods</italic>
  (those that involve updating a guess of <inline-formula><alternatives>
  <tex-math><![CDATA[x^*]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
  by evaluating the gradient <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∇</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>)
  achieve state-of-the-art performance in the global minimisation
  problem. However, in scenarios where <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  is <italic>non-convex</italic> (when <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  could have many <italic>local minima</italic>), where
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  is <italic>non-smooth</italic> (when <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∇</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is not well-defined), or where the evaluation of
  <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∇</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is impractical due to cost or complexity,
  <italic>derivative-free</italic> methods are needed. Numerous
  techniques exist for derivative-free optimisation, such as
  <italic>random</italic> or <italic>pattern search</italic>
  (<xref alt="Friedman &amp; Savage, 1947" rid="ref-friedman1947planning" ref-type="bibr">Friedman
  &amp; Savage, 1947</xref>;
  <xref alt="Hooke &amp; Jeeves, 1961" rid="ref-hooke1961direct" ref-type="bibr">Hooke
  &amp; Jeeves, 1961</xref>;
  <xref alt="Rastrigin, 1963" rid="ref-rastrigin1963convergence" ref-type="bibr">Rastrigin,
  1963</xref>), <italic>Bayesian optimisation</italic>
  (<xref alt="Močkus, 1975" rid="ref-movckus1975bayesian" ref-type="bibr">Močkus,
  1975</xref>) or <italic>simulated annealing</italic>
  (<xref alt="Henderson et al., 2003" rid="ref-henderson2003theory" ref-type="bibr">Henderson
  et al., 2003</xref>). Here, we focus on <italic>particle-based
  methods</italic>, specifically, consensus-based optimisation (CBO), as
  proposed by Pinnau et al.
  (<xref alt="2017" rid="ref-pinnau2017consensus" ref-type="bibr">2017</xref>),
  and the consensus-based taxonomy of related techniques, which we term
  <italic>CBX</italic>.</p>
  <p>CBO uses a finite number <inline-formula><alternatives>
  <tex-math><![CDATA[N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
  of <italic>agents</italic> (particles), <inline-formula><alternatives>
  <tex-math><![CDATA[x_t=(x_t^1,\dots,x_t^N)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>N</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  dependent on time <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
  to explore the landscape of <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  without evaluating any of its derivatives (as do other CBX methods).
  The agents evaluate the objective function at their current position,
  <inline-formula><alternatives>
  <tex-math><![CDATA[f(x_t^i)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and define a <italic>consensus point</italic>
  <inline-formula><alternatives>
  <tex-math><![CDATA[c_\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.
  This point is an approximation of the global minimiser
  <inline-formula><alternatives>
  <tex-math><![CDATA[x^*]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>,
  and is constructed by weighing each agent’s position against the
  <italic>Gibbs-like distribution</italic>
  <inline-formula><alternatives>
  <tex-math><![CDATA[\exp(-\alpha f)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>−</mml:mi><mml:mi>α</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  (<xref alt="Boltzmann, 1868" rid="ref-boltzmann1868studien" ref-type="bibr">Boltzmann,
  1868</xref>). More rigorously,</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  c_\alpha(x_t) =
  \frac{1}{ \sum_{i=1}^N \omega_\alpha(x_t^i) }
  \sum_{i=1}^N x_t^i \, \omega_\alpha(x_t^i),
  \quad\text{where}\quad
  \omega_\alpha(\,\cdot\,) = \mathrm{exp}(-\alpha f(\,\cdot\,)),
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>ω</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mspace width="0.167em"></mml:mspace><mml:msub><mml:mi>ω</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mtext mathvariant="normal">where</mml:mtext><mml:mspace width="1.0em"></mml:mspace><mml:msub><mml:mi>ω</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mspace width="0.167em"></mml:mspace><mml:mo>⋅</mml:mo><mml:mspace width="0.167em"></mml:mspace><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>−</mml:mi><mml:mi>α</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mspace width="0.167em"></mml:mspace><mml:mo>⋅</mml:mo><mml:mspace width="0.167em"></mml:mspace><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>for some <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha>0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  The exponential weights in the definition favour those points
  <inline-formula><alternatives>
  <tex-math><![CDATA[x_t^i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[f(x_t^i)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is lowest, and comparatively ignore the rest, particularly for larger
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>.
  If all the found values of the objective function are approximately
  the same, <inline-formula><alternatives>
  <tex-math><![CDATA[c_\alpha(x_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is roughly an arithmetic mean. Instead, if one particle is much better
  than the rest, <inline-formula><alternatives>
  <tex-math><![CDATA[c_\alpha(x_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  will be very close to its position.</p>
  <p>Once the consensus point is computed, the particles evolve in time
  following the <italic>stochastic differential equation</italic>
  (SDE)</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \mathrm{d}x_t^i =
  -\lambda\ \underbrace{
  \left( x_t^i - c_\alpha(x_t) \right) \mathrm{d}t
  }_{
  \text{consensus drift}
  }
  + \sigma\ \underbrace{
  \left\| x_t^i - c_\alpha(x_t) \right\| \mathrm{d}B_t^i
  }_{
  \text{scaled diffusion}
  },
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mi>λ</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:munder><mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo accent="true">⏟</mml:mo></mml:munder><mml:mtext mathvariant="normal">consensus drift</mml:mtext></mml:munder><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:munder><mml:munder><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:msubsup><mml:mi>B</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:mrow><mml:mo accent="true">⏟</mml:mo></mml:munder><mml:mtext mathvariant="normal">scaled diffusion</mml:mtext></mml:munder><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
  are positive parameters, and where <inline-formula><alternatives>
  <tex-math><![CDATA[B_t^i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>B</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>
  are independent Brownian motions in <inline-formula><alternatives>
  <tex-math><![CDATA[d]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>
  dimensions. The <italic>consensus drift</italic> is a deterministic
  term that drives each agent towards the consensus point, with rate
  <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>.
  Meanwhile, the <italic>scaled diffusion</italic> is a stochastic term
  that encourages exploration of the landscape. The scaling factor of
  the diffusion is proportional to the distance of the particle to the
  consensus point. Hence, whenever the position of a particle and the
  location of the weighted mean coincide, the particle stops moving. On
  the other hand, if the particle is far away from the consensus, its
  evolution has a stronger exploratory behaviour. While both the agents’
  positions and the consensus point evolve in time, it has been proven
  that all agents eventually reach the same position and that the
  consensus point <inline-formula><alternatives>
  <tex-math><![CDATA[c_\alpha(x_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is a good approximation of <inline-formula><alternatives>
  <tex-math><![CDATA[x^*]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>x</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
  (<xref alt="Carrillo et al., 2018" rid="ref-carrillo2018analytical" ref-type="bibr">Carrillo
  et al., 2018</xref>;
  <xref alt="Fornasier, Klock, et al., 2021" rid="ref-fornasier2021consensus" ref-type="bibr">Fornasier,
  Klock, et al., 2021</xref>). Other variations of the method, such as
  CBO with anisotropic noise
  (<xref alt="Carrillo et al., 2021" rid="ref-carrillo2021consensus" ref-type="bibr">Carrillo
  et al., 2021</xref>), <italic>polarised CBO</italic>
  (<xref alt="Bungert et al., 2024" rid="ref-bungert2022polarized" ref-type="bibr">Bungert
  et al., 2024</xref>), or <italic>consensus-based sampling</italic>
  (CBS)
  (<xref alt="Carrillo et al., 2022" rid="ref-carrillo2022consensus" ref-type="bibr">Carrillo
  et al., 2022</xref>) have also been proposed.</p>
  <p>In practice, the solution to the SDE above cannot be found exactly.
  Instead, an <italic>Euler–Maruyama scheme</italic>
  (<xref alt="Kloeden &amp; Platen, 1992" rid="ref-KP1992" ref-type="bibr">Kloeden
  &amp; Platen, 1992</xref>) is used to update the position of the
  agents. The update is given by</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  x^i \gets x^i
  -\lambda \,\Delta t
  \left( x^i - c_\alpha(x) \right)
  + \sigma\sqrt{\Delta t}\
  \left\| x^i - c_\alpha(x) \right\| \xi^i,
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>←</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mi>λ</mml:mi><mml:mspace width="0.167em"></mml:mspace><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:msqrt><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msqrt><mml:mspace width="0.222em"></mml:mspace><mml:mrow><mml:mo stretchy="true" form="prefix">∥</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">∥</mml:mo></mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\Delta t > 0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Δ</mml:mi><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  is the <italic>step size</italic> and <inline-formula><alternatives>
  <tex-math><![CDATA[\xi^i \sim \mathcal{N}(0,\mathrm{Id})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo>∼</mml:mo><mml:mi>𝒩</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">I</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  are independent, identically distributed, standard normal random
  vectors.</p>
  <p>As a particle-based family of methods, CBX is conceptually related
  to other optimisation approaches which take inspiration from biology,
  like <italic>particle-swarm optimisation</italic> (PSO)
  (<xref alt="Kennedy &amp; Eberhart, 1995" rid="ref-kennedy1995particle" ref-type="bibr">Kennedy
  &amp; Eberhart, 1995</xref>), from physics, like <italic>simulated
  annealing</italic> (SA)
  (<xref alt="Henderson et al., 2003" rid="ref-henderson2003theory" ref-type="bibr">Henderson
  et al., 2003</xref>), or from other heuristics
  (<xref alt="Bayraktar et al., 2013" rid="ref-bayraktar2013wind" ref-type="bibr">Bayraktar
  et al., 2013</xref>;
  <xref alt="Chandra Mohan &amp; Baskaran, 2012" rid="ref-mohan2012survey" ref-type="bibr">Chandra
  Mohan &amp; Baskaran, 2012</xref>;
  <xref alt="Karaboga et al., 2012" rid="ref-karaboga2014comprehensive" ref-type="bibr">Karaboga
  et al., 2012</xref>;
  <xref alt="Yang, 2009" rid="ref-yang2009firefly" ref-type="bibr">Yang,
  2009</xref>). However, unlike many such methods, CBX has been designed
  to be compatible with rigorous convergence analysis at the mean-field
  level (the infinite-particle limit, see
  <xref alt="Huang &amp; Qiu, 2022" rid="ref-huang2021MFLCBO" ref-type="bibr">Huang
  &amp; Qiu, 2022</xref>). Many convergence results have been shown,
  whether in the original formulation
  (<xref alt="Carrillo et al., 2018" rid="ref-carrillo2018analytical" ref-type="bibr">Carrillo
  et al., 2018</xref>;
  <xref alt="Fornasier, Klock, et al., 2021" rid="ref-fornasier2021consensus" ref-type="bibr">Fornasier,
  Klock, et al., 2021</xref>), for CBO with anisotropic noise
  (<xref alt="Carrillo et al., 2021" rid="ref-carrillo2021consensus" ref-type="bibr">Carrillo
  et al., 2021</xref>;
  <xref alt="Fornasier et al., 2022" rid="ref-fornasier2021convergence" ref-type="bibr">Fornasier
  et al., 2022</xref>), with memory effects
  (<xref alt="Riedl, 2023" rid="ref-riedl2022leveraging" ref-type="bibr">Riedl,
  2023</xref>), with truncated noise
  (<xref alt="Fornasier et al., 2024" rid="ref-fornasier2023consensus" ref-type="bibr">Fornasier
  et al., 2024</xref>), for polarised CBO
  (<xref alt="Bungert et al., 2024" rid="ref-bungert2022polarized" ref-type="bibr">Bungert
  et al., 2024</xref>), and PSO
  (<xref alt="Huang et al., 2023" rid="ref-qiu2022PSOconvergence" ref-type="bibr">Huang
  et al., 2023</xref>). The relation between CBO and <italic>stochastic
  gradient descent</italic> has been recently established by Riedl et
  al.
  (<xref alt="2023" rid="ref-riedl2023gradient" ref-type="bibr">2023</xref>),
  which suggests a previously unknown yet fundamental connection between
  derivative-free and gradient-based approaches.</p>
  <fig>
    <caption><p>Typical evolution of a CBO method minimising the Ackley
    function
    (<xref alt="Ackley, 1987" rid="ref-ackley2012connectionist" ref-type="bibr">Ackley,
    1987</xref>).</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="JOSS.pdf" />
  </fig>
  <p>CBX methods have been successfully applied and extended to several
  different settings, such as constrained optimisation problems
  (<xref alt="Borghi et al., 2023b" rid="ref-borghi2021constrained" ref-type="bibr">Borghi
  et al., 2023b</xref>;
  <xref alt="Fornasier, Huang, et al., 2021" rid="ref-fornasier2020consensus_sphere_convergence" ref-type="bibr">Fornasier,
  Huang, et al., 2021</xref>), multi-objective optimisation
  (<xref alt="Borghi et al., 2023a" rid="ref-borghi2022adaptive" ref-type="bibr">Borghi
  et al., 2023a</xref>;
  <xref alt="Klamroth et al., 2024" rid="ref-klamroth2022consensus" ref-type="bibr">Klamroth
  et al., 2024</xref>), saddle-point problems
  (<xref alt="Huang et al., 2024" rid="ref-huang2022consensus" ref-type="bibr">Huang
  et al., 2024</xref>), federated learning tasks
  (<xref alt="Carrillo et al., 2023" rid="ref-carrillo2023fedcbo" ref-type="bibr">Carrillo
  et al., 2023</xref>), uncertainty quantification
  (<xref alt="Althaus et al., 2023" rid="ref-althaus2023consensus" ref-type="bibr">Althaus
  et al., 2023</xref>), or sampling
  (<xref alt="Carrillo et al., 2022" rid="ref-carrillo2022consensus" ref-type="bibr">Carrillo
  et al., 2022</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In general, very few implementations of CBO already exist, and none
  have been designed with the generality of other CBX methods in mind.
  Here, we summarise the related software:</p>
  <p>Regarding Python, we refer to <monospace>PyPop7</monospace>
  (<xref alt="Duan et al., 2022" rid="ref-duan2023pypop7" ref-type="bibr">Duan
  et al., 2022</xref>) and <monospace>scikit-opt</monospace>
  (<xref alt="Guo, 2021" rid="ref-scikitopt" ref-type="bibr">Guo,
  2021</xref>) for a collection of various derivative-free optimisation
  strategies. For packages connected to Bayesian optimisation, we refer
  to <monospace>BayesO</monospace>
  (<xref alt="Kim &amp; Choi, 2023" rid="ref-Kim2023" ref-type="bibr">Kim
  &amp; Choi, 2023</xref>), <monospace>bayesian-optimization</monospace>
  (<xref alt="Nogueira, 2014–" rid="ref-Bayesian14" ref-type="bibr">Nogueira,
  2014–</xref>), <monospace>GPyOpt</monospace>
  (<xref alt="The GPyOpt authors, 2016" rid="ref-gpyopt2016" ref-type="bibr">The
  GPyOpt authors, 2016</xref>), <monospace>GPflowOpt</monospace>
  (<xref alt="Knudde et al., 2017" rid="ref-GPflowOpt2017" ref-type="bibr">Knudde
  et al., 2017</xref>), <monospace>pyGPGO</monospace>
  (<xref alt="Jiménez &amp; Ginebra, 2017" rid="ref-Jiménez2017" ref-type="bibr">Jiménez
  &amp; Ginebra, 2017</xref>), <monospace>PyBADS</monospace>
  (<xref alt="Singh &amp; Acerbi, 2024" rid="ref-Singh2024" ref-type="bibr">Singh
  &amp; Acerbi, 2024</xref>) and <monospace>BoTorch</monospace>
  (<xref alt="Balandat et al., 2020" rid="ref-balandat2020botorch" ref-type="bibr">Balandat
  et al., 2020</xref>). Furthermore, CMA-ES
  (<xref alt="Hansen &amp; Ostermeier, 1996" rid="ref-hansen1996adapting" ref-type="bibr">Hansen
  &amp; Ostermeier, 1996</xref>) was implemented in
  <monospace>pycma</monospace>
  (<xref alt="Hansen et al., 2019" rid="ref-hansen2019pycma" ref-type="bibr">Hansen
  et al., 2019</xref>). To the best of our knowledge the connection
  between consensus-based methods and evolution strategies is not fully
  understood, and is therefore an interesting future direction. PSO and
  SA implementations are already available in
  <monospace>PySwarms</monospace>
  (<xref alt="Miranda, 2018" rid="ref-miranda2018pyswarms" ref-type="bibr">Miranda,
  2018</xref>), <monospace>scikit-opt</monospace>
  (<xref alt="Guo, 2021" rid="ref-scikitopt" ref-type="bibr">Guo,
  2021</xref>), <monospace>DEAP</monospace>
  (<xref alt="Fortin et al., 2012" rid="ref-deapJMLR2012" ref-type="bibr">Fortin
  et al., 2012</xref>) and <monospace>pagmo</monospace>
  (<xref alt="Biscani et al., 2017" rid="ref-pagmo2017" ref-type="bibr">Biscani
  et al., 2017</xref>). They are widely used by the community and
  provide a rich framework for the respective methods. However,
  adjusting these implementations to CBO is not straightforward. The
  first publicly available Python packages implementing CBX algorithms
  were given by some of the authors together with collaborators. Tukh
  &amp; Riedl
  (<xref alt="2022" rid="ref-Igor_CBOinPython" ref-type="bibr">2022</xref>)
  implement standard CBO
  (<xref alt="Pinnau et al., 2017" rid="ref-pinnau2017consensus" ref-type="bibr">Pinnau
  et al., 2017</xref>), and the package <monospace>PolarCBO</monospace>
  (<xref alt="Roith et al., 2023" rid="ref-Roith_polarcbo" ref-type="bibr">Roith
  et al., 2023</xref>) provides an implementation of polarised CBO
  (<xref alt="Bungert et al., 2024" rid="ref-bungert2022polarized" ref-type="bibr">Bungert
  et al., 2024</xref>).
  <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/CBXpy/">CBXPy</ext-link>
  is a significant extension of the latter, which was tailored to the
  polarised variant. The code architecture was generalised, which
  allowed the implementation of the whole CBX family within a common
  framework.</p>
  <p>Regarding Julia, PSO and SA methods are, among others, implemented
  in <monospace>Optim.jl</monospace>
  (<xref alt="Mogensen &amp; Riseth, 2018" rid="ref-mogensen2018optim" ref-type="bibr">Mogensen
  &amp; Riseth, 2018</xref>), <monospace>Metaheuristics.jl</monospace>
  (<xref alt="Mejı́a-de-Dios &amp; Mezura-Montes, 2022" rid="ref-mejia2022metaheuristics" ref-type="bibr">Mejı́a-de-Dios
  &amp; Mezura-Montes, 2022</xref>), and
  <monospace>Manopt.jl</monospace>
  (<xref alt="Bergmann, 2022" rid="ref-Bergmann2022" ref-type="bibr">Bergmann,
  2022</xref>). PSO and SA are also included in the meta-library
  <monospace>Optimization.jl</monospace>
  (<xref alt="Dixit &amp; Rackauckas, 2023" rid="ref-DR2023" ref-type="bibr">Dixit
  &amp; Rackauckas, 2023</xref>), as well as Nelder–Mead, which is a
  direct search method. The latter is also implemented in
  <monospace>Manopt.jl</monospace>
  (<xref alt="Bergmann, 2022" rid="ref-Bergmann2022" ref-type="bibr">Bergmann,
  2022</xref>), which further provides a manifold variant of CMA-ES
  (<xref alt="Colutto et al., 2009" rid="ref-colutto2009cma" ref-type="bibr">Colutto
  et al., 2009</xref>). One of the authors gave the first specific Julia
  implementation of standard CBO <monospace>Consensus.jl</monospace>
  (<xref alt="Bailo, 2023" rid="ref-Bailo_consensus" ref-type="bibr">Bailo,
  2023</xref>). That package has now been deprecated in favour of
  <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/ConsensusBasedX.jl/">ConsensusBasedX.jl</ext-link>,
  which improves the performance of the CBO implementation with a
  type-stable and allocation-free implementation. The package also adds
  a CBS implementation, and overall presents a more general interface
  that accomodates the wider CBX class of methods.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p><ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/CBXpy/">CBXPy</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/ConsensusBasedX.jl/">ConsensusBasedX.jl</ext-link>
  provide a lightweight and high-level interface. An existing function
  can be optimised with just one call. Method selection, parameters,
  different approaches to particle initialisation, and termination
  criteria can be specified directly through this interface, offering a
  flexible point of entry for the casual user. Some of the methods
  provided are standard CBO
  (<xref alt="Pinnau et al., 2017" rid="ref-pinnau2017consensus" ref-type="bibr">Pinnau
  et al., 2017</xref>), CBO with mini-batching
  (<xref alt="Carrillo et al., 2021" rid="ref-carrillo2021consensus" ref-type="bibr">Carrillo
  et al., 2021</xref>), polarised CBO
  (<xref alt="Bungert et al., 2024" rid="ref-bungert2022polarized" ref-type="bibr">Bungert
  et al., 2024</xref>), CBO with memory effects
  (<xref alt="Grassi &amp; Pareschi, 2021" rid="ref-grassi2020particle" ref-type="bibr">Grassi
  &amp; Pareschi, 2021</xref>;
  <xref alt="Riedl, 2023" rid="ref-riedl2022leveraging" ref-type="bibr">Riedl,
  2023</xref>), and consensus-based sampling (CBS)
  (<xref alt="Carrillo et al., 2022" rid="ref-carrillo2022consensus" ref-type="bibr">Carrillo
  et al., 2022</xref>). Parallelisation tools are available.</p>
  <p>A more proficient user will benefit from the fully documented
  interface, which allows the specification of advanced options (e.g.,
  debug output, the noise model, or the numerical approach to the matrix
  square root of the weighted ensemble covariance matrix). Both
  libraries offer performance evaluation methods as well as
  visualisation tools.</p>
  <p>Ultimately, a low-level interface (including documentation and
  full-code examples) is provided. Both libraries have been designed to
  express common abstractions in the CBX family while allowing
  customisation. Users can easily implement new CBX methods or modify
  the behaviour of the existing implementation by strategically
  overriding certain hooks. The stepping of the methods can also be
  controlled manually.</p>
  <sec id="cbxpy">
    <title>CBXPy</title>
    <fig>
      <caption><p>CBXPy logo.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="CBXPy.png" />
    </fig>
    <p>Most of the
    <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/CBXpy/">CBXPy</ext-link>
    implementation uses basic Python functionality, and the agents are
    handled as an array-like structure. For certain specific features,
    like broadcasting-behaviour, array copying, and index selection, we
    fall back to the <monospace>numpy</monospace> implementation
    (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
    et al., 2020</xref>). However, it should be noted that an adaptation
    to other array or tensor libraries like PyTorch
    (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
    et al., 2019</xref>) is straightforward. Compatibility with the
    latter enables gradient-free deep learning directly on the GPU, as
    demonstrated in the documentation.
    </p>
    <p>The library is available on
    <ext-link ext-link-type="uri" xlink:href="https://github.com/pdips/CBXpy">GitHub</ext-link>
    and can be installed via <monospace>pip</monospace>. It is licensed
    under the MIT license. Below, we provide a short example on how to
    optimise a function with CBXPy.</p>
    <code language="python">from cbx.dynamics import CBO        # import the CBO class
f = lambda x: x[0]**2 + x[1]**2     # define the function to minimise
x = CBO(f, d=2).optimize()          # run the optimisation</code>
    <p>More examples and details on the implementation are available in
    the
    <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/CBXpy/">documentation</ext-link>.</p>
  </sec>
  <sec id="consensusbasedx.jl">
    <title>ConsensusBasedX.jl</title>
    <fig>
      <caption><p>ConsensusBasedX.jl logo.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="CBXjl.png" />
    </fig>
    <p><ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/ConsensusBasedX.jl/">ConsensusBasedX.jl</ext-link>
    has been almost entirely written in native Julia (with the exception
    of a single call to LAPACK). The code has been developed with
    performance in mind, thus the critical routines are fully
    type-stable and allocation-free. A specific tool is provided to
    benchmark a typical method iteration, which can be used to detect
    allocations. Through this tool, unit tests are in place to ensure
    zero allocations in all the provided methods. The benchmarking tool
    is also available to users, who can use it to test their
    implementations of <inline-formula><alternatives>
    <tex-math><![CDATA[f]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>,
    as well as any new CBX methods.</p>
    <p>Basic function minimisation can be performed by running:</p>
    <code language="julia">using ConsensusBasedX               # load the ConsensusBasedX package
f(x) = x[1]^2 + x[2]^2              # define the function to minimise
x = minimise(f, D = 2)              # run the minimisation</code>
    <p>The library is available on
    <ext-link ext-link-type="uri" xlink:href="https://github.com/PdIPS/ConsensusBasedX.jl">GitHub</ext-link>.
    It has been registered in the
    <ext-link ext-link-type="uri" xlink:href="https://github.com/JuliaRegistries/General">general
    Julia registry</ext-link>, and therefore it can be installed by
    running <monospace>]add ConsensusBasedX</monospace>. It is licensed
    under the MIT license. More examples and full instructions are
    available in the
    <ext-link ext-link-type="uri" xlink:href="https://pdips.github.io/ConsensusBasedX.jl/">documentation</ext-link>.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the Lorentz Center in Leiden for their kind hospitality
  during the workshop “Purpose-driven particle systems” in Spring 2023,
  where this work was initiated. RB was supported by the Advanced Grant
  Nonlocal-CPD (Nonlocal PDEs for Complex Particle Dynamics: Phase
  Transitions, Patterns and Synchronisation) of the European Research
  Council Executive Agency (ERC) under the European Union’s Horizon 2020
  research and innovation programme (grant agreement No. 883363) and by
  the EPSRC grant EP/T022132/1 “Spectral element methods for fractional
  differential equations, with applications in applied analysis and
  medical imaging”. KR acknowledges support from the German Federal
  Ministry of Education and Research and the Bavarian State Ministry for
  Science and the Arts. TR acknowledges support from DESY (Hamburg,
  Germany), a member of the Helmholtz Association HGF. This research was
  supported in part through the Maxwell computational resources operated
  at Deutsches Elektronen-Synchrotron DESY, Hamburg, Germany. UV
  acknowledges support from the Agence Nationale de la Recherche under
  grant ANR-23-CE40-0027 (IPSO).</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-boltzmann1868studien">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boltzmann</surname><given-names>Ludwig</given-names></name>
      </person-group>
      <article-title>Studien über das Gleichgewicht der lebendigen Kraft zwischen bewegten materiellen Punkten</article-title>
      <source>Wiener Berichte</source>
      <year iso-8601-date="1868">1868</year>
      <volume>58</volume>
      <fpage>517</fpage>
      <lpage>560</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bayraktar2013wind">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bayraktar</surname><given-names>Zikri</given-names></name>
        <name><surname>Komurcu</surname><given-names>Muge</given-names></name>
        <name><surname>Bossard</surname><given-names>Jeremy A</given-names></name>
        <name><surname>Werner</surname><given-names>Douglas H</given-names></name>
      </person-group>
      <article-title>The wind driven optimization technique and its application in electromagnetics</article-title>
      <source>IEEE transactions on antennas and propagation</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>61</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1109/TAP.2013.2238654</pub-id>
      <fpage>2745</fpage>
      <lpage>2757</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hansen2019pycma">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hansen</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Akimoto</surname><given-names>Youhei</given-names></name>
        <name><surname>Baudis</surname><given-names>Petr</given-names></name>
      </person-group>
      <article-title>CMA-ES/pycma on GitHub</article-title>
      <publisher-name>Zenodo, DOI:10.5281/zenodo.2559634</publisher-name>
      <year iso-8601-date="2019-02">2019</year><month>02</month>
      <uri>10.5281/zenodo.2559634</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.2559634</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hansen1996adapting">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hansen</surname><given-names>Nikolaus</given-names></name>
        <name><surname>Ostermeier</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation</article-title>
      <source>Proceedings of IEEE international conference on evolutionary computation</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="1996">1996</year>
      <pub-id pub-id-type="doi">10.1109/ICEC.1996.542381</pub-id>
      <fpage>312</fpage>
      <lpage>317</lpage>
    </element-citation>
  </ref>
  <ref id="ref-colutto2009cma">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Colutto</surname><given-names>Sebastian</given-names></name>
        <name><surname>Fruhauf</surname><given-names>Florian</given-names></name>
        <name><surname>Fuchs</surname><given-names>Matthias</given-names></name>
        <name><surname>Scherzer</surname><given-names>Otmar</given-names></name>
      </person-group>
      <article-title>The CMA-ES on Riemannian manifolds to reconstruct shapes in 3-d voxel images</article-title>
      <source>IEEE Transactions on Evolutionary Computation</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>14</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/TEVC.2009.2029567</pub-id>
      <fpage>227</fpage>
      <lpage>245</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kennedy1995particle">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kennedy</surname><given-names>James</given-names></name>
        <name><surname>Eberhart</surname><given-names>Russell</given-names></name>
      </person-group>
      <article-title>Particle swarm optimization</article-title>
      <source>Proceedings of ICNN’95-international conference on neural networks</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="1995">1995</year>
      <volume>4</volume>
      <pub-id pub-id-type="doi">10.1109/ICNN.1995.488968</pub-id>
      <fpage>1942</fpage>
      <lpage>1948</lpage>
    </element-citation>
  </ref>
  <ref id="ref-henderson2003theory">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Henderson</surname><given-names>Darrall</given-names></name>
        <name><surname>Jacobson</surname><given-names>Sheldon H</given-names></name>
        <name><surname>Johnson</surname><given-names>Alan W</given-names></name>
      </person-group>
      <article-title>The theory and practice of simulated annealing</article-title>
      <source>Handbook of metaheuristics</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <pub-id pub-id-type="doi">10.1007/0-306-48056-5_10</pub-id>
      <fpage>287</fpage>
      <lpage>319</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rastrigin1963convergence">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rastrigin</surname><given-names>LA</given-names></name>
      </person-group>
      <article-title>The convergence of the random search method in the extremal control of a many parameter system</article-title>
      <source>Automaton &amp; Remote Control</source>
      <year iso-8601-date="1963">1963</year>
      <volume>24</volume>
      <fpage>1337</fpage>
      <lpage>1342</lpage>
    </element-citation>
  </ref>
  <ref id="ref-friedman1947planning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friedman</surname><given-names>Milton</given-names></name>
        <name><surname>Savage</surname><given-names>Lo J</given-names></name>
      </person-group>
      <article-title>Planning experiments seeking maxima</article-title>
      <source>Techniques of statistical analysis</source>
      <publisher-name>McGraw-Hill: NY</publisher-name>
      <year iso-8601-date="1947">1947</year>
      <fpage>365</fpage>
      <lpage>372</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hooke1961direct">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hooke</surname><given-names>Robert</given-names></name>
        <name><surname>Jeeves</surname><given-names>Terry A</given-names></name>
      </person-group>
      <article-title>“Direct search” solution of numerical and statistical problems</article-title>
      <source>Journal of the ACM (JACM)</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="1961">1961</year>
      <volume>8</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1145/321062.321069</pub-id>
      <fpage>212</fpage>
      <lpage>229</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fornasier2021consensus">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Fornasier</surname><given-names>Massimo</given-names></name>
        <name><surname>Klock</surname><given-names>Timo</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>Consensus-based optimization methods converge globally</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2103.15130</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2103.15130</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-carrillo2022consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carrillo</surname><given-names>J A</given-names></name>
        <name><surname>Hoffmann</surname><given-names>Franca</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
        <name><surname>Vaes</surname><given-names>Urbain</given-names></name>
      </person-group>
      <article-title>Consensus-based sampling</article-title>
      <source>Studies in Applied Mathematics</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>148</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1111/sapm.12470</pub-id>
      <fpage>1069</fpage>
      <lpage>1140</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris2020array">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>Stéfan J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Kerkwijk</surname><given-names>Marten H. van</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>Río</surname><given-names>Jaime Fernández del</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Gérard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <volume>585</volume>
      <issue>7825</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gpyopt2016">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>The GPyOpt authors</string-name>
      </person-group>
      <article-title>GPyOpt: A Bayesian optimization framework in Python</article-title>
      <publisher-name>http://github.com/SheffieldML/GPyOpt</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-balandat2020botorch">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Balandat</surname><given-names>Maximilian</given-names></name>
        <name><surname>Karrer</surname><given-names>Brian</given-names></name>
        <name><surname>Jiang</surname><given-names>Daniel R.</given-names></name>
        <name><surname>Daulton</surname><given-names>Samuel</given-names></name>
        <name><surname>Letham</surname><given-names>Benjamin</given-names></name>
        <name><surname>Wilson</surname><given-names>Andrew Gordon</given-names></name>
        <name><surname>Bakshy</surname><given-names>Eytan</given-names></name>
      </person-group>
      <article-title>BoTorch: A framework for efficient Monte-Carlo Bayesian optimization</article-title>
      <source>Advances in neural information processing systems 33</source>
      <year iso-8601-date="2020">2020</year>
      <uri>http://arxiv.org/abs/1910.06403</uri>
    </element-citation>
  </ref>
  <ref id="ref-GPflowOpt2017">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Knudde</surname><given-names>Nicolas</given-names></name>
        <name><surname>van der Herten</surname><given-names>Joachim</given-names></name>
        <name><surname>Dhaene</surname><given-names>Tom</given-names></name>
        <name><surname>Couckuyt</surname><given-names>Ivo</given-names></name>
      </person-group>
      <article-title>GPflowOpt: A Bayesian Optimization Library using TensorFlow</article-title>
      <year iso-8601-date="2017">2017</year>
      <uri>https://arxiv.org/abs/1711.03845</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1711.03845</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Jiménez2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jiménez</surname><given-names>José</given-names></name>
        <name><surname>Ginebra</surname><given-names>Josep</given-names></name>
      </person-group>
      <article-title>pyGPGO: Bayesian optimization for Python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>2</volume>
      <issue>19</issue>
      <uri>10.21105/joss.00431</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00431</pub-id>
      <fpage>431</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bayesian14">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Nogueira</surname><given-names>Fernando</given-names></name>
      </person-group>
      <article-title>Bayesian Optimization: Open source constrained global optimization tool for Python</article-title>
      <uri> https://github.com/bayesian-optimization/BayesianOptimization</uri>
    </element-citation>
  </ref>
  <ref id="ref-scikitopt">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Guo</surname><given-names>Fei</given-names></name>
      </person-group>
      <article-title>scikit-opt</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/guofei9987/scikit-opt</uri>
    </element-citation>
  </ref>
  <ref id="ref-Kim2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kim</surname><given-names>Jungtaek</given-names></name>
        <name><surname>Choi</surname><given-names>Seungjin</given-names></name>
      </person-group>
      <article-title>BayesO: A Bayesian optimization framework in Python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>90</issue>
      <pub-id pub-id-type="doi">10.21105/joss.05320</pub-id>
      <fpage>5320</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-deapJMLR2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fortin</surname><given-names>Félix-Antoine</given-names></name>
        <name><surname>De Rainville</surname><given-names>François-Michel</given-names></name>
        <name><surname>Gardner</surname><given-names>Marc-André</given-names></name>
        <name><surname>Parizeau</surname><given-names>Marc</given-names></name>
        <name><surname>Gagné</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>DEAP: Evolutionary algorithms made easy</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2012-07">2012</year><month>07</month>
      <volume>13</volume>
      <fpage>2171</fpage>
      <lpage>2175</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pagmo2017">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Biscani</surname><given-names>Francesco</given-names></name>
        <name><surname>Izzo</surname><given-names>Dario</given-names></name>
        <name><surname>Märtens</surname><given-names>Marcus</given-names></name>
      </person-group>
      <article-title>Esa/pagmo2: Pagmo 2.6</article-title>
      <year iso-8601-date="2017-11">2017</year><month>11</month>
      <pub-id pub-id-type="doi">10.5281/zenodo.1054110</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Igor_CBOinPython">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Tukh</surname><given-names>Igor</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>CBO-in-python</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/Igor-Tukh/cbo-in-python</uri>
    </element-citation>
  </ref>
  <ref id="ref-Roith_polarcbo">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Roith</surname><given-names>Tim</given-names></name>
        <name><surname>Bungert</surname><given-names>Leon</given-names></name>
        <name><surname>Wacker</surname><given-names>Philipp</given-names></name>
      </person-group>
      <article-title>polarcbo</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/PdIPS/polarcbo</uri>
    </element-citation>
  </ref>
  <ref id="ref-duan2023pypop7">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Duan</surname><given-names>Qiqi</given-names></name>
        <name><surname>Zhou</surname><given-names>Guochen</given-names></name>
        <name><surname>Shao</surname><given-names>Chang</given-names></name>
        <name><surname>Wang</surname><given-names>Zhuowei</given-names></name>
        <name><surname>Feng</surname><given-names>Mingyang</given-names></name>
        <name><surname>Yang</surname><given-names>Yijun</given-names></name>
        <name><surname>Zhao</surname><given-names>Qi</given-names></name>
        <name><surname>Shi</surname><given-names>Yuhui</given-names></name>
      </person-group>
      <article-title>PyPop7: A pure-Python library for population-based black-box optimization</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2212.05652</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2212.05652</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bungert2022polarized">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bungert</surname><given-names>Leon</given-names></name>
        <name><surname>Roith</surname><given-names>Tim</given-names></name>
        <name><surname>Wacker</surname><given-names>Philipp</given-names></name>
      </person-group>
      <article-title>Polarized consensus-based dynamics for optimization and sampling</article-title>
      <source>Math. Program.</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2024-05">2024</year><month>05</month>
      <pub-id pub-id-type="doi">10.1007/s10107-024-02095-y</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Bergmann2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bergmann</surname><given-names>Ronny</given-names></name>
      </person-group>
      <article-title>Manopt.jl: Optimization on manifolds in Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>70</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03866</pub-id>
      <fpage>3866</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bailo_consensus">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bailo</surname><given-names>Rafael</given-names></name>
      </person-group>
      <article-title>Consensus.jl</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/rafaelbailo/Consensus.jl</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.7754236</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Singh2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Singh</surname><given-names>Gurjeet Sangra</given-names></name>
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>PyBADS: Fast and robust black-box optimization in Python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>9</volume>
      <issue>94</issue>
      <uri>10.21105/joss.05694</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05694</pub-id>
      <fpage>5694</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-carrillo2018analytical">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carrillo</surname><given-names>J A</given-names></name>
        <name><surname>Choi</surname><given-names>Young-Pil</given-names></name>
        <name><surname>Totzeck</surname><given-names>Claudia</given-names></name>
        <name><surname>Tse</surname><given-names>Oliver</given-names></name>
      </person-group>
      <article-title>An analytical framework for consensus-based global optimization method</article-title>
      <source>Mathematical Models and Methods in Applied Sciences</source>
      <year iso-8601-date="2018">2018</year>
      <volume>28</volume>
      <issue>6</issue>
      <issn>0218-2025</issn>
      <pub-id pub-id-type="doi">10.1142/S0218202518500276</pub-id>
      <fpage>1037</fpage>
      <lpage>1066</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-riedl2022leveraging">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>Leveraging memory effects and gradient information in consensus-based optimisation: On global convergence in mean-field law</article-title>
      <source>European Journal of Applied Mathematics</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1017/S0956792523000293</pub-id>
      <fpage>1</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-qiu2022PSOconvergence">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Hui</given-names></name>
        <name><surname>Qiu</surname><given-names>Jinniao</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>On the global convergence of particle swarm optimization methods</article-title>
      <source>Applied Mathematics &amp; Optimization</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>88</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1007/s00245-023-09983-3</pub-id>
      <fpage>30</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-grassi2020particle">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grassi</surname><given-names>Sara</given-names></name>
        <name><surname>Pareschi</surname><given-names>Lorenzo</given-names></name>
      </person-group>
      <article-title>From particle swarm optimization to consensus based optimization: Stochastic modeling and mean-field limit</article-title>
      <source>Mathematical Models and Methods in Applied Sciences</source>
      <year iso-8601-date="2021">2021</year>
      <volume>31</volume>
      <issue>8</issue>
      <issn>0218-2025</issn>
      <pub-id pub-id-type="doi">10.1142/S0218202521500342</pub-id>
      <fpage>1625</fpage>
      <lpage>1657</lpage>
    </element-citation>
  </ref>
  <ref id="ref-riedl2023gradient">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
        <name><surname>Klock</surname><given-names>Timo</given-names></name>
        <name><surname>Geldhauser</surname><given-names>Carina</given-names></name>
        <name><surname>Fornasier</surname><given-names>Massimo</given-names></name>
      </person-group>
      <article-title>Gradient is all you need?</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2306.09778</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2306.09778</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-carrillo2023fedcbo">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Carrillo</surname><given-names>J A</given-names></name>
        <name><surname>Garcia Trillos</surname><given-names>Nicolas</given-names></name>
        <name><surname>Li</surname><given-names>Sixu</given-names></name>
        <name><surname>Zhu</surname><given-names>Yuhua</given-names></name>
      </person-group>
      <article-title>FedCBO: Reaching group consensus in clustered federated learning through consensus-based optimization</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2305.02894</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2305.02894</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-borghi2022adaptive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Borghi</surname><given-names>Giacomo</given-names></name>
        <name><surname>Herty</surname><given-names>Michael</given-names></name>
        <name><surname>Pareschi</surname><given-names>Lorenzo</given-names></name>
      </person-group>
      <article-title>An adaptive consensus based method for multi-objective optimization with uniform Pareto front approximation</article-title>
      <source>Applied Mathematics &amp; Optimization</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>88</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1007/s00245-023-10036-y</pub-id>
      <fpage>1</fpage>
      <lpage>43</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fornasier2020consensus_sphere_convergence">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fornasier</surname><given-names>Massimo</given-names></name>
        <name><surname>Huang</surname><given-names>Hui</given-names></name>
        <name><surname>Pareschi</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Sünnen</surname><given-names>Philippe</given-names></name>
      </person-group>
      <article-title>Consensus-based optimization on the sphere: Convergence to global minimizers and machine learning</article-title>
      <source>Journal of Machine Learning Research (JMLR)</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issn>1532-4435</issn>
      <fpage>Paper No. 237, 55</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-borghi2021constrained">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Borghi</surname><given-names>Giacomo</given-names></name>
        <name><surname>Herty</surname><given-names>Michael</given-names></name>
        <name><surname>Pareschi</surname><given-names>Lorenzo</given-names></name>
      </person-group>
      <article-title>Constrained consensus-based optimization</article-title>
      <source>SIAM Journal on Optimization</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>33</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1137/22M1471304</pub-id>
      <fpage>211</fpage>
      <lpage>236</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huang2021MFLCBO">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Hui</given-names></name>
        <name><surname>Qiu</surname><given-names>Jinniao</given-names></name>
      </person-group>
      <article-title>On the mean-field limit for the consensus-based optimization</article-title>
      <source>Mathematical Methods in the Applied Sciences</source>
      <year iso-8601-date="2022">2022</year>
      <volume>45</volume>
      <issue>12</issue>
      <issn>0170-4214</issn>
      <pub-id pub-id-type="doi">10.1002/mma.8279</pub-id>
      <fpage>7814</fpage>
      <lpage>7831</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huang2022consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Hui</given-names></name>
        <name><surname>Qiu</surname><given-names>Jinniao</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>Consensus-based optimization for saddle point problems</article-title>
      <source>SIAM Journal on Control and Optimization</source>
      <year iso-8601-date="2024">2024</year>
      <volume>62</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1137/22M1543367</pub-id>
      <fpage>1093</fpage>
      <lpage>1121</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fornasier2023consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fornasier</surname><given-names>Massimo</given-names></name>
        <name><surname>Richtárik</surname><given-names>Peter</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
        <name><surname>Sun</surname><given-names>Lukang</given-names></name>
      </person-group>
      <article-title>Consensus-based optimisation with truncated noise</article-title>
      <source>European Journal of Applied Mathematics</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1017/S095679252400007X</pub-id>
      <fpage>1</fpage>
      <lpage>24</lpage>
    </element-citation>
  </ref>
  <ref id="ref-althaus2023consensus">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Althaus</surname><given-names>Konstantin</given-names></name>
        <name><surname>Papaioannou</surname><given-names>Iason</given-names></name>
        <name><surname>Ullmann</surname><given-names>Elisabeth</given-names></name>
      </person-group>
      <article-title>Consensus-based rare event estimation</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2304.09077</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2304.09077</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-DR2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Dixit</surname><given-names>Vaibhav Kumar</given-names></name>
        <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>Optimization.jl: A unified optimization package</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.5281/ZENODO.7738525</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-KP1992">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Kloeden</surname><given-names>Peter E.</given-names></name>
        <name><surname>Platen</surname><given-names>Eckhard</given-names></name>
      </person-group>
      <source>Numerical solution of stochastic differential equations</source>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <year iso-8601-date="1992">1992</year>
      <isbn>9783662126165</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-662-12616-5</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pinnau2017consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pinnau</surname><given-names>René</given-names></name>
        <name><surname>Totzeck</surname><given-names>Claudia</given-names></name>
        <name><surname>Tse</surname><given-names>Oliver</given-names></name>
        <name><surname>Martin</surname><given-names>Stephan</given-names></name>
      </person-group>
      <article-title>A consensus-based model for global optimization and its mean-field limit</article-title>
      <source>Mathematical Models and Methods in Applied Sciences</source>
      <publisher-name>World Scientific</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>27</volume>
      <issue>01</issue>
      <issn>1793-6314</issn>
      <pub-id pub-id-type="doi">10.1142/s0218202517400061</pub-id>
      <fpage>183</fpage>
      <lpage>204</lpage>
    </element-citation>
  </ref>
  <ref id="ref-movckus1975bayesian">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Močkus</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>On Bayesian methods for seeking the extremum</article-title>
      <source>Optimization techniques IFIP technical conference</source>
      <publisher-name>Springer; Springer Berlin Heidelberg</publisher-name>
      <year iso-8601-date="1975">1975</year>
      <isbn>9783662385272</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-662-38527-2_55</pub-id>
      <fpage>400</fpage>
      <lpage>404</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mohan2012survey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chandra Mohan</surname><given-names>B.</given-names></name>
        <name><surname>Baskaran</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>A survey: Ant colony optimization based recent research and implementation on several engineering domain</article-title>
      <source>Expert Systems with Applications</source>
      <publisher-name>Elsevier BV</publisher-name>
      <year iso-8601-date="2012-03">2012</year><month>03</month>
      <volume>39</volume>
      <issue>4</issue>
      <issn>0957-4174</issn>
      <pub-id pub-id-type="doi">10.1016/j.eswa.2011.09.076</pub-id>
      <fpage>4618</fpage>
      <lpage>4627</lpage>
    </element-citation>
  </ref>
  <ref id="ref-karaboga2014comprehensive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Karaboga</surname><given-names>Dervis</given-names></name>
        <name><surname>Gorkemli</surname><given-names>Beyza</given-names></name>
        <name><surname>Ozturk</surname><given-names>Celal</given-names></name>
        <name><surname>Karaboga</surname><given-names>Nurhan</given-names></name>
      </person-group>
      <article-title>A comprehensive survey: Artificial bee colony (ABC) algorithm and applications</article-title>
      <source>Artificial intelligence review</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2012-03">2012</year><month>03</month>
      <volume>42</volume>
      <issue>1</issue>
      <issn>1573-7462</issn>
      <pub-id pub-id-type="doi">10.1007/s10462-012-9328-0</pub-id>
      <fpage>21</fpage>
      <lpage>57</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yang2009firefly">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Xin-She</given-names></name>
      </person-group>
      <article-title>Firefly algorithms for multimodal optimization</article-title>
      <source>Lecture notes in computer science</source>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <isbn>9783642049446</isbn>
      <issn>1611-3349</issn>
      <pub-id pub-id-type="doi">10.1007/978-3-642-04944-6_14</pub-id>
      <fpage>169</fpage>
      <lpage>178</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ackley2012connectionist">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ackley</surname><given-names>David H.</given-names></name>
      </person-group>
      <source>A connectionist machine for genetic hillclimbing</source>
      <publisher-name>Springer US</publisher-name>
      <year iso-8601-date="1987">1987</year>
      <volume>28</volume>
      <isbn>9781461319979</isbn>
      <issn>0893-3405</issn>
      <pub-id pub-id-type="doi">10.1007/978-1-4613-1997-9</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-miranda2018pyswarms">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Miranda</surname><given-names>Lester James</given-names></name>
      </person-group>
      <article-title>PySwarms: A research toolkit for particle swarm optimization in Python</article-title>
      <source>The Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018-01">2018</year><month>01</month>
      <volume>3</volume>
      <issue>21</issue>
      <issn>2475-9066</issn>
      <pub-id pub-id-type="doi">10.21105/joss.00433</pub-id>
      <fpage>433</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-mogensen2018optim">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mogensen</surname><given-names>P</given-names></name>
        <name><surname>Riseth</surname><given-names>A</given-names></name>
      </person-group>
      <article-title>Optim: A mathematical optimization package for Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018-04">2018</year><month>04</month>
      <volume>3</volume>
      <issue>24</issue>
      <issn>2475-9066</issn>
      <pub-id pub-id-type="doi">10.21105/joss.00615</pub-id>
      <fpage>615</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-mejia2022metaheuristics">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mejı́a-de-Dios</surname><given-names>Jesús-Adolfo</given-names></name>
        <name><surname>Mezura-Montes</surname><given-names>Efrén</given-names></name>
      </person-group>
      <article-title>Metaheuristics: A Julia package for single-and multi-objective optimization</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022-10">2022</year><month>10</month>
      <volume>7</volume>
      <issue>78</issue>
      <issn>2475-9066</issn>
      <pub-id pub-id-type="doi">10.21105/joss.04723</pub-id>
      <fpage>4723</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-carrillo2021consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carrillo</surname><given-names>J A</given-names></name>
        <name><surname>Jin</surname><given-names>Shi</given-names></name>
        <name><surname>Li</surname><given-names>Lei</given-names></name>
        <name><surname>Zhu</surname><given-names>Yuhua</given-names></name>
      </person-group>
      <article-title>A consensus-based global optimization method for high dimensional machine learning problems</article-title>
      <source>ESAIM: Control, Optimisation and Calculus of Variations</source>
      <publisher-name>EDP Sciences</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>27</volume>
      <issn>1262-3377</issn>
      <pub-id pub-id-type="doi">10.1051/cocv/2020046</pub-id>
      <fpage>S5</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-fornasier2021convergence">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Fornasier</surname><given-names>Massimo</given-names></name>
        <name><surname>Klock</surname><given-names>Timo</given-names></name>
        <name><surname>Riedl</surname><given-names>Konstantin</given-names></name>
      </person-group>
      <article-title>Convergence of anisotropic consensus-based optimization in mean-field law</article-title>
      <source>Applications of evolutionary computation</source>
      <person-group person-group-type="editor">
        <name><surname>Jiménez Laredo</surname><given-names>Juan Luis</given-names></name>
        <name><surname>Hidalgo</surname><given-names>J. Ignacio</given-names></name>
        <name><surname>Babaagba</surname><given-names>Kehinde Oluwatoyin</given-names></name>
      </person-group>
      <publisher-name>Springer</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>978-3-031-02462-7</isbn>
      <issn>1611-3349</issn>
      <pub-id pub-id-type="doi">10.1007/978-3-031-02462-7_46</pub-id>
      <fpage>738</fpage>
      <lpage>754</lpage>
    </element-citation>
  </ref>
  <ref id="ref-klamroth2022consensus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Klamroth</surname><given-names>Kathrin</given-names></name>
        <name><surname>Stiglmayr</surname><given-names>Michael</given-names></name>
        <name><surname>Totzeck</surname><given-names>Claudia</given-names></name>
      </person-group>
      <article-title>Consensus-based optimization for multi-objective problems: A multi-swarm approach</article-title>
      <source>Journal of Global Optimization</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2024-02">2024</year><month>02</month>
      <issn>1573-2916</issn>
      <pub-id pub-id-type="doi">10.1007/s10898-024-01369-1</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
