<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6849</article-id>
<article-id pub-id-type="doi">10.21105/joss.06849</article-id>
<title-group>
<article-title>Sports2D: Compute 2D human pose and angles from a video
or a webcam</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6891-8331</contrib-id>
<name>
<surname>Pagnon</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-7710-8051</contrib-id>
<name>
<surname>Kim</surname>
<given-names>HunMin</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Centre for the Analysis of Motion, Entertainment Research
&amp; Applications (CAMERA), University of Bath, Claverton Down, Bath,
BA2 7AY, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Inha University, Yonghyeon Campus, 100 Inha-ro,
Michuhol-gu, Incheon 22212, South Korea</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<volume>9</volume>
<issue>101</issue>
<fpage>6849</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>markerless kinematics</kwd>
<kwd>motion capture</kwd>
<kwd>sports performance analysis</kwd>
<kwd>rtmpose</kwd>
<kwd>clinical gait analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Sports2D</monospace> provides a user-friendly solution
  for automatic and real-time analysis of multi-person human movement
  from a video or a webcam. This Python package uses 2D markerless pose
  estimation to detect joint coordinates from videos, and then computes
  2D joint and segment angles. </p>
  <p>The output incorporates annotated videos and image sequences
  overlaid with joint locations, joint angles, and segment angles, for
  each of the detected persons. For further analysis, this information
  is also stored in files that are editable with MS Excel® or any other
  spreadsheet editor (.trc for locations, .mot for angles, according to
  the OpenSim standard
  (<xref alt="Delp et al., 2007" rid="ref-Delp_2007" ref-type="bibr">Delp
  et al., 2007</xref>;
  <xref alt="Seth, 2018" rid="ref-Seth_2018" ref-type="bibr">Seth,
  2018</xref>)).</p>
  <p><monospace>Sports2D</monospace> may be useful for clinicians as a
  decision supports system (CDSS)
  (<xref alt="Bright et al., 2012" rid="ref-Bright_2012" ref-type="bibr">Bright
  et al., 2012</xref>), as well as for gait analysis
  (<xref alt="Whittle, 2014" rid="ref-Whittle_2014" ref-type="bibr">Whittle,
  2014</xref>) or ergonomic design
  (<xref alt="Patrizi et al., 2016" rid="ref-Patrizi_2016" ref-type="bibr">Patrizi
  et al., 2016</xref>). Sports coaches can also use it to quantify key
  performance indicators (KPIs)
  (<xref alt="O’Donoghue, 2008" rid="ref-ODonoghue_2008" ref-type="bibr">O’Donoghue,
  2008</xref>;
  <xref alt="Pagnon, Domalain, Robert, et al., 2022" rid="ref-Pagnon_2022b" ref-type="bibr">Pagnon,
  Domalain, Robert, et al., 2022</xref>), or to better understand,
  correct, or compare athletes’ movement patterns. Finally, it can be
  used by researchers as a simple tool for 2D biomechanical analysis on
  the fly. One of the multiple use cases would be to evaluate ACL injury
  risks from deceleration drills
  (<xref alt="Di Paolo et al., 2021" rid="ref-Di_2021" ref-type="bibr">Di
  Paolo et al., 2021</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Machine learning has recently accelerated the development and
  availability of markerless kinematics
  (<xref alt="Colyer et al., 2018" rid="ref-Colyer_2018" ref-type="bibr">Colyer
  et al., 2018</xref>;
  <xref alt="Zheng et al., 2023" rid="ref-Zheng_2023" ref-type="bibr">Zheng
  et al., 2023</xref>), which allows for the collection of kinematic
  data without the use of physical markers or manual annotation.</p>
  <p>A large part of these tools focus on 2D analysis, such as
  <monospace>OpenPose</monospace>
  (<xref alt="Cao et al., 2019" rid="ref-Cao_2019" ref-type="bibr">Cao
  et al., 2019</xref>), <monospace>BlazePose</monospace>
  (<xref alt="Bazarevsky et al., 2020" rid="ref-Bazarevsky_2020" ref-type="bibr">Bazarevsky
  et al., 2020</xref>), or <monospace>DeepLabCut</monospace>
  (<xref alt="Mathis et al., 2018" rid="ref-Mathis_2018" ref-type="bibr">Mathis
  et al., 2018</xref>). More recently, <monospace>RTMPose</monospace>
  (<xref alt="Jiang et al., 2023" rid="ref-Jiang_2023" ref-type="bibr">Jiang
  et al., 2023</xref>) offered a faster, more accurate, and more
  flexible alternative to the previous solutions. Still, although they
  bear the advantage of being open-source, none of these options are
  easily accessible to people who do not have a programming background,
  and the output is not directly usable for further kinematic
  investigation. Yet, clinical acceptance of new technologies is known
  to be influenced not only by their price value and their performance,
  but also by their perceived ease-of-use, the social influence around
  the customer, and other parameters described by the Unified Theory of
  Acceptance and Use of Technology (UTAUT2)
  (<xref alt="Venkatesh et al., 2012" rid="ref-Venkatesh_2012" ref-type="bibr">Venkatesh
  et al., 2012</xref>).</p>
  <fig>
    <caption><p>Example results from a demonstration
    video.<styled-content id="figU003ADemo_results"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="Demo_results.png" />
  </fig>
  <fig>
    <caption><p>Example joint angle
    output.<styled-content id="figU003ADemo_plots"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="Demo_plots.png" />
  </fig>
  <p>In fact, there is a clear trade-off between accuracy and
  ease-of-use. Some open-source tools focus on the accuracy of a 3D
  analysis by using multiple cameras, such as
  <monospace>Pose2Sim</monospace>
  (<xref alt="Pagnon, Domalain, &amp; Reveret, 2022" rid="ref-Pagnon_2022a" ref-type="bibr">Pagnon,
  Domalain, &amp; Reveret, 2022</xref>) or
  <monospace>OpenCap</monospace>
  (<xref alt="Uhlrich et al., 2022" rid="ref-Uhlrich_2022" ref-type="bibr">Uhlrich
  et al., 2022</xref>). These, however, require either a certain level
  of programming skills, a particular hardware setup, or to send data to
  a server that does not comply with the European rules of data
  protection (GDPR). Some other tools choose to put more emphasis on
  user-friendliness, and point out that 2D analysis is often sufficient
  when the analyzed motion mostly lies in the sagittal or frontal plane.
  <monospace>Sit2Stand</monospace>
  (<xref alt="Boswell et al., 2023" rid="ref-Boswell_2023" ref-type="bibr">Boswell
  et al., 2023</xref>) and <monospace>CP GaitLab</monospace>
  (<xref alt="Kidziński et al., 2020" rid="ref-Kidzinski_2020" ref-type="bibr">Kidziński
  et al., 2020</xref>) provide such tools, although they are focused on
  very specific tasks. <monospace>Kinovea</monospace>
  (<xref alt="Kinovea" rid="ref-Kinovea" ref-type="bibr">Kinovea</xref>),
  on the other hand, is a widely used software for sports performance
  analysis, which provides multiple additional features. However, it
  relies on tracking manual labels. This can be time-consuming when
  analyzing numerous videos, and it may also be lacking robustness when
  the tracked points are lost. It is also only available on Windows, and
  requires the user to transfer files prior to analysis.</p>
  <p><monospace>Sports2D</monospace> is an alternative solution that
  aims at filling this gap: it is free and open-source, straightforward
  to install and to run, can be run on any platform, can be run locally
  for data protection, and it automatically provides 2D joint and
  segment angles without the need for manual annotation. It is also
  robust and flexible, works in real-time and can be used to process one
  video, several videos at once, or webcam stream. Multi-person analysis
  is available, and the output is directly usable for further
  statistical analysis.</p>
</sec>
<sec id="workflow">
  <title>Workflow</title>
  <sec id="installation-and-usage">
    <title>Installation and usage</title>
    <p><monospace>Sports2d</monospace> is installed under Python via
    <monospace>pip install sports2d</monospace>. If a valid CUDA
    installation is found, Sports2D uses the GPU, otherwise it uses the
    CPU with OpenVino acceleration.</p>
    <p>A detailed installation and usage guide can be found on the
    repository: https://github.com/davidpagnon/Sports2D.</p>
  </sec>
  <sec id="sports2d-method-details">
    <title>Sports2D method details</title>
    <p><underline>Sports2D</underline>:</p>
    <list list-type="order">
      <list-item>
        <p>Reads stream from a webcam, from one video, or from a list of
        videos. It selects an optional specified time range to
        process.</p>
      </list-item>
      <list-item>
        <p>Sets up the RTMLib pose tracker with specified parameters. It
        can be run in lightweight, balanced, or performance mode, and
        for faster inference, keypoints can be tracked for a certain
        number of frames instead of detected. Any RTMPose model can be
        used.</p>
      </list-item>
      <list-item>
        <p>Tracks people so that their IDs are consistent across frames.
        A person is associated to another in the next frame when they
        are at a small distance. IDs remain consistent even if the
        person disappears for a few frames. This carefully crafted
        <monospace>sports2d</monospace> tracker runs at a comparable
        speed as the RTMlib one but is much more robust. The user can
        still choose the RTMLib method if they need it by using the
        <monospace>tracking_mode</monospace> argument.</p>
      </list-item>
      <list-item>
        <p>Retrieves the keypoints with high enough confidence, and only
        keeps the persons with enough average high-confidence.</p>
      </list-item>
      <list-item>
        <p>Computes the selected joint and segment angles, and flips
        them on the left/right side if the respective foot is pointing
        to the left/right. The user can select which angles they want to
        compute, display, and save.</p>
      </list-item>
      <list-item>
        <p>Draws bounding boxes around each person and writes their IDs
        Draws the skeleton and the keypoints, with a green to red color
        scale to account for their confidence
        Draws joint and segment angles on the body, and writes the
        values either near the joint/segment, or on the upper-left of
        the image with a progress bar</p>
      </list-item>
      <list-item>
        <p>Interpolates missing pose and angle sequences if gaps are not
        too large. Filters them with the selected filter (among
        <monospace>Butterworth</monospace>,
        <monospace>Gaussian</monospace>, <monospace>LOESS</monospace>,
        or <monospace>Median</monospace>) and their parameters</p>
      </list-item>
      <list-item>
        <p>Optionally shows processed images, saves them, or saves them
        as a video
        Optionally plots pose and angle data before and after processing
        for comparison
        Optionally saves poses for each person as a TRC file, and angles
        as a MOT file</p>
      </list-item>
    </list>
    <p></p>
    <p><underline>**The Demo video</underline> that Sports2D is tested
    on is voluntarily challenging, in order to demonstrate the
    robustness of the process after sorting, interpolation and
    filtering. It contains:</p>
    <list list-type="bullet">
      <list-item>
        <p>One person walking in the sagittal plane</p>
      </list-item>
      <list-item>
        <p>One person in the frontal plane. This person then performs a
        flip while being backlit, both of which are challenging for the
        pose detection algorithm</p>
      </list-item>
      <list-item>
        <p>One tiny person flickering in the background who needs to be
        ignored</p>
      </list-item>
    </list>
    <p></p>
    <p><underline>Joint and segment angle estimation</underline>:</p>
    <p>Specific joint and segment angles can be chosen. They are
    consistent regardless of the direction the participant is facing:
    the participant is considered to look to the left when their toes
    are to the left of their heels, and to the right otherwise.
    Resulting angles can be filtered in the same way as point
    coordinates, and they can also be plotted.</p>
    <p>Joint angle conventions are as follows
    (<xref alt="[fig:joint_angle_conventions]" rid="figU003Ajoint_angle_conventions">[fig:joint_angle_conventions]</xref>):</p>
    <list list-type="bullet">
      <list-item>
        <p>Ankle dorsiflexion: Between heel and big toe, and ankle and
        knee.
        <italic>-90° when the foot is aligned with the
        shank.</italic></p>
      </list-item>
      <list-item>
        <p>Knee flexion: Between hip, knee, and ankle.
        <italic>0° when the shank is aligned with the
        thigh.</italic></p>
      </list-item>
      <list-item>
        <p>Hip flexion: Between knee, hip, and shoulder.
        <italic>0° when the trunk is aligned with the
        thigh.</italic></p>
      </list-item>
      <list-item>
        <p>Shoulder flexion: Between hip, shoulder, and elbow.
        <italic>180° when the arm is aligned with the
        trunk.</italic></p>
      </list-item>
      <list-item>
        <p>Elbow flexion: Between wrist, elbow, and shoulder.
        <italic>0° when the forearm is aligned with the
        arm.</italic></p>
      </list-item>
    </list>
    <p>Segment angles are measured anticlockwise between the horizontal
    and the segment lines:</p>
    <list list-type="bullet">
      <list-item>
        <p>Foot: Between heel and big toe.</p>
      </list-item>
      <list-item>
        <p>Shank: Between knee and ankle.</p>
      </list-item>
      <list-item>
        <p>Thigh: Between hip and knee.</p>
      </list-item>
      <list-item>
        <p>Pelvis: Between left and right hip</p>
      </list-item>
      <list-item>
        <p>Trunk: Between hip midpoint and shoulder midpoint</p>
      </list-item>
      <list-item>
        <p>Shoulders: Between left and right shoulder</p>
      </list-item>
      <list-item>
        <p>Head: Between neck and top of the head</p>
      </list-item>
      <list-item>
        <p>Arm: Between shoulder and elbow.</p>
      </list-item>
      <list-item>
        <p>Forearm: Between elbow and wrist.</p>
      </list-item>
    </list>
    <fig>
      <caption><p>Joint angle
      conventions<styled-content id="figU003Ajoint_angle_conventions"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="joint_convention.png" />
    </fig>
  </sec>
</sec>
<sec id="limitations">
  <title>Limitations</title>
  <p>The user of <monospace>Sports2D</monospace> should be aware of the
  following limitations:</p>
  <list list-type="bullet">
    <list-item>
      <p>Results are acceptable only if the participants move in the 2D
      plane, either in the frontal plane or in the sagittal one. If you
      need research-grade markerless joint kinematics, consider using
      several cameras, and constraining angles to a biomechanically
      accurate model. See <monospace>Pose2Sim</monospace>
      (<xref alt="Pagnon, Domalain, &amp; Reveret, 2022" rid="ref-Pagnon_2022a" ref-type="bibr">Pagnon,
      Domalain, &amp; Reveret, 2022</xref>) for example.</p>
    </list-item>
    <list-item>
      <p>Angle estimation is only as good as the pose estimation
      algorithm, i.e., it is not perfect
      (<xref alt="Wade et al., 2022" rid="ref-Wade_2022" ref-type="bibr">Wade
      et al., 2022</xref>), especially if motion blur is significant
      such as on some broadcast videos. </p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>I would like to acknowledge Rob Olivar, a sports coach who
  enlightened me about the need for such a tool.
  I also acknowledge the work of the dedicated people involved in the
  many major open-source software programs and packages used by
  <monospace>Sports2D</monospace>, such as
  <monospace>Python</monospace>, <monospace>RTMPPose</monospace>,
  <monospace>OpenCV</monospace>
  (<xref alt="Bradski, 2000" rid="ref-Bradski_2000" ref-type="bibr">Bradski,
  2000</xref>), among others.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Bazarevsky_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bazarevsky</surname><given-names>Valentin</given-names></name>
        <name><surname>Grishchenko</surname><given-names>Ivan</given-names></name>
        <name><surname>Raveendran</surname><given-names>Karthik</given-names></name>
        <name><surname>Zhu</surname><given-names>Tyler</given-names></name>
        <name><surname>Zhang</surname><given-names>Fan</given-names></name>
        <name><surname>Grundmann</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>Blazepose: On-device real-time body pose tracking</article-title>
      <source>arXiv preprint arXiv:2006.10204</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.10204</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Boswell_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boswell</surname><given-names>Melissa A</given-names></name>
        <name><surname>Kidziński</surname><given-names>Łukasz</given-names></name>
        <name><surname>Hicks</surname><given-names>Jennifer L</given-names></name>
        <name><surname>Uhlrich</surname><given-names>Scott D</given-names></name>
        <name><surname>Falisse</surname><given-names>Antoine</given-names></name>
        <name><surname>Delp</surname><given-names>Scott L</given-names></name>
      </person-group>
      <article-title>Smartphone videos of the sit-to-stand test predict osteoarthritis and health outcomes in a nationwide study</article-title>
      <source>npj Digital Medicine</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>6</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41746-023-00775-1</pub-id>
      <fpage>32</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bradski_2000">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bradski</surname><given-names>G.</given-names></name>
      </person-group>
      <article-title>The OpenCV library</article-title>
      <source>Dr. Dobb’s Journal of Software Tools</source>
      <year iso-8601-date="2000">2000</year>
    </element-citation>
  </ref>
  <ref id="ref-Bright_2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bright</surname><given-names>Tiffani J</given-names></name>
        <name><surname>Wong</surname><given-names>Anthony</given-names></name>
        <name><surname>Dhurjati</surname><given-names>Ravi</given-names></name>
        <name><surname>Bristow</surname><given-names>Erin</given-names></name>
        <name><surname>Bastian</surname><given-names>Lori</given-names></name>
        <name><surname>Coeytaux</surname><given-names>Remy R</given-names></name>
        <name><surname>Samsa</surname><given-names>Gregory</given-names></name>
        <name><surname>Hasselblad</surname><given-names>Vic</given-names></name>
        <name><surname>Williams</surname><given-names>John W</given-names></name>
        <name><surname>Musty</surname><given-names>Michael D</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Effect of clinical decision-support systems: A systematic review</article-title>
      <source>Annals of internal medicine</source>
      <publisher-name>American College of Physicians</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>157</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.7326/0003-4819-157-1-201207030-00450</pub-id>
      <fpage>29</fpage>
      <lpage>43</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cao_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cao</surname><given-names>Zhe</given-names></name>
        <name><surname>Hidalgo</surname><given-names>Gines</given-names></name>
        <name><surname>Simon</surname><given-names>Tomas</given-names></name>
        <name><surname>Wei</surname><given-names>Shih-En</given-names></name>
        <name><surname>Sheikh</surname><given-names>Yaser</given-names></name>
      </person-group>
      <article-title>OpenPose: Realtime multi-person 2D pose estimation using part affinity fields</article-title>
      <source>IEEE transactions on pattern analysis and machine intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>43</volume>
      <issue>1</issue>
      <uri>https://arxiv.org/abs/1611.08050</uri>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2929257</pub-id>
      <fpage>172</fpage>
      <lpage>186</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Colyer_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Colyer</surname><given-names>Steffi L</given-names></name>
        <name><surname>Evans</surname><given-names>Murray</given-names></name>
        <name><surname>Cosker</surname><given-names>Darren P</given-names></name>
        <name><surname>Salo</surname><given-names>Aki IT</given-names></name>
      </person-group>
      <article-title>A review of the evolution of vision-based motion analysis and the integration of advanced computer vision methods towards developing a markerless system</article-title>
      <source>Sports medicine-open</source>
      <publisher-name>SpringerOpen</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>4</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1186/s40798-018-0139-y</pub-id>
      <fpage>1</fpage>
      <lpage>15</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Delp_2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Delp</surname><given-names>Scott L</given-names></name>
        <name><surname>Anderson</surname><given-names>Frank C</given-names></name>
        <name><surname>Arnold</surname><given-names>Allison S</given-names></name>
        <name><surname>Loan</surname><given-names>Peter</given-names></name>
        <name><surname>Habib</surname><given-names>Ayman</given-names></name>
        <name><surname>John</surname><given-names>Chand T</given-names></name>
        <name><surname>Guendelman</surname><given-names>Eran</given-names></name>
        <name><surname>Thelen</surname><given-names>Darryl G</given-names></name>
      </person-group>
      <article-title>OpenSim: Open-source software to create and analyze dynamic simulations of movement</article-title>
      <source>IEEE transactions on biomedical engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>54</volume>
      <issue>11</issue>
      <uri>https://ieeexplore.ieee.org/abstract/document/4352056</uri>
      <pub-id pub-id-type="doi">10.1109/TBME.2007.901024</pub-id>
      <fpage>1940</fpage>
      <lpage>1950</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Di_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Di Paolo</surname><given-names>Stefano</given-names></name>
        <name><surname>Zaffagnini</surname><given-names>Stefano</given-names></name>
        <name><surname>Tosarelli</surname><given-names>Filippo</given-names></name>
        <name><surname>Aggio</surname><given-names>Fabrizio</given-names></name>
        <name><surname>Bragonzoni</surname><given-names>Laura</given-names></name>
        <name><surname>Grassi</surname><given-names>Alberto</given-names></name>
        <name><surname>Della Villa</surname><given-names>Francesco</given-names></name>
      </person-group>
      <article-title>A 2D qualitative movement assessment of a deceleration task detects football players with high knee joint loading</article-title>
      <source>Knee Surgery, Sports Traumatology, Arthroscopy</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>29</volume>
      <pub-id pub-id-type="doi">10.1007/s00167-021-06709-2</pub-id>
      <fpage>4032</fpage>
      <lpage>4040</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Jiang_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jiang</surname><given-names>Tao</given-names></name>
        <name><surname>Lu</surname><given-names>Peng</given-names></name>
        <name><surname>Zhang</surname><given-names>Li</given-names></name>
        <name><surname>Ma</surname><given-names>Ningsheng</given-names></name>
        <name><surname>Han</surname><given-names>Rui</given-names></name>
        <name><surname>Lyu</surname><given-names>Chengqi</given-names></name>
        <name><surname>Li</surname><given-names>Yining</given-names></name>
        <name><surname>Chen</surname><given-names>Kai</given-names></name>
      </person-group>
      <article-title>RTMPose: Real-time multi-person pose estimation based on MMPose</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2303.07399</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2303.07399</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kidzinski_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kidziński</surname><given-names>Łukasz</given-names></name>
        <name><surname>Yang</surname><given-names>Bryan</given-names></name>
        <name><surname>Hicks</surname><given-names>Jennifer L</given-names></name>
        <name><surname>Rajagopal</surname><given-names>Apoorva</given-names></name>
        <name><surname>Delp</surname><given-names>Scott L</given-names></name>
        <name><surname>Schwartz</surname><given-names>Michael H</given-names></name>
      </person-group>
      <article-title>Deep neural networks enable quantitative movement analysis using single-camera videos</article-title>
      <source>Nature communications</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>11</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41467-020-17807-z</pub-id>
      <fpage>4054</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Kinovea">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kinovea</surname></name>
      </person-group>
      <article-title>Kinovea - a microscope for your videos</article-title>
      <publisher-name>https://www.kinovea.org/features.html; GitHub</publisher-name>
      <uri>https://www.kinovea.org/features.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-Mathis_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mathis</surname><given-names>Alexander</given-names></name>
        <name><surname>Mamidanna</surname><given-names>Pranav</given-names></name>
        <name><surname>Cury</surname><given-names>Kevin M</given-names></name>
        <name><surname>Abe</surname><given-names>Taiga</given-names></name>
        <name><surname>Murthy</surname><given-names>Venkatesh N</given-names></name>
        <name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name>
        <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>
      <source>Nature neuroscience</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>21</volume>
      <issue>9</issue>
      <uri>https://www.nature.com/articles/s41593-018-0209-y</uri>
      <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
      <fpage>1281</fpage>
      <lpage>1289</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ODonoghue_2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>O’Donoghue</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Principal components analysis in the selection of key performance indicators in sport</article-title>
      <source>International Journal of Performance Analysis in Sport</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>8</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1080/24748668.2008.11868456</pub-id>
      <fpage>145</fpage>
      <lpage>155</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Pagnon_2022a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pagnon</surname><given-names>David</given-names></name>
        <name><surname>Domalain</surname><given-names>Mathieu</given-names></name>
        <name><surname>Reveret</surname><given-names>Lionel</given-names></name>
      </person-group>
      <article-title>Pose2Sim: An open-source python package for multiview markerless kinematics</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>77</issue>
      <uri>https://joss.theoj.org/papers/10.21105/joss.04362</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04362</pub-id>
      <fpage>4362</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Pagnon_2022b">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pagnon</surname><given-names>David</given-names></name>
        <name><surname>Domalain</surname><given-names>Mathieu</given-names></name>
        <name><surname>Robert</surname><given-names>Thomas</given-names></name>
        <name><surname>Lahkar</surname><given-names>Bhrigu-Kumar</given-names></name>
        <name><surname>Moussa</surname><given-names>Issa</given-names></name>
        <name><surname>Saulière</surname><given-names>Guillaume</given-names></name>
        <name><surname>Goyallon</surname><given-names>Thibault</given-names></name>
        <name><surname>Reveret</surname><given-names>Lionel</given-names></name>
      </person-group>
      <article-title>A 3D markerless protocol with action cameras – Key performance indicators in boxing</article-title>
      <source>2022 congress of the european college of sport science (ECSS)</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://hal.archives-ouvertes.fr/hal-03790926</uri>
    </element-citation>
  </ref>
  <ref id="ref-Patrizi_2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Patrizi</surname><given-names>Alfredo</given-names></name>
        <name><surname>Pennestrı̀</surname><given-names>Ettore</given-names></name>
        <name><surname>Valentini</surname><given-names>Pier Paolo</given-names></name>
      </person-group>
      <article-title>Comparison between low-cost marker-less and high-end marker-based motion capture systems for the computer-aided assessment of working ergonomics</article-title>
      <source>Ergonomics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>59</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1080/00140139.2015.1057238</pub-id>
      <fpage>155</fpage>
      <lpage>162</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Seth_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Seth</surname><given-names>Jennifer L. AND Uchida</given-names><suffix>Ajay AND Hicks</suffix></name>
      </person-group>
      <article-title>OpenSim: Simulating musculoskeletal dynamics and neuromuscular control to study human and animal movement</article-title>
      <source>PLOS Computational Biology</source>
      <publisher-name>Public Library of Science</publisher-name>
      <year iso-8601-date="2018-07">2018</year><month>07</month>
      <volume>14</volume>
      <issue>7</issue>
      <uri>https://doi.org/10.1371/journal.pcbi.1006223</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006223</pub-id>
      <fpage>1</fpage>
      <lpage>20</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Uhlrich_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Uhlrich</surname><given-names>Scott D.</given-names></name>
        <name><surname>Falisse</surname><given-names>Antoine</given-names></name>
        <name><surname>Kidziński</surname><given-names>Łukasz</given-names></name>
        <name><surname>Muccini</surname><given-names>Julie</given-names></name>
        <name><surname>Ko</surname><given-names>Michael</given-names></name>
        <name><surname>Chaudhari</surname><given-names>Akshay S.</given-names></name>
        <name><surname>Hicks</surname><given-names>Jennifer L.</given-names></name>
        <name><surname>Delp</surname><given-names>Scott L.</given-names></name>
      </person-group>
      <article-title>OpenCap: 3D human movement dynamics from smartphone videos</article-title>
      <publisher-name>bioRxiv</publisher-name>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <uri>https://www.biorxiv.org/content/10.1101/2022.07.07.499061v1</uri>
      <pub-id pub-id-type="doi">10.1101/2022.07.07.499061</pub-id>
      <fpage>2022.07.07.499061</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Venkatesh_2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Venkatesh</surname><given-names>Viswanath</given-names></name>
        <name><surname>Thong</surname><given-names>James YL</given-names></name>
        <name><surname>Xu</surname><given-names>Xin</given-names></name>
      </person-group>
      <article-title>Consumer acceptance and use of information technology: Extending the unified theory of acceptance and use of technology</article-title>
      <source>MIS quarterly</source>
      <publisher-name>JSTOR</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <pub-id pub-id-type="doi">10.2307/41410412</pub-id>
      <fpage>157</fpage>
      <lpage>178</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wade_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wade</surname><given-names>Logan</given-names></name>
        <name><surname>Needham</surname><given-names>Laurie</given-names></name>
        <name><surname>McGuigan</surname><given-names>Polly</given-names></name>
        <name><surname>Bilzon</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>Applications and limitations of current markerless motion capture methods for clinical gait biomechanics</article-title>
      <source>PeerJ</source>
      <publisher-name>PeerJ Inc.</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.7717/peerj.12995</pub-id>
      <fpage>e12995</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Whittle_2014">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Whittle</surname><given-names>Michael W</given-names></name>
      </person-group>
      <source>Gait analysis: An introduction</source>
      <publisher-name>Butterworth-Heinemann</publisher-name>
      <year iso-8601-date="2014">2014</year>
    </element-citation>
  </ref>
  <ref id="ref-Zheng_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zheng</surname><given-names>Ce</given-names></name>
        <name><surname>Wu</surname><given-names>Wenhan</given-names></name>
        <name><surname>Chen</surname><given-names>Chen</given-names></name>
        <name><surname>Yang</surname><given-names>Taojiannan</given-names></name>
        <name><surname>Zhu</surname><given-names>Sijie</given-names></name>
        <name><surname>Shen</surname><given-names>Ju</given-names></name>
        <name><surname>Kehtarnavaz</surname><given-names>Nasser</given-names></name>
        <name><surname>Shah</surname><given-names>Mubarak</given-names></name>
      </person-group>
      <article-title>Deep learning-based human pose estimation: A survey</article-title>
      <source>ACM Computing Surveys</source>
      <publisher-name>ACM New York, NY</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>56</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1145/3603618</pub-id>
      <fpage>1</fpage>
      <lpage>37</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
