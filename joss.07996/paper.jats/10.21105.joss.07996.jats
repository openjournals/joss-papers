<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7996</article-id>
<article-id pub-id-type="doi">10.21105/joss.07996</article-id>
<title-group>
<article-title>MultiVae: A Python package for Multimodal Variational
Autoencoders on Partial Datasets.</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-3176-6461</contrib-id>
<name>
<surname>Senellart</surname>
<given-names>Agathe</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chadebec</surname>
<given-names>Clément</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Allassonnière</surname>
<given-names>Stéphanie</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Université Paris Cité, Inria, Inserm, HeKA, F-75015 Paris,
France</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-03-03">
<day>3</day>
<month>3</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>110</issue>
<fpage>7996</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Pytorch</kwd>
<kwd>Variational Autoencoders</kwd>
<kwd>Multimodality</kwd>
<kwd>Missing data</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In recent years, multimodal machine learning has seen significant
  growth, especially in representation learning and data generation.
  Recently, Multimodal Variational Autoencoders (VAEs) have been
  attracting growing interest for both tasks, thanks to their
  versatility, scalability, and interpretability as latent variable
  models. They are particularly useful in <italic>partially
  observed</italic> settings, such as medical applications, where
  available datasets are often incomplete
  (<xref alt="Antelmi et al., 2019" rid="ref-antelmiU003A2019" ref-type="bibr">Antelmi
  et al., 2019</xref>;
  <xref alt="Lawry Aguila et al., 2023" rid="ref-aguilaU003Apoe" ref-type="bibr">Lawry
  Aguila et al., 2023</xref>).</p>
  <p>We introduce MultiVae, an open-source Python library offering
  unified implementations of multimodal VAEs. It is designed for easy
  and customizable use of these models on fully or partially observed
  data. It facilitates the development and benchmarking of new
  algorithms by including standard benchmark datasets, evaluation
  metrics and tools for monitoring and sharing models.</p>
  <sec id="multimodal-variational-autoencoders">
    <title>Multimodal Variational Autoencoders</title>
    <p>Multimodal VAEs aim to: (1) Learn a shared representation from
    multiple modalities; (2) Generate one missing modality from
    available ones.</p>
    <p>These models learn a latent representation
    <inline-formula><alternatives>
    <tex-math><![CDATA[z]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
    of all modalities in a lower dimensional space and learn to
    <italic>decode</italic> <inline-formula><alternatives>
    <tex-math><![CDATA[z]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
    to generate each modality. Let <inline-formula><alternatives>
    <tex-math><![CDATA[X = (x_1, x_2, ... x_M)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    contain <inline-formula><alternatives>
    <tex-math><![CDATA[M]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>
    modalities. In the VAE setting, we define an
    <italic>encoder</italic> distribution <inline-formula><alternatives>
    <tex-math><![CDATA[q_{\phi}(z|X)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    projecting the observations to the latent space, and decoders
    distributions <inline-formula><alternatives>
    <tex-math><![CDATA[(p_{\theta}(x_i|z))_{1 \leq i \leq M}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
    translating the latent code <inline-formula><alternatives>
    <tex-math><![CDATA[z]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
    back to observations. Those distributions are parameterized by
    neural networks that are trained to minimize an objective function
    derived from variational inference. See Kingma &amp; Welling
    (<xref alt="2014" rid="ref-kingma" ref-type="bibr">2014</xref>) to
    learn more about the VAE framework and Suzuki &amp; Matsuo
    (<xref alt="2022" rid="ref-suzuki_survey_2022" ref-type="bibr">2022</xref>)
    for a survey on multimodal VAEs.</p>
    <p>A key differentiator of multimodal VAEs relies in the choice of
    the encoder <inline-formula><alternatives>
    <tex-math><![CDATA[q_{\phi}(z|X)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ϕ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
    They fall into three main categories, depicted in
    <xref alt="[fig:typesvae]" rid="figU003Atypesvae">[fig:typesvae]</xref>.
    <italic>Aggregated models</italic>
    (<xref alt="Shi et al., 2019" rid="ref-shiU003A2019" ref-type="bibr">Shi
    et al., 2019</xref>;
    <xref alt="Sutter et al., 2021" rid="ref-sutterU003A2021" ref-type="bibr">Sutter
    et al., 2021</xref>;
    <xref alt="Wu &amp; Goodman, 2018" rid="ref-wuU003A2018" ref-type="bibr">Wu
    &amp; Goodman, 2018</xref>) use a mean or product operation to
    combine modalities, <italic>Joint models</italic>
    (<xref alt="Senellart et al., 2023" rid="ref-senellartU003A2023" ref-type="bibr">Senellart
    et al., 2023</xref>;
    <xref alt="Suzuki et al., 2016" rid="ref-suzukiU003A2016" ref-type="bibr">Suzuki
    et al., 2016</xref>;
    <xref alt="Vedantam et al., 2018" rid="ref-vedantamU003A2018" ref-type="bibr">Vedantam
    et al., 2018</xref>) use a neural network taking all modalities as
    input, and <italic>Coordinated models</italic>
    (<xref alt="Tian &amp; Engel, 2019" rid="ref-tianU003A2019" ref-type="bibr">Tian
    &amp; Engel, 2019</xref>;
    <xref alt="Wang et al., 2017" rid="ref-wang_deep_2017" ref-type="bibr">Wang
    et al., 2017</xref>) use separate latent spaces with additional
    similarity constraints.</p>
    <fig>
      <caption><p>Different types of multimodal
      VAEs.<styled-content id="figU003Atypesvae"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="mvae_models_diagrams.png" />
    </fig>
    <p>MultiVae unifies these approaches in a modular and extensible
    way. Notably, aggregated models offer a natural way of
    <italic>learning</italic> on incomplete datasets: for an incomplete
    sample <inline-formula><alternatives>
    <tex-math><![CDATA[X]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>,
    the encoding <inline-formula><alternatives>
    <tex-math><![CDATA[z]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
    and the objective function can be computed using only available
    modalities. MultiVae is the first library to provide implementations
    of these models with built-in support for missing data, using masks
    during loss computation.</p>
  </sec>
  <sec id="data-augmentation">
    <title>Data Augmentation</title>
    <p>Another application of VAEs is Data Augmentation (DA): by
    sampling new latent codes <inline-formula><alternatives>
    <tex-math><![CDATA[z]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
    and decoding them, <italic>fully synthetic multimodal</italic>
    samples can be generated to augment a dataset. This approach has
    been successfully used with unimodal VAEs to augment datasets for
    data-intensive deep learning applications
    (<xref alt="Chadebec et al., 2023" rid="ref-chadebec_DA" ref-type="bibr">Chadebec
    et al., 2023</xref>). However, it remains underexplored in the
    multimodal setting. MultiVae includes a
    <monospace>multivae.samplers</monospace> module with several
    sampling strategies to further explore the generative abilities of
    these models.</p>
  </sec>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Despite the usefulness of multimodal VAEs, the lack of easy-to-use
  and verified implementations might hinder applicative research.
  MultiVae offers unified implementations, designed to be accessible
  even for non-specialists. We ensure reliability by reproducing key
  results from original papers whenever possible.</p>
  <p>Related libraries contain implementations of Multimodal VAEs: the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/gabinsane/multimodal-vae-comparison">Multimodal
  VAE Comparison Toolkit</ext-link>
  (<xref alt="Sejnova et al., 2024" rid="ref-sejnovaU003A2024" ref-type="bibr">Sejnova
  et al., 2024</xref>),
  <ext-link ext-link-type="uri" xlink:href="https://github.com/masa-su/pixyz/blob/main/examples/jmvae.ipynb">Pixyz</ext-link>
  (<xref alt="Masahiro Suzuki &amp; Matsuo, 2023" rid="ref-suzuki2023pixyz" ref-type="bibr">Masahiro
  Suzuki &amp; Matsuo, 2023</xref>) and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/alawryaguila/multi-view-AE">multi-view-ae</ext-link>
  (<xref alt="Aguila et al., 2023" rid="ref-Aguila2023" ref-type="bibr">Aguila
  et al., 2023</xref>) that is most closely related to us and released
  while we were developing MultiVae.</p>
  <p>We compare in a summarizing table below, the different features of
  each work. MultiVae differs and complements existing software packages
  in key ways: it supports <bold>incomplete datasets</bold>, which we
  consider essential for real-life applications, as well as
  <bold>generative samplers</bold>, <bold>benchmark datasets</bold> and
  <bold>metrics</bold> to facilitate research. It contains a large range
  of models with a great flexibility on parameters’ choices and
  including all implementation details present in the original codes
  that improve performance.</p>
  <sec id="list-of-models-and-features">
    <title>List of Models and Features</title>
    <p>We list models and features in each work. Symbol
    (<inline-formula><alternatives>
    <tex-math><![CDATA[\checkmark]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*)
    indicates that the implementation includes additional options
    unavailable in the others.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="49%" />
          <col width="13%" />
          <col width="13%" />
          <col width="13%" />
          <col width="13%" />
        </colgroup>
        <thead>
          <tr>
            <th>Models/ Features</th>
            <th>Ours</th>
            <th>Aguila et al.
            (<xref alt="2023" rid="ref-Aguila2023" ref-type="bibr">2023</xref>)</th>
            <th>Sejnova et al.
            (<xref alt="2024" rid="ref-sejnovaU003A2024" ref-type="bibr">2024</xref>)</th>
            <th>Masahiro Suzuki &amp; Matsuo
            (<xref alt="2023" rid="ref-suzuki2023pixyz" ref-type="bibr">2023</xref>)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>JMVAE
            (<xref alt="Suzuki et al., 2016" rid="ref-suzukiU003A2016" ref-type="bibr">Suzuki
            et al., 2016</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>MVAE
            (<xref alt="Wu &amp; Goodman, 2018" rid="ref-wuU003A2018" ref-type="bibr">Wu
            &amp; Goodman, 2018</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>MMVAE
            (<xref alt="Shi et al., 2019" rid="ref-shiU003A2019" ref-type="bibr">Shi
            et al., 2019</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
          </tr>
          <tr>
            <td>MoPoE
            (<xref alt="Sutter et al., 2021" rid="ref-sutterU003A2021" ref-type="bibr">Sutter
            et al., 2021</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
          </tr>
          <tr>
            <td>DMVAE
            (<xref alt="Lee &amp; Pavlovic, 2021" rid="ref-leeU003A2021" ref-type="bibr">Lee
            &amp; Pavlovic, 2021</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
          </tr>
          <tr>
            <td>MVTCAE
            (<xref alt="Hwang et al., 2021" rid="ref-hwang2021multi" ref-type="bibr">Hwang
            et al., 2021</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>MMVAE+
            (<xref alt="Palumbo et al., 2023" rid="ref-palumbo_mmvae_2023" ref-type="bibr">Palumbo
            et al., 2023</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>*</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>CMVAE
            (<xref alt="Palumbo et al., 2024" rid="ref-palumbo2024deep" ref-type="bibr">Palumbo
            et al., 2024</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>Nexus
            (<xref alt="Vasco et al., 2022" rid="ref-vasco2022leveraging" ref-type="bibr">Vasco
            et al., 2022</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>CVAE
            (<xref alt="Kingma &amp; Welling, 2014" rid="ref-kingma" ref-type="bibr">Kingma
            &amp; Welling, 2014</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>MHVAE
            (<xref alt="Dorent et al., 2023" rid="ref-dorentU003A2023" ref-type="bibr">Dorent
            et al., 2023</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>TELBO
            (<xref alt="Vedantam et al., 2018" rid="ref-vedantamU003A2018" ref-type="bibr">Vedantam
            et al., 2018</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>JNF
            (<xref alt="Senellart et al., 2023" rid="ref-senellartU003A2023" ref-type="bibr">Senellart
            et al., 2023</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>CRMVAE
            (<xref alt="Suzuki &amp; Matsuo, 2023" rid="ref-suzukiU003A2023U003Amitigating" ref-type="bibr">Suzuki
            &amp; Matsuo, 2023</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>MCVAE
            (<xref alt="Antelmi et al., 2019" rid="ref-antelmiU003A2019" ref-type="bibr">Antelmi
            et al., 2019</xref>)</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>mAAE</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>DVCCA
            (<xref alt="Wang et al., 2017" rid="ref-wang_deep_2017" ref-type="bibr">Wang
            et al., 2017</xref>)</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>DCCAE
            (<xref alt="Wang et al., 2015" rid="ref-dccae" ref-type="bibr">Wang
            et al., 2015</xref>)</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>mWAE</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>mmJSD
            (<xref alt="Sutter et al., 2020" rid="ref-sutterU003Ammjsd" ref-type="bibr">Sutter
            et al., 2020</xref>)</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>gPoE
            (<xref alt="Lawry Aguila et al., 2023" rid="ref-aguilaU003Apoe" ref-type="bibr">Lawry
            Aguila et al., 2023</xref>)</td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>Support of Incomplete datasets</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>GMM Sampler</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>MAF Sampler, IAF Sampler</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td><bold>Metrics</bold>: {Likelihood, Coherences, FIDs,
            Reconstruction, Clustering}</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
          <tr>
            <td>Benchmark Datasets</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
          </tr>
          <tr>
            <td>Model sharing via Hugging Face</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\checkmark]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            <td></td>
            <td></td>
            <td></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
</sec>
<sec id="code-quality-and-documentation">
  <title>Code Quality and Documentation</title>
  <p>MultiVae is available on
  <ext-link ext-link-type="uri" xlink:href="https://github.com/AgatheSenellart/MultiVae">GitHub</ext-link>
  and Pypi, with full documentation at
  <ext-link ext-link-type="uri" xlink:href="https://multivae.readthedocs.io/">https://multivae.readthedocs.io/</ext-link>.
  The code is unit-tested with 94% coverage. We provide
  <bold>tutorials</bold> either as notebooks or scripts allowing users
  to get started easily. To further showcase how to use our library for
  research applications, we provide detailed <italic>case
  studies</italic> in the documentation.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We are grateful to the authors of the initial implementations of
  the models included in MultiVae. This work benefited from state grant
  managed by the Agence Nationale de la Recherche under the France 2030
  program, AN-23-IACL-0008. This research has been partly supported by
  the European Union under the (2023-2030) ERC Synergy Grant
  101071601.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-wuU003A2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Mike</given-names></name>
        <name><surname>Goodman</surname><given-names>Noah</given-names></name>
      </person-group>
      <article-title>Multimodal Generative Models for Scalable Weakly-Supervised Learning</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-15">2022</year><month>04</month><day>15</day></date-in-citation>
      <volume>31</volume>
      <uri>https://proceedings.neurips.cc/paper/2018/hash/1102a326d5f7c9e04fc3c89d0ede88c9-Abstract.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-suzukiU003A2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Nakayama</surname><given-names>Kotaro</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Joint multimodal learning with deep generative models</article-title>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/abs/1611.01891</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1611.01891</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-palumbo_mmvae_2023">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Palumbo</surname><given-names>Emanuele</given-names></name>
        <name><surname>Daunhawer</surname><given-names>Imant</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia E</given-names></name>
      </person-group>
      <article-title>MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises</article-title>
      <source>The eleventh international conference on learning representations</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://openreview.net/forum?id=sdQGxouELX</uri>
    </element-citation>
  </ref>
  <ref id="ref-vedantamU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vedantam</surname><given-names>Ramakrishna</given-names></name>
        <name><surname>Fischer</surname><given-names>Ian</given-names></name>
        <name><surname>Huang</surname><given-names>Jonathan</given-names></name>
        <name><surname>Murphy</surname><given-names>Kevin</given-names></name>
      </person-group>
      <article-title>Generative Models of Visually Grounded Imagination</article-title>
      <source>arXiv:1705.10762 [cs, stat]</source>
      <year iso-8601-date="2018-11">2018</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-05-12">2022</year><month>05</month><day>12</day></date-in-citation>
      <uri>http://arxiv.org/abs/1705.10762</uri>
    </element-citation>
  </ref>
  <ref id="ref-kingma">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P.</given-names></name>
        <name><surname>Welling</surname><given-names>Max</given-names></name>
      </person-group>
      <article-title>Auto-Encoding Variational Bayes</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2014-05">2014</year><month>05</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-20">2022</year><month>07</month><day>20</day></date-in-citation>
      <uri>http://arxiv.org/abs/1312.6114</uri>
      <pub-id pub-id-type="doi">10.61603/ceas.v2i1.33</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-sutterU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sutter</surname><given-names>Thomas M.</given-names></name>
        <name><surname>Daunhawer</surname><given-names>Imant</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia E.</given-names></name>
      </person-group>
      <article-title>Generalized Multimodal ELBO</article-title>
      <source>ICLR</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-shiU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shi</surname><given-names>Yuge</given-names></name>
        <name><surname>Siddharth</surname><given-names>N.</given-names></name>
        <name><surname>Paige</surname><given-names>Brooks</given-names></name>
        <name><surname>Torr</surname><given-names>Philip H. S.</given-names></name>
      </person-group>
      <article-title>Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models</article-title>
      <source>arXiv:1911.03393 [cs, stat]</source>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-04-20">2022</year><month>04</month><day>20</day></date-in-citation>
      <uri>http://arxiv.org/abs/1911.03393</uri>
    </element-citation>
  </ref>
  <ref id="ref-wang_deep_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Weiran</given-names></name>
        <name><surname>Yan</surname><given-names>Xinchen</given-names></name>
        <name><surname>Lee</surname><given-names>Honglak</given-names></name>
        <name><surname>Livescu</surname><given-names>Karen</given-names></name>
      </person-group>
      <article-title>Deep Variational Canonical Correlation Analysis</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2017-02">2017</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-01-18">2023</year><month>01</month><day>18</day></date-in-citation>
      <uri>http://arxiv.org/abs/1610.03454</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1610.03454</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-vasco2022leveraging">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vasco</surname><given-names>Miguel</given-names></name>
        <name><surname>Yin</surname><given-names>Hang</given-names></name>
        <name><surname>Melo</surname><given-names>Francisco S</given-names></name>
        <name><surname>Paiva</surname><given-names>Ana</given-names></name>
      </person-group>
      <article-title>Leveraging hierarchy in multimodal generative models for effective cross-modality inference</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>146</volume>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2021.11.019</pub-id>
      <fpage>238</fpage>
      <lpage>255</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hwang2021multi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hwang</surname><given-names>HyeongJoo</given-names></name>
        <name><surname>Kim</surname><given-names>Geon-Hyeong</given-names></name>
        <name><surname>Hong</surname><given-names>Seunghoon</given-names></name>
        <name><surname>Kim</surname><given-names>Kee-Eung</given-names></name>
      </person-group>
      <article-title>Multi-view representation learning via total correlation objective</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
      <fpage>12194</fpage>
      <lpage>12207</lpage>
    </element-citation>
  </ref>
  <ref id="ref-senellartU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Senellart</surname><given-names>Agathe</given-names></name>
        <name><surname>Chadebec</surname><given-names>Clément</given-names></name>
        <name><surname>Allassonnière</surname><given-names>Stéphanie</given-names></name>
      </person-group>
      <article-title>Improving multimodal joint variational autoencoders through normalizing flows and correlation analysis</article-title>
      <source>arXiv preprint arXiv:2305.11832</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-Aguila2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Aguila</surname><given-names>Ana Lawry</given-names></name>
        <name><surname>Jayme</surname><given-names>Alejandra</given-names></name>
        <name><surname>Montaña-Brown</surname><given-names>Nina</given-names></name>
        <name><surname>Heuveline</surname><given-names>Vincent</given-names></name>
        <name><surname>Altmann</surname><given-names>Andre</given-names></name>
      </person-group>
      <article-title>Multi-view-AE: A python package for multi-view autoencoder models</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>85</issue>
      <uri>https://doi.org/10.21105/joss.05093</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05093</pub-id>
      <fpage>5093</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-suzuki_survey_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>A survey of multimodal deep generative models</article-title>
      <source>Advanced Robotics</source>
      <year iso-8601-date="2022-03">2022</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-12-09">2022</year><month>12</month><day>09</day></date-in-citation>
      <volume>36</volume>
      <issue>5-6</issue>
      <issn>0169-1864</issn>
      <uri>http://arxiv.org/abs/2207.02127</uri>
      <pub-id pub-id-type="doi">10.1080/01691864.2022.2035253</pub-id>
      <fpage>261</fpage>
      <lpage>278</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tianU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tian</surname><given-names>Yingtao</given-names></name>
        <name><surname>Engel</surname><given-names>Jesse</given-names></name>
      </person-group>
      <article-title>Latent Translation: Crossing Modalities by Bridging Generative Models</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-chadebec_DA">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chadebec</surname><given-names>Clément</given-names></name>
        <name><surname>Thibeau-Sutre</surname><given-names>Elina</given-names></name>
        <name><surname>Burgos</surname><given-names>Ninon</given-names></name>
        <name><surname>Allassonnière</surname><given-names>Stéphanie</given-names></name>
      </person-group>
      <article-title>Data augmentation in high dimensional low sample size setting using a geometry-based variational autoencoder</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2023">2023</year>
      <volume>45</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2022.3185773</pub-id>
      <fpage>2879</fpage>
      <lpage>2896</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sejnovaU003A2024">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Sejnova</surname><given-names>Gabriela</given-names></name>
        <name><surname>Vavrecka</surname><given-names>Michal</given-names></name>
        <name><surname>Stepanova</surname><given-names>Karla</given-names></name>
        <name><surname>Taniguchi</surname><given-names>Tadahiro</given-names></name>
      </person-group>
      <article-title>Benchmarking multimodal variational autoencoders: CdSprites+ dataset and toolkit</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2209.03048</uri>
    </element-citation>
  </ref>
  <ref id="ref-suzuki2023pixyz">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Masahiro Suzuki</surname><given-names>Takaaki Kaneko</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Pixyz: A python library for developing deep generative models</article-title>
      <source>Advanced Robotics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>0</volume>
      <pub-id pub-id-type="doi">10.1080/01691864.2023.2244568</pub-id>
      <fpage>1</fpage>
      <lpage>16</lpage>
    </element-citation>
  </ref>
  <ref id="ref-leeU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Mihee</given-names></name>
        <name><surname>Pavlovic</surname><given-names>Vladimir</given-names></name>
      </person-group>
      <article-title>Private-shared disentangled multimodal VAE for learning of latent representations</article-title>
      <year iso-8601-date="2021">2021</year>
      <volume></volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.1109/CVPRW53098.2021.00185</pub-id>
      <fpage>1692</fpage>
      <lpage>1700</lpage>
    </element-citation>
  </ref>
  <ref id="ref-palumbo2024deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Palumbo</surname><given-names>Emanuele</given-names></name>
        <name><surname>Manduchi</surname><given-names>Laura</given-names></name>
        <name><surname>Laguna</surname><given-names>Sonia</given-names></name>
        <name><surname>Chopard</surname><given-names>Daphné</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia E</given-names></name>
      </person-group>
      <article-title>Deep generative clustering with multimodal diffusion variational autoencoders</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://openreview.net/forum?id=k5THrhXDV3</uri>
    </element-citation>
  </ref>
  <ref id="ref-dorentU003A2023">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Dorent</surname><given-names>Reuben</given-names></name>
        <name><surname>Haouchine</surname><given-names>Nazim</given-names></name>
        <name><surname>Kogl</surname><given-names>Fryderyk</given-names></name>
        <name><surname>Joutard</surname><given-names>Samuel</given-names></name>
        <name><surname>Juvekar</surname><given-names>Parikshit</given-names></name>
        <name><surname>Torio</surname><given-names>Erickson</given-names></name>
        <name><surname>Golby</surname><given-names>Alexandra J.</given-names></name>
        <name><surname>Ourselin</surname><given-names>Sebastien</given-names></name>
        <name><surname>Frisken</surname><given-names>Sarah</given-names></name>
        <name><surname>Vercauteren</surname><given-names>Tom</given-names></name>
        <name><surname>Kapur</surname><given-names>Tina</given-names></name>
        <name><surname>Wells</surname><given-names>William M.</given-names></name>
      </person-group>
      <article-title>Unified brain MR-ultrasound synthesis using multi-modal hierarchical representations</article-title>
      <source>Medical image computing and computer assisted intervention – MICCAI 2023</source>
      <publisher-name>Springer Nature Switzerland</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <isbn>9783031439995</isbn>
      <issn>1611-3349</issn>
      <uri>http://dx.doi.org/10.1007/978-3-031-43999-5_43</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-031-43999-5_43</pub-id>
      <fpage>448</fpage>
      <lpage>458</lpage>
    </element-citation>
  </ref>
  <ref id="ref-suzukiU003A2023U003Amitigating">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Mitigating the limitations of multimodal VAEs with coordination-based approach</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://openreview.net/forum?id=Rn8u4MYgeNJ</uri>
    </element-citation>
  </ref>
  <ref id="ref-antelmiU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Antelmi</surname><given-names>Luigi</given-names></name>
        <name><surname>Ayache</surname><given-names>Nicholas</given-names></name>
        <name><surname>Robert</surname><given-names>Philippe</given-names></name>
        <name><surname>Lorenzi</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data</article-title>
      <person-group person-group-type="editor">
        <name><surname>Chaudhuri</surname><given-names>Kamalika</given-names></name>
        <name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>97</volume>
      <uri>https://proceedings.mlr.press/v97/antelmi19a.html</uri>
      <fpage>302</fpage>
      <lpage>311</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sutterU003Ammjsd">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sutter</surname><given-names>Thomas M.</given-names></name>
        <name><surname>Daunhawer</surname><given-names>Imant</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia E.</given-names></name>
      </person-group>
      <article-title>Multimodal generative learning utilizing jensen-shannon-divergence</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2020">2020</year>
      <volume>abs/2006.08242</volume>
      <uri>https://arxiv.org/abs/2006.08242</uri>
    </element-citation>
  </ref>
  <ref id="ref-aguilaU003Apoe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lawry Aguila</surname><given-names>Ana</given-names></name>
        <name><surname>Chapman</surname><given-names>James</given-names></name>
        <name><surname>Altmann</surname><given-names>Andre</given-names></name>
      </person-group>
      <article-title>Multi-modal variational autoencoders for&amp;nbsp;normative modelling across multiple imaging modalities</article-title>
      <publisher-name>Springer-Verlag</publisher-name>
      <publisher-loc>Berlin, Heidelberg</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <isbn>978-3-031-43906-3</isbn>
      <uri>https://doi.org/10.1007/978-3-031-43907-0_41</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-031-43907-0_41</pub-id>
      <fpage>425</fpage>
      <lpage>434</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dccae">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Weiran</given-names></name>
        <name><surname>Arora</surname><given-names>Raman</given-names></name>
        <name><surname>Livescu</surname><given-names>Karen</given-names></name>
        <name><surname>Bilmes</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>On deep multi-view representation learning</article-title>
      <source>Proceedings of the 32nd international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>Bach</surname><given-names>Francis</given-names></name>
        <name><surname>Blei</surname><given-names>David</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <publisher-loc>Lille, France</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <volume>37</volume>
      <uri>https://proceedings.mlr.press/v37/wangb15.html</uri>
      <fpage>1083</fpage>
      <lpage>1092</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
