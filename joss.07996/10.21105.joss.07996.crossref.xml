<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250603095606-59c42d133e29a6e07ed84522f999e2f67e35aac6</doi_batch_id>
    <timestamp>20250603095606</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>06</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>110</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>MultiVae: A Python package for Multimodal Variational Autoencoders on Partial Datasets.</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Agathe</given_name>
            <surname>Senellart</surname>
            <affiliations>
              <institution><institution_name>Université Paris Cité, Inria, Inserm, HeKA, F-75015 Paris, France</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0000-3176-6461</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Clément</given_name>
            <surname>Chadebec</surname>
            <affiliations>
              <institution><institution_name>Université Paris Cité, Inria, Inserm, HeKA, F-75015 Paris, France</institution_name></institution>
            </affiliations>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Stéphanie</given_name>
            <surname>Allassonnière</surname>
            <affiliations>
              <institution><institution_name>Université Paris Cité, Inria, Inserm, HeKA, F-75015 Paris, France</institution_name></institution>
            </affiliations>
          </person_name>
        </contributors>
        <publication_date>
          <month>06</month>
          <day>03</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>7996</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.07996</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.15577722</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/7996</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.07996</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.07996</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.07996.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="wu:2018">
            <article_title>Multimodal Generative Models for Scalable Weakly-Supervised Learning</article_title>
            <author>Wu</author>
            <journal_title>Advances in Neural Information Processing Systems</journal_title>
            <volume>31</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Wu, M., &amp; Goodman, N. (2018). Multimodal Generative Models for Scalable Weakly-Supervised Learning. Advances in Neural Information Processing Systems, 31. https://proceedings.neurips.cc/paper/2018/hash/1102a326d5f7c9e04fc3c89d0ede88c9-Abstract.html</unstructured_citation>
          </citation>
          <citation key="suzuki:2016">
            <article_title>Joint multimodal learning with deep generative models</article_title>
            <author>Suzuki</author>
            <doi>10.48550/arXiv.1611.01891</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Suzuki, M., Nakayama, K., &amp; Matsuo, Y. (2016). Joint multimodal learning with deep generative models. https://doi.org/10.48550/arXiv.1611.01891</unstructured_citation>
          </citation>
          <citation key="palumbo_mmvae_2023">
            <article_title>MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises</article_title>
            <author>Palumbo</author>
            <journal_title>The eleventh international conference on learning representations</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Palumbo, E., Daunhawer, I., &amp; Vogt, J. E. (2023). MMVAE+: Enhancing the generative quality of multimodal VAEs without compromises. The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=sdQGxouELX</unstructured_citation>
          </citation>
          <citation key="vedantam:2018">
            <article_title>Generative Models of Visually Grounded Imagination</article_title>
            <author>Vedantam</author>
            <journal_title>arXiv:1705.10762 [cs, stat]</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Vedantam, R., Fischer, I., Huang, J., &amp; Murphy, K. (2018). Generative Models of Visually Grounded Imagination. arXiv:1705.10762 [Cs, Stat]. http://arxiv.org/abs/1705.10762</unstructured_citation>
          </citation>
          <citation key="kingma">
            <article_title>Auto-Encoding Variational Bayes</article_title>
            <author>Kingma</author>
            <doi>10.61603/ceas.v2i1.33</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Kingma, D. P., &amp; Welling, M. (2014). Auto-Encoding Variational Bayes. arXiv. https://doi.org/10.61603/ceas.v2i1.33</unstructured_citation>
          </citation>
          <citation key="sutter:2021">
            <article_title>Generalized Multimodal ELBO</article_title>
            <author>Sutter</author>
            <journal_title>ICLR</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Sutter, T. M., Daunhawer, I., &amp; Vogt, J. E. (2021). Generalized Multimodal ELBO. ICLR.</unstructured_citation>
          </citation>
          <citation key="shi:2019">
            <article_title>Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models</article_title>
            <author>Shi</author>
            <journal_title>arXiv:1911.03393 [cs, stat]</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Shi, Y., Siddharth, N., Paige, B., &amp; Torr, P. H. S. (2019). Variational Mixture-of-Experts Autoencoders for Multi-Modal Deep Generative Models. arXiv:1911.03393 [Cs, Stat]. http://arxiv.org/abs/1911.03393</unstructured_citation>
          </citation>
          <citation key="wang_deep_2017">
            <article_title>Deep Variational Canonical Correlation Analysis</article_title>
            <author>Wang</author>
            <doi>10.48550/arXiv.1610.03454</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Wang, W., Yan, X., Lee, H., &amp; Livescu, K. (2017). Deep Variational Canonical Correlation Analysis. https://doi.org/10.48550/arXiv.1610.03454</unstructured_citation>
          </citation>
          <citation key="vasco2022leveraging">
            <article_title>Leveraging hierarchy in multimodal generative models for effective cross-modality inference</article_title>
            <author>Vasco</author>
            <journal_title>Neural Networks</journal_title>
            <volume>146</volume>
            <doi>10.1016/j.neunet.2021.11.019</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Vasco, M., Yin, H., Melo, F. S., &amp; Paiva, A. (2022). Leveraging hierarchy in multimodal generative models for effective cross-modality inference. Neural Networks, 146, 238–255. https://doi.org/10.1016/j.neunet.2021.11.019</unstructured_citation>
          </citation>
          <citation key="hwang2021multi">
            <article_title>Multi-view representation learning via total correlation objective</article_title>
            <author>Hwang</author>
            <journal_title>Advances in Neural Information Processing Systems</journal_title>
            <volume>34</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Hwang, H., Kim, G.-H., Hong, S., &amp; Kim, K.-E. (2021). Multi-view representation learning via total correlation objective. Advances in Neural Information Processing Systems, 34, 12194–12207.</unstructured_citation>
          </citation>
          <citation key="senellart:2023">
            <article_title>Improving multimodal joint variational autoencoders through normalizing flows and correlation analysis</article_title>
            <author>Senellart</author>
            <journal_title>arXiv preprint arXiv:2305.11832</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Senellart, A., Chadebec, C., &amp; Allassonnière, S. (2023). Improving multimodal joint variational autoencoders through normalizing flows and correlation analysis. arXiv Preprint arXiv:2305.11832.</unstructured_citation>
          </citation>
          <citation key="Aguila2023">
            <article_title>Multi-view-AE: A python package for multi-view autoencoder models</article_title>
            <author>Aguila</author>
            <journal_title>Journal of Open Source Software</journal_title>
            <issue>85</issue>
            <volume>8</volume>
            <doi>10.21105/joss.05093</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Aguila, A. L., Jayme, A., Montaña-Brown, N., Heuveline, V., &amp; Altmann, A. (2023). Multi-view-AE: A python package for multi-view autoencoder models. Journal of Open Source Software, 8(85), 5093. https://doi.org/10.21105/joss.05093</unstructured_citation>
          </citation>
          <citation key="suzuki_survey_2022">
            <article_title>A survey of multimodal deep generative models</article_title>
            <author>Suzuki</author>
            <journal_title>Advanced Robotics</journal_title>
            <issue>5-6</issue>
            <volume>36</volume>
            <doi>10.1080/01691864.2022.2035253</doi>
            <issn>0169-1864</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Suzuki, M., &amp; Matsuo, Y. (2022). A survey of multimodal deep generative models. Advanced Robotics, 36(5-6), 261–278. https://doi.org/10.1080/01691864.2022.2035253</unstructured_citation>
          </citation>
          <citation key="tian:2019">
            <article_title>Latent Translation: Crossing Modalities by Bridging Generative Models</article_title>
            <author>Tian</author>
            <journal_title>ArXiv</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Tian, Y., &amp; Engel, J. (2019). Latent Translation: Crossing Modalities by Bridging Generative Models. ArXiv.</unstructured_citation>
          </citation>
          <citation key="chadebec_DA">
            <article_title>Data augmentation in high dimensional low sample size setting using a geometry-based variational autoencoder</article_title>
            <author>Chadebec</author>
            <journal_title>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal_title>
            <issue>3</issue>
            <volume>45</volume>
            <doi>10.1109/TPAMI.2022.3185773</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Chadebec, C., Thibeau-Sutre, E., Burgos, N., &amp; Allassonnière, S. (2023). Data augmentation in high dimensional low sample size setting using a geometry-based variational autoencoder. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2879–2896. https://doi.org/10.1109/TPAMI.2022.3185773</unstructured_citation>
          </citation>
          <citation key="sejnova:2024">
            <article_title>Benchmarking multimodal variational autoencoders: CdSprites+ dataset and toolkit</article_title>
            <author>Sejnova</author>
            <cYear>2024</cYear>
            <unstructured_citation>Sejnova, G., Vavrecka, M., Stepanova, K., &amp; Taniguchi, T. (2024). Benchmarking multimodal variational autoencoders: CdSprites+ dataset and toolkit. https://arxiv.org/abs/2209.03048</unstructured_citation>
          </citation>
          <citation key="suzuki2023pixyz">
            <article_title>Pixyz: A python library for developing deep generative models</article_title>
            <author>Masahiro Suzuki</author>
            <journal_title>Advanced Robotics</journal_title>
            <volume>0</volume>
            <doi>10.1080/01691864.2023.2244568</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Masahiro Suzuki, T. K., &amp; Matsuo, Y. (2023). Pixyz: A python library for developing deep generative models. In Advanced Robotics (No. 0; Vol. 0, pp. 1–16). Taylor &amp; Francis. https://doi.org/10.1080/01691864.2023.2244568</unstructured_citation>
          </citation>
          <citation key="lee:2021">
            <article_title>Private-shared disentangled multimodal VAE for learning of latent representations</article_title>
            <author>Lee</author>
            <doi>10.1109/CVPRW53098.2021.00185</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Lee, M., &amp; Pavlovic, V. (2021). Private-shared disentangled multimodal VAE for learning of latent representations. 1692–1700. https://doi.org/10.1109/CVPRW53098.2021.00185</unstructured_citation>
          </citation>
          <citation key="palumbo2024deep">
            <article_title>Deep generative clustering with multimodal diffusion variational autoencoders</article_title>
            <author>Palumbo</author>
            <cYear>2024</cYear>
            <unstructured_citation>Palumbo, E., Manduchi, L., Laguna, S., Chopard, D., &amp; Vogt, J. E. (2024). Deep generative clustering with multimodal diffusion variational autoencoders. https://openreview.net/forum?id=k5THrhXDV3</unstructured_citation>
          </citation>
          <citation key="dorent:2023">
            <article_title>Unified brain MR-ultrasound synthesis using multi-modal hierarchical representations</article_title>
            <author>Dorent</author>
            <journal_title>Medical image computing and computer assisted intervention – MICCAI 2023</journal_title>
            <doi>10.1007/978-3-031-43999-5_43</doi>
            <issn>1611-3349</issn>
            <isbn>9783031439995</isbn>
            <cYear>2023</cYear>
            <unstructured_citation>Dorent, R., Haouchine, N., Kogl, F., Joutard, S., Juvekar, P., Torio, E., Golby, A. J., Ourselin, S., Frisken, S., Vercauteren, T., Kapur, T., &amp; Wells, W. M. (2023). Unified brain MR-ultrasound synthesis using multi-modal hierarchical representations. In Medical image computing and computer assisted intervention – MICCAI 2023 (pp. 448–458). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-43999-5_43</unstructured_citation>
          </citation>
          <citation key="suzuki:2023:mitigating">
            <article_title>Mitigating the limitations of multimodal VAEs with coordination-based approach</article_title>
            <author>Suzuki</author>
            <cYear>2023</cYear>
            <unstructured_citation>Suzuki, M., &amp; Matsuo, Y. (2023). Mitigating the limitations of multimodal VAEs with coordination-based approach. https://openreview.net/forum?id=Rn8u4MYgeNJ</unstructured_citation>
          </citation>
          <citation key="antelmi:2019">
            <article_title>Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data</article_title>
            <author>Antelmi</author>
            <volume>97</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Antelmi, L., Ayache, N., Robert, P., &amp; Lorenzi, M. (2019). Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data. 97, 302–311. https://proceedings.mlr.press/v97/antelmi19a.html</unstructured_citation>
          </citation>
          <citation key="sutter:mmjsd">
            <article_title>Multimodal generative learning utilizing jensen-shannon-divergence</article_title>
            <author>Sutter</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/2006.08242</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Sutter, T. M., Daunhawer, I., &amp; Vogt, J. E. (2020). Multimodal generative learning utilizing jensen-shannon-divergence. CoRR, abs/2006.08242. https://arxiv.org/abs/2006.08242</unstructured_citation>
          </citation>
          <citation key="aguila:poe">
            <article_title>Multi-modal variational autoencoders for&amp;nbsp;normative modelling across multiple imaging modalities</article_title>
            <author>Lawry Aguila</author>
            <doi>10.1007/978-3-031-43907-0_41</doi>
            <isbn>978-3-031-43906-3</isbn>
            <cYear>2023</cYear>
            <unstructured_citation>Lawry Aguila, A., Chapman, J., &amp; Altmann, A. (2023). Multi-modal variational autoencoders for&amp;nbsp;normative modelling across multiple imaging modalities. 425–434. https://doi.org/10.1007/978-3-031-43907-0_41</unstructured_citation>
          </citation>
          <citation key="dccae">
            <article_title>On deep multi-view representation learning</article_title>
            <author>Wang</author>
            <journal_title>Proceedings of the 32nd international conference on machine learning</journal_title>
            <volume>37</volume>
            <cYear>2015</cYear>
            <unstructured_citation>Wang, W., Arora, R., Livescu, K., &amp; Bilmes, J. (2015). On deep multi-view representation learning. In F. Bach &amp; D. Blei (Eds.), Proceedings of the 32nd international conference on machine learning (Vol. 37, pp. 1083–1092). PMLR. https://proceedings.mlr.press/v37/wangb15.html</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
