<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7613</article-id>
<article-id pub-id-type="doi">10.21105/joss.07613</article-id>
<title-group>
<article-title><monospace>aweSOM</monospace>: a CPU/GPU-accelerated
Self-organizing Map and Statistically Combined Ensemble Framework for
Machine-learning Clustering Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6600-2517</contrib-id>
<name>
<surname>Ha</surname>
<given-names>Trung</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3226-4575</contrib-id>
<name>
<surname>Nättilä</surname>
<given-names>Joonas</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2685-2434</contrib-id>
<name>
<surname>Davelaar</surname>
<given-names>Jordy</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
<xref ref-type="aff" rid="aff-7"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Astronomy, University of
Massachusetts-Amherst, Amherst, MA 01003, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Center for Computational Astrophysics, Flatiron Institute,
162 Fifth Avenue, New York, NY 10010, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Physics, University of North Texas, Denton,
TX 76203, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Physics, University of Helsinki, P.O. Box 64,
University of Helsinki, FI-00014, Finland</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Physics Department and Columbia Astrophysics Laboratory,
Columbia University, 538 West 120th Street, New York, NY 10027,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Department of Astrophysical Sciences, Peyton Hall,
Princeton University, Princeton, NJ 08544, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>NASA Hubble Fellowship Program, Einstein
Fellow</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-10-08">
<day>8</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>107</issue>
<fpage>7613</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>astronomy</kwd>
<kwd>plasma</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We introduce <monospace>aweSOM</monospace>, an open-source Python
  package for machine learning (ML) clustering and classification, using
  a Self-organizing Maps (SOM,
  <xref alt="Kohonen, 1990" rid="ref-kohonen1990" ref-type="bibr">Kohonen,
  1990</xref>) algorithm that incorporates CPU/GPU acceleration to
  accommodate large (<inline-formula><alternatives>
  <tex-math><![CDATA[N > 10^6]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&gt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[N]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
  is the number of data points), multidimensional datasets.
  <monospace>aweSOM</monospace> consists of two main modules, one that
  handles the initialization and training of the SOM, and another that
  stacks the results of multiple SOM realizations to obtain more
  statistically robust clusters.</p>
  <p>Existing Python-based SOM implementations (e.g.,
  <monospace>POPSOM</monospace>, Yuan
  (<xref alt="2018" rid="ref-yuan2018" ref-type="bibr">2018</xref>);
  <monospace>MiniSom</monospace>, Vettigli
  (<xref alt="2018" rid="ref-minisom" ref-type="bibr">2018</xref>);
  <monospace>sklearn-som</monospace>) primarily serve as
  proof-of-concept demonstrations, optimized for smaller datasets, but
  lacking scalability for large, multidimensional data.
  <monospace>aweSOM</monospace> provides a solution for this gap in
  capability, with good performance scaling up to
  <inline-formula><alternatives>
  <tex-math><![CDATA[\sim 10^8]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>8</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  individual points, and capable of utilizing multiple features per
  point. We compare the code performance against the legacy
  implementations it is based on, and find a
  <inline-formula><alternatives>
  <tex-math><![CDATA[10 - 100 \times]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>10</mml:mn><mml:mo>−</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  speed up, as well as significantly improved memory efficiency, due to
  several built-in optimizations.</p>
  <p>As a companion to this paper, Ha et al.
  (<xref alt="2024" rid="ref-ha2024" ref-type="bibr">2024</xref>)
  demonstrates the capabilities of <monospace>aweSOM</monospace> in
  analyzing the physics of plasma turbulence. Detailed instructions on
  how to install, test, and replicate the results of the paper are
  available in the online
  <ext-link ext-link-type="uri" xlink:href="https://awesom.readthedocs.io/en/latest/">documentation</ext-link>.
  Also included in the documentation is an example of applying
  <monospace>aweSOM</monospace> to the Iris dataset
  (<xref alt="Fisher, 1936" rid="ref-iris53" ref-type="bibr">Fisher,
  1936</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <sec id="the-self-organizing-map-algorithm">
    <title>The self-organizing map algorithm</title>
    <p>A SOM algorithm is an unsupervised ML technique that excels at
    dimensionality reduction, clustering, and classification tasks. It
    consists of a 2-dimensional (2D) lattice of nodes. Each node
    contains a weight vector that matches the dimensionality of the
    input data. A SOM performs clustering by adapting the weight vectors
    of nodes, progressively reshaping the lattice’s topology to match
    the intrinsic clustering of the input data. In this manner, a SOM
    lattice can capture multidimensional correlations in the input
    data.</p>
    <p>SOM is commonly used in various real-world applications, such as
    in the financial sector (e.g.,
    <xref alt="Alshantti &amp; Rasheed, 2021" rid="ref-Alshantti2021" ref-type="bibr">Alshantti
    &amp; Rasheed, 2021</xref>;
    <xref alt="Pei et al., 2023" rid="ref-Pei2023" ref-type="bibr">Pei
    et al., 2023</xref>), in environmental surveys (e.g.,
    <xref alt="Alvarez-Guerra et al., 2008" rid="ref-Alvarez2008" ref-type="bibr">Alvarez-Guerra
    et al., 2008</xref>;
    <xref alt="Li et al., 2020" rid="ref-Li2020" ref-type="bibr">Li et
    al., 2020</xref>), in medical technology (e.g.,
    <xref alt="Hautaniemi et al., 2003" rid="ref-Hautaniemi2003" ref-type="bibr">Hautaniemi
    et al., 2003</xref>;
    <xref alt="Kawaguchi et al., 2024" rid="ref-Kawaguchi2024" ref-type="bibr">Kawaguchi
    et al., 2024</xref>), among others. <monospace>aweSOM</monospace> is
    originally developed to be used in analyzing astrophysical
    simulations, but can be applied to a wide variety of real-world
    data.</p>
    <sec id="popsom">
      <title><monospace>POPSOM</monospace></title>
      <p>We base the SOM module of <monospace>aweSOM</monospace> on
      <monospace>POPSOM</monospace>
      (<xref alt="Hamel, 2019" rid="ref-hamel2019" ref-type="bibr">Hamel,
      2019</xref>;
      <xref alt="Yuan, 2018" rid="ref-yuan2018" ref-type="bibr">Yuan,
      2018</xref>), a R-based SOM model. <monospace>POPSOM</monospace>
      was developed as a single-threaded, stochastic training algorithm
      with built-in visualization capabilities. However, due to its
      single-threaded nature, the algorithm does not scale well with
      large datasets. When <inline-formula><alternatives>
      <tex-math><![CDATA[N \gtrsim 10^6]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≳</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      <monospace>POPSOM</monospace> is often unable to complete the
      training process as the dimensionality of the input data increases
      due to its much higher memory usage. As an example, we generated a
      mock dataset with <inline-formula><alternatives>
      <tex-math><![CDATA[N = 10^6]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[F = 6]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      dimensions, then trained it on a lattice of
      <inline-formula><alternatives>
      <tex-math><![CDATA[X = 63]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mn>63</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
      and <inline-formula><alternatives>
      <tex-math><![CDATA[Y = 32]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[X, Y]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      are the dimensions of the lattice, using one Intel Icelake node
      with 64 cores and 1 TB memory. <monospace>POPSOM</monospace>
      completed the training in <inline-formula><alternatives>
      <tex-math><![CDATA[\approx 2200]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>2200</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      s and consumed <inline-formula><alternatives>
      <tex-math><![CDATA[\approx 600]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>600</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      GB of system memory at its peak.</p>
    </sec>
    <sec id="rewriting-popsom-into-awesom">
      <title>Rewriting <monospace>POPSOM</monospace> into
      <monospace>aweSOM</monospace></title>
      <p>To combat the long training time and excessive memory usage, we
      rewrite <monospace>POPSOM</monospace> with multiple
      optimizations/parallelizations. We replaced legacy code with
      modern <monospace>NumPy</monospace> functions for updating the
      lattice (a 3D array) and eliminated the use of
      <monospace>pandas</monospace> DataFrames
      (<xref alt="The pandas development team, 2024" rid="ref-pandas" ref-type="bibr">The
      pandas development team, 2024</xref>), which consume significantly
      more memory. The weight vector modifications in the DataFrame were
      also less efficient compared to the NumPy arrays used in aweSOM.
      Furthermore, for the steps where parallelization could be
      leveraged (such as when the cluster labels are mapped to the
      lattice, then to the input data), we integrate
      <monospace>Numba</monospace>
      (<xref alt="Lam et al., 2015" rid="ref-numba" ref-type="bibr">Lam
      et al., 2015</xref>) to take advantage of its Just-In-Time (JIT)
      compiler and simple parallelization of loops. In the same example
      as above, <monospace>aweSOM</monospace> took
      <inline-formula><alternatives>
      <tex-math><![CDATA[\approx 200]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      s and consumed <inline-formula><alternatives>
      <tex-math><![CDATA[\approx 450]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>≈</mml:mo><mml:mn>450</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      MB of memory to complete the training and clustering. In addition
      to the <inline-formula><alternatives>
      <tex-math><![CDATA[\sim 10 \times]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>10</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      speedup, <monospace>aweSOM</monospace> is also
      <inline-formula><alternatives>
      <tex-math><![CDATA[\sim 10^3 \times]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>×</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      more memory-efficient.</p>
      <p>The left hand side of
      <xref alt="[fig:sce_scaling]" rid="figU003Asce_scaling">[fig:sce_scaling]</xref>
      shows a graph of the performance between
      <monospace>aweSOM</monospace> and the legacy
      <monospace>POPSOM</monospace> implementation over a range of
      <inline-formula><alternatives>
      <tex-math><![CDATA[N]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>F</mml:mi></mml:math></alternatives></inline-formula>,
      performed on one Intel Icelake compute node with 64 CPU cores and
      1 TB memory. While <monospace>POPSOM</monospace> initially
      performs slightly faster than <monospace>aweSOM</monospace> for
      <inline-formula><alternatives>
      <tex-math><![CDATA[N \lesssim 10^4]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≲</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      this changes when <inline-formula><alternatives>
      <tex-math><![CDATA[N]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
      exceeds <inline-formula><alternatives>
      <tex-math><![CDATA[5 \times 10^5]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      after that <monospace>aweSOM</monospace> consistently outperforms
      <monospace>POPSOM</monospace> by approximately a factor of
      <inline-formula><alternatives>
      <tex-math><![CDATA[10]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>10</mml:mn></mml:math></alternatives></inline-formula>.
      Critically, <monospace>POPSOM</monospace> fails to complete its
      clusters mapping for <inline-formula><alternatives>
      <tex-math><![CDATA[N \gtrsim 10^6, F > 4]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≳</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mi>F</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      because the memory buffer of the test node was exceeded.</p>
    </sec>
  </sec>
  <sec id="the-statistically-combined-ensemble-method">
    <title>The statistically combined ensemble method</title>
    <p>The statistically combined ensemble (SCE) method was developed by
    Bussov &amp; Nättilä
    (<xref alt="2021" rid="ref-bussov2021" ref-type="bibr">2021</xref>)
    to stack the result of multiple independent clustering realizations
    into a statically significant set of clusters. This method
    represents a form of ensemble learning. Additionally, SCE can also
    be used independently from the base SOM algorithm, and is compatible
    with any general unsupervised classification algorithm.</p>
    <sec id="the-legacy-sce-implementation">
      <title>The legacy SCE implementation</title>
      <p>In its original version, the SCE was saved as a nested
      dictionary of boolean arrays, each of which contains the spatial
      similarity index <inline-formula><alternatives>
      <tex-math><![CDATA[g]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>g</mml:mi></mml:math></alternatives></inline-formula>
      between cluster <inline-formula><alternatives>
      <tex-math><![CDATA[C]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula>
      and cluster <inline-formula><alternatives>
      <tex-math><![CDATA[C']]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>C</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      The total number of operations scales as
      <inline-formula><alternatives>
      <tex-math><![CDATA[N_{C}^R]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msubsup><mml:mi>N</mml:mi><mml:mi>C</mml:mi><mml:mi>R</mml:mi></mml:msubsup></mml:math></alternatives></inline-formula>,
      where <inline-formula><alternatives>
      <tex-math><![CDATA[N_C]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is the number of clusters in each realization, and
      <inline-formula><alternatives>
      <tex-math><![CDATA[R]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>R</mml:mi></mml:math></alternatives></inline-formula>
      is the number of realizations. For example, in our use case
      involving plasma simulation data
      (<xref alt="Ha et al., 2024" rid="ref-ha2024" ref-type="bibr">Ha
      et al., 2024</xref>), each SOM realization produces on average 7
      clusters, and the SCE analysis incorporates 36 realizations,
      resulting in approximately <inline-formula><alternatives>
      <tex-math><![CDATA[7^{36} \sim 10^{30}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mn>7</mml:mn><mml:mn>36</mml:mn></mml:msup><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>30</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      array-to-array comparisons.</p>
    </sec>
    <sec id="integrating-sce-into-awesom-with-jax">
      <title>Integrating SCE into <monospace>aweSOM</monospace> with
      <monospace>JAX</monospace></title>
      <p>To mitigate this bottleneck, we rewrite the legacy SCE code
      with <monospace>JAX</monospace>
      (<xref alt="Bradbury et al., 2018" rid="ref-jax" ref-type="bibr">Bradbury
      et al., 2018</xref>) to significantly enhance the performance of
      array-to-array comparisons (which are matrix multiplications) by
      leveraging the GPU’s parallel-computing advantage over the CPU. We
      implement this optimization by replacing the original nested
      dictionaries with data arrays. Then, every instance of matrix
      operation using <monospace>NumPy</monospace> is converted to
      <monospace>jax.numpy</monospace>. Additionally, we implement
      internal checks such that the SCE code automatically reverts to
      <monospace>NumPy</monospace> if GPU-accelerated
      <monospace>JAX</monospace> is not available.</p>
      <p>Similar to the SOM implementation, the SCE implementation in
      <monospace>aweSOM</monospace> demonstrates excellent scalability
      as the number of data points increases. The right hand side of
      <xref alt="[fig:sce_scaling]" rid="figU003Asce_scaling">[fig:sce_scaling]</xref>
      shows a graph of the performance between the two implementations
      given <inline-formula><alternatives>
      <tex-math><![CDATA[R = 20]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
      At <inline-formula><alternatives>
      <tex-math><![CDATA[N < 5 \times 10^4]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      the legacy code is faster due to the overhead from loading
      <monospace>JAX</monospace> and the JIT compiler. However,
      <monospace>aweSOM</monospace> quickly exceeds the performance of
      the legacy code, and begins to approach its maximum speed-up of
      <inline-formula><alternatives>
      <tex-math><![CDATA[\sim 100 \times]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>100</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      at <inline-formula><alternatives>
      <tex-math><![CDATA[N \gtrsim 10^7]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≳</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>7</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      (performed on one NVIDIA A100-40GB GPU). On the other hand, when
      running on CPU-only with <monospace>NumPy</monospace>,
      <monospace>aweSOM</monospace> consistently shows a
      <inline-formula><alternatives>
      <tex-math><![CDATA[2 \times]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      speed improvement over the legacy code. Altogether, it is best to
      use <monospace>aweSOM</monospace> with
      <monospace>Numpy</monospace> when <inline-formula><alternatives>
      <tex-math><![CDATA[N \lesssim 10^5]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≲</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
      and with <monospace>JAX</monospace> when
      <inline-formula><alternatives>
      <tex-math><![CDATA[N \gtrsim 10^5]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>≳</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>.</p>
      <fig>
        <caption><p>Performance scaling for
        <monospace>aweSOM</monospace> vs. the legacy SOM (left) and SCE
        (right) implementation. The top panels show the time for each
        implementation to complete analysis of
        <inline-formula><alternatives>
        <tex-math><![CDATA[N]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
        number of data points. On the right panel, the dotted line
        extending from the olive line shows linear extrapolations from
        the data in order to estimate the speedup. The bottom panels
        show the ratio between the time taken by the legacy code divided
        by the time taken by <monospace>aweSOM</monospace>. In the SOM
        analysis, we consider a dataset with
        <inline-formula><alternatives>
        <tex-math><![CDATA[F = 6]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
        and <inline-formula><alternatives>
        <tex-math><![CDATA[F = 10]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
        dimensions. In the SCE analysis, we test the scaling of both a
        GPU-accelerated implementation (with <monospace>JAX</monospace>)
        and a CPU-only implementation (with
        <monospace>NumPy</monospace>).
        <styled-content id="figU003Asce_scaling"></styled-content></p></caption>
        <graphic mimetype="application" mime-subtype="pdf" xlink:href="joss_scaling.pdf" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors would like to thank Kaze Wong for the valuable input in
  setting up <monospace>JAX</monospace> for the SCE analysis. The
  authors would also like to thank Shirley Ho and Lorenzo Sironi for the
  useful discussions. TH acknowledges support from a pre-doctoral
  program at the Center for Computational Astrophysics, which is part of
  the Flatiron Institute. JN is supported by an ERC grant (ILLUMINATOR,
  101114623). JD is supported by NASA through the NASA Hubble Fellowship
  grant HST-HF2-51552.001-A, awarded by the Space Telescope Science
  Institute, which is operated by the Association of Universities for
  Research in Astronomy, Incorporated, under NASA contract NAS5-26555.
  <monospace>aweSOM</monospace> was developed and primarily run at
  facilities supported by the Scientific Computing Core at the Flatiron
  Institute. Research at the Flatiron Institute is supported by the
  Simons Foundation.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-hamel2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hamel</surname><given-names>Lutz</given-names></name>
      </person-group>
      <article-title>VSOM: Efficient, Stochastic Self-organizing Map Training</article-title>
      <source>Intelligent Systems and Applications</source>
      <person-group person-group-type="editor">
        <name><surname>Arai</surname><given-names>Kohei</given-names></name>
        <name><surname>Kapoor</surname><given-names>Supriya</given-names></name>
        <name><surname>Bhatia</surname><given-names>Rahul</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <isbn>978-3-030-01057-7</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-030-01057-7_60</pub-id>
      <fpage>805</fpage>
      <lpage>821</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yuan2018">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Yuan</surname><given-names>Li</given-names></name>
      </person-group>
      <article-title>Implementation of Self-Organizing Maps with Python</article-title>
      <publisher-name>University of Rhode Island</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-09-24">2024</year><month>09</month><day>24</day></date-in-citation>
      <uri>https://digitalcommons.uri.edu/theses/1244</uri>
      <pub-id pub-id-type="doi">10.23860/thesis-yuan-li-2018</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bussov2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bussov</surname><given-names>Maarja</given-names></name>
        <name><surname>Nättilä</surname><given-names>Joonas</given-names></name>
      </person-group>
      <article-title>Segmentation of turbulent computational fluid dynamics simulations with unsupervised ensemble learning</article-title>
      <source>Signal Processing: Image Communication</source>
      <year iso-8601-date="2021-11">2021</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-03-30">2024</year><month>03</month><day>30</day></date-in-citation>
      <volume>99</volume>
      <issn>0923-5965</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0923596521002150</uri>
      <pub-id pub-id-type="doi">10.1016/j.image.2021.116450</pub-id>
      <fpage>116450</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kohonen1990">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kohonen</surname><given-names>T.</given-names></name>
      </person-group>
      <article-title>The self-organizing map</article-title>
      <source>Proceedings of the IEEE</source>
      <year iso-8601-date="1990-09">1990</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-05-15">2024</year><month>05</month><day>15</day></date-in-citation>
      <volume>78</volume>
      <issue>9</issue>
      <issn>1558-2256</issn>
      <uri>https://ieeexplore.ieee.org/abstract/document/58325?casa_token=tYJbvCiScXgAAAAA:e0OqIPDn1kbiKn8KQpq_5-r00iYB1fwi7ULOa4WuPC588gnVLAHVo-B8PPP2UmlgvFyr9YgEMQM</uri>
      <pub-id pub-id-type="doi">10.1109/5.58325</pub-id>
      <fpage>1464</fpage>
      <lpage>1480</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ha2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ha</surname><given-names>Trung</given-names></name>
        <name><surname>Nättilä</surname><given-names>Joonas</given-names></name>
        <name><surname>Davelaar</surname><given-names>Jordy</given-names></name>
        <name><surname>Sironi</surname><given-names>Lorenzo</given-names></name>
      </person-group>
      <article-title>Machine-learning characterization of intermittency in plasma turbulence: Single and double sheet structures</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2410.01878</uri>
    </element-citation>
  </ref>
  <ref id="ref-jax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-numba">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lam</surname><given-names>Siu Kwan</given-names></name>
        <name><surname>Pitrou</surname><given-names>Antoine</given-names></name>
        <name><surname>Seibert</surname><given-names>Stanley</given-names></name>
      </person-group>
      <article-title>Numba: A LLVM-based python JIT compiler</article-title>
      <source>Proceedings of the second workshop on the LLVM compiler infrastructure in HPC</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>9781450340052</isbn>
      <uri>https://doi.org/10.1145/2833157.2833162</uri>
      <pub-id pub-id-type="doi">10.1145/2833157.2833162</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-minisom">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Vettigli</surname><given-names>Giuseppe</given-names></name>
      </person-group>
      <article-title>MiniSom: Minimalistic and NumPy-based implementation of the self organizing map</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/JustGlowing/minisom/</uri>
    </element-citation>
  </ref>
  <ref id="ref-iris53">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Fisher</surname><given-names>R. A.</given-names></name>
      </person-group>
      <article-title>Iris</article-title>
      <publisher-name>UCI Machine Learning Repository</publisher-name>
      <year iso-8601-date="1936">1936</year>
      <pub-id pub-id-type="doi">10.24432/C56C76</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pandas">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <string-name>The pandas development team</string-name>
      </person-group>
      <article-title>Pandas-dev/pandas: pandas</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2024-09">2024</year><month>09</month>
      <uri>https://doi.org/10.5281/zenodo.13819579</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.13819579</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Alshantti2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alshantti</surname><given-names>Abdallah</given-names></name>
        <name><surname>Rasheed</surname><given-names>Adil</given-names></name>
      </person-group>
      <article-title>Self-organising map based framework for investigating accounts suspected of money laundering</article-title>
      <source>Frontiers in Artificial Intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>4</volume>
      <issn>2624-8212</issn>
      <uri>https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2021.761925</uri>
      <pub-id pub-id-type="doi">10.3389/frai.2021.761925</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Pei2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pei</surname><given-names>Dehao</given-names></name>
        <name><surname>Luo</surname><given-names>Chao</given-names></name>
        <name><surname>Liu</surname><given-names>Xiaomei</given-names></name>
      </person-group>
      <article-title>Financial trading decisions based on deep fuzzy self-organizing map</article-title>
      <source>Applied Soft Computing</source>
      <year iso-8601-date="2023">2023</year>
      <volume>134</volume>
      <issn>1568-4946</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S1568494622010213</uri>
      <pub-id pub-id-type="doi">10.1016/j.asoc.2022.109972</pub-id>
      <fpage>109972</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Alvarez2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alvarez-Guerra</surname><given-names>Manuel</given-names></name>
        <name><surname>González-Piñuela</surname><given-names>Cristina</given-names></name>
        <name><surname>Andrés</surname><given-names>Ana</given-names></name>
        <name><surname>Galán</surname><given-names>Berta</given-names></name>
        <name><surname>Viguri</surname><given-names>Javier R.</given-names></name>
      </person-group>
      <article-title>Assessment of self-organizing map artificial neural networks for the classification of sediment quality</article-title>
      <source>Environment International</source>
      <year iso-8601-date="2008">2008</year>
      <volume>34</volume>
      <issue>6</issue>
      <issn>0160-4120</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S016041200800007X</uri>
      <pub-id pub-id-type="doi">10.1016/j.envint.2008.01.006</pub-id>
      <fpage>782</fpage>
      <lpage>790</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Li2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Kenan</given-names></name>
        <name><surname>Sward</surname><given-names>Katherine</given-names></name>
        <name><surname>Deng</surname><given-names>Huiyu</given-names></name>
        <name><surname>Morrison</surname><given-names>John</given-names></name>
        <name><surname>Habre</surname><given-names>Rima</given-names></name>
        <name><surname>Chiang</surname><given-names>Yao-Yi</given-names></name>
        <name><surname>Ambite</surname><given-names>Jose-Luis</given-names></name>
        <name><surname>Wilson</surname><given-names>John</given-names></name>
        <name><surname>Eckel</surname><given-names>Sandrah</given-names></name>
      </person-group>
      <article-title>Using dynamic time warping self-organizing maps to characterize diurnal patterns in environmental exposures</article-title>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <pub-id pub-id-type="doi">10.21203/rs.3.rs-87487/v1</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Kawaguchi2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kawaguchi</surname><given-names>Takahiro</given-names></name>
        <name><surname>Ono</surname><given-names>Koki</given-names></name>
        <name><surname>Hikawa</surname><given-names>Hiroomi</given-names></name>
      </person-group>
      <article-title>Electroencephalogram-based facial gesture recognition using self-organizing map</article-title>
      <source>Sensors</source>
      <year iso-8601-date="2024">2024</year>
      <volume>24</volume>
      <issue>9</issue>
      <issn>1424-8220</issn>
      <uri>https://www.mdpi.com/1424-8220/24/9/2741</uri>
      <pub-id pub-id-type="doi">10.3390/s24092741</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Hautaniemi2003">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hautaniemi</surname><given-names>Sampsa</given-names></name>
        <name><surname>Yli-Harja</surname><given-names>Olli</given-names></name>
        <name><surname>Astola</surname><given-names>Jaakko</given-names></name>
        <name><surname>Kauraniemi</surname><given-names>Päivikki</given-names></name>
        <name><surname>Kallioniemi</surname><given-names>Anne</given-names></name>
        <name><surname>Wolf</surname><given-names>Maija</given-names></name>
        <name><surname>Ruiz</surname><given-names>Jimmy</given-names></name>
        <name><surname>Mousses</surname><given-names>Spyro</given-names></name>
        <name><surname>Kallioniemi</surname><given-names>Olli-P.</given-names></name>
      </person-group>
      <article-title>Analysis and visualization of gene expression microarray data in human cancer using self-organizing maps</article-title>
      <source>Machine Learning</source>
      <publisher-name>Kluwer Academic Publishers</publisher-name>
      <publisher-loc>USA</publisher-loc>
      <year iso-8601-date="2003">2003</year>
      <volume>52</volume>
      <issue>1–2</issue>
      <issn>0885-6125</issn>
      <uri>https://doi.org/10.1023/A:1023941307670</uri>
      <pub-id pub-id-type="doi">10.1023/A:1023941307670</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
