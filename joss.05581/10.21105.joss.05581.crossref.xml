<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230804T171634-85a59c051bf1374e64743772b1f5681da23d2c03</doi_batch_id>
    <timestamp>20230804171634</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>08</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>88</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>EcoAssist: A no-code platform to train and deploy
custom YOLOv5 object detection models</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Peter</given_name>
            <surname>van Lunteren</surname>
            <ORCID>https://orcid.org/0000-0001-5488-4225</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>08</month>
          <day>04</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5581</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05581</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7223363</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5581</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05581</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05581</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05581.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="gyurov">
            <article_title>MegaDetector-GUI: A desktop application that
makes using MegaDetector’s model easier</article_title>
            <author>Gyurov</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Gyurov, P. (2022). MegaDetector-GUI:
A desktop application that makes using MegaDetector’s model easier. In
GitHub repository. GitHub.
https://github.com/petargyurov/megadetector-gui</unstructured_citation>
          </citation>
          <citation key="mcwilliam">
            <article_title>MegaDetector-interface: GUI created for the
microsoft MegaDetector</article_title>
            <author>McWilliam</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>McWilliam, N. (2021).
MegaDetector-interface: GUI created for the microsoft MegaDetector. In
GitHub repository. GitHub.
https://github.com/NaomiMcWilliam/Megadetector-Interface</unstructured_citation>
          </citation>
          <citation key="evans">
            <article_title>CamTrap-detector: Detect animals, humans and
vehicles in camera trap imagery</article_title>
            <author>Evans</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Evans, B. C. (2023).
CamTrap-detector: Detect animals, humans and vehicles in camera trap
imagery. In GitHub repository. GitHub.
https://github.com/bencevans/camtrap-detector</unstructured_citation>
          </citation>
          <citation key="greenberg">
            <article_title>Design patterns for wildlife-related camera
trap image analysis</article_title>
            <author>Greenberg</author>
            <journal_title>Ecology and Evolution</journal_title>
            <issue>24</issue>
            <volume>9</volume>
            <doi>10.1002/ece3.5767</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Greenberg, S., Godin, T., &amp;
Whittington, J. (2019). Design patterns for wildlife-related camera trap
image analysis. Ecology and Evolution, 9(24), 13706–13730.
https://doi.org/10.1002/ece3.5767</unstructured_citation>
          </citation>
          <citation key="heartexlabs">
            <article_title>labelImg: Label studio is a modern,
multi-modal data annotation tool</article_title>
            <author>Heartexlabs</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Heartexlabs. (2022). labelImg: Label
studio is a modern, multi-modal data annotation tool. In GitHub
repository. GitHub.
https://github.com/heartexlabs/labelImg</unstructured_citation>
          </citation>
          <citation key="beery">
            <article_title>Efficient pipeline for camera trap image
review</article_title>
            <author>Beery</author>
            <cYear>2019</cYear>
            <unstructured_citation>Beery, S., Morris, D., &amp; Yang, S.
(2019). Efficient pipeline for camera trap image review.
https://arxiv.org/abs/1907.06772</unstructured_citation>
          </citation>
          <citation key="ceballos">
            <article_title>Vertebrates on the brink as indicators of
biological annihilation and the sixth mass extinction</article_title>
            <author>Ceballos</author>
            <journal_title>Proceedings of the National Academy of
Sciences</journal_title>
            <issue>24</issue>
            <volume>117</volume>
            <doi>10.1073/pnas.1922686117</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Ceballos, G., Ehrlich, P. R., &amp;
Raven, P. H. (2020). Vertebrates on the brink as indicators of
biological annihilation and the sixth mass extinction. Proceedings of
the National Academy of Sciences, 117(24), 13596–13602.
https://doi.org/10.1073/pnas.1922686117</unstructured_citation>
          </citation>
          <citation key="tuia">
            <article_title>Perspectives in machine learning for wildlife
conservation</article_title>
            <author>Tuia</author>
            <journal_title>Nature communications</journal_title>
            <issue>1</issue>
            <volume>13</volume>
            <doi>10.1038/s41467-022-27980-y</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Tuia, D., Kellenberger, B., Beery,
S., Costelloe, B. R., Zuffi, S., Risse, B., Mathis, A., Mathis, M. W.,
Langevelde, F. van, Burghardt, T., Kays, R., Klinck, H., Wikelski, M.,
Couzin, I. D., Horn, G. van, Crofoot, M. C., Stewart, C. V., &amp;
Berger-Wolf, T. (2022). Perspectives in machine learning for wildlife
conservation. Nature Communications, 13(1), 792.
https://doi.org/10.1038/s41467-022-27980-y</unstructured_citation>
          </citation>
          <citation key="schneider">
            <article_title>Deep learning object detection methods for
ecological camera trap data</article_title>
            <author>Schneider</author>
            <journal_title>2018 15th conference on computer and robot
vision (CRV)</journal_title>
            <doi>10.1109/crv.2018.00052</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Schneider, S., Taylor, G. W., &amp;
Kremer, S. (2018, May). Deep learning object detection methods for
ecological camera trap data. 2018 15th Conference on Computer and Robot
Vision (CRV).
https://doi.org/10.1109/crv.2018.00052</unstructured_citation>
          </citation>
          <citation key="gomez">
            <article_title>Towards automatic wild animal monitoring:
Identification of animal species in camera-trap images using very deep
convolutional neural networks</article_title>
            <author>Gomez Villa</author>
            <journal_title>Ecological Informatics</journal_title>
            <volume>41</volume>
            <doi>10.1016/j.ecoinf.2017.07.004</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Gomez Villa, A., Salazar, A., &amp;
Vargas, F. (2017). Towards automatic wild animal monitoring:
Identification of animal species in camera-trap images using very deep
convolutional neural networks. Ecological Informatics, 41, 24–32.
https://doi.org/10.1016/j.ecoinf.2017.07.004</unstructured_citation>
          </citation>
          <citation key="norouzzadeh">
            <article_title>Automatically identifying, counting, and
describing wild animals in camera-trap images with deep
learning</article_title>
            <author>Norouzzadeh</author>
            <journal_title>Proceedings of the National Academy of
Sciences</journal_title>
            <issue>25</issue>
            <volume>115</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Norouzzadeh, M. S., Nguyen, A.,
Kosmala, M., Swanson, A., Palmer, M. S., Packer, C., &amp; Clune, J.
(2018). Automatically identifying, counting, and describing wild animals
in camera-trap images with deep learning. Proceedings of the National
Academy of Sciences, 115(25), E5716–E5725.</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
