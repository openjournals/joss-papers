<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5581</article-id>
<article-id pub-id-type="doi">10.21105/joss.05581</article-id>
<title-group>
<article-title>EcoAssist: A no-code platform to train and deploy custom
YOLOv5 object detection models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5488-4225</contrib-id>
<name>
<surname>van Lunteren</surname>
<given-names>Peter</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Researcher, The Netherlands</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-04-25">
<day>25</day>
<month>4</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>88</issue>
<fpage>5581</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>artificial intelligence</kwd>
<kwd>machine learning</kwd>
<kwd>deep learning</kwd>
<kwd>object detection</kwd>
<kwd>yolov5</kwd>
<kwd>annotation tool</kwd>
<kwd>cameratraps</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>EcoAssist</monospace> is a tkinter based graphical user
  interface (GUI) which is designed to enable object detection for
  novices. The package allows the user to annotate images, train and
  deploy custom models, and post-process imagery without having to write
  code (<xref alt="[fig:1]" rid="figU003A1">[fig:1]</xref>).
  Installation is automated and dependencies will be installed in a
  virtual environment to avoid conflicts. Annotation is done via the
  open-source package <monospace>labelImg</monospace>
  (<xref alt="Heartexlabs, 2022" rid="ref-heartexlabs" ref-type="bibr">Heartexlabs,
  2022</xref>) and post-processing features include folder separation,
  detection visualization, cropping, label creation, and exporting
  results to CSV files. It will automatically run on NVIDIA or Apple
  Silicon GPU and is available for Microsoft Windows, Apple macOS, and
  Linux. The main target audience is ecologists working with motion
  triggered camera traps, although it is not limited to detect animals
  as object detection models are generic. The package is developed to be
  used in conjunction with Timelapse
  (<xref alt="Greenberg et al., 2019" rid="ref-greenberg" ref-type="bibr">Greenberg
  et al., 2019</xref>) (an image analyzer for camera traps) and
  MegaDetector
  (<xref alt="Beery et al., 2019" rid="ref-beery" ref-type="bibr">Beery
  et al., 2019</xref>) (a trained object detection model to localize
  animals in camera trap imagery). More features are planned and will be
  added in the future.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Given the unprecedented global decline of animal diversity
  (<xref alt="Ceballos et al., 2020" rid="ref-ceballos" ref-type="bibr">Ceballos
  et al., 2020</xref>), conservationists urgently need tools for
  accurate and fast assessment of wildlife diversity
  (<xref alt="Tuia et al., 2022" rid="ref-tuia" ref-type="bibr">Tuia et
  al., 2022</xref>). A commonly used method for this is deploying
  ecological motion triggered camera traps
  (<xref alt="Schneider et al., 2018" rid="ref-schneider" ref-type="bibr">Schneider
  et al., 2018</xref>). However, the analysis of such data is often
  expensive, labor-intensive, and time-consuming
  (<xref alt="Beery et al., 2019" rid="ref-beery" ref-type="bibr">Beery
  et al., 2019</xref>). Fortunately, optimizing this workflow with
  machine learning techniques has proven to be an effective method
  (<xref alt="Gomez Villa et al., 2017" rid="ref-gomez" ref-type="bibr">Gomez
  Villa et al., 2017</xref>;
  <xref alt="Norouzzadeh et al., 2018" rid="ref-norouzzadeh" ref-type="bibr">Norouzzadeh
  et al., 2018</xref>;
  <xref alt="Schneider et al., 2018" rid="ref-schneider" ref-type="bibr">Schneider
  et al., 2018</xref>).</p>
  <p>One such example is the MegaDetector model
  (<xref alt="Beery et al., 2019" rid="ref-beery" ref-type="bibr">Beery
  et al., 2019</xref>). This model is able to drastically reduce the
  workload by facilitating the removal of empty camera trap images.
  Although the model does not identify the species, it offers a
  simplification of the annotation process for users wanting to train
  their own project-specific species classifier by providing bounding
  box coordinates. Still, the only way to interact with the model is
  exclusively through Python code and command-line interfaces. Users
  without a programming background might find it difficult to implement
  this software. These users, thus, might miss out on valuable and
  open-source techniques without GUIs such as
  <monospace>EcoAssist</monospace> - which are designed to overcome this
  limitation.</p>
  <p>Besides <monospace>EcoAssist</monospace> there are three other GUIs
  able to deploy the MegaDetector model (i.e.,
  <xref alt="Evans, 2023" rid="ref-evans" ref-type="bibr">Evans,
  2023</xref>;
  <xref alt="Gyurov, 2022" rid="ref-gyurov" ref-type="bibr">Gyurov,
  2022</xref>;
  <xref alt="McWilliam, 2021" rid="ref-mcwilliam" ref-type="bibr">McWilliam,
  2021</xref>). However, none of these packages offer features to
  annotate and train custom models, nor do they accept the deployment of
  custom models. Furthermore, Gyurov
  (<xref alt="2022" rid="ref-gyurov" ref-type="bibr">2022</xref>) and
  McWilliam
  (<xref alt="2021" rid="ref-mcwilliam" ref-type="bibr">2021</xref>) do
  not offer functionality on systems other than Microsoft Windows.</p>
</sec>
<sec id="generic-platform">
  <title>Generic platform</title>
  <p>Although <monospace>EcoAssist</monospace> was originally designed
  for ecologists working with camera trap imagery, it evolved to be a
  more generic platform to be used by any researcher wanting to work
  with object detection models. All features are available with any kind
  of object, which makes it an interesting tool for many academic
  disciplines. The package has proved its ease of use and ability to
  efficiently analyze large datasets and is currently used by dozens of
  research institutions worldwide. Its user-friendly design, simplicity,
  and support will likely extract further scientific interest.</p>
  <fig>
    <caption><p>The annotation (top left), training (top right),
    deployment (bottom left), and documentation window (bottom
    right).<styled-content id="figU003A1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/fig1.png" />
  </fig>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-gyurov">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gyurov</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>MegaDetector-GUI: A desktop application that makes using MegaDetectorâ€™s model easier</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/petargyurov/megadetector-gui</uri>
    </element-citation>
  </ref>
  <ref id="ref-mcwilliam">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>McWilliam</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>MegaDetector-interface: GUI created for the microsoft MegaDetector</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/NaomiMcWilliam/Megadetector-Interface</uri>
    </element-citation>
  </ref>
  <ref id="ref-evans">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Evans</surname><given-names>B. C.</given-names></name>
      </person-group>
      <article-title>CamTrap-detector: Detect animals, humans and vehicles in camera trap imagery</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/bencevans/camtrap-detector</uri>
    </element-citation>
  </ref>
  <ref id="ref-greenberg">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Greenberg</surname><given-names>Saul</given-names></name>
        <name><surname>Godin</surname><given-names>Theresa</given-names></name>
        <name><surname>Whittington</surname><given-names>Jesse</given-names></name>
      </person-group>
      <article-title>Design patterns for wildlife-related camera trap image analysis</article-title>
      <source>Ecology and Evolution</source>
      <publisher-name>Wiley</publisher-name>
      <year iso-8601-date="2019-12">2019</year><month>12</month>
      <volume>9</volume>
      <issue>24</issue>
      <pub-id pub-id-type="doi">10.1002/ece3.5767</pub-id>
      <fpage>13706</fpage>
      <lpage>13730</lpage>
    </element-citation>
  </ref>
  <ref id="ref-heartexlabs">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Heartexlabs</surname></name>
      </person-group>
      <article-title>labelImg: Label studio is a modern, multi-modal data annotation tool</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/heartexlabs/labelImg</uri>
    </element-citation>
  </ref>
  <ref id="ref-beery">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Beery</surname><given-names>Sara</given-names></name>
        <name><surname>Morris</surname><given-names>Dan</given-names></name>
        <name><surname>Yang</surname><given-names>Siyu</given-names></name>
      </person-group>
      <article-title>Efficient pipeline for camera trap image review</article-title>
      <year iso-8601-date="2019-07">2019</year><month>07</month>
      <uri>https://arxiv.org/abs/1907.06772</uri>
    </element-citation>
  </ref>
  <ref id="ref-ceballos">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ceballos</surname><given-names>Gerardo</given-names></name>
        <name><surname>Ehrlich</surname><given-names>Paul R</given-names></name>
        <name><surname>Raven</surname><given-names>Peter H</given-names></name>
      </person-group>
      <article-title>Vertebrates on the brink as indicators of biological annihilation and the sixth mass extinction</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <publisher-name>Proceedings of the National Academy of Sciences</publisher-name>
      <year iso-8601-date="2020-06">2020</year><month>06</month>
      <volume>117</volume>
      <issue>24</issue>
      <pub-id pub-id-type="doi">10.1073/pnas.1922686117</pub-id>
      <fpage>13596</fpage>
      <lpage>13602</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tuia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tuia</surname><given-names>Devis</given-names></name>
        <name><surname>Kellenberger</surname><given-names>Benjamin</given-names></name>
        <name><surname>Beery</surname><given-names>Sara</given-names></name>
        <name><surname>Costelloe</surname><given-names>Blair R</given-names></name>
        <name><surname>Zuffi</surname><given-names>Silvia</given-names></name>
        <name><surname>Risse</surname><given-names>Benjamin</given-names></name>
        <name><surname>Mathis</surname><given-names>Alexander</given-names></name>
        <name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name>
        <name><surname>Langevelde</surname><given-names>Frank van</given-names></name>
        <name><surname>Burghardt</surname><given-names>Tilo</given-names></name>
        <name><surname>Kays</surname><given-names>Roland</given-names></name>
        <name><surname>Klinck</surname><given-names>Holger</given-names></name>
        <name><surname>Wikelski</surname><given-names>Martin</given-names></name>
        <name><surname>Couzin</surname><given-names>Iain D</given-names></name>
        <name><surname>Horn</surname><given-names>Grant van</given-names></name>
        <name><surname>Crofoot</surname><given-names>Margaret C</given-names></name>
        <name><surname>Stewart</surname><given-names>Charles V</given-names></name>
        <name><surname>Berger-Wolf</surname><given-names>Tanya</given-names></name>
      </person-group>
      <article-title>Perspectives in machine learning for wildlife conservation</article-title>
      <source>Nature communications</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2022-02">2022</year><month>02</month>
      <volume>13</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41467-022-27980-y</pub-id>
      <fpage>792</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-schneider">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schneider</surname><given-names>Stefan</given-names></name>
        <name><surname>Taylor</surname><given-names>Graham W</given-names></name>
        <name><surname>Kremer</surname><given-names>Stefan</given-names></name>
      </person-group>
      <article-title>Deep learning object detection methods for ecological camera trap data</article-title>
      <source>2018 15th conference on computer and robot vision (CRV)</source>
      <publisher-name>IEEE</publisher-name>
      <publisher-loc>Toronto, ON, Canada</publisher-loc>
      <year iso-8601-date="2018-05">2018</year><month>05</month>
      <pub-id pub-id-type="doi">10.1109/crv.2018.00052</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gomez">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gomez Villa</surname><given-names>Alexander</given-names></name>
        <name><surname>Salazar</surname><given-names>Augusto</given-names></name>
        <name><surname>Vargas</surname><given-names>Francisco</given-names></name>
      </person-group>
      <article-title>Towards automatic wild animal monitoring: Identification of animal species in camera-trap images using very deep convolutional neural networks</article-title>
      <source>Ecological Informatics</source>
      <year iso-8601-date="2017-03">2017</year><month>03</month>
      <volume>41</volume>
      <pub-id pub-id-type="doi">10.1016/j.ecoinf.2017.07.004</pub-id>
      <fpage>24</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-norouzzadeh">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Norouzzadeh</surname><given-names>Mohammad Sadegh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Anh</given-names></name>
        <name><surname>Kosmala</surname><given-names>Margaret</given-names></name>
        <name><surname>Swanson</surname><given-names>Alexandra</given-names></name>
        <name><surname>Palmer</surname><given-names>Meredith S</given-names></name>
        <name><surname>Packer</surname><given-names>Craig</given-names></name>
        <name><surname>Clune</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>Automatically identifying, counting, and describing wild animals in camera-trap images with deep learning</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <year iso-8601-date="2018-06">2018</year><month>06</month>
      <volume>115</volume>
      <issue>25</issue>
      <fpage>E5716</fpage>
      <lpage>E5725</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
