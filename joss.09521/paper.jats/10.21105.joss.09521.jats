<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9521</article-id>
<article-id pub-id-type="doi">10.21105/joss.09521</article-id>
<title-group>
<article-title>Bijx: Bijections and normalizing flows with
JAX/NNX</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2369-1436</contrib-id>
<name>
<surname>Gerdes</surname>
<given-names>Mathis</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4651-095X</contrib-id>
<name>
<surname>Cheng</surname>
<given-names>Miranda C. N.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Amsterdam, Amsterdam, The
Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Academia Sinica, Taipei, Taiwan</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-09-07">
<day>7</day>
<month>9</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>116</issue>
<fpage>9521</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>JAX</kwd>
<kwd>Flax NNX</kwd>
<kwd>normalizing flows</kwd>
<kwd>continuous normalizing flows</kwd>
<kwd>lattice field theory</kwd>
<kwd>Lie groups</kwd>
<kwd>Fourier analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Normalizing flows (NF) are a powerful class of generative models
  that allow for both efficient sampling and exact likelihood evaluation
  (<xref alt="Kobyzev et al., 2021" rid="ref-kobyzev2021normalizing" ref-type="bibr">Kobyzev
  et al., 2021</xref>;
  <xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>;
  <xref alt="Rezende &amp; Mohamed, 2015" rid="ref-rezende2015variational" ref-type="bibr">Rezende
  &amp; Mohamed, 2015</xref>). Here we present
  <monospace>bijx</monospace>, a JAX library
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>) built on Flax NNX
  (<xref alt="Heek et al., 2024" rid="ref-flax2020github" ref-type="bibr">Heek
  et al., 2024</xref>) that combines a general normalizing flow API with
  specialized tools for physics, especially in the context of lattice
  quantum field theory. The API mirrors standard patterns (e.g.,
  <monospace>Bijection</monospace>, <monospace>Distribution</monospace>,
  <monospace>Chain</monospace>, <monospace>Transformed</monospace>)
  while making every component an <monospace>nnx.Module</monospace> for
  coherent parameter/state management and seamless use with modern JAX
  ML tools. Beyond this general core, <monospace>bijx</monospace>
  provides first-class support for continuous normalizing flows (via
  <monospace>diffrax</monospace>), structure-preserving Crouch–Grossmann
  integrators for matrix Lie groups, symmetry-aware vector fields for
  lattice data, and utilities for Fourier and Lie algebra valued data.
  The package is available via <monospace>pip</monospace>, with source
  code on GitHub (https://github.com/mathisgerdes/bijx). Each capability
  can be used independently: the general NF API, continuous normalizing
  flow (CNF) interface, Lie-group integrators, and physics utilities are
  modular and separable. General ML users can adopt the standard flow
  API and CNF interface without the physics utilities, and vice
  versa.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The JAX ecosystem includes several flow libraries such as
  <monospace>distrax</monospace>
  (<xref alt="Babuschkin et al., 2020" rid="ref-deepmind2020jax" ref-type="bibr">Babuschkin
  et al., 2020</xref>) and <monospace>flowjax</monospace>
  (<xref alt="Ward et al., 2025" rid="ref-flowjax" ref-type="bibr">Ward
  et al., 2025</xref>), but these primarily target discrete flows on
  Euclidean spaces and general ML applications. Many physics problems
  call for manifold-aware integration, continuous-time flows, and strict
  symmetry handling, which are not first-class in these packages.</p>
  <p><monospace>bijx</monospace> pursues an
  <monospace>nnx.Module</monospace>-centric design, a consistent runtime
  shape inference convention tailored to physics use-cases, and
  manifold-preserving ODE solvers as core abstractions. Packaging the
  physics capabilities inside a general, familiar flow API lowers
  barriers for users: one can prototype with standard components and opt
  into the specialized tools when needed. At the same time, the latter
  are also available independently and can be used in other
  contexts.</p>
  <p><monospace>bijx</monospace> consolidates, generalizes, and
  streamlines research code from prior work on equivariant models for
  lattice field theory and continuous flows for gauge theories
  (<xref alt="Gerdes et al., 2023" rid="ref-gerdes2022learning" ref-type="bibr">Gerdes
  et al., 2023</xref>,
  <xref alt="2024" rid="ref-gerdes2024equivariant" ref-type="bibr">2024</xref>).
  While the package itself is newly organized, the specialized
  components reflect methods validated in these research contexts and
  are actively used in ongoing research in this domain.</p>
  <p>Primary contributions beyond existing JAX libraries:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Continuous flows:</bold> A unified interface for CNFs
      (<xref alt="Chen et al., 2018" rid="ref-chen2018node" ref-type="bibr">Chen
      et al., 2018</xref>) leveraging <monospace>diffrax</monospace>
      (<xref alt="Kidger, 2021" rid="ref-kidger2021on" ref-type="bibr">Kidger,
      2021</xref>), inheriting flexible adjoints and solver choices from
      the latter.</p>
    </list-item>
    <list-item>
      <p><bold>Lie-group integration:</bold> Structure-preserving
      Crouch–Grossmann integrators
      (<xref alt="Crouch &amp; Grossman, 1993" rid="ref-crouch1993numerical" ref-type="bibr">Crouch
      &amp; Grossman, 1993</xref>) for matrix Lie groups, differentiable
      via adjoint sensitivity.</p>
    </list-item>
    <list-item>
      <p><bold>Symmetry-aware vector fields:</bold> Convolutional CNF
      architectures for lattice data, applicable to other grid-like
      domains.</p>
    </list-item>
    <list-item>
      <p><bold>Physics-domain tools:</bold> Fourier degrees of freedom
      handling, matrix Lie group operations, and symmetry-aware data
      transforms.</p>
    </list-item>
  </list>
</sec>
<sec id="design-and-functionality">
  <title>Design and Functionality</title>
  <p><monospace>bijx</monospace> favors composable primitives over
  monolithic training pipelines, as we anticipate its use in (physics)
  research contexts. Users can assemble flows from small parts to match
  their problem, e.g. following one of the examples outlined in the
  documentation.</p>
  <p>Core abstractions follow standard patterns:
  <monospace>Bijection</monospace> and
  <monospace>Distribution</monospace> compose via
  <monospace>Chain</monospace> and <monospace>Transformed</monospace>.
  All components are <monospace>nnx.Module</monospace>s, giving
  consistent parameter/state handling,
  <monospace>jit</monospace>/<monospace>vmap</monospace> compatibility,
  and clean integration with modern tooling. Runtime shape inference
  across batch/event/channel dimensions and an optional
  auto-vectorization utility enable flexible shape handling.</p>
  <p>Time-dependent vector fields <inline-formula><alternatives>
  <tex-math><![CDATA[f(t, x; \theta)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  typically represented by neural networks, can be turned into
  continuous flows via the CNF interface. This leverages
  <monospace>diffrax</monospace>, exposing multiple integrators and
  adjoint methods. For matrix Lie groups and Lie algebra–valued states,
  specialized Crouch–Grossmann integrators yield structure-preserving
  dynamics and remain fully differentiable.</p>
  <p>The library includes building blocks such as a highly general
  <monospace>GeneralCouplingLayer</monospace> that reconstructs
  arbitrary bijection templates from parameters (automatic parameter
  extraction), supports indexing or multiplicative masks, and offers
  optional auto-vectorization. It also provides symmetry-aware
  convolutions for lattice data and utilities for vectorization and
  batching. Extensive tests and tutorials support both standard and
  physics-focused workflows.</p>
  <p>The library encourages extension by implementing minimal
  <monospace>Bijection</monospace> and
  <monospace>Distribution</monospace> interfaces, with a consistent
  runtime shape convention and <monospace>nnx.Module</monospace> state
  handling. Performance-critical paths are JIT-compiled and
  <monospace>vmap</monospace>-friendly, minimizing Python overhead. A
  continuous integration pipeline runs unit tests and doctests across
  configurations, and versioned documentation provides API reference and
  tutorials.</p>
  <p>An optional <monospace>bijx.flowjax</monospace> module provides a
  lightweight adapter that enables bidirectional reuse of bijections and
  distributions between <monospace>bijx</monospace> and FlowJAX. This
  exemplifies how <monospace>bijx</monospace> complements, rather than
  replaces, existing tools, and how similar patterns can be implemented
  for other flow libraries.</p>
</sec>
<sec id="illustrative-example">
  <title>Illustrative Example</title>
  <p>The documentation (https://mathisgerdes.github.io/bijx/) includes
  basic 2D and higher-dimensional examples using the general API.
  Following a very basic linear flow to showcase the API, here we
  highlight a physics use-case from the tutorials to illustrate the
  specialized capabilities.</p>
  <p>A typical workflow composes bijections and wraps a base
  distribution.</p>
  <code language="python">import jax, jax.numpy as jnp
import bijx

prior = bijx.IndependentNormal(event_shape=(2,))
flow = bijx.Chain(
    bijx.Shift(jnp.array([1.0, -1.0])),
    bijx.Scaling(jnp.array([0.5, 2.0])),
)
dist = bijx.Transformed(prior, flow)
y, logp = dist.sample(batch_shape=(1024,), rng=jax.random.key(0))</code>
  <p>Moving on to a physics use-case, we train a continuous flow on the
  SU(3) Lie group to learn a Wilson-like target density,
  <disp-formula><alternatives>
  <tex-math><![CDATA[p(U) \propto \exp\left(-\beta \sum_{i=1}^3 c_i \operatorname{Re}[\operatorname{tr}(U^i)]\right).]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>U</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>∝</mml:mo><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>−</mml:mi><mml:mi>β</mml:mi><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>3</mml:mn></mml:munderover><mml:msub><mml:mi>c</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>Re</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mo>tr</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  Here we choose <inline-formula><alternatives>
  <tex-math><![CDATA[c_1 = 0.17]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.17</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[c_2 = -0.65]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mn>0.65</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[c_3 = 1.22]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.22</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  Conjugation-invariance is enforced by building the vector field as the
  gradient of a “potential” defined in terms of invariant features, and
  dynamics are integrated with the Crouch–Grossmann solver to remain on
  the manifold. Starting from the Haar measure, the model is optimized
  to match the target by minimizing the reverse Kullback-Leibler
  divergence. As shown in
  <xref alt="[fig:su3]" rid="figU003Asu3">[fig:su3]</xref>, the
  resulting density captures the target’s shape and symmetries.</p>
  <fig id="figU003Asu3">
    <caption><p>Uniform Haar density on SU(3) (left), the learned SU(3)
    density (middle), and the target density (right), shown in
    eigenvalue-angle space.</p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="su3.jpg" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank collaborators who used this software and provided early
  feedback on the design of various components of this library.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-jax2018github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/jax-ml/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-flax2020github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Heek</surname><given-names>Jonathan</given-names></name>
        <name><surname>Levskaya</surname><given-names>Anselm</given-names></name>
        <name><surname>Oliver</surname><given-names>Avital</given-names></name>
        <name><surname>Ritter</surname><given-names>Marvin</given-names></name>
        <name><surname>Rondepierre</surname><given-names>Bertrand</given-names></name>
        <name><surname>Steiner</surname><given-names>Andreas</given-names></name>
        <name><surname>Zee</surname><given-names>Marc van</given-names></name>
      </person-group>
      <article-title>Flax: A neural network library and ecosystem for JAX</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>http://github.com/google/flax</uri>
    </element-citation>
  </ref>
  <ref id="ref-kidger2021on">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Kidger</surname><given-names>Patrick</given-names></name>
      </person-group>
      <article-title>On Neural Differential Equations</article-title>
      <publisher-name>University of Oxford</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-flowjax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Ward</surname><given-names>Daniel</given-names></name>
        <name><surname>Hickling</surname><given-names>Tennessee</given-names></name>
        <name><surname>Mould</surname><given-names>Matthew</given-names></name>
        <name><surname>Seyboldt</surname><given-names>Adrian</given-names></name>
        <name><surname>Turok</surname><given-names>Gilead</given-names></name>
      </person-group>
      <article-title>FlowJAX: Distributions and normalizing flows in jax</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://github.com/danielward27/flowjax</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.10402073</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-rezende2015variational">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rezende</surname><given-names>Danilo</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
      </person-group>
      <article-title>Variational inference with normalizing flows</article-title>
      <source>Proceedings of the 32nd international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/abs/1505.05770</uri>
      <fpage>1530</fpage>
      <lpage>1538</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kobyzev2021normalizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kobyzev</surname><given-names>Ivan</given-names></name>
        <name><surname>Prince</surname><given-names>Simon J. D.</given-names></name>
        <name><surname>Brubaker</surname><given-names>Marcus A.</given-names></name>
      </person-group>
      <article-title>Normalizing flows: An introduction and review of current methods</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>43</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2020.2992934</pub-id>
      <fpage>3964</fpage>
      <lpage>3979</lpage>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2021normalizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>57</issue>
      <fpage>1</fpage>
      <lpage>64</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chen2018node">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Rubanova</surname><given-names>Yulia</given-names></name>
        <name><surname>Bettencourt</surname><given-names>Jesse</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Neural Ordinary Differential Equations</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
    </element-citation>
  </ref>
  <ref id="ref-deepmind2020jax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Babuschkin</surname><given-names>Igor</given-names></name>
        <name><surname>Baumli</surname><given-names>Kate</given-names></name>
        <name><surname>Bell</surname><given-names>Alison</given-names></name>
        <name><surname>Bhupatiraju</surname><given-names>Surya</given-names></name>
        <name><surname>Bruce</surname><given-names>Jake</given-names></name>
        <name><surname>Buchlovsky</surname><given-names>Peter</given-names></name>
        <name><surname>Budden</surname><given-names>David</given-names></name>
        <name><surname>Cai</surname><given-names>Trevor</given-names></name>
        <name><surname>Clark</surname><given-names>Aidan</given-names></name>
        <name><surname>Danihelka</surname><given-names>Ivo</given-names></name>
        <name><surname>Fantacci</surname><given-names>Claudio</given-names></name>
        <name><surname>Godwin</surname><given-names>Jonathan</given-names></name>
        <name><surname>Jones</surname><given-names>Chris</given-names></name>
        <name><surname>Hemsley</surname><given-names>Ross</given-names></name>
        <name><surname>Hennigan</surname><given-names>Tom</given-names></name>
        <name><surname>Hessel</surname><given-names>Matteo</given-names></name>
        <name><surname>Hou</surname><given-names>Shaobo</given-names></name>
        <name><surname>Kapturowski</surname><given-names>Steven</given-names></name>
        <name><surname>Kemaev</surname><given-names>Iurii</given-names></name>
        <name><surname>King</surname><given-names>Michael</given-names></name>
        <name><surname>Kunesch</surname><given-names>Markus</given-names></name>
        <name><surname>Martens</surname><given-names>Lena</given-names></name>
        <name><surname>Merzic</surname><given-names>Hamza</given-names></name>
        <name><surname>Mikulik</surname><given-names>Vladimir</given-names></name>
        <name><surname>Norman</surname><given-names>Tamara</given-names></name>
        <name><surname>Quan</surname><given-names>John</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Ring</surname><given-names>Roman</given-names></name>
        <name><surname>Ruiz</surname><given-names>Francisco</given-names></name>
        <name><surname>Sanchez</surname><given-names>Alvaro</given-names></name>
        <name><surname>Schneider</surname><given-names>Rosalia</given-names></name>
        <name><surname>Sezener</surname><given-names>Eren</given-names></name>
        <name><surname>Spencer</surname><given-names>Stephen</given-names></name>
        <name><surname>Srinivasan</surname><given-names>Srivatsan</given-names></name>
        <name><surname>Wang</surname><given-names>Luyu</given-names></name>
        <name><surname>Stokowiec</surname><given-names>Wojciech</given-names></name>
        <name><surname>Viola</surname><given-names>Fabio</given-names></name>
      </person-group>
      <article-title>The DeepMind JAX Ecosystem</article-title>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-gerdes2022learning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gerdes</surname><given-names>Mathis</given-names></name>
        <name><surname>de Haan</surname><given-names>Pim</given-names></name>
        <name><surname>Rainone</surname><given-names>Corrado</given-names></name>
        <name><surname>Bondesan</surname><given-names>Roberto</given-names></name>
        <name><surname>Cheng</surname><given-names>Miranda C. N.</given-names></name>
      </person-group>
      <article-title>Learning Lattice Quantum Field Theories with Equivariant Continuous Flows</article-title>
      <source>SciPost Phys.</source>
      <year iso-8601-date="2023-12">2023</year><month>12</month>
      <volume>15</volume>
      <issue>6</issue>
      <uri>https://arxiv.org/abs/2207.00283</uri>
      <pub-id pub-id-type="doi">10.21468/SciPostPhys.15.6.238</pub-id>
      <fpage>238</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gerdes2024equivariant">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gerdes</surname><given-names>Mathis</given-names></name>
        <name><surname>Haan</surname><given-names>Pim de</given-names></name>
        <name><surname>Bondesan</surname><given-names>Roberto</given-names></name>
        <name><surname>Cheng</surname><given-names>Miranda C. N.</given-names></name>
      </person-group>
      <article-title>Continuous normalizing flows for lattice gauge theories</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2024-10">2024</year><month>10</month>
      <uri>https://arxiv.org/abs/2410.13161</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2410.13161</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-crouch1993numerical">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Crouch</surname><given-names>P. E.</given-names></name>
        <name><surname>Grossman</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>Numerical integration of ordinary differential equations on manifolds</article-title>
      <source>Journal of Nonlinear Science</source>
      <year iso-8601-date="1993-12">1993</year><month>12</month>
      <volume>3</volume>
      <issue>1</issue>
      <issn>1432-1467</issn>
      <pub-id pub-id-type="doi">10.1007/BF02429858</pub-id>
      <fpage>1</fpage>
      <lpage>33</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
