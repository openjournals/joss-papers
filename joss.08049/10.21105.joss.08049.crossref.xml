<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20251103143517-2f8c702b477bf0b45f846e490bc720784ceaebb3</doi_batch_id>
    <timestamp>20251103143517</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>11</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>115</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Nkululeko 1.0: A Python package to predict speaker characteristics with a high-level interface</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Felix</given_name>
            <surname>Burkhardt</surname>
            <affiliations>
              <institution><institution_name>audEERING GmbH, Germany</institution_name></institution>
              <institution><institution_name>TU Berlin, Germany</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-2689-0545</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Bagus Tris</given_name>
            <surname>Atmaja</surname>
            <affiliations>
              <institution><institution_name>Nara Institute of Science and Technology (NAIST), Japan</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0003-1560-2824</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>11</month>
          <day>03</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8049</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08049</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.17349217</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8049</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08049</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08049</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08049.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="scikit-learn:2011">
            <article_title>Scikit-learn: Machine learning in Python</article_title>
            <author>Pedregosa</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <volume>12</volume>
            <doi>10.5555/1953048.2078195</doi>
            <cYear>2011</cYear>
            <unstructured_citation>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830. https://doi.org/10.5555/1953048.2078195</unstructured_citation>
          </citation>
          <citation key="nkululeko:2022">
            <article_title>Nkululeko: A tool for rapid speaker characteristics detection</article_title>
            <author>Burkhardt</author>
            <journal_title>2022 Language Resources and Evaluation Conference, LREC 2022</journal_title>
            <isbn>9791095546726</isbn>
            <cYear>2022</cYear>
            <unstructured_citation>Burkhardt, F., Wagner, J., Wierstorf, H., Eyben, F., &amp; Schuller, B. (2022). Nkululeko: A tool for rapid speaker characteristics detection. 2022 Language Resources and Evaluation Conference, LREC 2022, 1925–1932. ISBN: 9791095546726</unstructured_citation>
          </citation>
          <citation key="McFee:2015">
            <article_title>librosa: Audio and Music Signal Analysis in Python</article_title>
            <author>McFee</author>
            <journal_title>Proc. 14th Python Sci. Conf.</journal_title>
            <issue>Scipy</issue>
            <doi>10.25080/majora-7b98e3ed-003</doi>
            <cYear>2015</cYear>
            <unstructured_citation>McFee, B., Raffel, C., Liang, D., Ellis, D., McVicar, M., Battenberg, E., &amp; Nieto, O. (2015). librosa: Audio and Music Signal Analysis in Python. Proc. 14th Python Sci. Conf., Scipy, 18–24. https://doi.org/10.25080/majora-7b98e3ed-003</unstructured_citation>
          </citation>
          <citation key="torch:2020">
            <volume_title>Deep learning with PyTorch</volume_title>
            <author>Chaudhary</author>
            <doi>10.4018/978-1-7998-3095-5.ch003</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Chaudhary, A., Chouhan, K. S., Gajrani, J., &amp; Sharma, B. (2020). Deep learning with PyTorch. https://doi.org/10.4018/978-1-7998-3095-5.ch003</unstructured_citation>
          </citation>
          <citation key="Yang:2021">
            <article_title>SUPERB: Speech Processing Universal PERformance Benchmark</article_title>
            <author>Yang</author>
            <journal_title>Interspeech 2021</journal_title>
            <doi>10.21437/Interspeech.2021-1775</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Yang, S., Chi, P.-H., Chuang, Y.-S., Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J., Chang, X., Lin, G.-T., Huang, T.-H., Tseng, W.-C., Lee, K., Liu, D.-R., Huang, Z., Dong, S., Li, S.-W., Watanabe, S., Mohamed, A., &amp; Lee, H. (2021). SUPERB: Speech Processing Universal PERformance Benchmark. Interspeech 2021, 1194–1198. https://doi.org/10.21437/Interspeech.2021-1775</unstructured_citation>
          </citation>
          <citation key="Giannakopoulos:2015">
            <article_title>pyAudioAnalysis: An open-source Python library for audio signal analysis</article_title>
            <author>Giannakopoulos</author>
            <journal_title>PLoS One</journal_title>
            <issue>12</issue>
            <volume>10</volume>
            <doi>10.1371/journal.pone.0144610</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Giannakopoulos, T. (2015). pyAudioAnalysis: An open-source Python library for audio signal analysis. PLoS One, 10(12), 1–17. https://doi.org/10.1371/journal.pone.0144610</unstructured_citation>
          </citation>
          <citation key="Watanabe:2018">
            <article_title>ESPNet: End-to-end speech processing toolkit</article_title>
            <author>Watanabe</author>
            <journal_title>Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</journal_title>
            <issue>September</issue>
            <volume>2018-Septe</volume>
            <doi>10.21437/Interspeech.2018-1456</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N. E. Y., Heymann, J., Wiesner, M., Chen, N., Renduchintala, A., &amp; Ochiai, T. (2018). ESPNet: End-to-end speech processing toolkit. Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH, 2018-Septe(September), 2207–2211. https://doi.org/10.21437/Interspeech.2018-1456</unstructured_citation>
          </citation>
          <citation key="speechbrain:2021">
            <article_title>SpeechBrain: A general-purpose speech toolkit</article_title>
            <author>Ravanelli</author>
            <doi>10.48550/arXiv.2106.04624</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., … Bengio, Y. (2021). SpeechBrain: A general-purpose speech toolkit. https://doi.org/10.48550/arXiv.2106.04624</unstructured_citation>
          </citation>
          <citation key="spotlight:2023">
            <article_title>Spotlight</article_title>
            <author>Suwelack</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Suwelack, S. (2023). Spotlight. In GitHub repository. https://github.com/Renumics/spotlight/; GitHub.</unstructured_citation>
          </citation>
          <citation key="opensmile:2010">
            <article_title>openSMILE – the munich versatile and fast open-source audio feature extractor</article_title>
            <author>Eyben</author>
            <journal_title>MM’10 - Proceedings of the ACM Multimedia 2010 International Conference</journal_title>
            <doi>10.1145/1873951.1874246</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Eyben, F., Wöllmer, M., &amp; Schuller, B. (2010). openSMILE – the munich versatile and fast open-source audio feature extractor. MM’10 - Proceedings of the ACM Multimedia 2010 International Conference, 1459–1462. https://doi.org/10.1145/1873951.1874246</unstructured_citation>
          </citation>
          <citation key="wav2vec:2020">
            <article_title>wav2vec 2.0: A framework for self-supervised learning of speech representations</article_title>
            <author>Baevski</author>
            <journal_title>Advances in neural information processing systems</journal_title>
            <volume>33</volume>
            <doi>10.48550/arXiv.2006.11477</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Baevski, A., Zhou, Y., Mohamed, A., &amp; Auli, M. (2020). wav2vec 2.0: A framework for self-supervised learning of speech representations. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, &amp; H. Lin (Eds.), Advances in neural information processing systems (Vol. 33, pp. 12449–12460). Curran Associates, Inc. https://doi.org/10.48550/arXiv.2006.11477</unstructured_citation>
          </citation>
          <citation key="burkhardt:2022-syntact">
            <article_title>SyntAct : A Synthesized Database of Basic Emotions</article_title>
            <author>Burkhardt</author>
            <journal_title>Proc. Work. Dataset creat. Low. Lang. Within 13th lang. Resour. Eval. conf.</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Burkhardt, F., Eyben, F., &amp; Schuller, W. (2022). SyntAct : A Synthesized Database of Basic Emotions. In Jonne Sälevä &amp; C. Lignos (Eds.), Proc. Work. Dataset creat. Low. Lang. Within 13th lang. Resour. Eval. conf. European Language Resources Association.</unstructured_citation>
          </citation>
          <citation key="Atmaja:2024a">
            <article_title>Uncertainty-based ensemble learning for speech classification</article_title>
            <author>Atmaja</author>
            <journal_title>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</journal_title>
            <doi>10.1109/O-COCOSDA64382.2024.10800111</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Atmaja, B. T., Sasou, A., &amp; Burkhardt, F. (2024). Uncertainty-based ensemble learning for speech classification. 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-Ordination and Standardisation of Speech Databases and Assessment Techniques (o-COCOSDA), 1–6. https://doi.org/10.1109/O-COCOSDA64382.2024.10800111</unstructured_citation>
          </citation>
          <citation key="Burkhardt:2024">
            <article_title>Check your audio data: Nkululeko for bias detection</article_title>
            <author>Burkhardt</author>
            <journal_title>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</journal_title>
            <doi>10.1109/O-COCOSDA64382.2024.10800580</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Burkhardt, F., Atmaja, B. T., Derington, A., &amp; Eyben, F. (2024). Check your audio data: Nkululeko for bias detection. 2024 27th Conference of the Oriental COCOSDA International Committee for the Co-Ordination and Standardisation of Speech Databases and Assessment Techniques (o-COCOSDA), 1–6. https://doi.org/10.1109/O-COCOSDA64382.2024.10800580</unstructured_citation>
          </citation>
          <citation key="Atmaja:2025">
            <article_title>Pathological voice detection from sustained vowels: Handcrafted vs. Self-supervised learning</article_title>
            <author>Atmaja</author>
            <journal_title>2025 IEEE international conference on acoustics, speech, and signal processing workshops (ICASSPW)</journal_title>
            <doi>10.1109/ICASSPW65056.2025.11011272</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Atmaja, B. T., &amp; Sasou, A. (2025). Pathological voice detection from sustained vowels: Handcrafted vs. Self-supervised learning. 2025 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW). https://doi.org/10.1109/ICASSPW65056.2025.11011272</unstructured_citation>
          </citation>
          <citation key="Atmaja:2025b">
            <article_title>Performance-weighted ensemble learning for speech classification</article_title>
            <author>Atmaja</author>
            <journal_title>2025 international conference on artificial intelligence in information and communication (ICAIIC)</journal_title>
            <doi>10.1109/ICAIIC64266.2025.10920862</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Atmaja, B. T., Burkhardt, F., &amp; Sasou, A. (2025). Performance-weighted ensemble learning for speech classification. 2025 International Conference on Artificial Intelligence in Information and Communication (ICAIIC). https://doi.org/10.1109/ICAIIC64266.2025.10920862</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
