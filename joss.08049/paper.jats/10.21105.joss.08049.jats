<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8049</article-id>
<article-id pub-id-type="doi">10.21105/joss.08049</article-id>
<title-group>
<article-title>Nkululeko 1.0: A Python package to predict speaker
characteristics with a high-level interface</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2689-0545</contrib-id>
<name>
<surname>Burkhardt</surname>
<given-names>Felix</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1560-2824</contrib-id>
<name>
<surname>Atmaja</surname>
<given-names>Bagus Tris</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>audEERING GmbH, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>TU Berlin, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Nara Institute of Science and Technology (NAIST),
Japan</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-04-15">
<day>15</day>
<month>4</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>115</issue>
<fpage>8049</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>speech</kwd>
<kwd>machine learning</kwd>
<kwd>data exploration</kwd>
<kwd>speaker characteristics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Nkululeko</monospace>
  (<xref alt="Burkhardt, Wagner, et al., 2022" rid="ref-nkululekoU003A2022" ref-type="bibr">Burkhardt,
  Wagner, et al., 2022</xref>) is a Python toolkit for audio-based
  machine learning that uses a command-line interface and configuration
  files, eliminating the need for users to write code. Built on sklearn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learnU003A2011" ref-type="bibr">Pedregosa
  et al., 2011</xref>) and PyTorch
  (<xref alt="Chaudhary et al., 2020" rid="ref-torchU003A2020" ref-type="bibr">Chaudhary
  et al., 2020</xref>), it enables training and evaluation of speech
  databases with state-of-the-art machine learning approaches and
  acoustic features. Key capabilities include model demonstration,
  database storage with predicted labels, and bias detection through
  correlation analysis of target labels (e.g., depression) with speaker
  characteristics (age, gender) or signal quality metrics.</p>
</sec>
<sec id="design-choices">
  <title>Design Choices</title>
  <p>Nkululeko targets <bold>novice users</bold> interested in speaker
  characteristics detection (emotion, age, gender) without programming
  expertise, focusing on <bold>education</bold> and
  <bold>research</bold>. Core design principles include: (1) exploring
  combinations of acoustic features, models, and preprocessing for
  optimal performance; (2) database analysis with visualizations; (3)
  inference on audio files or streams. Users run experiments via a
  single command:
  <monospace>nkululeko.MODULE_NAME --config CONFIG_FILE.ini</monospace>.</p>
</sec>
<sec id="how-does-it-work">
  <title>How Does It Work?</title>
  <p>Nkululeko is a Python command-line tool that uses INI configuration
  files to specify experiments. Data is imported via CSV format (file
  path, speaker ID, gender, task labels) or audformat. The functionality
  is encapsulated by software modules that are called on the command
  line. Key modules include:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>nkululeko</bold>: machine learning experiments combining
      features and learners (e.g., opensmile with SVM);</p>
    </list-item>
    <list-item>
      <p><bold>explore</bold>: data exploration and analysis with
      visualizations;</p>
    </list-item>
    <list-item>
      <p><bold>predict</bold>: predict features like speaker
      diarization, signal distortion ratio, mean opinion score,
      age/gender with deep learning models;</p>
    </list-item>
    <list-item>
      <p><bold>segment</bold>: segment database based on VAD (voice
      activity detection);</p>
    </list-item>
    <list-item>
      <p><bold>ensemble</bold>: combine several models to improve
      performance;</p>
    </list-item>
    <list-item>
      <p><bold>demo</bold>: demonstrate the current best model on
      command line or files;</p>
    </list-item>
    <list-item>
      <p><bold>augment</bold>: augment training data for bias
      reduction;</p>
    </list-item>
    <list-item>
      <p><bold>optim</bold>: search model’s best hyperparamaters;</p>
    </list-item>
    <list-item>
      <p><bold>flags</bold>: run several experiments at once.</p>
    </list-item>
  </list>
  <p>Configuration files contain sections: DATA (database location,
  target labels), FEATS (acoustic features: opensmile
  (<xref alt="Eyben et al., 2010" rid="ref-opensmileU003A2010" ref-type="bibr">Eyben
  et al., 2010</xref>), wav2vec 2.0
  (<xref alt="Baevski et al., 2020" rid="ref-wav2vecU003A2020" ref-type="bibr">Baevski
  et al., 2020</xref>)), MODEL (classifiers/regressors), and PLOT
  (visualization). The overall workflow is shown in
  <xref alt="[fig:nkulu_flow]" rid="figU003Ankulu_flow">[fig:nkulu_flow]</xref>.
  Results include images, text reports, and auto-generated LaTeX/PDF
  documentation.</p>
  <fig>
    <caption><p>Nkululeko’s workflow: from raw dataset to experiment
    results
    <styled-content id="figU003Ankulu_flow"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="./assets/nkulu_flow-crop.pdf" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Open-source tools accelerate science through security,
  customizability, and transparency. While several open-source tools
  exist for audio analysis—librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeU003A2015" ref-type="bibr">McFee
  et al., 2015</xref>), TorchAudio
  (<xref alt="Yang et al., 2021" rid="ref-YangU003A2021" ref-type="bibr">Yang
  et al., 2021</xref>), pyAudioAnalysis
  (<xref alt="Giannakopoulos, 2015" rid="ref-GiannakopoulosU003A2015" ref-type="bibr">Giannakopoulos,
  2015</xref>), ESPNET
  (<xref alt="Watanabe et al., 2018" rid="ref-WatanabeU003A2018" ref-type="bibr">Watanabe
  et al., 2018</xref>), and SpeechBrain
  (<xref alt="Ravanelli et al., 2021" rid="ref-speechbrainU003A2021" ref-type="bibr">Ravanelli
  et al., 2021</xref>)—none specialize in speech analysis with
  high-level interfaces for novices. Nkululeko fills this gap with key
  principles:</p>
  <list list-type="order">
    <list-item>
      <p>minimal programming skills (CSV data preparation and
      command-line execution);</p>
    </list-item>
    <list-item>
      <p>standardized data formats (CSV and AUDFORMAT);</p>
    </list-item>
    <list-item>
      <p>replicability through shareable configuration files;</p>
    </list-item>
    <list-item>
      <p>high-level INI-file interface requiring no Python coding;</p>
    </list-item>
    <list-item>
      <p>transparency via comprehensive debug output and automated
      reporting.</p>
    </list-item>
  </list>
  <p>Nkululeko interfaces with Spotlight
  (<xref alt="Suwelack, 2023" rid="ref-spotlightU003A2023" ref-type="bibr">Suwelack,
  2023</xref>) for enhanced metadata visualization, combining
  complementary functionalities.</p>
</sec>
<sec id="usage-in-existing-research">
  <title>Usage in Existing Research</title>
  <p>Nkululeko has been used in several research projects since 2022
  (<xref alt="Burkhardt, Wagner, et al., 2022" rid="ref-nkululekoU003A2022" ref-type="bibr">Burkhardt,
  Wagner, et al., 2022</xref>):</p>
  <list list-type="bullet">
    <list-item>
      <p>Burkhardt, Eyben, et al.
      (<xref alt="2022" rid="ref-burkhardtU003A2022-syntact" ref-type="bibr">2022</xref>)
      evaluated synthesized emotional speech databases;</p>
    </list-item>
    <list-item>
      <p>Burkhardt et al.
      (<xref alt="2024" rid="ref-BurkhardtU003A2024" ref-type="bibr">2024</xref>)
      demonstrated bias detection in UACorpus and Androids datasets;</p>
    </list-item>
    <list-item>
      <p>Atmaja et al.
      (<xref alt="2024" rid="ref-AtmajaU003A2024a" ref-type="bibr">2024</xref>)
      showcased ensemble learning with uncertainty estimation;</p>
    </list-item>
    <list-item>
      <p>Atmaja &amp; Sasou
      (<xref alt="2025" rid="ref-AtmajaU003A2025" ref-type="bibr">2025</xref>)
      evaluated handcrafted acoustic features and self-supervised
      learning for pathological voice detection with early/late fusion
      strategies;</p>
    </list-item>
    <list-item>
      <p>Atmaja et al.
      (<xref alt="2025" rid="ref-AtmajaU003A2025b" ref-type="bibr">2025</xref>)
      extended ensemble evaluations with performance weighting across
      five tasks and ten datasets.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge support from: European SHIFT project (Grant
  101060660); European EASIER project (Grant 101016982); Project
  JPNP20006 (NEDO, Japan); Project 24K02967 (JSPS). We thank audEERING
  GmbH for partial funding.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-scikit-learnU003A2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <pub-id pub-id-type="doi">10.5555/1953048.2078195</pub-id>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nkululekoU003A2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Wagner</surname><given-names>Johannes</given-names></name>
        <name><surname>Wierstorf</surname><given-names>Hagen</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Schuller</surname><given-names>Björn</given-names></name>
      </person-group>
      <article-title>Nkululeko: A tool for rapid speaker characteristics detection</article-title>
      <source>2022 Language Resources and Evaluation Conference, LREC 2022</source>
      <publisher-name>European Language Resources Association (ELRA)</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <isbn>9791095546726</isbn>
      <fpage>1925</fpage>
      <lpage>1932</lpage>
    </element-citation>
  </ref>
  <ref id="ref-McFeeU003A2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Raffel</surname><given-names>Colin</given-names></name>
        <name><surname>Liang</surname><given-names>Dawen</given-names></name>
        <name><surname>Ellis</surname><given-names>Daniel</given-names></name>
        <name><surname>McVicar</surname><given-names>Matt</given-names></name>
        <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
      </person-group>
      <article-title>librosa: Audio and Music Signal Analysis in Python</article-title>
      <source>Proc. 14th Python Sci. Conf.</source>
      <year iso-8601-date="2015">2015</year>
      <issue>Scipy</issue>
      <pub-id pub-id-type="doi">10.25080/majora-7b98e3ed-003</pub-id>
      <fpage>18</fpage>
      <lpage>24</lpage>
    </element-citation>
  </ref>
  <ref id="ref-torchU003A2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Chaudhary</surname><given-names>Anmol</given-names></name>
        <name><surname>Chouhan</surname><given-names>Kuldeep Singh</given-names></name>
        <name><surname>Gajrani</surname><given-names>Jyoti</given-names></name>
        <name><surname>Sharma</surname><given-names>Bhavna</given-names></name>
      </person-group>
      <source>Deep learning with PyTorch</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.4018/978-1-7998-3095-5.ch003</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-YangU003A2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Shu-wen</given-names></name>
        <name><surname>Chi</surname><given-names>Po-Han</given-names></name>
        <name><surname>Chuang</surname><given-names>Yung-Sung</given-names></name>
        <name><surname>Lai</surname><given-names>Cheng-I Jeff</given-names></name>
        <name><surname>Lakhotia</surname><given-names>Kushal</given-names></name>
        <name><surname>Lin</surname><given-names>Yist Y.</given-names></name>
        <name><surname>Liu</surname><given-names>Andy T.</given-names></name>
        <name><surname>Shi</surname><given-names>Jiatong</given-names></name>
        <name><surname>Chang</surname><given-names>Xuankai</given-names></name>
        <name><surname>Lin</surname><given-names>Guan-Ting</given-names></name>
        <name><surname>Huang</surname><given-names>Tzu-Hsien</given-names></name>
        <name><surname>Tseng</surname><given-names>Wei-Cheng</given-names></name>
        <name><surname>Lee</surname><given-names>Ko-tik</given-names></name>
        <name><surname>Liu</surname><given-names>Da-Rong</given-names></name>
        <name><surname>Huang</surname><given-names>Zili</given-names></name>
        <name><surname>Dong</surname><given-names>Shuyan</given-names></name>
        <name><surname>Li</surname><given-names>Shang-Wen</given-names></name>
        <name><surname>Watanabe</surname><given-names>Shinji</given-names></name>
        <name><surname>Mohamed</surname><given-names>Abdelrahman</given-names></name>
        <name><surname>Lee</surname><given-names>Hung-yi</given-names></name>
      </person-group>
      <article-title>SUPERB: Speech Processing Universal PERformance Benchmark</article-title>
      <source>Interspeech 2021</source>
      <publisher-name>ISCA</publisher-name>
      <year iso-8601-date="2021-08">2021</year><month>08</month>
      <pub-id pub-id-type="doi">10.21437/Interspeech.2021-1775</pub-id>
      <fpage>1194</fpage>
      <lpage>1198</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GiannakopoulosU003A2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Giannakopoulos</surname><given-names>Theodoros</given-names></name>
      </person-group>
      <article-title>pyAudioAnalysis: An open-source Python library for audio signal analysis</article-title>
      <source>PLoS One</source>
      <year iso-8601-date="2015">2015</year>
      <volume>10</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0144610</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-WatanabeU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Watanabe</surname><given-names>Shinji</given-names></name>
        <name><surname>Hori</surname><given-names>Takaaki</given-names></name>
        <name><surname>Karita</surname><given-names>Shigeki</given-names></name>
        <name><surname>Hayashi</surname><given-names>Tomoki</given-names></name>
        <name><surname>Nishitoba</surname><given-names>Jiro</given-names></name>
        <name><surname>Unno</surname><given-names>Yuya</given-names></name>
        <name><surname>Soplin</surname><given-names>Nelson Enrique Yalta</given-names></name>
        <name><surname>Heymann</surname><given-names>Jahn</given-names></name>
        <name><surname>Wiesner</surname><given-names>Matthew</given-names></name>
        <name><surname>Chen</surname><given-names>Nanxin</given-names></name>
        <name><surname>Renduchintala</surname><given-names>Adithya</given-names></name>
        <name><surname>Ochiai</surname><given-names>Tsubasa</given-names></name>
      </person-group>
      <article-title>ESPNet: End-to-end speech processing toolkit</article-title>
      <source>Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</source>
      <year iso-8601-date="2018">2018</year>
      <volume>2018-Septe</volume>
      <issue>September</issue>
      <pub-id pub-id-type="doi">10.21437/Interspeech.2018-1456</pub-id>
      <fpage>2207</fpage>
      <lpage>2211</lpage>
    </element-citation>
  </ref>
  <ref id="ref-speechbrainU003A2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ravanelli</surname><given-names>Mirco</given-names></name>
        <name><surname>Parcollet</surname><given-names>Titouan</given-names></name>
        <name><surname>Plantinga</surname><given-names>Peter</given-names></name>
        <name><surname>Rouhe</surname><given-names>Aku</given-names></name>
        <name><surname>Cornell</surname><given-names>Samuele</given-names></name>
        <name><surname>Lugosch</surname><given-names>Loren</given-names></name>
        <name><surname>Subakan</surname><given-names>Cem</given-names></name>
        <name><surname>Dawalatabad</surname><given-names>Nauman</given-names></name>
        <name><surname>Heba</surname><given-names>Abdelwahab</given-names></name>
        <name><surname>Zhong</surname><given-names>Jianyuan</given-names></name>
        <name><surname>Chou</surname><given-names>Ju-Chieh</given-names></name>
        <name><surname>Yeh</surname><given-names>Sung-Lin</given-names></name>
        <name><surname>Fu</surname><given-names>Szu-Wei</given-names></name>
        <name><surname>Liao</surname><given-names>Chien-Feng</given-names></name>
        <name><surname>Rastorgueva</surname><given-names>Elena</given-names></name>
        <name><surname>Grondin</surname><given-names>François</given-names></name>
        <name><surname>Aris</surname><given-names>William</given-names></name>
        <name><surname>Na</surname><given-names>Hwidong</given-names></name>
        <name><surname>Gao</surname><given-names>Yan</given-names></name>
        <name><surname>Mori</surname><given-names>Renato De</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>SpeechBrain: A general-purpose speech toolkit</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2106.04624</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2106.04624</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-spotlightU003A2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Suwelack</surname><given-names>Stefan</given-names></name>
      </person-group>
      <article-title>Spotlight</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/Renumics/spotlight/; GitHub</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-opensmileU003A2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Wöllmer</surname><given-names>Martin</given-names></name>
        <name><surname>Schuller</surname><given-names>Björn</given-names></name>
      </person-group>
      <article-title>openSMILE – the munich versatile and fast open-source audio feature extractor</article-title>
      <source>MM’10 - Proceedings of the ACM Multimedia 2010 International Conference</source>
      <year iso-8601-date="2010-01">2010</year><month>01</month>
      <pub-id pub-id-type="doi">10.1145/1873951.1874246</pub-id>
      <fpage>1459</fpage>
      <lpage>1462</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wav2vecU003A2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Baevski</surname><given-names>Alexei</given-names></name>
        <name><surname>Zhou</surname><given-names>Yuhao</given-names></name>
        <name><surname>Mohamed</surname><given-names>Abdelrahman</given-names></name>
        <name><surname>Auli</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>wav2vec 2.0: A framework for self-supervised learning of speech representations</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Ranzato</surname><given-names>M.</given-names></name>
        <name><surname>Hadsell</surname><given-names>R.</given-names></name>
        <name><surname>Balcan</surname><given-names>M. F.</given-names></name>
        <name><surname>Lin</surname><given-names>H.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <uri>https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.11477</pub-id>
      <fpage>12449</fpage>
      <lpage>12460</lpage>
    </element-citation>
  </ref>
  <ref id="ref-burkhardtU003A2022-syntact">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Schuller</surname><given-names>W</given-names></name>
      </person-group>
      <article-title>SyntAct : A Synthesized Database of Basic Emotions</article-title>
      <source>Proc. Work. Dataset creat. Low. Lang. Within 13th lang. Resour. Eval. conf.</source>
      <person-group person-group-type="editor">
        <string-name>Jonne Sälevä</string-name>
        <name><surname>Lignos</surname><given-names>Constantine</given-names></name>
      </person-group>
      <publisher-name>European Language Resources Association</publisher-name>
      <publisher-loc>Marseille, France</publisher-loc>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2024a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
      </person-group>
      <article-title>Uncertainty-based ensemble learning for speech classification</article-title>
      <source>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/O-COCOSDA64382.2024.10800111</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BurkhardtU003A2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Derington</surname><given-names>Anna</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
      </person-group>
      <article-title>Check your audio data: Nkululeko for bias detection</article-title>
      <source>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/O-COCOSDA64382.2024.10800580</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2025">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
      </person-group>
      <article-title>Pathological voice detection from sustained vowels: Handcrafted vs. Self-supervised learning</article-title>
      <source>2025 IEEE international conference on acoustics, speech, and signal processing workshops (ICASSPW)</source>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.1109/ICASSPW65056.2025.11011272</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2025b">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
      </person-group>
      <article-title>Performance-weighted ensemble learning for speech classification</article-title>
      <source>2025 international conference on artificial intelligence in information and communication (ICAIIC)</source>
      <publisher-loc>Fukuoka</publisher-loc>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.1109/ICAIIC64266.2025.10920862</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
