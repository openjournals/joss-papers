<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8049</article-id>
<article-id pub-id-type="doi">10.21105/joss.08049</article-id>
<title-group>
<article-title>Nkululeko 1.0: A Python package to predict speaker
characteristics with a high-level interface</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2689-0545</contrib-id>
<name>
<surname>Burkhardt</surname>
<given-names>Felix</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1560-2824</contrib-id>
<name>
<surname>Atmaja</surname>
<given-names>Bagus Tris</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>audEERING GmbH, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>TU Berlin, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Nara Institute of Science and Technology (NAIST),
Japan</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-04-15">
<day>15</day>
<month>4</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>114</issue>
<fpage>8049</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>speech</kwd>
<kwd>machine learning</kwd>
<kwd>data exploration</kwd>
<kwd>speaker characteristics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Nkululeko</monospace>
  (<xref alt="Burkhardt, Wagner, et al., 2022" rid="ref-nkululekoU003A2022" ref-type="bibr">Burkhardt,
  Wagner, et al., 2022</xref>) is a toolkit for audio-based machine
  learning explorations using a command line interface and a
  configuration file, based on machine learning packages like sklearn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learnU003A2011" ref-type="bibr">Pedregosa
  et al., 2011</xref>) and pytorch
  (<xref alt="Chaudhary et al., 2020" rid="ref-torchU003A2020" ref-type="bibr">Chaudhary
  et al., 2020</xref>). It is written in Python without the need to
  write Python code by the users. The main features are: training and
  evaluation of labelled speech databases with state-of-the-art machine
  learning approach and acoustic feature extractors, a live
  demonstration interface, and the possibility to store databases with
  predicted labels. Based on this, the framework can also be used to
  check on bias in databases by exploring correlations of target labels,
  like, e.g., depression or diagnosis, with predicted, or additionally
  given, labels like age, gender, signal distortion ratio, or mean
  opinion score in.</p>
</sec>
<sec id="design-choices">
  <title>Design choices</title>
  <p>The program is intended for <bold>novice</bold> people interested
  in speaker characteristics detection (e.g., emotion, age, and gender)
  without being proficient in (Python) programming language. Its main
  target is for <bold>education</bold> and <bold>research</bold> with
  the main features as follows:</p>
  <list list-type="bullet">
    <list-item>
      <p>Finding good combinations of variables, e.g., acoustic
      features, models (classifier or regressor), feature
      standardization, augmentation, etc., for speaker characteristics
      detection (e.g., emotion);</p>
    </list-item>
    <list-item>
      <p>Characteristics of the database, such as distribution of
      gender, age, emotion, duration, data size, and so on, with their
      visualization;</p>
    </list-item>
    <list-item>
      <p>Inference of speaker characteristics from a given audio file or
      streaming audio (can also be said as “weak” labeling for
      semi-supervised learning).</p>
    </list-item>
  </list>
  <p>Hence, one should be able to use Nkululeko after installing and
  preparing/downloading their data in the correct format in a single
  line (add <monospace>python -m</monospace> if working in a development
  environment without installing it).</p>
  <code language="bash">$ nkululeko.MODULE_NAME --config CONFIG_FILE.ini</code>
</sec>
<sec id="how-does-it-work">
  <title>How does it work?</title>
  <p><monospace>nkululeko</monospace> is a command line tool written in
  Python, best used in conjunction with the Visual Studio Code editor
  (but can be run stand-alone). To use it, a text editor is needed to
  edit the experiment configuration. You would then run
  <monospace>nkululeko</monospace> <bold>experiment</bold> like
  this:</p>
  <code language="bash">$ nkululeko.explore --config conf.ini</code>
  <p>and inspect the results afterward; they are represented as images,
  texts, and even a fully automatically compiled PDF report written in
  LaTeX.</p>
  <p><monospace>nkululeko</monospace>’s data import format is based on a
  simple CSV formalism, or alternatively, for a more detailed
  representation including data schemata,
  audformat.<xref ref-type="fn" rid="fn1">1</xref> Basically, to be used
  by <monospace>nkululeko</monospace>, the data format should include
  the audio file path of <bold>speech dataset</bold> (usually in WAV
  format) and a task-specific label. Optionally, speaker ID and gender
  labels help with speech data. An example of a database (in
  <bold>CSV</bold> format) labelled with emotion is</p>
  <preformat>file, speaker, gender, emotion
x/sample.wav, s1, female, happy
...</preformat>
  <p>As the main goal of <monospace>nkululeko</monospace> is to avoid
  the need to learn programming, experiments are specified by means of a
  configuration file. The functionality is encapsulated by software
  <italic>modules</italic> (interfaces) that are to be called on the
  command line. We list the most important ones here:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>nkululeko</bold>: do machine learning experiments
      combining features and learners (e.g. opensmile with SVM);</p>
    </list-item>
    <list-item>
      <p><bold>demo</bold>: demo the current best model on the command
      line or some files;</p>
    </list-item>
    <list-item>
      <p><bold>testing</bold>: run the current best model on a specified
      test set;</p>
    </list-item>
    <list-item>
      <p><bold>explore</bold>: perform data exploration (used mainly in
      this paper)</p>
    </list-item>
    <list-item>
      <p><bold>augment</bold>: augment the current training data. This
      could also be used to reduce bias in the data, for example, by
      adding noise to audio samples that belong to a specific
      category;</p>
    </list-item>
    <list-item>
      <p><bold>aug_train</bold>: augment the training data and train the
      model with the augmented data;</p>
    </list-item>
    <list-item>
      <p><bold>predict</bold>: predict features like speaker
      diarization, signal distortion ratio, mean opinion score,
      arousal/valence, age/gender (for databases that miss this
      information), with deep neural nets models, e.g. as a basis for
      the <italic>explore</italic> module;</p>
    </list-item>
    <list-item>
      <p><bold>segment</bold>: segment a database based on VAD (voice
      activity detection);</p>
    </list-item>
    <list-item>
      <p><bold>ensemble</bold>: ensemble several models to improve
      performance.</p>
    </list-item>
  </list>
  <p>The <bold>configuration file</bold> (in INI format) consists of a
  set of key-value pairs that are organised into several sections.
  Almost all keys have default values, so they do not have to be
  specified.</p>
  <p>The following table gives examples of default values for important
  configuration keys:</p>
  <table-wrap>
    <table>
      <thead>
        <tr>
          <th>Section</th>
          <th>Key</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>MODEL</td>
          <td>batch_size</td>
          <td>8</td>
        </tr>
        <tr>
          <td>MODEL</td>
          <td>learning_rate</td>
          <td>0.0001</td>
        </tr>
        <tr>
          <td>MODEL</td>
          <td>drop</td>
          <td>0.1 (MLP) or False</td>
        </tr>
        <tr>
          <td>MODEL</td>
          <td>max_duration</td>
          <td>8.0</td>
        </tr>
        <tr>
          <td>MODEL</td>
          <td>push_to_hub</td>
          <td>False</td>
        </tr>
        <tr>
          <td>FEATS</td>
          <td>store_format</td>
          <td>pkl</td>
        </tr>
        <tr>
          <td>FEATS</td>
          <td>set</td>
          <td>eGeMAPSv02</td>
        </tr>
        <tr>
          <td>PLOT</td>
          <td>format</td>
          <td>png</td>
        </tr>
        <tr>
          <td>PLOT</td>
          <td>ccc</td>
          <td>False</td>
        </tr>
        <tr>
          <td>DATA</td>
          <td>target</td>
          <td>emotion</td>
        </tr>
        <tr>
          <td>DATA</td>
          <td>labels</td>
          <td>all labels</td>
        </tr>
        <tr>
          <td>EXP</td>
          <td>type</td>
          <td>classification</td>
        </tr>
        <tr>
          <td>EXP</td>
          <td>epochs</td>
          <td>1</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>You can override these defaults by specifying your own values in
  the configuration file. Here is a sample listing of an INI file
  (<monospace>conf.ini</monospace>) with a database section:</p>
  <code language="ini files">[EXP]
name = explore-androids
[DATA]
databases = ['androids']
androids = /data/androids/androids.csv
target = depression
labels = ['depressed', 'control']
samples_per_speaker = 20
min_length = 2
[PREDICT]
sample_selection = all
targets = ['pesq', 'sdr', 'stoi', 'mos']
[EXPL]
value_counts = [['gender'], ['age'], ['est_sdr'], ['est_pesq'], ['est_mos']]
[REPORT]
latex = androids-report</code>
  <p>As can be seen, several values simply contain Python data
  structures like arrays or dictionaries. Within this example, an
  experiment is specified with the name
  <italic>explore-androids</italic>, and a <bold>result</bold> folder
  with this name will be created, containing all figures and textual
  results, including an automatically generated Latex and PDF report on
  the findings. The overall flow of basic Nkululeko experiments can be
  shown in
  <xref alt="[fig:nkulu_flow]" rid="figU003Ankulu_flow">[fig:nkulu_flow]</xref>.</p>
  <fig>
    <caption><p>Nkululeko’s workflow: from a raw dataset into experiment
    results
    <styled-content id="figU003Ankulu_flow"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="./assets/nkulu_flow-crop.pdf" />
  </fig>
  <p>The <italic>DATA</italic> section sets the location of the database
  and specifies filters on the sample, in this case limiting the data to
  20 samples per speaker at most and at least 2 seconds long. In this
  section, the split sets (training, development, and test) are also
  specified. There is a special feature named <italic>balance
  splits</italic> that lets the user specify criteria that should be
  used to stratify the splits, for example, based on signal distortion
  ratio.</p>
  <p>With the <italic>predict</italic> module, specific features like,
  for example, signal distortion ratio or mean opinion score are to be
  predicted by deep learning models. The results are then used by a
  following call to the <italic>explore</italic> module to check whether
  these features, as well as some ground truth features
  (<italic>age</italic> and <italic>gender</italic>), correlate with the
  target variable (<italic>depressed</italic> in the given example) in
  any way.</p>
  <p>The <monospace>nkululeko</monospace> configuration can specify
  further sections:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>FEATS</bold> to specify acoustic features (e.g. opensmile
      (<xref alt="Eyben et al., 2010" rid="ref-opensmileU003A2010" ref-type="bibr">Eyben
      et al., 2010</xref>) or deep learning embeddings; e.g. wav2vec 2.0
      (<xref alt="Baevski et al., 2020" rid="ref-wav2vecU003A2020" ref-type="bibr">Baevski
      et al., 2020</xref>)) that should be used to represent the audio
      files.</p>
    </list-item>
    <list-item>
      <p><bold>MODEL</bold> to specify statistical models for regression
      or classification of audio data.</p>
    </list-item>
  </list>
</sec>
<sec id="example-of-usage">
  <title>Example of usage</title>
  <p>In the previous section, we have seen how to specify an experiment
  in an INI file that can be run with, for instance,
  <monospace>explore</monospace> and <monospace>segment</monospace>
  modules. Here, we show how to run the experiment
  (<monospace>nkululeko.nkululeko</monospace>) with the built-in dataset
  (Polish Speech Emotions dataset) from the installation until getting
  the results.</p>
  <p>First, users could clone the GitHub repository of Nkululeko.</p>
  <code language="bash">$ git clone https://github.com/felixbur/nkululeko.git
$ cd nkululeko</code>
  <p>Then, install nkululeko with <monospace>pip</monospace>. It is
  recommended that a virtual environment be used to avoid conflicts with
  other Python packages.</p>
  <code language="bash">$ python -m venv .env
$ source .env/bin/activate
$ pip install nkululeko</code>
  <p>Next, extract <monospace>polish_speech_emotions.zip</monospace>
  inside the Nkululeko data folder
  (<monospace>nkululeko/data/polish</monospace>) with right click
  regardless of the operating system (or using
  <monospace>unzip</monospace> command in the terminal like below).
  Then, run the following command in the terminal:</p>
  <code language="bash">$ cd data/polish
$ unzip polish_speech_emotions.zip
$ python3 process_database.py
$ cd ../..
$ nkululeko.nkululeko --config data/polish/exp.ini</code>
  <p>That’s it! The results will be stored in the
  <monospace>results/exp_polish_os</monospace> folder as stated in
  <monospace>exp.ini</monospace>. Below is an example of the debug
  output of the command:</p>
  <code language="bash">DEBUG: nkululeko: running exp_polish_os from config data/polish/exp.ini, 
nkululeko version 0.91.0
...
DEBUG: reporter: 
               precision    recall  f1-score   support

       anger     0.6944    0.8333    0.7576        30
     neutral     0.5000    0.4333    0.4643        30
        fear     0.6429    0.6000    0.6207        30

    accuracy                         0.6222        90
   macro avg     0.6124    0.6222    0.6142        90
weighted avg     0.6124    0.6222    0.6142        90

DEBUG: reporter: labels: ['anger', 'neutral', 'fear']
DEBUG: reporter: result per class (F1 score): [0.758, 0.464, 0.621] 
from epoch: 0
DEBUG: experiment: Done, used 7.439 seconds
DONE</code>
</sec>
<sec id="what-has-been-added-since-the-last-publication">
  <title>What has been added since the last publication</title>
  <p>Besides many small changes, mainly three big additions extended
  Nkululeko’s functionality since the last published papers. We
  introduce them in the next subsections.</p>
  <sec id="finetune-transformer-models">
    <title>Finetune transformer models</title>
    <p>With
    <ext-link ext-link-type="uri" xlink:href="https://github.com/felixbur/nkululeko">nkululeko</ext-link>
    since version 0.85.0 you can finetune a transformer model with
    <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/docs/transformers/training">huggingface</ext-link>
    (and even
    <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/docs/hub/models-uploading">publish
    it there if you like</ext-link>).</p>
    <p>Finetuning in this context means to train the (pre-trained)
    transformer layers with your new training data labels, as opposed to
    only using the last layer as embeddings.</p>
    <p>The only thing you need to do is to set your MODEL type to
    <italic>finetune</italic>:</p>
    <preformat>[FEATS]
type = []
[MODEL]
type = finetune</preformat>
    <p>The acoustic features can/should be empty, because the
    transformer model starts with CNN layers to model the acoustics
    frame-wise. The frames are then pooled by the model for the whole
    utterance.</p>
    <p>The default base model is the one from
    <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/facebook/wav2vec2-large-robust-ft-swbd-300h">facebook</ext-link>,
    but you can specify a different one like this:</p>
    <code language="ini files">[MODEL]
type = finetune
pretrained_model = microsoft/wavlm-base

duration = 10.5</code>
    <p>The parameter <italic>max_duration</italic> is also optional
    (default=8) and means the maximum duration of your samples /
    segments (in seconds) that will be used, starting from 0. The rest
    is disregarded.</p>
    <p>You can use the usual deep learning parameters:</p>
    <code language="ini files">[MODEL]
learning_rate = .001
batch_size = 16
device = cuda:3
measure = mse
loss = mse</code>
    <p>but all of them have defaults.</p>
    <p>The loss function is fixed to</p>
    <list list-type="bullet">
      <list-item>
        <p>weighted cross entropy for classification</p>
      </list-item>
      <list-item>
        <p>concordance correlation coefficient for regression</p>
      </list-item>
    </list>
    <p>The resulting best model and the HuggingFace logs (which can be
    read by
    <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/tensorboard">tensorboard</ext-link>)
    are stored in the project folder.</p>
    <p>If you like to have your model published, set:</p>
    <code language="ini files">[MODEL]
push_to_hub = True</code>
  </sec>
  <sec id="ensemble-classification">
    <title>Ensemble classification</title>
    <p>Since
    <ext-link ext-link-type="uri" xlink:href="https://github.com/felixbur/nkululeko">nkululeko</ext-link>
    version 0.88.0 you can combine experiment results and report on the
    outcome, by using the <bold>ensemble</bold> module.</p>
    <p>For example, you would like to know if the combination of expert
    features and learned embeddings works better than one of those. You
    could then do:</p>
    <code language="bash">python -m nkululeko.ensemble \
--method max_class \
examples/exp_emodb_praat_xgb.ini \
examples/exp_emodb_ast_xgb.ini \
examples/exp_emodb_wav2vec_xgb.in</code>
    <p>(all in one line) and would then get the results for a majority
    voting of the three results for Praat, AST, and Wav2vec2
    features.</p>
    <p>Other methods to combine the different predictors, are
    <italic>mean</italic>, <italic>max</italic>, <italic>sum</italic>,
    <italic>max_class</italic>, <italic>uncertainty_threshold</italic>,
    <italic>uncertainty_weighted</italic>,
    <italic>confidence_weighted</italic>:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>majority_voting</bold>: The modality function for
        classification: predict the category that most classifiers agree
        on.</p>
      </list-item>
      <list-item>
        <p><bold>mean</bold>: For classification: compute the arithmetic
        mean of probabilities from all predictors for each label, use
        the highest probability to infer the label.</p>
      </list-item>
      <list-item>
        <p><bold>max</bold>: For classification: use the maximum value
        of probabilities from all predictors for each label, use the
        highest probability to infer the label.</p>
      </list-item>
      <list-item>
        <p><bold>sum</bold>: For classification: use the sum of
        probabilities from all predictors for each label, use the
        highest probability to infer the label.</p>
      </list-item>
      <list-item>
        <p><bold>max_class</bold>: For classification: compare the
        highest probabilities of all models across classes (instead of
        same class as in max_ensemble) and return the highest
        probability and the class</p>
      </list-item>
      <list-item>
        <p><bold>uncertainty_threshold</bold>: For classification:
        predict the class with the lowest uncertainty if lower than a
        threshold (default to 1.0, meaning no threshold), else calculate
        the mean of uncertainties for all models per class and predict
        the lowest.</p>
      </list-item>
      <list-item>
        <p><bold>uncertainty_weighted</bold>: For classification: weigh
        each class with the inverse of its uncertainty (1/uncertainty),
        normalize the weights per model, then multiply each class model
        probability with their normalized weights and use the maximum
        one to infer the label.</p>
      </list-item>
      <list-item>
        <p><bold>confidence_weighted</bold>: Weighted ensemble based on
        confidence (1-uncertainty), normalized for all samples per
        model. Like before, but use confidence (instead of the inverse
        of uncertainty) as weights.</p>
      </list-item>
    </list>
  </sec>
  <sec id="predicting-speaker-id">
    <title>Predicting speaker ID</title>
    <p>To have labels for the individual speakers in a database is
    extremely important, because if you mix the same speakers in
    training and testing data splits, it is very possible that your
    model simply learned some speaker idiosyncrasies instead of some
    underlying principle. If you don’t have these labels, you could at
    least try to infer them with a pre-trained model.</p>
    <p>With
    <ext-link ext-link-type="uri" xlink:href="https://github.com/felixbur/nkululeko">nkululeko</ext-link>
    since version 0.93.0 the
    <ext-link ext-link-type="uri" xlink:href="https://github.com/pyannote/pyannote-audio">pyannote</ext-link>
    segmentation package is interfaced (as an alternative to
    <ext-link ext-link-type="uri" xlink:href="https://github.com/snakers4/silero-vad">silero</ext-link>).</p>
    <p>There are two modules that you can use for this:</p>
    <list list-type="bullet">
      <list-item>
        <p>SEGMENT</p>
      </list-item>
      <list-item>
        <p>PREDICT</p>
      </list-item>
    </list>
    <p>The (huge) difference is that the SEGMENT module looks at each
    file in the input data and looks for speakers per file (can be only
    one large file), while the PREDICT module concatenates all input
    data and looks for different speakers in the whole database.</p>
    <p>In any case, best run it on a GPU, as CPU will be very slow (and
    there is no progress bar).</p>
    <p>If you specify the <italic>method</italic> in [SEGMENT] section
    and the
    <ext-link ext-link-type="uri" xlink:href="https://huggingface.co/docs/hub/security-tokens"><italic>hf_token</italic></ext-link>
    (needed for the pyannote model) in the [MODEL] section</p>
    <code language="ini files">[SEGMENT]
method = pyannote
segment_target = _segmented
sample_selection = all
[MODEL]
hf_token = &lt;my hugging face token&gt;</code>
    <p>your resulting segmentation will have the predicted
    <monospace>speaker id</monospace> attached. Be aware that this is
    really slow on CPU, so best run on GPU and declare so in the [MODEL]
    section:</p>
    <code language="ini files">[MODEL]
hf_token = &lt;my hugging face token&gt;
device=gpu # or cuda:0</code>
    <p>As a result, a new plot would appear in the image folder: the
    distribution of speakers that were found.</p>
    <p>Simply select <italic>speaker</italic> as the prediction
    target:</p>
    <code language="ini files">[PREDICT]
targets = [&quot;speaker&quot;]</code>
    <p>Generally, the
    <ext-link ext-link-type="uri" xlink:href="https://blog.syntheticspeech.de/2023/08/16/nkululeko-how-to-predict-labels-for-your-data-from-existing-models-and-check-them/">PREDICT
    module is described here</ext-link>.</p>
  </sec>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Open-source tools are believed to be one of the reasons for
  accelerated science and technology. They are more secure, easy to
  customise, and transparent. There are several open-source tools that
  exist for acoustic, sound, and audio analysis, such as librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeU003A2015" ref-type="bibr">McFee
  et al., 2015</xref>), TorchAudio
  (<xref alt="Yang et al., 2021" rid="ref-YangU003A2021" ref-type="bibr">Yang
  et al., 2021</xref>), pyAudioAnalysis
  (<xref alt="Giannakopoulos, 2015" rid="ref-GiannakopoulosU003A2015" ref-type="bibr">Giannakopoulos,
  2015</xref>), ESPNET
  (<xref alt="Watanabe et al., 2018" rid="ref-WatanabeU003A2018" ref-type="bibr">Watanabe
  et al., 2018</xref>), and SpeechBrain
  (<xref alt="Ravanelli et al., 2021" rid="ref-speechbrainU003A2021" ref-type="bibr">Ravanelli
  et al., 2021</xref>). However, none of them are specialised in speech
  analysis with high-level interfaces for novices in the speech
  processing area.</p>
  <p>One exception is Spotlight
  (<xref alt="Suwelack, 2023" rid="ref-spotlightU003A2023" ref-type="bibr">Suwelack,
  2023</xref>), an open-source tool that visualises metadata
  distributions in audio data. An existing interface between
  <monospace>nkululeko</monospace> and Spotlight can be used to combine
  the visualisations of Spotlight with the functionalities of
  Nkululeko.</p>
  <p>Nkululeko follows these principles:</p>
  <list list-type="bullet">
    <list-item>
      <p><italic>Minimum programming skills</italic>: The only
      programming skills required are preparing the data in the correct
      (CSV) format and running the command line tool. For AUDFORMAT, no
      preparation is needed.</p>
    </list-item>
    <list-item>
      <p><italic>Standardised data format and label</italic>: The data
      format is based on CSV and AUDFORMAT, which are widely used
      formats for data exchange. The standard headers are like ‘file’,
      ‘speaker’, ‘emotion’, ‘age’, and ‘language’, and can be
      customised. Data could be saved anywhere on the computer, but the
      recipe for the data preparation is advised to be saved in
      <monospace>nkululeko/data</monospace> folder (and/or make a soft
      link to the original data location).</p>
    </list-item>
    <list-item>
      <p><italic>Replicability</italic>: the experiments are specified
      in a configuration file, which can be shared with others including
      the splitting of training, development, and test partition. All
      results are stored in a folder with the same name as the
      experiment.</p>
    </list-item>
    <list-item>
      <p><italic>High-level interface</italic>: the user specifies the
      experiment in an INI file, which is a simple text file that can be
      edited with any text editor. The user does not need to write
      Python code for experiments.</p>
    </list-item>
    <list-item>
      <p><italic>Transparency</italic>: as CLI, nkululeko <italic>always
      output debug</italic>, in which info, warning, and error will be
      obviously displayed in the terminal (and should be easily
      understood). The results are stored in the experiment folder for
      further investigations and are represented as images, texts, and
      even a fully automatically compiled PDF report written in
      Latex.</p>
    </list-item>
  </list>
</sec>
<sec id="usage-in-existing-research">
  <title>Usage in existing research</title>
  <p>Nkululeko has been used in several research projects since its
  first appearance in 2022
  (<xref alt="Burkhardt, Wagner, et al., 2022" rid="ref-nkululekoU003A2022" ref-type="bibr">Burkhardt,
  Wagner, et al., 2022</xref>). The following list gives an overview of
  the research papers that have used Nkululeko:</p>
  <list list-type="bullet">
    <list-item>
      <p>(<xref alt="Burkhardt, Eyben, et al., 2022" rid="ref-burkhardtU003A2022-syntact" ref-type="bibr">Burkhardt,
      Eyben, et al., 2022</xref>): This paper reported a database
      development of synthesized speech for basic emotions and its
      evaluation using the Nkululeko toolkit.</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Burkhardt et al., 2024" rid="ref-BurkhardtU003A2024" ref-type="bibr">Burkhardt
      et al., 2024</xref>): This paper shows how to use Nkululeko for
      bias detection. The findings on two datasets, UACorpus and
      Androids, show that some features are correlated with the target
      label, e.g., depression, and can be used to detect bias in the
      database.</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Atmaja et al., 2024" rid="ref-AtmajaU003A2024a" ref-type="bibr">Atmaja
      et al., 2024</xref>): This paper shows Nkululeko’s capability for
      ensemble learning with a focus on uncertainty estimation.</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Atmaja &amp; Sasou, 2025" rid="ref-AtmajaU003A2025" ref-type="bibr">Atmaja
      &amp; Sasou, 2025</xref>): In this paper, evaluations of different
      handcrafted acoustic features and SSL approaches for pathological
      voice detection tasks were reported, highlighting the ease of
      using Nkululeko to perform extensive experiments, including
      combinations of different features at different levels (early and
      late fusions).</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Atmaja et al., 2025" rid="ref-AtmajaU003A2025b" ref-type="bibr">Atmaja
      et al., 2025</xref>): This paper extends the previous ensemble
      learning evaluations with performance weighting (using weighted
      and unweighted accuracies) on five tasks and ten datasets.</p>
    </list-item>
  </list>
</sec>
<sec id="changes">
  <title>Changes</title>
  <p>Nkululeko has been described in three papers so far; we give a
  short overview on the updates since then.</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>2022 Paper:</bold> F. Burkhardt, Johannes Wagner, Hagen
      Wierstorf, Florian Eyben and Björn Schuller: Nkululeko: A Tool For
      Rapid Speaker Characteristics Detection, Proc. Proc. LREC, 2022.
      <bold>New features:</bold> First version mainly focussing on basic
      machine learning experiments that combine <italic>expert</italic>
      acoustic features (like Praat or opensmile features) with
      traditional learning approaches.</p>
    </list-item>
    <list-item>
      <p><bold>2023 Paper:</bold> F. Burkhardt, Florian Eyben and Björn
      Schuller: Nkululeko: Machine Learning Experiments on Speaker
      Characteristics Without Programming, Proc. Interspeech, 2023.
      <bold>New features:</bold> Mainly extending the acoustic features
      to deep-learning based (like TRILL, Hubert or wav2vec2) and the
      models by neural net architectures like MLP or CNN.</p>
    </list-item>
    <list-item>
      <p><bold>2024 Paper:</bold> F. Burkhardt, Bagus Tris Atmaja, Anna
      Derington, Florian Eyben and Björn Schuller: Check Your Audio
      Data: Nkululeko for Bias Detection, Proc. Oriental COCOSDA, 2024.
      <bold>New features:</bold> Introducing the concept of interfaces
      (or <italic>modules</italic>), focusing on the
      <italic>explore-module</italic> that features automatic data
      statistics and bias analysis.</p>
    </list-item>
    <list-item>
      <p><bold>Since then:</bold> Besides many minor enhancements;
      ensemble learning, Wav2vec2 model finetuning, adding automatic
      speaker identification, extending augmentation and
      segmentation.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge support from these various projects:</p>
  <list list-type="bullet">
    <list-item>
      <p>European SHIFT (<italic>MetamorphoSis of cultural Heritage Into
      augmented hypermedia assets For enhanced accessibiliTy and
      inclusion</italic>) project (Grant Agreement number:
      101060660);</p>
    </list-item>
    <list-item>
      <p>European EASIER (<italic>Intelligent Automatic Sign Language
      Translation</italic>) project (Grant Agreement number:
      101016982);</p>
    </list-item>
    <list-item>
      <p>Project JPNP20006 commissioned by the New Energy and Industrial
      Technology Development Organization (NEDO), Japan;</p>
    </list-item>
    <list-item>
      <p>Project 24K02967 from the Japan Society for the Promotion of
      Science (JSPS).</p>
    </list-item>
  </list>
  <p>We thank audEERING GmbH for partial funding.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-scikit-learnU003A2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <pub-id pub-id-type="doi">10.5555/1953048.2078195</pub-id>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nkululekoU003A2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Wagner</surname><given-names>Johannes</given-names></name>
        <name><surname>Wierstorf</surname><given-names>Hagen</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Schuller</surname><given-names>Björn</given-names></name>
      </person-group>
      <article-title>Nkululeko: A tool for rapid speaker characteristics detection</article-title>
      <source>2022 Language Resources and Evaluation Conference, LREC 2022</source>
      <publisher-name>European Language Resources Association (ELRA)</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <isbn>9791095546726</isbn>
      <fpage>1925</fpage>
      <lpage>1932</lpage>
    </element-citation>
  </ref>
  <ref id="ref-McFeeU003A2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Raffel</surname><given-names>Colin</given-names></name>
        <name><surname>Liang</surname><given-names>Dawen</given-names></name>
        <name><surname>Ellis</surname><given-names>Daniel</given-names></name>
        <name><surname>McVicar</surname><given-names>Matt</given-names></name>
        <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
      </person-group>
      <article-title>librosa: Audio and Music Signal Analysis in Python</article-title>
      <source>Proc. 14th Python Sci. Conf.</source>
      <year iso-8601-date="2015">2015</year>
      <issue>Scipy</issue>
      <pub-id pub-id-type="doi">10.25080/majora-7b98e3ed-003</pub-id>
      <fpage>18</fpage>
      <lpage>24</lpage>
    </element-citation>
  </ref>
  <ref id="ref-torchU003A2020">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Chaudhary</surname><given-names>Anmol</given-names></name>
        <name><surname>Chouhan</surname><given-names>Kuldeep Singh</given-names></name>
        <name><surname>Gajrani</surname><given-names>Jyoti</given-names></name>
        <name><surname>Sharma</surname><given-names>Bhavna</given-names></name>
      </person-group>
      <source>Deep learning with PyTorch</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.4018/978-1-7998-3095-5.ch003</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-YangU003A2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Shu-wen</given-names></name>
        <name><surname>Chi</surname><given-names>Po-Han</given-names></name>
        <name><surname>Chuang</surname><given-names>Yung-Sung</given-names></name>
        <name><surname>Lai</surname><given-names>Cheng-I Jeff</given-names></name>
        <name><surname>Lakhotia</surname><given-names>Kushal</given-names></name>
        <name><surname>Lin</surname><given-names>Yist Y.</given-names></name>
        <name><surname>Liu</surname><given-names>Andy T.</given-names></name>
        <name><surname>Shi</surname><given-names>Jiatong</given-names></name>
        <name><surname>Chang</surname><given-names>Xuankai</given-names></name>
        <name><surname>Lin</surname><given-names>Guan-Ting</given-names></name>
        <name><surname>Huang</surname><given-names>Tzu-Hsien</given-names></name>
        <name><surname>Tseng</surname><given-names>Wei-Cheng</given-names></name>
        <name><surname>Lee</surname><given-names>Ko-tik</given-names></name>
        <name><surname>Liu</surname><given-names>Da-Rong</given-names></name>
        <name><surname>Huang</surname><given-names>Zili</given-names></name>
        <name><surname>Dong</surname><given-names>Shuyan</given-names></name>
        <name><surname>Li</surname><given-names>Shang-Wen</given-names></name>
        <name><surname>Watanabe</surname><given-names>Shinji</given-names></name>
        <name><surname>Mohamed</surname><given-names>Abdelrahman</given-names></name>
        <name><surname>Lee</surname><given-names>Hung-yi</given-names></name>
      </person-group>
      <article-title>SUPERB: Speech Processing Universal PERformance Benchmark</article-title>
      <source>Interspeech 2021</source>
      <publisher-name>ISCA</publisher-name>
      <year iso-8601-date="2021-08">2021</year><month>08</month>
      <pub-id pub-id-type="doi">10.21437/Interspeech.2021-1775</pub-id>
      <fpage>1194</fpage>
      <lpage>1198</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GiannakopoulosU003A2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Giannakopoulos</surname><given-names>Theodoros</given-names></name>
      </person-group>
      <article-title>pyAudioAnalysis: An open-source python library for audio signal analysis</article-title>
      <source>PLoS One</source>
      <year iso-8601-date="2015">2015</year>
      <volume>10</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0144610</pub-id>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-WatanabeU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Watanabe</surname><given-names>Shinji</given-names></name>
        <name><surname>Hori</surname><given-names>Takaaki</given-names></name>
        <name><surname>Karita</surname><given-names>Shigeki</given-names></name>
        <name><surname>Hayashi</surname><given-names>Tomoki</given-names></name>
        <name><surname>Nishitoba</surname><given-names>Jiro</given-names></name>
        <name><surname>Unno</surname><given-names>Yuya</given-names></name>
        <name><surname>Soplin</surname><given-names>Nelson Enrique Yalta</given-names></name>
        <name><surname>Heymann</surname><given-names>Jahn</given-names></name>
        <name><surname>Wiesner</surname><given-names>Matthew</given-names></name>
        <name><surname>Chen</surname><given-names>Nanxin</given-names></name>
        <name><surname>Renduchintala</surname><given-names>Adithya</given-names></name>
        <name><surname>Ochiai</surname><given-names>Tsubasa</given-names></name>
      </person-group>
      <article-title>ESPNet: End-to-end speech processing toolkit</article-title>
      <source>Proc. Annu. Conf. Int. Speech Commun. Assoc. INTERSPEECH</source>
      <year iso-8601-date="2018">2018</year>
      <volume>2018-Septe</volume>
      <issue>September</issue>
      <pub-id pub-id-type="doi">10.21437/Interspeech.2018-1456</pub-id>
      <fpage>2207</fpage>
      <lpage>2211</lpage>
    </element-citation>
  </ref>
  <ref id="ref-speechbrainU003A2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ravanelli</surname><given-names>Mirco</given-names></name>
        <name><surname>Parcollet</surname><given-names>Titouan</given-names></name>
        <name><surname>Plantinga</surname><given-names>Peter</given-names></name>
        <name><surname>Rouhe</surname><given-names>Aku</given-names></name>
        <name><surname>Cornell</surname><given-names>Samuele</given-names></name>
        <name><surname>Lugosch</surname><given-names>Loren</given-names></name>
        <name><surname>Subakan</surname><given-names>Cem</given-names></name>
        <name><surname>Dawalatabad</surname><given-names>Nauman</given-names></name>
        <name><surname>Heba</surname><given-names>Abdelwahab</given-names></name>
        <name><surname>Zhong</surname><given-names>Jianyuan</given-names></name>
        <name><surname>Chou</surname><given-names>Ju-Chieh</given-names></name>
        <name><surname>Yeh</surname><given-names>Sung-Lin</given-names></name>
        <name><surname>Fu</surname><given-names>Szu-Wei</given-names></name>
        <name><surname>Liao</surname><given-names>Chien-Feng</given-names></name>
        <name><surname>Rastorgueva</surname><given-names>Elena</given-names></name>
        <name><surname>Grondin</surname><given-names>François</given-names></name>
        <name><surname>Aris</surname><given-names>William</given-names></name>
        <name><surname>Na</surname><given-names>Hwidong</given-names></name>
        <name><surname>Gao</surname><given-names>Yan</given-names></name>
        <name><surname>Mori</surname><given-names>Renato De</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>SpeechBrain: A general-purpose speech toolkit</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2106.04624</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2106.04624</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-spotlightU003A2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Suwelack</surname><given-names>Stefan</given-names></name>
      </person-group>
      <article-title>Spotlight</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/Renumics/spotlight/; GitHub</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-opensmileU003A2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Wöllmer</surname><given-names>Martin</given-names></name>
        <name><surname>Schuller</surname><given-names>Björn</given-names></name>
      </person-group>
      <article-title>openSMILE – the munich versatile and fast open-source audio feature extractor</article-title>
      <source>MM’10 - Proceedings of the ACM Multimedia 2010 International Conference</source>
      <year iso-8601-date="2010-01">2010</year><month>01</month>
      <pub-id pub-id-type="doi">10.1145/1873951.1874246</pub-id>
      <fpage>1459</fpage>
      <lpage>1462</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wav2vecU003A2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Baevski</surname><given-names>Alexei</given-names></name>
        <name><surname>Zhou</surname><given-names>Yuhao</given-names></name>
        <name><surname>Mohamed</surname><given-names>Abdelrahman</given-names></name>
        <name><surname>Auli</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>wav2vec 2.0: A framework for self-supervised learning of speech representations</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Ranzato</surname><given-names>M.</given-names></name>
        <name><surname>Hadsell</surname><given-names>R.</given-names></name>
        <name><surname>Balcan</surname><given-names>M. F.</given-names></name>
        <name><surname>Lin</surname><given-names>H.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <uri>https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.11477</pub-id>
      <fpage>12449</fpage>
      <lpage>12460</lpage>
    </element-citation>
  </ref>
  <ref id="ref-burkhardtU003A2022-syntact">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
        <name><surname>Schuller</surname><given-names>W</given-names></name>
      </person-group>
      <article-title>SyntAct : A Synthesized Database of Basic Emotions</article-title>
      <source>Proc. Work. Dataset creat. Low. Lang. Within 13th lang. Resour. Eval. conf.</source>
      <person-group person-group-type="editor">
        <string-name>Jonne Sälevä</string-name>
        <name><surname>Lignos</surname><given-names>Constantine</given-names></name>
      </person-group>
      <publisher-name>European Language Resources Association</publisher-name>
      <publisher-loc>Marseille, France</publisher-loc>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2024a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
      </person-group>
      <article-title>Uncertainty-based ensemble learning for speech classification</article-title>
      <source>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/O-COCOSDA64382.2024.10800111</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BurkhardtU003A2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Derington</surname><given-names>Anna</given-names></name>
        <name><surname>Eyben</surname><given-names>Florian</given-names></name>
      </person-group>
      <article-title>Check your audio data: Nkululeko for bias detection</article-title>
      <source>2024 27th conference of the oriental COCOSDA international committee for the co-ordination and standardisation of speech databases and assessment techniques (o-COCOSDA)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/O-COCOSDA64382.2024.10800580</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2025">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
      </person-group>
      <article-title>Pathological voice detection from sustained vowels: Handcrafted vs. Self-supervised learning</article-title>
      <source>2025 IEEE international conference on acoustics, speech, and signal processing workshops (ICASSPW)</source>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.1109/ICASSPW65056.2025.11011272</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-AtmajaU003A2025b">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Atmaja</surname><given-names>Bagus Tris</given-names></name>
        <name><surname>Burkhardt</surname><given-names>Felix</given-names></name>
        <name><surname>Sasou</surname><given-names>Akira</given-names></name>
      </person-group>
      <article-title>Performance-weighted ensemble learning for speech classification</article-title>
      <source>2025 international conference on artificial intelligence in information and communication (ICAIIC)</source>
      <publisher-loc>Fukuoka</publisher-loc>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.1109/ICAIIC64266.2025.10920862</pub-id>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://audeering.github.io/audformat/">https://audeering.github.io/audformat/</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
