<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3652</article-id>
<article-id pub-id-type="doi">10.21105/joss.03652</article-id>
<title-group>
<article-title>TX<inline-formula><alternatives>
<tex-math><![CDATA[^2]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>:
Transformer eXplainability and eXploration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0002-5036-5433</contrib-id>
<name>
<surname>Martindale</surname>
<given-names>Nathan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4320-5818</contrib-id>
<name>
<surname>Stewart</surname>
<given-names>Scott L.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Oak Ridge National Laboratory</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-12-21">
<day>21</day>
<month>12</month>
<year>2021</year>
</pub-date>
<volume>6</volume>
<issue>68</issue>
<fpage>3652</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>explainability</kwd>
<kwd>natural language processing</kwd>
<kwd>deep networks</kwd>
<kwd>transformers</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The Transformer eXplainability and eXploration
  (<xref alt="Martindale &amp; Stewart, 2021" rid="ref-tx2" ref-type="bibr">Martindale
  &amp; Stewart, 2021</xref>), or TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  software package, is a library designed for artificial intelligence
  researchers to better understand the performance of transformer models
  (<xref alt="Vaswani et al., 2017" rid="ref-vaswani2017attention" ref-type="bibr">Vaswani
  et al., 2017</xref>) used for sequence classification. The tool is
  capable of integrating with a trained transformer model and a dataset
  split into training and testing populations to produce an ipywidget
  (<xref alt="Project Jupyter Contributors, 2021" rid="ref-ipywidgets" ref-type="bibr">Project
  Jupyter Contributors, 2021</xref>) dashboard with a number of
  visualizations to understand model performance with an emphasis on
  explainability and interpretability. The
  TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  package is primarily intended to integrate into a workflow centered
  around Jupyter Notebooks
  (<xref alt="Kluyver et al., 2016" rid="ref-jupyternotebook" ref-type="bibr">Kluyver
  et al., 2016</xref>), and currently assumes the use of PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>) and Hugging Face transformers library
  (<xref alt="Wolf et al., 2020" rid="ref-hf-transformers" ref-type="bibr">Wolf
  et al., 2020</xref>). The dashboard includes visualization and data
  exploration features to aid researchers, including an interactive UMAP
  embedding graph
  (<xref alt="McInnes et al., 2018" rid="ref-mcinnes2018umap" ref-type="bibr">McInnes
  et al., 2018</xref>) to understand classification clusters, a word
  salience map that can be updated as researchers alter textual entries
  in near real time, a set of tools to understand word frequency and
  importance based on the clusters in the UMAP embedding graph, and a
  set of traditional confusion matrix analysis tools.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Transformers, although particularly effective on a wide variety of
  natural language processing tasks, have the same challenge of many
  deep network approaches in that it is difficult to glean insight into
  why certain classification decisions are made
  (<xref alt="Aken et al., 2020" rid="ref-aken2020visbert" ref-type="bibr">Aken
  et al., 2020</xref>). Various works have explored the value of
  analyzing the attention layers in order to provide explainability in
  the output of a transformer network
  (<xref alt="Vig, 2019" rid="ref-vig2019multiscale" ref-type="bibr">Vig,
  2019</xref>). However, analyzing attention alone can be insufficient
  when attempting to gain broader insight into why a transformer is
  performing a certain way with a specific dataset
  (<xref alt="Jain &amp; Wallace, 2019" rid="ref-jain2019attention" ref-type="bibr">Jain
  &amp; Wallace, 2019</xref>). TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  aims to address this challenge by providing a model developer with a
  number of tools to explore why a certain transformer performs in a
  certain way for a specific dataset. This tool can help a developer
  determine, among other things, whether or not a specific transformer
  has gained a generalized understanding of the semantic meaning behind
  textual entries in a specific dataset. It can also help with studying
  the impact of language distribution shifts over time on transformer
  sequence classification performance.</p>
  <p>Existing tools, such as Google PAIR’s Language Interpretability
  Tool
  (<xref alt="Tenney et al., 2020" rid="ref-tenney2020lit" ref-type="bibr">Tenney
  et al., 2020</xref>), also provide a platform to use multiple
  visualizations to study transformer model performance.
  TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  differs from these tools with its emphasis on cluster analysis and
  easier customization of both the model interaction and dashboard
  itself within a Jupyter Notebook. The close integration with Jupyter
  Notebook is advantageous for those researchers who already rely
  heavily on the tools within the Jupyter ecosystem. Like the Language
  Interpretability Tool, TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  offers a projection map with all of the data points; however it goes
  further in breaking down the visual clusters and providing separate
  visualizations for understanding the language per cluster.
  Additionally, the TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  design promotes easy modification or customization depending on the
  researcher’s needs, as researchers can completely change the
  presentation order of plots within the ipywidget and even add
  additional visualizations if desired.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p>The primary visualization for the widget is a UMAP embedding graph
  that projects the multidimensional sequence embedding space into 2D
  clusters. This plots multiple controls that can be used to understand
  how the sequence classifier is working, including the ability to show
  or hide training data, highlight certain keywords, and focus on
  misclassifications. Below the UMAP plot, the dashboard includes a set
  of tools for exploring textual data including a word salience map that
  shows information on specific train or test data entries. The salience
  map serves as a proxy for word importance and is computed by
  recalculating the soft classifications of a particular entry in the
  corpus multiple times with each word individually removed. The
  background coloring in the map indicates the degree of impact word
  removal has on the classification result, with a darker background
  highlight corresponding to greater importance. The dashboard also
  includes a text entry box that is prepopulated with the text from the
  entry shown in the salience map. The user can use this text box to
  explore the impact of word addition or removal by modifying the entry.
  The change is reflected both in the salience map plot as well as with
  a change in the data point in the UMAP embedding graph.</p>
  <p>The dashboard also includes a set of visual clustering analysis
  tools. Any clustering algorithm from sklearn’s
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) clustering module can be used to assign clusters
  to the data once it is projected into the UMAP embedding. The
  dashboard displays cluster labels, along with inter-cluster word
  frequency, and each words’ importance on the classification result.
  The salience scores for each word are calculated in aggregate for each
  cluster, again by iterating with the classifier while individual words
  are removed. There are also some sampling buttons that allow for a
  data example to be randomly pulled from a specific cluster so that it
  can be examined by the entry-specific salience map tool. Finally, it
  is also possible to output traditional confusion matrices as well as
  various evaluation scores (e.g., f1-score, accuracy, precision) as
  part of the dashboard.</p>
</sec>
<sec id="integration">
  <title>Integration</title>
  <p>TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  includes two main classes: a wrapper class and a dashboard class. The
  wrapper class wraps around the transformer/classification model and
  acts as an interface between the dashboard and the transformer. The
  wrapper is in charge of computing and caching all the necessary data
  for the dashboard visualizations. The dashboard class is responsible
  for setting up and rendering the widget layout and handling dashboard
  interactivity. The flow of interactions between the
  TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  library and a Jupyter Notebook can be seen in
  <xref alt="Figure 1" rid="figU003Ainteractivity">Figure 1</xref>.</p>
  <fig>
    <caption><p>Flow of interactions between a Jupyter Notebook and the
    tx^2
    library.<styled-content id="figU003Ainteractivity"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/interaction_flow.png" xlink:title="" />
  </fig>
  <p>The wrapper communicates with the transformer through a set of four
  functions as seen in
  <xref alt="Figure 2" rid="figU003Aintegration">Figure 2</xref>. These
  functions include an embedding function that returns a single sequence
  of embeddings for each input text, a classification function that
  returns the predicted output class for each input text, a soft
  classification function that returns some output value for each class
  for each input text, and an encoding function that converts the text
  into model inputs.</p>
  <fig>
    <caption><p>Example of integrating a transformer with the tx^2
    wrapper.<styled-content id="figU003Aintegration"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/example_interaction.png" xlink:title="" />
  </fig>
  <p>The default implementation for TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  assumes a huggingface pretrained model. If this use case fits the
  purposes of the user, they can use the default implementations for
  these functions. Otherwise, the user will need to redefine the
  functions to handle their use case while ensuring that the new
  functions return the necessary data and the correct format.</p>
</sec>
<sec id="audience">
  <title>Audience</title>
  <p>The target audience for the TX<inline-formula><alternatives>
  <tex-math><![CDATA[^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi></mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  tool are machine learning or artificial intelligence researchers
  focused on natural language processing with transformers, and who are
  comfortable operating within the Jupyter ecosystem for demonstration
  or exploration. This open-source software is licensed under a BSD-3
  clause license, is registered on
  <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.11578/dc.20210129.1">DOE
  Code</ext-link>, and is available on
  <ext-link ext-link-type="uri" xlink:href="https://github.com/ORNL/tx2">GitHub</ext-link>.
  The package is also pip installable with
  <monospace>pip install tx2</monospace> with Sphinx
  (<xref alt="Sphinx Team, 2021" rid="ref-sphinx" ref-type="bibr">Sphinx
  Team, 2021</xref>) built
  <ext-link ext-link-type="uri" xlink:href="https://ornl.github.io/tx2/index.html">documentation</ext-link>.
  Finally, linting for this project is performed using black
  (<xref alt="Python Software Foundation, 2021" rid="ref-black-linter" ref-type="bibr">Python
  Software Foundation, 2021</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors would like to acknowledge the US Department of Energy,
  National Nuclear Security Administration’s Office of Defense Nuclear
  Nonproliferation Research and Development (NA-22) for supporting this
  work.</p>
  <p>This manuscript has been authored by UT-Battelle, LLC, under
  contract DE-AC05-00OR22725 with the US Department of Energy (DOE). The
  US government retains and the publisher, by accepting the article for
  publication, acknowledges that the US government retains a
  nonexclusive, paid-up, irrevocable, worldwide license to publish or
  reproduce the published form of this manuscript, or allow others to do
  so, for US government purposes. DOE will provide public access to
  these results of federally sponsored research in accordance with the
  DOE Public Access Plan
  (http://energy.gov/downloads/doe-public-access-plan).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-tx2">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Martindale</surname><given-names>Nathan</given-names></name>
        <name><surname>Stewart</surname><given-names>Scott L.</given-names></name>
      </person-group>
      <article-title>Transformer eXplainability and eXploration</article-title>
      <publisher-name>[Computer Software] https://doi.org/10.11578/dc.20210129.1</publisher-name>
      <year iso-8601-date="2021-01">2021</year><month>01</month>
    </element-citation>
  </ref>
  <ref id="ref-tenney2020lit">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tenney</surname><given-names>Ian</given-names></name>
        <name><surname>Wexler</surname><given-names>James</given-names></name>
        <name><surname>Bastings</surname><given-names>Jasmijn</given-names></name>
        <name><surname>Bolukbasi</surname><given-names>Tolga</given-names></name>
        <name><surname>Coenen</surname><given-names>Andy</given-names></name>
        <name><surname>Gehrmann</surname><given-names>Sebastian</given-names></name>
        <name><surname>Jiang</surname><given-names>Ellen</given-names></name>
        <name><surname>Pushkarna</surname><given-names>Mahima</given-names></name>
        <name><surname>Radebaugh</surname><given-names>Carey</given-names></name>
        <name><surname>Reif</surname><given-names>Emily</given-names></name>
        <name><surname>Yuan</surname><given-names>Ann</given-names></name>
      </person-group>
      <article-title>The language interpretability tool: Extensible, interactive visualizations and analysis for NLP models</article-title>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://www.aclweb.org/anthology/2020.emnlp-demos.15</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2020.emnlp-demos.15</pub-id>
      <fpage>107</fpage>
      <lpage>118</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jupyternotebook">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kluyver</surname><given-names>Thomas</given-names></name>
        <name><surname>Ragan-Kelley</surname><given-names>Benjamin</given-names></name>
        <name><surname>Pérez</surname><given-names>Fernando</given-names></name>
        <name><surname>Granger</surname><given-names>Brian</given-names></name>
        <name><surname>Bussonnier</surname><given-names>Matthias</given-names></name>
        <name><surname>Frederic</surname><given-names>Jonathan</given-names></name>
        <name><surname>Kelley</surname><given-names>Kyle</given-names></name>
        <name><surname>Hamrick</surname><given-names>Jessica</given-names></name>
        <name><surname>Grout</surname><given-names>Jason</given-names></name>
        <name><surname>Corlay</surname><given-names>Sylvain</given-names></name>
        <name><surname>Ivanov</surname><given-names>Paul</given-names></name>
        <name><surname>Avila</surname><given-names>Damián</given-names></name>
        <name><surname>Abdalla</surname><given-names>Safia</given-names></name>
        <name><surname>Willing</surname><given-names>Carol</given-names></name>
        <name><surname>team</surname><given-names>Jupyter development</given-names></name>
      </person-group>
      <article-title>Jupyter notebooks - a publishing format for reproducible computational workflows</article-title>
      <source>Positioning and power in academic publishing: Players, agents and agendas</source>
      <person-group person-group-type="editor">
        <name><surname>Loizides</surname><given-names>Fernando</given-names></name>
        <name><surname>Scmidt</surname><given-names>Birgit</given-names></name>
      </person-group>
      <publisher-name>IOS Press</publisher-name>
      <publisher-loc>Netherlands</publisher-loc>
      <year iso-8601-date="2016">2016</year>
      <uri>https://eprints.soton.ac.uk/403913/</uri>
      <fpage>87</fpage>
      <lpage>90</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hf-transformers">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>Rémi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>Davison</surname><given-names>Joe</given-names></name>
        <name><surname>Shleifer</surname><given-names>Sam</given-names></name>
        <name><surname>Platen</surname><given-names>Patrick von</given-names></name>
        <name><surname>Ma</surname><given-names>Clara</given-names></name>
        <name><surname>Jernite</surname><given-names>Yacine</given-names></name>
        <name><surname>Plu</surname><given-names>Julien</given-names></name>
        <name><surname>Xu</surname><given-names>Canwen</given-names></name>
        <name><surname>Scao</surname><given-names>Teven Le</given-names></name>
        <name><surname>Gugger</surname><given-names>Sylvain</given-names></name>
        <name><surname>Drame</surname><given-names>Mariama</given-names></name>
        <name><surname>Lhoest</surname><given-names>Quentin</given-names></name>
        <name><surname>Rush</surname><given-names>Alexander M.</given-names></name>
      </person-group>
      <article-title>Transformers: State-of-the-art natural language processing</article-title>
      <source>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <uri>https://www.aclweb.org/anthology/2020.emnlp-demos.6</uri>
      <fpage>38</fpage>
      <lpage>45</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mcinnes2018umap">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McInnes</surname><given-names>Leland</given-names></name>
        <name><surname>Healy</surname><given-names>John</given-names></name>
        <name><surname>Saul</surname><given-names>Nathaniel</given-names></name>
        <name><surname>Grossberger</surname><given-names>Lukas</given-names></name>
      </person-group>
      <article-title>UMAP: Uniform manifold approximation and projection</article-title>
      <source>The Journal of Open Source Software</source>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>29</issue>
      <pub-id pub-id-type="doi">10.21105/joss.00861</pub-id>
      <fpage>861</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vig2019multiscale">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vig</surname><given-names>Jesse</given-names></name>
      </person-group>
      <article-title>A multiscale visualization of attention in the transformer model</article-title>
      <source>Proceedings of the 57th annual meeting of the association for computational linguistics: System demonstrations</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Florence, Italy</publisher-loc>
      <year iso-8601-date="2019-07">2019</year><month>07</month>
      <uri>https://www.aclweb.org/anthology/P19-3007</uri>
      <pub-id pub-id-type="doi">10.18653/v1/P19-3007</pub-id>
      <fpage>37</fpage>
      <lpage>42</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jain2019attention">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jain</surname><given-names>Sarthak</given-names></name>
        <name><surname>Wallace</surname><given-names>Byron C</given-names></name>
      </person-group>
      <article-title>Attention is not explanation</article-title>
      <source>arXiv preprint arXiv:1902.10186</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-aken2020visbert">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Aken</surname><given-names>Betty van</given-names></name>
        <name><surname>Winter</surname><given-names>Benjamin</given-names></name>
        <name><surname>Löser</surname><given-names>Alexander</given-names></name>
        <name><surname>Gers</surname><given-names>Felix A.</given-names></name>
      </person-group>
      <article-title>VisBERT: Hidden-state visualizations for transformers</article-title>
      <source>Companion proceedings of the web conference 2020</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2020">2020</year>
      <isbn>9781450370240</isbn>
      <uri>https://doi.org/10.1145/3366424.3383542</uri>
      <pub-id pub-id-type="doi">10.1145/3366424.3383542</pub-id>
      <fpage>207</fpage>
      <lpage>211</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scikit-learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-black-linter">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Python Software Foundation</string-name>
      </person-group>
      <article-title>Black - the uncompromising code formatter</article-title>
      <publisher-name>https://github.com/psf/black</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-ipywidgets">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Project Jupyter Contributors</string-name>
      </person-group>
      <article-title>Ipywidgets: Interactive HTML widgets</article-title>
      <publisher-name>https://github.com/jupyter-widgets/ipywidgets</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-sphinx">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Sphinx Team</string-name>
      </person-group>
      <article-title>Sphinx</article-title>
      <publisher-name>https://github.com/sphinx-doc/sphinx</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-vaswani2017attention">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention is all you need</article-title>
      <source>arXiv preprint arXiv:1706.03762</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
