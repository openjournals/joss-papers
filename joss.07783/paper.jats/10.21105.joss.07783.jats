<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7783</article-id>
<article-id pub-id-type="doi">10.21105/joss.07783</article-id>
<title-group>
<article-title>MM-PoE: Multiple Choice Reasoning via. Process of
Elimination using Multi-Modal Models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0004-6179-389X</contrib-id>
<name>
<surname>Chakrabarty</surname>
<given-names>Sayak</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5781-3032</contrib-id>
<name>
<surname>Pal</surname>
<given-names>Souradip</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Northwestern University</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Purdue University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-10-22">
<day>22</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>108</issue>
<fpage>7783</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>machine learning</kwd>
<kwd>large language models</kwd>
<kwd>multi-modal</kwd>
<kwd>python</kwd>
<kwd>multiple choice reasoning</kwd>
<kwd>visual question answering</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>This paper introduces Multiple Choice Reasoning via. Process of
  Elimination using Multi-Modal models, also known as Multi-Modal
  Process of Elimination (MM-PoE), is a method to enhance vision
  language models’ performance on multiple choice visual reasoning tasks
  by employing a two-step scoring system that first eliminates incorrect
  options and then predicts from the remaining ones. Our experiments
  across three question-answering datasets show the method’s
  effectiveness, particularly in visual reasoning tasks. This method
  addresses one of the key limitations of the paper
  (<xref alt="Ma &amp; Du, 2023" rid="ref-ma-du-2023-poe" ref-type="bibr">Ma
  &amp; Du, 2023</xref>) by extending to tasks involving
  multi-modalities and also includes experimentation techniques for
  few-shot settings.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Large Language models (LLMs) excel at in-context learning for
  multiple-choice reasoning tasks but often treat all options equally,
  unlike humans who typically eliminate incorrect choices before
  selecting the correct answer. The same is true for vision language
  models (VLMs) in case of visual question-answering tasks with multiple
  choices. This discrepancy can limit the effectiveness of vision
  language models in accurately solving such tasks. To address this, we
  introduce Multi-Modal Process of Elimination (MM-PoE), a two-step
  scoring method designed to enhance VLM performance by mimicking human
  reasoning strategies in multi-modal settings.</p>
  <p>In the first step, the method evaluates and scores each option,
  systematically eliminating those that appear incorrect. The second
  step involves masking these eliminated options, allowing the VLM to
  focus solely on the remaining viable choices to make a final
  prediction. Our zero-shot experiments across three datasets
  demonstrate MM-PoE’s effectiveness, particularly excelling in logical
  reasoning scenarios. Additionally, MM-PoE proves adaptable to few-shot
  settings and is compatible with the current state-of-the-art vision
  language models (VLMs).</p>
  <p>Using this tool, researchers and practitioners can experiment and
  significantly improve the accuracy and reliability of VLMs in multiple
  choice reasoning tasks, making it a valuable tool for advancing
  machine learning models for visual reasoning.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the Field</title>
  <p>A common strategy for answering multiple-choice questions,
  especially under examination conditions, involves a process of
  elimination where incorrect answers are systematically discarded to
  narrow down the choices to the most likely correct ones. This
  approach, grounded in everyday test-taking strategies
  (<xref alt="Zhang et al., 2023" rid="ref-zhang2023dynamically" ref-type="bibr">Zhang
  et al., 2023</xref>), contrasts with how current language models (LMs)
  and vision language models (VLMs) handle multiple-choice reasoning
  tasks. Typically, VLMs evaluate each option independently or
  collectively without actively discarding less likely answers,
  potentially reducing their effectiveness in distinguishing the best
  choice from plausible distractors.</p>
  <p>This paper argues that vision language models can benefit from an
  explicit two-step reasoning process akin to human problem-solving
  techniques. The proposed method, known as Multi-Modal Process of
  Elimination (MM-PoE), enhances the decision-making process by first
  scoring and then eliminating options that are seemingly incorrect
  before focusing on selecting the correct answer from the remaining
  choices. This method is designed to align with natural human reasoning
  by replicating how individuals often approach multiple-choice
  questions, particularly under the constraint of time and accuracy, as
  frequently experienced in academic testing environments.</p>
  <p>Our hypothesis posits that vision language models, when equipped
  with a mechanism to discard implausible answers systematically, can
  achieve better performance on multiple-choice visual reasoning tasks.
  This is particularly relevant in the context of logical reasoning,
  where the elimination of clearly incorrect options can simplify the
  decision process and potentially lead to more accurate outcomes. This
  idea is supported by previous work demonstrating the effectiveness of
  LMs in various reasoning tasks when adapted to more human-like
  reasoning methods
  (<xref alt="Holtzman et al., 2021" rid="ref-holtzman2021surface" ref-type="bibr">Holtzman
  et al., 2021</xref>).</p>
  <p>In the development of MM-PoE, we draw inspiration from the
  established capabilities of LMs to handle complex reasoning tasks
  (<xref alt="Brown et al., 2020" rid="ref-brown2020language" ref-type="bibr">Brown
  et al., 2020</xref>) and the known strategies that humans employ in
  test-taking scenarios as depicted in
  (<xref alt="Ma &amp; Du, 2023" rid="ref-ma-du-2023-poe" ref-type="bibr">Ma
  &amp; Du, 2023</xref>). The approach builds on the foundational work
  in language modeling likelihood
  (<xref alt="Brown et al., 2020" rid="ref-brown2020language" ref-type="bibr">Brown
  et al., 2020</xref>), which demonstrates the LMs’ ability to perform
  in-context learning. By incorporating a structured process to
  eliminate unlikely choices in a multi-modal setting, MM-PoE aims to
  refine this capability, making it more targeted and efficient in
  dealing with the nuanced challenges presented by multiple-choice
  questions.</p>
  <p>The effectiveness of this approach is underscored through zero-shot
  and few-shot experiments across a diverse set of reasoning datasets,
  illustrating that the integration of human-like elimination strategies
  can significantly enhance the performance of vision language models.
  This paper aims to show that by mimicking human reasoning processes,
  we can make VLMs not only perform better on standardized visual
  reasoning tasks but also behave in ways that are more interpretable
  and aligned with human cognitive processes.</p>
</sec>
<sec id="methodology">
  <title>Methodology</title>
  <p>The Multi-Modal Process of Elimination (MM-PoE) introduced in this
  paper operates on a two-step mechanism
  (<xref alt="Datta &amp; Chakrabarty, 2024" rid="ref-datta2024consistency" ref-type="bibr">Datta
  &amp; Chakrabarty, 2024</xref>) designed to enhance the
  decision-making capabilities of vision language models (VLMs) in
  multiple-choice visual reasoning tasks. This method employs a novel
  approach to option elimination followed by a focused prediction phase.
  The strategy is rooted in the belief that separating the elimination
  of clearly incorrect options from the choice of the best remaining
  option will improve overall task performance.</p>
  <sec id="problem-setting">
    <title>Problem Setting</title>
    <p>Given a multiple-choice visual reasoning task, we define the
    problem setting as follows:</p>
    <list list-type="bullet">
      <list-item>
        <p>Let <inline-formula><alternatives>
        <tex-math><![CDATA[x]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
        be the question or context provided.</p>
      </list-item>
      <list-item>
        <p>Let <inline-formula><alternatives>
        <tex-math><![CDATA[h]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>
        be the image provided.</p>
      </list-item>
      <list-item>
        <p>Let <inline-formula><alternatives>
        <tex-math><![CDATA[Y = \{y_1, y_2, \ldots, y_n\}]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
        be the set of multiple-choice options available.</p>
      </list-item>
      <list-item>
        <p>Let <inline-formula><alternatives>
        <tex-math><![CDATA[y]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
        be the correct answer from <inline-formula><alternatives>
        <tex-math><![CDATA[Y]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>.</p>
      </list-item>
    </list>
    <p>The goal is to develop an in-context learning method that
    accurately selects <inline-formula><alternatives>
    <tex-math><![CDATA[y]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>
    from <inline-formula><alternatives>
    <tex-math><![CDATA[Y]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
    given <inline-formula><alternatives>
    <tex-math><![CDATA[x]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[h]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>.</p>
  </sec>
  <sec id="two-step-scoring-method">
    <title>Two-Step Scoring Method</title>
    <sec id="step-1-elimination">
      <title>Step 1: Elimination</title>
      <p>In the first step of the MM-PoE method, each option
      <inline-formula><alternatives>
      <tex-math><![CDATA[y_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is scored based on a specified metric. The score function,
      <inline-formula><alternatives>
      <tex-math><![CDATA[\text{score}(x, h, y_i)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">score</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
      evaluates each option’s plausibility given the question
      <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      and image <inline-formula><alternatives>
      <tex-math><![CDATA[h]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>.
      The scores are used to eliminate options deemed less likely to be
      correct. Specifically, options whose scores are below the average
      score are eliminated. This is calculated as follows:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      s_i = \text{score}(x, h, y_i)
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext mathvariant="normal">score</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      Y_{\text{wrong}} = \{y_i | s_i < \text{avg}(s_1, \ldots, s_n)\}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mtext mathvariant="normal">wrong</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mtext mathvariant="normal">avg</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This elimination strategy intuitively aligns with how humans
      often discard options that seem clearly incorrect before carefully
      considering the remaining choices.</p>
    </sec>
    <sec id="step-2-prediction">
      <title>Step 2: Prediction</title>
      <p>The second step involves making the final choice from the
      non-eliminated options. This step utilizes a binary mask to
      exclude the eliminated options during the prediction phase. The
      mask for each option <inline-formula><alternatives>
      <tex-math><![CDATA[y_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is defined as follows:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      m_i = \begin{cases} 
      0 & \text{if } y_i \in Y_{\text{wrong}} \\
      1 & \text{otherwise}
      \end{cases}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mtext mathvariant="normal">if </mml:mtext><mml:mspace width="0.333em"></mml:mspace></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mtext mathvariant="normal">wrong</mml:mtext></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>The masked context <inline-formula><alternatives>
      <tex-math><![CDATA[x_{\text{mask}}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant="normal">mask</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      is then constructed by modifying the original context
      <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      to include only the options for which
      <inline-formula><alternatives>
      <tex-math><![CDATA[m_i = 1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
      Each option is scored again, but this time within the context that
      explicitly excludes the eliminated options, possibly by using a
      template <inline-formula><alternatives>
      <tex-math><![CDATA[T]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>T</mml:mi></mml:math></alternatives></inline-formula>
      that masks out <inline-formula><alternatives>
      <tex-math><![CDATA[Y_{\text{wrong}}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Y</mml:mi><mml:mtext mathvariant="normal">wrong</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      in the presentation of the options:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      x_{\text{mask}} = T(x, Y, \text{mask})
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant="normal">mask</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:mtext mathvariant="normal">mask</mml:mtext><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>The final predicted answer <inline-formula><alternatives>
      <tex-math><![CDATA[\hat{y}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
      is then the option with the highest score among the remaining
      options:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      \hat{y} = \arg\max_{i | m_i = 1} \text{score}(x_{\text{mask}}, h, y_i)
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo>max</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munder><mml:mtext mathvariant="normal">score</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mtext mathvariant="normal">mask</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
    </sec>
  </sec>
</sec>
<sec id="experimental-setup">
  <title>Experimental Setup</title>
  <p>To evaluate the effectiveness of the Multi-Modal Process of
  Elimination (MM-PoE), we designed an experimental framework that tests
  the method across a diverse set of visual reasoning datasets. This
  setup aims to compare MM-PoE with existing scoring methods to
  highlight its potential improvements in accuracy and reasoning
  capability. Our experiments primarily focused on a zero-shot setting
  to evaluate the generalization capabilities of MM-PoE without any
  task-specific tuning. Accuracy was used as the main metric for
  performance evaluation, with results averaged over multiple seeds to
  ensure robustness.</p>
  <p>To further explore the versatility of MM-PoE, we also examined its
  performance in few-shot settings by incorporating examples into the
  model’s input, aiming to observe any changes in effectiveness when
  provided with context-specific demonstrations.</p>
  <sec id="data">
    <title>Data</title>
    <p>Our experiments were conducted on three different multiple-choice
    visual reasoning datasets - Visual Question Answering (VQA)
    (<xref alt="Antol et al., 2015" rid="ref-VQA" ref-type="bibr">Antol
    et al., 2015</xref>), ScienceQA
    (<xref alt="Lu et al., 2022" rid="ref-lu2022learn" ref-type="bibr">Lu
    et al., 2022</xref>), and Diagram Understanding (AI2D)
    (<xref alt="Kembhavi et al., 2016" rid="ref-Kembhavi2016ADI" ref-type="bibr">Kembhavi
    et al., 2016</xref>), selected to cover a broad spectrum of
    reasoning types and complexities. These tasks include both
    traditional visual reasoning tasks and more specialized ones
    designed to test specific reasoning skills. To ensure a
    comprehensive evaluation, we used train sets from established
    benchmarks when available; otherwise, we utilized development sets.
    In case of varying number of options in the multiple-choice answers
    for SceinceQA and AI2D datasets, we filtered questions containing
    image context and exactly four options.</p>
    <table-wrap>
      <table>
        <thead>
          <tr>
            <th>Dataset</th>
            <th>#Options</th>
            <th>Train</th>
            <th>Dev</th>
            <th>Test</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>VQA</td>
            <td>18</td>
            <td>248,349</td>
            <td>121,512</td>
            <td>244,302</td>
          </tr>
          <tr>
            <td>ScienceQA</td>
            <td>4</td>
            <td>12726</td>
            <td>4241</td>
            <td>4241</td>
          </tr>
          <tr>
            <td>AI2D</td>
            <td>4</td>
            <td>3921</td>
            <td>982</td>
            <td>-</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="model">
    <title>Model</title>
    <p>For the core experiments, we utilized the GIT and BLIP models,
    chosen for their balance between computational efficiency and
    performance in instruction-tuned vision language tasks. These models
    have demonstrated strong capabilities in handling various
    multi-modal tasks and serve as a robust platform for evaluating our
    MM-PoE method.</p>
  </sec>
  <sec id="baselines">
    <title>Baselines</title>
    <p>We compared MM-PoE against five baseline scoring methods to
    assess its relative performance:</p>
    <list list-type="order">
      <list-item>
        <p><bold>Language Modeling (LM):</bold> This baseline uses the
        raw vision language modeling likelihood as the scoring
        function.</p>
      </list-item>
      <list-item>
        <p><bold>Average Language Modeling (AVG):</bold> This method
        averages the log probabilities across all tokens in the
        option.</p>
      </list-item>
      <list-item>
        <p><bold>Calibration:</bold> This involves adjusting the VLM
        scores based on calibration techniques that aim to correct for
        the model’s confidence.</p>
      </list-item>
      <list-item>
        <p><bold>Channel:</bold> Channel methods score each option based
        on how likely the question is given the option, which reverses
        the typical conditional probability used in LMs.</p>
      </list-item>
      <list-item>
        <p><bold>Multiple Choice Prompting (MCP):</bold> This approach
        formats the input by presenting the question followed by all
        options, prompting the model to select the most likely
        option.</p>
      </list-item>
    </list>
    <p>Each method provides a different approach to scoring options,
    allowing for a comprehensive comparison of how each interacts with
    the structure and strategy of MM-PoE.</p>
  </sec>
  <sec id="implementation">
    <title>Implementation</title>
    <p>The effectiveness of MM-PoE hinges on the robustness of the
    scoring function and the accuracy of the elimination step. The
    scoring function can be any VLM-based likelihood estimator, such as
    vision language modeling likelihood, or any of its alternatives like
    average log probability or calibrated log probability. Our
    implementation tests multiple such scoring functions to identify the
    most effective ones in both eliminating implausible options and
    accurately selecting the final answer.</p>
    <p>The MM-PoE method is designed to be model-agnostic, meaning it
    can be implemented using any existing VLM capable of scoring text
    options, and it is flexible enough to be adapted to different types
    of multiple-choice visual answering questions across various
    domains. The scoring functions were carefully chosen based on their
    theoretical alignment with the two-step elimination and prediction
    philosophy of MM-PoE. We conducted extensive parameter tuning and
    optimization to maximize the performance of both the elimination
    step and the final prediction accuracy.</p>
    <p>This experiment setup was designed to rigorously test the
    effectiveness of MM-PoE across a range of visual reasoning tasks and
    compare its performance against standard baseline methods. The
    results of these experiments are intended to demonstrate the
    potential benefits of integrating a process of elimination approach
    into vision language model reasoning strategies for multiple-choice
    questions.</p>
  </sec>
  <sec id="results">
    <title>Results</title>
    <p>MM-PoE consistently outperformed or matched the best-performing
    baselines across all datasets, showing particular strength in
    logical reasoning. The method’s effectiveness in separating
    elimination and prediction tasks was crucial to its success.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="31%" />
          <col width="13%" />
          <col width="7%" />
          <col width="7%" />
          <col width="16%" />
          <col width="11%" />
          <col width="7%" />
          <col width="7%" />
        </colgroup>
        <thead>
          <tr>
            <th align="center">Model</th>
            <th align="center">Dataset</th>
            <th align="center">LM</th>
            <th align="center">AVG</th>
            <th align="center">Calibration</th>
            <th align="center">Channel</th>
            <th align="center">MCP</th>
            <th align="center">PoE</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center">microsoft/git-base-vqav2</td>
            <td align="center">ScienceQA</td>
            <td align="center">27.4</td>
            <td align="center">17.8</td>
            <td align="center">23.2</td>
            <td align="center">24.6</td>
            <td align="center">25.8</td>
            <td align="center">27.2</td>
          </tr>
          <tr>
            <td align="center">microsoft/git-base-vqav2</td>
            <td align="center">AI2D</td>
            <td align="center">25.4</td>
            <td align="center">26.2</td>
            <td align="center">26.4</td>
            <td align="center">25.4</td>
            <td align="center">25.3</td>
            <td align="center">26.5</td>
          </tr>
          <tr>
            <td align="center">microsoft/git-base-textvqa</td>
            <td align="center">ScienceQA</td>
            <td align="center">21.8</td>
            <td align="center">20.4</td>
            <td align="center">25.8</td>
            <td align="center">23.4</td>
            <td align="center">23.6</td>
            <td align="center">28.2</td>
          </tr>
          <tr>
            <td align="center">microsoft/git-base-textvqa</td>
            <td align="center">AI2D</td>
            <td align="center">26.5</td>
            <td align="center">27.6</td>
            <td align="center">20.8</td>
            <td align="center">26.2</td>
            <td align="center">24.2</td>
            <td align="center">26.8</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p><bold>Table 1</bold>: Comparison of Multiple-Choice Prompting
    (MCP) and Process of Elimination (PoE) accuracy scores on 2 visual
    question answering datasets for the
    <monospace>microsoft/git-base-vqav2</monospace> and
    <monospace>microsoft/git-base-textvqa</monospace> models in the
    zero-shot settings. Each dataset has a different number of answer
    choices. PoE mostly outperforms MCP on all the visual reasoning
    tasks for the two multi-modal models mentioned.</p>
  </sec>
  <sec id="examples">
    <title>Examples</title>
    <sec id="scienceqa-example">
      <title>ScienceQA Example</title>
      <p></p>
      <p><bold>Question</bold>: Which of these states is farthest north?
      <bold>Options</bold>: West Virginia, Louisiana, Arizona, Oklahoma
      <bold>Ground Truth Option</bold>: West Virginia</p>
      <p><bold>Predicted Masks</bold>: West Virginia, Louisiana, [MASK],
      [MASK] <bold>Predicted Option</bold>: West Virginia</p>
    </sec>
    <sec id="ai2d-example">
      <title>AI2D Example</title>
      <p></p>
      <p><bold>Question</bold>: Are phytoplankton predators or prey in
      this food chain? <bold>Options</bold>: producer, predator, prey,
      NA <bold>Ground Truth Option</bold>: prey</p>
      <p><bold>Predicted Masks</bold>: [MASK], predator, prey, NA
      <bold>Predicted Option</bold>: prey</p>
    </sec>
  </sec>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>MM-PoE demonstrates a significant improvement in handling multiple
  choice visual reasoning tasks by mimicking a human-like process of
  elimination approach. Future work will focus on enhancing its
  generalizability and efficiency, possibly extending to handle better
  masking strategies.</p>
</sec>
<sec id="ethics-statement">
  <title>Ethics Statement</title>
  <p>While this method uses publicly available data and models, users
  should be aware of potential biases in the data and model outputs.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to extend our sincere gratitude to Northwestern
  University for providing access to their servers and GPU resources,
  which were instrumental in conducting this research. The computational
  power and infrastructure made available by the university enabled the
  efficient processing and analysis of large datasets, significantly
  contributing to the success of the project. Without this support, the
  research would not have been possible at the scale or speed required.
  We deeply appreciate the university’s commitment to fostering a
  collaborative research environment and supporting technological
  innovation.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-brown2020language">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Tom</given-names></name>
        <name><surname>Mann</surname><given-names>Benjamin</given-names></name>
        <name><surname>Ryder</surname><given-names>Nick</given-names></name>
        <name><surname>Subbiah</surname><given-names>Melanie</given-names></name>
        <name><surname>Kaplan</surname><given-names>Jared D</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
        <name><surname>Neelakantan</surname><given-names>Arvind</given-names></name>
        <name><surname>Shyam</surname><given-names>Pranav</given-names></name>
        <name><surname>Sastry</surname><given-names>Girish</given-names></name>
        <name><surname>Askell</surname><given-names>Amanda</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Language models are few-shot learners</article-title>
      <source>Advances in neural information processing systems</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2005.14165</pub-id>
      <fpage>1877</fpage>
      <lpage>1901</lpage>
    </element-citation>
  </ref>
  <ref id="ref-datta2024consistency">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Datta</surname><given-names>Arghya</given-names></name>
        <name><surname>Chakrabarty</surname><given-names>Sayak</given-names></name>
      </person-group>
      <article-title>On the consistency of maximum likelihood estimation of probabilistic principal component analysis</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2024">2024</year>
      <volume>36</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2311.05046</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ma-du-2023-poe">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ma</surname><given-names>Chenkai</given-names></name>
        <name><surname>Du</surname><given-names>Xinya</given-names></name>
      </person-group>
      <article-title>POE: Process of elimination for multiple choice reasoning</article-title>
      <source>Proceedings of the 2023 conference on empirical methods in natural language processing</source>
      <person-group person-group-type="editor">
        <name><surname>Bouamor</surname><given-names>Houda</given-names></name>
        <name><surname>Pino</surname><given-names>Juan</given-names></name>
        <name><surname>Bali</surname><given-names>Kalika</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Singapore</publisher-loc>
      <year iso-8601-date="2023-12">2023</year><month>12</month>
      <uri>https://aclanthology.org/2023.emnlp-main.273/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2023.emnlp-main.273</pub-id>
      <fpage>4487</fpage>
      <lpage>4496</lpage>
    </element-citation>
  </ref>
  <ref id="ref-holtzman2021surface">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Holtzman</surname><given-names>Ari</given-names></name>
        <name><surname>West</surname><given-names>Peter</given-names></name>
        <name><surname>Shwartz</surname><given-names>Vered</given-names></name>
        <name><surname>Choi</surname><given-names>Yejin</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
      </person-group>
      <article-title>Surface form competition: Why the highest probability answer isn’t always right</article-title>
      <source>Proceedings of the 2021 conference on empirical methods in natural language processing</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.18653/v1/2021.emnlp-main.564</pub-id>
      <fpage>7038</fpage>
      <lpage>7051</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhang2023dynamically">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Youzhi</given-names></name>
        <name><surname>Chakrabarty</surname><given-names>Sayak</given-names></name>
        <name><surname>Liu</surname><given-names>Rui</given-names></name>
        <name><surname>Pugliese</surname><given-names>Andrea</given-names></name>
        <name><surname>Subrahmanian</surname><given-names>VS</given-names></name>
      </person-group>
      <article-title>SockDef: A dynamically adaptive defense to a novel attack on review fraud detection engines</article-title>
      <source>IEEE Transactions on Computational Social Systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/TCSS.2023.3321345</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-VQA">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Antol</surname><given-names>Stanislaw</given-names></name>
        <name><surname>Agrawal</surname><given-names>Aishwarya</given-names></name>
        <name><surname>Lu</surname><given-names>Jiasen</given-names></name>
        <name><surname>Mitchell</surname><given-names>Margaret</given-names></name>
        <name><surname>Batra</surname><given-names>Dhruv</given-names></name>
        <name><surname>Zitnick</surname><given-names>C. Lawrence</given-names></name>
        <name><surname>Parikh</surname><given-names>Devi</given-names></name>
      </person-group>
      <article-title>VQA: Visual question answering</article-title>
      <source>International conference on computer vision (ICCV)</source>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.1109/ICCV.2015.279</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kembhavi2016ADI">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kembhavi</surname><given-names>Aniruddha</given-names></name>
        <name><surname>Salvato</surname><given-names>Michael</given-names></name>
        <name><surname>Kolve</surname><given-names>Eric</given-names></name>
        <name><surname>Seo</surname><given-names>Minjoon</given-names></name>
        <name><surname>Hajishirzi</surname><given-names>Hannaneh</given-names></name>
        <name><surname>Farhadi</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>A diagram is worth a dozen images</article-title>
      <source>Computer vision–ECCV 2016: 14th european conference, amsterdam, the netherlands, october 11–14, 2016, proceedings, part IV 14</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <uri>https://api.semanticscholar.org/CorpusID:2682274</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-319-46493-0_15</pub-id>
      <fpage>235</fpage>
      <lpage>251</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lu2022learn">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Pan</given-names></name>
        <name><surname>Mishra</surname><given-names>Swaroop</given-names></name>
        <name><surname>Xia</surname><given-names>Tony</given-names></name>
        <name><surname>Qiu</surname><given-names>Liang</given-names></name>
        <name><surname>Chang</surname><given-names>Kai-Wei</given-names></name>
        <name><surname>Zhu</surname><given-names>Song-Chun</given-names></name>
        <name><surname>Tafjord</surname><given-names>Oyvind</given-names></name>
        <name><surname>Clark</surname><given-names>Peter</given-names></name>
        <name><surname>Kalyan</surname><given-names>Ashwin</given-names></name>
      </person-group>
      <article-title>Learn to explain: Multimodal reasoning via thought chains for science question answering</article-title>
      <source>The 36th conference on neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2209.09513</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
