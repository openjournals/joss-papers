<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5291</article-id>
<article-id pub-id-type="doi">10.21105/joss.05291</article-id>
<title-group>
<article-title><monospace>Melissa</monospace>: coordinating large-scale
ensemble runs for deep learning and sensitivity analyses</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3708-4135</contrib-id>
<name>
<surname>Schouler</surname>
<given-names>Marc</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5618-8629</contrib-id>
<name>
<surname>Caulk</surname>
<given-names>Robert Alexander</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5386-5997</contrib-id>
<name>
<surname>Meyer</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Terraz</surname>
<given-names>Théophile</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Conrads</surname>
<given-names>Christoph</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Friedemann</surname>
<given-names>Sebastian</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3216-4769</contrib-id>
<name>
<surname>Agarwal</surname>
<given-names>Achal</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Baldonado</surname>
<given-names>Juan Manuel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pogodziński</surname>
<given-names>Bartłomiej</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3524-3160</contrib-id>
<name>
<surname>Sekuła</surname>
<given-names>Anna</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ribes</surname>
<given-names>Alejandro</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Raffin</surname>
<given-names>Bruno</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG,
France</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Industrial AI Laboratory SINCLAIR, EDF Lab Paris-Saclay,
France</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Institute of Bioorganic Chemistry Polish Academy of
Sciences, Poznań Supercomputing and Networking Center</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-02-08">
<day>8</day>
<month>2</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>5291</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>supercomputing</kwd>
<kwd>sensitivity analysis</kwd>
<kwd>deep learning</kwd>
<kwd>distributed systems</kwd>
<kwd>orchestration</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Large-scale ensemble runs typically consist of executing thousands
  of physical simulation instances according to a range of different
  input parameters. These ensemble runs enable sensitivity analyses,
  deep surrogate trainings, reinforcement learning, and data
  assimilation, but they rely on volumes of data that are too large to
  store. For example, a recent data assimilation ensemble study
  generated 1.3 PB of data
  (<xref alt="Yashiro et al., 2020" rid="ref-yashiro2020" ref-type="bibr">Yashiro
  et al., 2020</xref>). These enormous volumes of data hinder scientific
  analyses in two ways. First, I/O is the slowest component in
  supercomputers; the incongruence between slow read/write speeds
  compared to the rapid generation of data leads to a degradation and
  plateau of performance. Second, file systems on supercomputers are not
  designed to allocate such large volumes of data to singular studies.
  To avoid this I/O limitation, scientists reduce their study size by
  running low resolution simulations or downsampling output data in
  space and time. However, the I/O problem only becomes more pronounced
  as the speed and size of supercomputers continues to advance faster
  than I/O speeds of storage disks.</p>
</sec>
<sec id="summary">
  <title>Summary</title>
  <p><ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/">Melissa</ext-link>
  is a file avoiding, fault tolerant, and elastic framework, generalized
  to perform ensemble runs such as <italic>large scale sensitivity
  analysis</italic> and <italic>large scale deep surrogate
  training</italic> on supercomputers. Some of the largest Melissa
  studies so far employed up to 30k cores to execute 80k parallel
  simulations while avoiding up to 288 TB of intermediate data storage
  (see
  (<xref alt="Ribés et al., 2022" rid="ref-insitu-book-chapter" ref-type="bibr">Ribés
  et al., 2022</xref>)). These large-scale studies avoid intermediate
  file storage due to Melissa’s “online” (also referred to as in-transit
  and on-the-fly) data handling approach. As shown in Fig. 1, Melissa’s
  architecture relies on three interacting components, the launcher, the
  server, and the client:</p>
  <list list-type="order">
    <list-item>
      <p>Melissa client: the parallel numerical simulation code turned
      into a client. Each client sends its output to the server as soon
      as available. Clients are independent jobs.</p>
    </list-item>
    <list-item>
      <p>Melissa server: a parallelized process in charge of processing
      the data upon arrival from the distributed and parallelized
      clients (e.g., computing statistics or training a neural
      network).</p>
    </list-item>
    <list-item>
      <p>Melissa Launcher: the front-end Python script in charge of
      orchestrating the execution of the study. This piece of code
      interacts directly with <monospace>OpenMPI</monospace> or with the
      cluster scheduler (e.g., <monospace>slurm</monospace> or
      <monospace>OAR</monospace>) to submit and monitor the proper
      execution of all instances.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Melissa architecture. Specificities of sensitivity and
    deep learning applications appear side by side.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/melissa-architecture.png" />
  </fig>
  <p>The Melissa server component is designed to be specialized for
  various types of ensemble runs:</p>
  <sec id="sensitivity-analysis-melissa-sa">
    <title>Sensitivity Analysis (melissa-sa)</title>
    <p>Melissa’s sensitivity analysis server is built around two key
    concepts: iterative (sometimes also called incremental) statistics
    algorithms and asynchronous client/server model for data transfer.
    Simulation outputs are never stored on disk. Instead, they are sent
    via NxM communication patterns from the simulations to a
    parallelized server (Fig. 1). This method of data aggregation
    enables the calculation of rapid statistical fields in an iterative
    fashion, without storing any data to disk. Avoiding disk storage
    opens up the ability to compute oblivious statistical maps for all
    mesh elements, for every time step and on a full resolution study.
    Melissa comes with iterative algorithms for computing various
    statistical quantities (e.g., mean, variance, skewness, kurtosis,
    and Sobol indices) and can easily be extended with new
    algorithms.</p>
  </sec>
  <sec id="deep-surrogate-training-melissa-dl">
    <title>Deep Surrogate Training (melissa-dl)</title>
    <p>Melissa’s deep learning server adopts a similar philosophy.
    Clients communicate data in a round-robin fashion to the
    parallelized server (Fig. 1). The multi-threaded server then puts
    and pulls data samples in and out of a buffer (Fig. 2), which is
    used for building training batches. Melissa can perform data
    distributed parallelism training on several GPUs, associating a
    buffer to each of them. To ensure proper memory management during
    execution, samples are selected and evicted according to a
    predefined policy. This strategy enables the online training method
    shown in Fig. 2. Furthermore, the Melissa architecture is designed
    to accommodate popular deep learning libraries such as
    <ext-link ext-link-type="uri" xlink:href="https://pytorch.org/">PyTorch</ext-link>
    and
    <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">Tensorflow</ext-link>.</p>
    <fig>
      <caption><p>Overview of Melissa’s deep learning framework
      (<xref alt="Meyer et al., 2023" rid="ref-meyer2023" ref-type="bibr">Meyer
      et al., 2023</xref>)</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/assets/melissa-dl.png" />
    </fig>
  </sec>
</sec>
<sec id="use-in-academia">
  <title>Use in academia</title>
  <p>The original Sensitivity Analysis framework was published by Terraz
  et al.
  (<xref alt="2017" rid="ref-terraz2017" ref-type="bibr">2017</xref>),
  which set a foundation for a variety of subsequent studies. Ribés et
  al. (<xref alt="2019" rid="ref-ribes2019" ref-type="bibr">2019</xref>)
  used melissa for parameter augmented ensembles of curves, Ribés &amp;
  Raffin
  (<xref alt="2020" rid="ref-ribes2020" ref-type="bibr">2020</xref>)
  used Melissa to help demonstrate challenges of in-situ analyses, and
  Friedemann &amp; Raffin
  (<xref alt="2022" rid="ref-friedemann2022" ref-type="bibr">2022</xref>)
  used the architecture of Melissa to build data assimilation software.
  Tangential work includes Guilloteau et al.
  (<xref alt="2022" rid="ref-guilloteau2022" ref-type="bibr">2022</xref>),
  which coupled Melissa with NixOS to demonstrate distributed
  environments. More recently, Melissa’s Deep Surrogate training was
  used to demonstrate improved training compared to offline analogs
  (<xref alt="Meyer et al., 2023" rid="ref-meyer2023" ref-type="bibr">Meyer
  et al., 2023</xref>).</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Melissa is unique in many ways, but there are a group of other
  open-source codes aiming to help scientists manage large scale
  analyses on supercomputers. For example, Merlin
  (<xref alt="Merlin, 2022" rid="ref-merlin2022" ref-type="bibr"><italic>Merlin</italic>,
  2022</xref>) and Radical Pilot
  (<xref alt="Merzky et al., 2021" rid="ref-rpilot2021" ref-type="bibr">Merzky
  et al., 2021</xref>) are supercomputing tools designed to help reduce
  friction in large scale ensemble runs dependent on file system I/O.
  Meanwhile, a group of frameworks exist that are aimed at distributing
  Python processes across clusters including Ray
  (<xref alt="Moritz et al., 2017" rid="ref-ray2018" ref-type="bibr">Moritz
  et al., 2017</xref>) and Dask
  (<xref alt="Dask Development Team, 2016" rid="ref-Dask" ref-type="bibr">Dask
  Development Team, 2016</xref>), but they do not support MPI-based
  applications and are not file avoiding. Finally, a group of in-situ
  processing tools exist that do not support ensemble runs including
  DataSpace
  (<xref alt="Docan et al., 2010" rid="ref-DataSpace" ref-type="bibr">Docan
  et al., 2010</xref>), Decaf
  (<xref alt="Yildiz et al., 2022" rid="ref-Decaf" ref-type="bibr">Yildiz
  et al., 2022</xref>), and Damaris
  (<xref alt="Dorier et al., 2012" rid="ref-Damaris" ref-type="bibr">Dorier
  et al., 2012</xref>). Although all these software packages are useful
  for particular applications, they do not fulfill all three main tasks
  Melissa was built for: large scale data generation, scheduler
  handling, and file-avoiding data processing.</p>
</sec>
<sec id="using-melissa">
  <title>Using Melissa</title>
  <sec id="installing-melissa">
    <title>Installing Melissa</title>
    <p>Melissa includes
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/">online
    documentation</ext-link> geared for new and advanced users alike.
    For example,
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/install/">installation
    instructions</ext-link> help users get started no matter which
    supercomputer they are working on. The typical installation is done
    via a <monospace>cmake</monospace> command. However, a
    <monospace>spack</monospace> install is also available.</p>
  </sec>
  <sec id="configuring-melissa">
    <title>Configuring Melissa</title>
    <p>As
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/new-use-case/">highlighted
    in the documentation</ext-link>, running a Melissa analysis requires
    the user to:</p>
    <list list-type="order">
      <list-item>
        <p>Instrument the simulation code with the Melissa API (3 base
        calls: init, send, and finalize) so it can become a Melissa
        client.</p>
      </list-item>
    </list>
    <list list-type="bullet">
      <list-item>
        <p>Typically the calls to the
        <monospace>melissa_send()</monospace> are performed inside the
        simulation loop. For example, each time step of a physical
        simulation may contain <monospace>melissa_send()</monospace>
        where it sends the physical quantities associated with domain at
        that time-step. This data will be the data that Melissa
        <monospace>server</monospace> collects and analyzes in an online
        fashion (iterative statistics or online training).</p>
      </list-item>
      <list-item>
        <p>As of now, Melissa provides an API compatible with solvers
        developed in the most popular HPC languages: C, Fortran, and
        Python.</p>
      </list-item>
    </list>
    <list list-type="order">
      <list-item>
        <label>2.</label>
        <p>Configure the analysis. This includes defining the design of
        experiment (i.e., how to draw the parameters for each simulation
        execution), selecting which statistics to compute, or specifying
        the Neural Network architecture, the training algorithm, and
        parameters in case of deep-surrogate training.</p>
      </list-item>
    </list>
    <list list-type="bullet">
      <list-item>
        <p>The Melissa interface comprises two components: the
        configuration file (<monospace>config.json</monospace>) and the
        custom user class (<monospace>custom_server.py</monospace>). The
        configuration file is a
        <ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/melissa/melissa/-/blob/JOSS_v2/examples/heat-pde/heat-pde-dl/config_mpi.json">json
        dictionary</ext-link> that contains all the study controls
        (e.g., number of clients to launch, which statistics to compute,
        batch_size, etc.) <monospace>config.json</monospace> also
        contains instructions on how to execute the instrumented solver
        as well as all the custom launcher controls for the user’s
        specific scheduler. Meanwhile, the
        <ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/melissa/melissa/-/blob/JOSS_v2/examples/heat-pde/heat-pde-dl/heatpde_dl_server.py"><monospace>custom_server.py</monospace></ext-link>
        is where a user customizes the machinery inside Melissa. For
        example, the <monospace>custom_server.py</monospace> may include
        specific deep-learning training loops/network architectures,
        custom iterative statistics, pre- and post-processing steps for
        the data, intermediate logging, etc.</p>
      </list-item>
    </list>
    <list list-type="order">
      <list-item>
        <label>3.</label>
        <p>Start the Melissa launcher on the terminal or on the
        front-end of the supercomputer. Melissa takes care of requesting
        resources to execute the server and runner, monitoring the
        execution, and restarting failing components when necessary.</p>
      </list-item>
    </list>
  </sec>
  <sec id="running-melissa">
    <title>Running Melissa</title>
    <p>After the user has instrumented their simulation code and
    configured their custom server, the study is launched with a
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/first-dl-study/">single
    command</ext-link>:</p>
    <code language="bash">melissa-launcher -c config.json</code>
  </sec>
  <sec id="monitoring-melissa">
    <title>Monitoring Melissa</title>
    <p>Melissa also contains a variety of monitoring/logging features to
    help users track live studies and post-processes completed studies.
    One feature is called the
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/monitoring-melissa/#using-the-melissa-monitor-command"><monospace>melissa monitor</monospace></ext-link>,
    which is designed to run in terminals directly on supercomputers.
    This feature displays the number of waiting, running, terminated,
    and failed jobs. Meanwhile, for deep-learning studies, Melissa has
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/monitoring-melissa/#tensorboard-logging-for-deep-learning">tensorboard
    integration</ext-link>, which allows users to track the training
    loss and other custom metrics in real-time.</p>
  </sec>
  <sec id="melissa-test-suite-and-ci">
    <title>Melissa test suite and CI</title>
    <p>The Melissa source code contains a
    <ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/melissa/melissa/-/pipelines">robust
    CI</ext-link>, which builds the source, builds/publishes the
    documentation, runs unit tests, and runs full integration tests.
    This CI serves to maintain code quality while advancing developments
    in an open-source fashion between a group of developers.</p>
  </sec>
  <sec id="examples-and-exhibits">
    <title>Examples and exhibits</title>
    <p>Melissa was already successfully coupled with state-of-the-art
    PDE solvers (e.g.,
    <ext-link ext-link-type="uri" xlink:href="https://www.code-saturne.org/cms/web/">Code-Saturne</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://fenicsproject.org/">FEniCS</ext-link>)
    and the source code provides ready to use examples of the
    <ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/melissa/melissa/-/tree/JOSS_v2/examples/heat-pde">heat
    equation</ext-link> and the
    <ext-link ext-link-type="uri" xlink:href="https://gitlab.inria.fr/melissa/melissa/-/tree/JOSS_v2/examples/lorenz">Lorenz
    system</ext-link>. These examples include training deep-learning
    surrogates using distributed GPUs, and iterative statistics.
    Further, Melissa includes a fully reproducible
    <ext-link ext-link-type="uri" xlink:href="https://melissa.gitlabpages.inria.fr/melissa/online-vs-offline-demo/">online
    vs offline</ext-link> deep learning comparison. Finally, if users
    seek active support, they are encouraged to join our
    <ext-link ext-link-type="uri" xlink:href="https://melissa.discourse.group/">Discourse
    forum</ext-link> and ask questions to the development team.</p>
  </sec>
</sec>
<sec id="acknowledgment">
  <title>Acknowledgment</title>
  <p>The development of Melissa was made possible thanks to several
  funding sources that include:</p>
  <list list-type="bullet">
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://www.eocoe.eu/">EOCOE
      II</ext-link>: This project has received funding from the European
      Union’s Horizon 2020 research and innovation programme under grant
      agreement No 824158</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://regale-project.eu/">REGALE</ext-link>:
      This project has received funding from the European
      High-Performance Computing Joint Undertaking (JU) under grant
      agreement No 956560</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-merlin2022">
    <element-citation publication-type="webpage">
      <article-title>Merlin</article-title>
      <year iso-8601-date="2022">2022</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-12-10">2022</year><month>12</month><day>10</day></date-in-citation>
      <uri>https://merlin.readthedocs.io/en/latest/index.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-yashiro2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yashiro</surname><given-names>Hisashi</given-names></name>
        <name><surname>Terasaki</surname><given-names>Koji</given-names></name>
        <name><surname>Kawai</surname><given-names>Yuta</given-names></name>
        <name><surname>Kudo</surname><given-names>Shuhei</given-names></name>
        <name><surname>Miyoshi</surname><given-names>Takemasa</given-names></name>
        <name><surname>Imamura</surname><given-names>Toshiyuki</given-names></name>
        <name><surname>Minami</surname><given-names>Kazuo</given-names></name>
        <name><surname>Inoue</surname><given-names>Hikaru</given-names></name>
        <name><surname>Nishiki</surname><given-names>Tatsuo</given-names></name>
        <name><surname>Saji</surname><given-names>Takayuki</given-names></name>
        <name><surname>Satoh</surname><given-names>Masaki</given-names></name>
        <name><surname>Tomita</surname><given-names>Hirofumi</given-names></name>
      </person-group>
      <article-title>A 1024-member ensemble data assimilation with 3.5-km mesh global weather simulations</article-title>
      <source>SC20: International conference for high performance computing, networking, storage and analysis</source>
      <year iso-8601-date="2020">2020</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/SC41405.2020.00005</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-insitu-book-chapter">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Ribés</surname><given-names>Alejandro</given-names></name>
        <name><surname>Terraz</surname><given-names>Théophile</given-names></name>
        <name><surname>Fournier</surname><given-names>Yvan</given-names></name>
        <name><surname>Iooss</surname><given-names>Bertrand</given-names></name>
        <name><surname>Raffin</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>Unlocking large scale uncertainty quantification with in transit iterative statistics</article-title>
      <source>In situ visualization for computational science</source>
      <person-group person-group-type="editor">
        <name><surname>Childs</surname><given-names>Hank</given-names></name>
        <name><surname>Bennett</surname><given-names>Janine C.</given-names></name>
        <name><surname>Garth</surname><given-names>Christoph</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>978-3-030-81627-8</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-030-81627-8_6</pub-id>
      <fpage>113</fpage>
      <lpage>136</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Damaris">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dorier</surname><given-names>Matthieu</given-names></name>
        <name><surname>Antoniu</surname><given-names>Gabriel</given-names></name>
        <name><surname>Cappello</surname><given-names>Franck</given-names></name>
        <name><surname>Snir</surname><given-names>Marc</given-names></name>
        <name><surname>Orf</surname><given-names>Leigh</given-names></name>
      </person-group>
      <article-title>Damaris: How to Efficiently Leverage Multicore Parallelism to Achieve Scalable, Jitter-free I/O</article-title>
      <source>CLUSTER 2012 - IEEE International Conference on Cluster Computing</source>
      <publisher-name>IEEE</publisher-name>
      <publisher-loc>Beijing, China</publisher-loc>
      <year iso-8601-date="2012-09">2012</year><month>09</month>
      <uri>https://hal.inria.fr/hal-00715252</uri>
      <pub-id pub-id-type="doi">10.1109/CLUSTER.2012.26</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-DataSpace">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Docan</surname><given-names>Ciprian</given-names></name>
        <name><surname>Parashar</surname><given-names>Manish</given-names></name>
        <name><surname>Klasky</surname><given-names>Scott</given-names></name>
      </person-group>
      <article-title>Dataspaces: An interaction and coordination framework for coupled simulation workflows</article-title>
      <source>Proceedings of the 19th ACM international symposium on high performance distributed computing</source>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.1145/1851476.1851481</pub-id>
      <fpage>25</fpage>
      <lpage>36</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Decaf">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yildiz</surname><given-names>Orcun</given-names></name>
        <name><surname>Dreher</surname><given-names>Matthieu</given-names></name>
        <name><surname>Peterka</surname><given-names>Tom</given-names></name>
      </person-group>
      <article-title>Decaf: Decoupled dataflows for in situ workflows</article-title>
      <source>In situ visualization for computational science</source>
      <person-group person-group-type="editor">
        <name><surname>Childs</surname><given-names>Hank</given-names></name>
        <name><surname>Bennett</surname><given-names>Janine C.</given-names></name>
        <name><surname>Garth</surname><given-names>Christoph</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>978-3-030-81627-8</isbn>
      <fpage>137</fpage>
      <lpage>158</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ribes2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ribés</surname><given-names>Alejandro</given-names></name>
        <name><surname>Raffin</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>The challenges of in situ analysis for multiple simulations</article-title>
      <source>ISAV 2020 - In Situ Infrastructures for Enabling Extreme-Scale Analysis and Visualization</source>
      <publisher-loc>Atlanta, United States</publisher-loc>
      <year iso-8601-date="2020-11">2020</year><month>11</month>
      <uri>https://hal.inria.fr/hal-02968789</uri>
      <pub-id pub-id-type="doi">10.1145/3426462.3426468</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Dask">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>Dask Development Team</string-name>
      </person-group>
      <source>Dask: Library for dynamic task scheduling</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://dask.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-terraz2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Terraz</surname><given-names>Théophile</given-names></name>
        <name><surname>Ribes</surname><given-names>Alejandro</given-names></name>
        <name><surname>Fournier</surname><given-names>Yvan</given-names></name>
        <name><surname>Iooss</surname><given-names>Bertrand</given-names></name>
        <name><surname>Raffin</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>Melissa: Large scale in transit sensitivity analysis avoiding intermediate files</article-title>
      <source>The International Conference for High Performance Computing, Networking, Storage and Analysis (SC17)</source>
      <publisher-loc>Denver, United States</publisher-loc>
      <year iso-8601-date="2017-11">2017</year><month>11</month>
      <uri>https://hal.inria.fr/hal-01607479</uri>
      <fpage>1 </fpage>
      <lpage> 14</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ray2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
        <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
        <name><surname>Wang</surname><given-names>Stephanie</given-names></name>
        <name><surname>Tumanov</surname><given-names>Alexey</given-names></name>
        <name><surname>Liaw</surname><given-names>Richard</given-names></name>
        <name><surname>Liang</surname><given-names>Eric</given-names></name>
        <name><surname>Paul</surname><given-names>William</given-names></name>
        <name><surname>Jordan</surname><given-names>Michael I.</given-names></name>
        <name><surname>Stoica</surname><given-names>Ion</given-names></name>
      </person-group>
      <article-title>Ray: A distributed framework for emerging AI applications</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2017">2017</year>
      <volume>abs/1712.05889</volume>
      <uri>http://arxiv.org/abs/1712.05889</uri>
    </element-citation>
  </ref>
  <ref id="ref-friedemann2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friedemann</surname><given-names>Sebastian</given-names></name>
        <name><surname>Raffin</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>An elastic framework for ensemble-based large-scale data assimilation</article-title>
      <source>The international journal of high performance computing applications</source>
      <publisher-name>SAGE Publications Sage UK: London, England</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>36</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1177/10943420221110507</pub-id>
      <fpage>543</fpage>
      <lpage>563</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ribes2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ribés</surname><given-names>Alejandro</given-names></name>
        <name><surname>Pouderoux</surname><given-names>Joachim</given-names></name>
        <name><surname>Iooss</surname><given-names>Bertrand</given-names></name>
      </person-group>
      <article-title>A visual sensitivity analysis for parameter-augmented ensembles of curves</article-title>
      <source>Journal of Verification, Validation and Uncertainty Quantification</source>
      <publisher-name>American Society of Mechanical Engineers Digital Collection</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>4</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1115/1.4046020</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-guilloteau2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Guilloteau</surname><given-names>Quentin</given-names></name>
        <name><surname>Bleuzen</surname><given-names>Jonathan</given-names></name>
        <name><surname>Poquet</surname><given-names>Millian</given-names></name>
        <name><surname>Richard</surname><given-names>Olivier</given-names></name>
      </person-group>
      <article-title>Painless transposition of reproducible distributed environments with NixOS compose</article-title>
      <source>2022 IEEE international conference on cluster computing (CLUSTER)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/CLUSTER51413.2022.00051</pub-id>
      <fpage>1</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-meyer2023">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Meyer</surname><given-names>Lucas</given-names></name>
        <name><surname>Schouler</surname><given-names>Marc</given-names></name>
        <name><surname>Caulk</surname><given-names>Robert A.</given-names></name>
        <name><surname>Ribes</surname><given-names>Alejandro</given-names></name>
        <name><surname>Raffin</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>Training deep surrogate models with large scale online learning</article-title>
      <source>2023 ICML international conference of machine learning</source>
      <publisher-name>ICML</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <fpage>Accepted</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-rpilot2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Merzky</surname><given-names>André</given-names></name>
        <name><surname>Turilli</surname><given-names>Matteo</given-names></name>
        <name><surname>Titov</surname><given-names>Mikhail</given-names></name>
        <name><surname>Al-Saadi</surname><given-names>Aymen</given-names></name>
        <name><surname>Jha</surname><given-names>Shantenu</given-names></name>
      </person-group>
      <article-title>Design and performance characterization of RADICAL-pilot on leadership-class platforms</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2021">2021</year>
      <volume>abs/2103.00091</volume>
      <uri>https://arxiv.org/abs/2103.00091</uri>
      <pub-id pub-id-type="doi">10.1109/TPDS.2021.3105994</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
