<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250128152255-5087210e66a5a1684348d021f6dcd1223bdf021b</doi_batch_id>
    <timestamp>20250128152255</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>105</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>scene_synthesizer: A Python Library for Procedural Scene Generation in Robot Manipulation</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Clemens</given_name>
            <surname>Eppner</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-5398-4037</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Adithyavairavan</given_name>
            <surname>Murali</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Caelan</given_name>
            <surname>Garrett</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-6474-1276</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Rowland</given_name>
            <surname>Oâ€™Flaherty</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Tucker</given_name>
            <surname>Hermans</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0003-2496-2768</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Wei</given_name>
            <surname>Yang</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0003-3975-2472</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Dieter</given_name>
            <surname>Fox</surname>
            <affiliations>
              <institution><institution_name>NVIDIA Research</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0009-4694-9127</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>28</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>7561</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.07561</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.14736778</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/7561</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.07561</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.07561</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.07561.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="hess2021procedural">
            <article_title>A procedural world generation framework for systematic evaluation of continual learning</article_title>
            <author>Hess</author>
            <journal_title>arXiv preprint arXiv:2106.02585</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Hess, T., Mundt, M., Pliushch, I., &amp; Ramesh, V. (2021). A procedural world generation framework for systematic evaluation of continual learning. arXiv Preprint arXiv:2106.02585.</unstructured_citation>
          </citation>
          <citation key="Denninger2023">
            <article_title>BlenderProc2: A procedural pipeline for photorealistic rendering</article_title>
            <author>Denninger</author>
            <journal_title>Journal of Open Source Software</journal_title>
            <issue>82</issue>
            <volume>8</volume>
            <doi>10.21105/joss.04901</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Denninger, M., Winkelbauer, D., Sundermeyer, M., Boerdijk, W., Knauer, M., Strobl, K. H., Humt, M., &amp; Triebel, R. (2023). BlenderProc2: A procedural pipeline for photorealistic rendering. Journal of Open Source Software, 8(82), 4901. https://doi.org/10.21105/joss.04901</unstructured_citation>
          </citation>
          <citation key="Mo_2019_CVPR">
            <article_title>PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding</article_title>
            <author>Mo</author>
            <journal_title>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</journal_title>
            <doi>10.1109/cvpr.2019.00100</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Mo, K., Zhu, S., Chang, A. X., Yi, L., Tripathi, S., Guibas, L. J., &amp; Su, H. (2019). PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr.2019.00100</unstructured_citation>
          </citation>
          <citation key="ehsani2021manipulathor">
            <article_title>ManipulaTHOR: A framework for visual object manipulation</article_title>
            <author>Ehsani</author>
            <journal_title>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</journal_title>
            <doi>10.1109/cvpr46437.2021.00447</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Ehsani, K., Han, W., Herrasti, A., VanderBilt, E., Weihs, L., Kolve, E., Kembhavi, A., &amp; Mottaghi, R. (2021). ManipulaTHOR: A framework for visual object manipulation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr46437.2021.00447</unstructured_citation>
          </citation>
          <citation key="procthor">
            <article_title>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</article_title>
            <author>Deitke</author>
            <journal_title>Advances in neural information processing systems (NeurIPS)</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Deitke, M., VanderBilt, E., Herrasti, A., Weihs, L., Salvador, J., Ehsani, K., Han, W., Kolve, E., Farhadi, A., Kembhavi, A., &amp; Mottaghi, R. (2022). ProcTHOR: Large-Scale Embodied AI Using Procedural Generation. Advances in Neural Information Processing Systems (NeurIPS).</unstructured_citation>
          </citation>
          <citation key="greff2021kubric">
            <article_title>Kubric: A scalable dataset generator</article_title>
            <author>Greff</author>
            <journal_title>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</journal_title>
            <doi>10.1109/cvpr52688.2022.00373</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Greff, K., Belletti, F., Beyer, L., Doersch, C., Du, Y., Duckworth, D., Fleet, D. J., Gnanapragasam, D., Golemo, F., Herrmann, C., Kipf, T., Kundu, A., Lagun, D., Laradji, I., Liu, H.-T. (Derek), Meyer, H., Miao, Y., Nowrouzezahrai, D., Oztireli, C., â€¦ Tagliasacchi, A. (2022). Kubric: A scalable dataset generator. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr52688.2022.00373</unstructured_citation>
          </citation>
          <citation key="infinigen2023infinite">
            <article_title>Infinite photorealistic worlds using procedural generation</article_title>
            <author>Raistrick</author>
            <journal_title>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</journal_title>
            <doi>10.1109/cvpr52729.2023.01215</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Raistrick, A., Lipson, L., Ma, Z., Mei, L., Wang, M., Zuo, Y., Kayan, K., Wen, H., Han, B., Wang, Y., Newell, A., Law, H., Goyal, A., Yang, K., &amp; Deng, J. (2023). Infinite photorealistic worlds using procedural generation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 12630â€“12641. https://doi.org/10.1109/cvpr52729.2023.01215</unstructured_citation>
          </citation>
          <citation key="robocasa2024">
            <article_title>RoboCasa: Large-scale simulation of everyday tasks for generalist robots</article_title>
            <author>Nasiriany</author>
            <journal_title>Robotics: Science and systems</journal_title>
            <cYear>2024</cYear>
            <unstructured_citation>Nasiriany, S., Maddukuri, A., Zhang, L., Parikh, A., Lo, A., Joshi, A., Mandlekar, A., &amp; Zhu, Y. (2024). RoboCasa: Large-scale simulation of everyday tasks for generalist robots. Robotics: Science and Systems.</unstructured_citation>
          </citation>
          <citation key="schult24controlroom3d">
            <article_title>ControlRoom3D: Room generation using semantic proxy rooms</article_title>
            <author>Schult</author>
            <journal_title>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</journal_title>
            <doi>10.1109/cvpr52733.2024.00593</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Schult, J., Tsai, S., HÃ¶llein, L., Wu, B., Wang, J., Ma, C.-Y., Li, K., Wang, X., Wimbauer, F., He, Z., Zhang, P., Leibe, B., Vajda, P., &amp; Hou, J. (2024). ControlRoom3D: Room generation using semantic proxy rooms. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). https://doi.org/10.1109/cvpr52733.2024.00593</unstructured_citation>
          </citation>
          <citation key="hoellein2023text2room">
            <article_title>Text2Room: Extracting textured 3D meshes from 2D text-to-image models</article_title>
            <author>HÃ¶llein</author>
            <journal_title>Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</journal_title>
            <doi>10.1109/iccv51070.2023.00727</doi>
            <cYear>2023</cYear>
            <unstructured_citation>HÃ¶llein, L., Cao, A., Owens, A., Johnson, J., &amp; NieÃŸner, M. (2023). Text2Room: Extracting textured 3D meshes from 2D text-to-image models. Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 7909â€“7920. https://doi.org/10.1109/iccv51070.2023.00727</unstructured_citation>
          </citation>
          <citation key="yuan2024robopointvisionlanguagemodelspatial">
            <article_title>RoboPoint: A vision-language model for spatial affordance prediction for robotics</article_title>
            <author>Yuan</author>
            <cYear>2024</cYear>
            <unstructured_citation>Yuan, W., Duan, J., Blukis, V., Pumacay, W., Krishna, R., Murali, A., Mousavian, A., &amp; Fox, D. (2024). RoboPoint: A vision-language model for spatial affordance prediction for robotics. https://arxiv.org/abs/2406.10721</unstructured_citation>
          </citation>
          <citation key="fishman2022motionpolicynetworks">
            <article_title>Motion policy networks</article_title>
            <author>Fishman</author>
            <cYear>2022</cYear>
            <unstructured_citation>Fishman, A., Murali, A., Eppner, C., Peele, B., Boots, B., &amp; Fox, D. (2022). Motion policy networks. https://arxiv.org/abs/2210.12209</unstructured_citation>
          </citation>
          <citation key="murali2023cabinet">
            <article_title>CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation</article_title>
            <author>Murali</author>
            <journal_title>Proceedings of the IEEE international conference on robotics and automation (ICRA)</journal_title>
            <doi>10.1109/icra48891.2023.10161528</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Murali, A., Mousavian, A., Eppner, C., Fishman, A., &amp; Fox, D. (2023, May). CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation. Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). https://doi.org/10.1109/icra48891.2023.10161528</unstructured_citation>
          </citation>
          <citation key="dalal2023optimus">
            <article_title>Imitating task and motion planning with visuomotor transformers</article_title>
            <author>Dalal</author>
            <journal_title>Conference on Robot Learning</journal_title>
            <cYear>2023</cYear>
            <unstructured_citation>Dalal, M., Mandlekar, A., Garrett, C., Handa, A., Salakhutdinov, R., &amp; Fox, D. (2023). Imitating task and motion planning with visuomotor transformers. Conference on Robot Learning.</unstructured_citation>
          </citation>
          <citation key="scenic2020">
            <article_title>Scenic: A language for scenario specification and data generation</article_title>
            <author>Fremont</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/2010.06580</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Fremont, D. J., Kim, E., Dreossi, T., Ghosh, S., Yue, X., Sangiovanni-Vincentelli, A. L., &amp; Seshia, S. A. (2020). Scenic: A language for scenario specification and data generation. CoRR, abs/2010.06580. https://arxiv.org/abs/2010.06580</unstructured_citation>
          </citation>
          <citation key="yuan2023m2t2multitaskmaskedtransformer">
            <article_title>M2T2: Multi-task masked transformer for object-centric pick and place</article_title>
            <author>Yuan</author>
            <cYear>2023</cYear>
            <unstructured_citation>Yuan, W., Murali, A., Mousavian, A., &amp; Fox, D. (2023). M2T2: Multi-task masked transformer for object-centric pick and place. https://arxiv.org/abs/2311.00926</unstructured_citation>
          </citation>
          <citation key="robotix2019">
            <article_title>The RobotriX: An eXtremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions</article_title>
            <author>Garcia-Garcia</author>
            <doi>10.1109/iros.2018.8594495</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Garcia-Garcia, A., Martinez-Gonzalez, P., Oprea, S., Castro-Vargas, J. A., Orts-Escolano, S., Garcia-Rodriguez, J., &amp; Jover-Alvarez, A. (2019). The RobotriX: An eXtremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions. https://doi.org/10.1109/iros.2018.8594495</unstructured_citation>
          </citation>
          <citation key="trimesh">
            <article_title>Trimesh</article_title>
            <author>Dawson-Haggerty et al.</author>
            <unstructured_citation>Dawson-Haggerty et al. (n.d.). Trimesh (Version 3.2.0). https://trimesh.org/</unstructured_citation>
          </citation>
          <citation key="garrett2024simpler">
            <article_title>Techniques for training machine learning models using robot simulation data</article_title>
            <author>Garrett</author>
            <cYear>2024</cYear>
            <unstructured_citation>Garrett, C. R., Ramos, F. T., Akinola, I., Degirmenci, A., Eppner, C., Fox, D., Hermans, T. R., Mandlekar, A. U., Mousavian, A., Narang, Y. S., Oâ€™Flaherty, R. W., Sundaralingam, B., &amp; Yang, W. (2024). Techniques for training machine learning models using robot simulation data (Patent No. US20240338598A1). https://patentimages.storage.googleapis.com/98/5d/bb/6ac5fcbb5c0745/US20240338598A1.pdf</unstructured_citation>
          </citation>
          <citation key="yang2024physcenephysicallyinteractable3d">
            <article_title>PhyScene: Physically interactable 3D scene synthesis for embodied AI</article_title>
            <author>Yang</author>
            <doi>10.1109/cvpr52733.2024.01539</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Yang, Y., Jia, B., Zhi, P., &amp; Huang, S. (2024). PhyScene: Physically interactable 3D scene synthesis for embodied AI. https://doi.org/10.1109/cvpr52733.2024.01539</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
