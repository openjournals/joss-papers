<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7561</article-id>
<article-id pub-id-type="doi">10.21105/joss.07561</article-id>
<title-group>
<article-title>scene_synthesizer: A Python Library for Procedural Scene
Generation in Robot Manipulation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5398-4037</contrib-id>
<name>
<surname>Eppner</surname>
<given-names>Clemens</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Murali</surname>
<given-names>Adithyavairavan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6474-1276</contrib-id>
<name>
<surname>Garrett</surname>
<given-names>Caelan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>O’Flaherty</surname>
<given-names>Rowland</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2496-2768</contrib-id>
<name>
<surname>Hermans</surname>
<given-names>Tucker</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3975-2472</contrib-id>
<name>
<surname>Yang</surname>
<given-names>Wei</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-4694-9127</contrib-id>
<name>
<surname>Fox</surname>
<given-names>Dieter</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>NVIDIA Research</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-04-30">
<day>30</day>
<month>4</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>105</issue>
<fpage>7561</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Robotics</kwd>
<kwd>Scene generation</kwd>
<kwd>Synthetic data</kwd>
<kwd>Dataset creation</kwd>
<kwd>Neural networks</kwd>
<kwd>Indoor Scene</kwd>
<kwd>Simulation</kwd>
<kwd>Pipeline</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>scene_synthesizer</monospace> is a library for writing
  procedural scene generators in Python with a special focus on robot
  manipulation. The resulting scenes can be exported to various formats,
  enabling physics simulation or rendering data pipelines for training
  robotics or vision models.</p>
  <fig>
    <caption><p>A synthetic kitchen scene. From top to bottom, left to
    right: Shown in the debug viewer, exported as mesh file in MeshLab,
    simulated in pybullet, Mujoco, Isaac Sim, and Isaac
    Lab.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="paper_teaser.png" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Simulation is an ever increasing data source for training deep
  learning models. In robotics, simulations have been successfully used
  to learn behaviors such as navigation, walking, flying or
  manipulation. The value of data generation in simulation mainly
  depends on the diversity and scale of scene layouts. Existing datasets
  (<xref alt="Ehsani et al., 2021" rid="ref-ehsani2021manipulathor" ref-type="bibr">Ehsani
  et al., 2021</xref>;
  <xref alt="Garcia-Garcia et al., 2019" rid="ref-robotix2019" ref-type="bibr">Garcia-Garcia
  et al., 2019</xref>;
  <xref alt="Mo et al., 2019" rid="ref-Mo_2019_CVPR" ref-type="bibr">Mo
  et al., 2019</xref>;
  <xref alt="Nasiriany et al., 2024" rid="ref-robocasa2024" ref-type="bibr">Nasiriany
  et al., 2024</xref>) are limited in that regard, whereas purely
  generative models still lack the ability to create scenes that can be
  used in physics simulator
  (<xref alt="Höllein et al., 2023" rid="ref-hoellein2023text2room" ref-type="bibr">Höllein
  et al., 2023</xref>;
  <xref alt="Schult et al., 2024" rid="ref-schult24controlroom3d" ref-type="bibr">Schult
  et al., 2024</xref>;
  <xref alt="Yang et al., 2024" rid="ref-yang2024physcenephysicallyinteractable3d" ref-type="bibr">Yang
  et al., 2024</xref>). Other procedural pipelines either focus on
  learning visual models
  (<xref alt="Denninger et al., 2023" rid="ref-Denninger2023" ref-type="bibr">Denninger
  et al., 2023</xref>;
  <xref alt="Greff et al., 2022" rid="ref-greff2021kubric" ref-type="bibr">Greff
  et al., 2022</xref>;
  <xref alt="Raistrick et al., 2023" rid="ref-infinigen2023infinite" ref-type="bibr">Raistrick
  et al., 2023</xref>), address specific use-cases such as autonomous
  driving
  (<xref alt="Fremont et al., 2020" rid="ref-scenic2020" ref-type="bibr">Fremont
  et al., 2020</xref>;
  <xref alt="Hess et al., 2021" rid="ref-hess2021procedural" ref-type="bibr">Hess
  et al., 2021</xref>), or make it hard to be extended and customized
  since they are tightly integrated with a particular simulation
  platform
  (<xref alt="Deitke et al., 2022" rid="ref-procthor" ref-type="bibr">Deitke
  et al., 2022</xref>). With <monospace>scene_synthesizer</monospace> we
  present a library that simplifies the process of writing scene
  randomizers in Python, with a particular focus on physics simulations
  for robot manipulation. It is fully simulator-agnostic.</p>
</sec>
<sec id="features-functionality">
  <title>Features &amp; Functionality</title>
  <p><monospace>scene_synthesizer</monospace> leverages
  <monospace>trimesh</monospace>
  (<xref alt="Dawson-Haggerty et al., n.d." rid="ref-trimesh" ref-type="bibr">Dawson-Haggerty
  et al., n.d.</xref>) for its scene representation, enabling access to
  a wide range of existing geometric algorithms. Assets can be either
  loaded from files (supporting all standard mesh formats, including
  those for articulated structures like USD, URDF, and MJCF) or
  instantiated procedurally from a library of 30 predefined objects,
  most of which are kitchen-themed. Asset placement is facilitated by
  defining object-agnostic anchor points or through automated labeling
  of support surfaces and containment volumes. The software allows users
  to specify physical properties such as collision geometry, mass,
  density, center of mass, friction, and joint constraints, including
  parameters like stiffness, damping, limits, maximum efforts, and
  velocities. Scenes can be fully articulated, with six common kitchen
  layouts provided as examples. While many included procedural assets
  and layouts are tailored to the kitchen domain, the software is
  versatile and does not impose constraints on the type of scenes that
  can be generated. The synthesized scenes can be exported to formats
  like USD and URDF, making them compatible with various physics
  simulators. <monospace>scene_synthesizer</monospace> has few
  dependencies and is easily extendable and customizable.</p>
</sec>
<sec id="example-use-cases">
  <title>Example Use Cases</title>
  <p>We have used <monospace>scene_synthesizer</monospace> to train
  neural robot motion planners
  (<xref alt="Fishman et al., 2022" rid="ref-fishman2022motionpolicynetworks" ref-type="bibr">Fishman
  et al., 2022</xref>), neural collision checkers
  (<xref alt="Murali et al., 2023" rid="ref-murali2023cabinet" ref-type="bibr">Murali
  et al., 2023</xref>), pick-and-place policies
  (<xref alt="Yuan et al., 2023" rid="ref-yuan2023m2t2multitaskmaskedtransformer" ref-type="bibr">Yuan
  et al., 2023</xref>), visuomotor policies
  (<xref alt="Dalal et al., 2023" rid="ref-dalal2023optimus" ref-type="bibr">Dalal
  et al., 2023</xref>), to fine-tune Vision-Language Models
  (<xref alt="Yuan et al., 2024" rid="ref-yuan2024robopointvisionlanguagemodelspatial" ref-type="bibr">Yuan
  et al., 2024</xref>), and in planning-based data generation pipelines
  (<xref alt="Garrett et al., 2024" rid="ref-garrett2024simpler" ref-type="bibr">Garrett
  et al., 2024</xref>). In all these examples, the data was generated in
  simulation using procedurally created scenes featuring object-filled
  shelves, tables, microwaves, countertops, cabinets, drawers, etc.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Jan Czarnowski for providing code during his internship
  which found its way into this project. We thank Adam Fishman, Ankur
  Handa, Shangru Li, Fabio Ramos, and Arsalan Mousavian for valuable
  feedback during the development of this project. We thank Melanie
  Miulli for selecting MDL materials.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-hess2021procedural">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hess</surname><given-names>Timm</given-names></name>
        <name><surname>Mundt</surname><given-names>Martin</given-names></name>
        <name><surname>Pliushch</surname><given-names>Iuliia</given-names></name>
        <name><surname>Ramesh</surname><given-names>Visvanathan</given-names></name>
      </person-group>
      <article-title>A procedural world generation framework for systematic evaluation of continual learning</article-title>
      <source>arXiv preprint arXiv:2106.02585</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-Denninger2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Denninger</surname><given-names>Maximilian</given-names></name>
        <name><surname>Winkelbauer</surname><given-names>Dominik</given-names></name>
        <name><surname>Sundermeyer</surname><given-names>Martin</given-names></name>
        <name><surname>Boerdijk</surname><given-names>Wout</given-names></name>
        <name><surname>Knauer</surname><given-names>Markus</given-names></name>
        <name><surname>Strobl</surname><given-names>Klaus H.</given-names></name>
        <name><surname>Humt</surname><given-names>Matthias</given-names></name>
        <name><surname>Triebel</surname><given-names>Rudolph</given-names></name>
      </person-group>
      <article-title>BlenderProc2: A procedural pipeline for photorealistic rendering</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>82</issue>
      <uri>https://doi.org/10.21105/joss.04901</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04901</pub-id>
      <fpage>4901</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Mo_2019_CVPR">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Mo</surname><given-names>Kaichun</given-names></name>
        <name><surname>Zhu</surname><given-names>Shilin</given-names></name>
        <name><surname>Chang</surname><given-names>Angel X.</given-names></name>
        <name><surname>Yi</surname><given-names>Li</given-names></name>
        <name><surname>Tripathi</surname><given-names>Subarna</given-names></name>
        <name><surname>Guibas</surname><given-names>Leonidas J.</given-names></name>
        <name><surname>Su</surname><given-names>Hao</given-names></name>
      </person-group>
      <article-title>PartNet: A large-scale benchmark for fine-grained and hierarchical part-level 3D object understanding</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1109/cvpr.2019.00100</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ehsani2021manipulathor">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ehsani</surname><given-names>Kiana</given-names></name>
        <name><surname>Han</surname><given-names>Winson</given-names></name>
        <name><surname>Herrasti</surname><given-names>Alvaro</given-names></name>
        <name><surname>VanderBilt</surname><given-names>Eli</given-names></name>
        <name><surname>Weihs</surname><given-names>Luca</given-names></name>
        <name><surname>Kolve</surname><given-names>Eric</given-names></name>
        <name><surname>Kembhavi</surname><given-names>Aniruddha</given-names></name>
        <name><surname>Mottaghi</surname><given-names>Roozbeh</given-names></name>
      </person-group>
      <article-title>ManipulaTHOR: A framework for visual object manipulation</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1109/cvpr46437.2021.00447</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-procthor">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Deitke</surname><given-names>Matt</given-names></name>
        <name><surname>VanderBilt</surname><given-names>Eli</given-names></name>
        <name><surname>Herrasti</surname><given-names>Alvaro</given-names></name>
        <name><surname>Weihs</surname><given-names>Luca</given-names></name>
        <name><surname>Salvador</surname><given-names>Jordi</given-names></name>
        <name><surname>Ehsani</surname><given-names>Kiana</given-names></name>
        <name><surname>Han</surname><given-names>Winson</given-names></name>
        <name><surname>Kolve</surname><given-names>Eric</given-names></name>
        <name><surname>Farhadi</surname><given-names>Ali</given-names></name>
        <name><surname>Kembhavi</surname><given-names>Aniruddha</given-names></name>
        <name><surname>Mottaghi</surname><given-names>Roozbeh</given-names></name>
      </person-group>
      <article-title>ProcTHOR: Large-Scale Embodied AI Using Procedural Generation</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-greff2021kubric">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Greff</surname><given-names>Klaus</given-names></name>
        <name><surname>Belletti</surname><given-names>Francois</given-names></name>
        <name><surname>Beyer</surname><given-names>Lucas</given-names></name>
        <name><surname>Doersch</surname><given-names>Carl</given-names></name>
        <name><surname>Du</surname><given-names>Yilun</given-names></name>
        <name><surname>Duckworth</surname><given-names>Daniel</given-names></name>
        <name><surname>Fleet</surname><given-names>David J</given-names></name>
        <name><surname>Gnanapragasam</surname><given-names>Dan</given-names></name>
        <name><surname>Golemo</surname><given-names>Florian</given-names></name>
        <name><surname>Herrmann</surname><given-names>Charles</given-names></name>
        <name><surname>Kipf</surname><given-names>Thomas</given-names></name>
        <name><surname>Kundu</surname><given-names>Abhijit</given-names></name>
        <name><surname>Lagun</surname><given-names>Dmitry</given-names></name>
        <name><surname>Laradji</surname><given-names>Issam</given-names></name>
        <name><surname>Liu</surname><given-names>Hsueh-Ti (Derek)</given-names></name>
        <name><surname>Meyer</surname><given-names>Henning</given-names></name>
        <name><surname>Miao</surname><given-names>Yishu</given-names></name>
        <name><surname>Nowrouzezahrai</surname><given-names>Derek</given-names></name>
        <name><surname>Oztireli</surname><given-names>Cengiz</given-names></name>
        <name><surname>Pot</surname><given-names>Etienne</given-names></name>
        <name><surname>Radwan</surname><given-names>Noha</given-names></name>
        <name><surname>Rebain</surname><given-names>Daniel</given-names></name>
        <name><surname>Sabour</surname><given-names>Sara</given-names></name>
        <name><surname>Sajjadi</surname><given-names>Mehdi S. M.</given-names></name>
        <name><surname>Sela</surname><given-names>Matan</given-names></name>
        <name><surname>Sitzmann</surname><given-names>Vincent</given-names></name>
        <name><surname>Stone</surname><given-names>Austin</given-names></name>
        <name><surname>Sun</surname><given-names>Deqing</given-names></name>
        <name><surname>Vora</surname><given-names>Suhani</given-names></name>
        <name><surname>Wang</surname><given-names>Ziyu</given-names></name>
        <name><surname>Wu</surname><given-names>Tianhao</given-names></name>
        <name><surname>Yi</surname><given-names>Kwang Moo</given-names></name>
        <name><surname>Zhong</surname><given-names>Fangcheng</given-names></name>
        <name><surname>Tagliasacchi</surname><given-names>Andrea</given-names></name>
      </person-group>
      <article-title>Kubric: A scalable dataset generator</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/cvpr52688.2022.00373</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-infinigen2023infinite">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Raistrick</surname><given-names>Alexander</given-names></name>
        <name><surname>Lipson</surname><given-names>Lahav</given-names></name>
        <name><surname>Ma</surname><given-names>Zeyu</given-names></name>
        <name><surname>Mei</surname><given-names>Lingjie</given-names></name>
        <name><surname>Wang</surname><given-names>Mingzhe</given-names></name>
        <name><surname>Zuo</surname><given-names>Yiming</given-names></name>
        <name><surname>Kayan</surname><given-names>Karhan</given-names></name>
        <name><surname>Wen</surname><given-names>Hongyu</given-names></name>
        <name><surname>Han</surname><given-names>Beining</given-names></name>
        <name><surname>Wang</surname><given-names>Yihan</given-names></name>
        <name><surname>Newell</surname><given-names>Alejandro</given-names></name>
        <name><surname>Law</surname><given-names>Hei</given-names></name>
        <name><surname>Goyal</surname><given-names>Ankit</given-names></name>
        <name><surname>Yang</surname><given-names>Kaiyu</given-names></name>
        <name><surname>Deng</surname><given-names>Jia</given-names></name>
      </person-group>
      <article-title>Infinite photorealistic worlds using procedural generation</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/cvpr52729.2023.01215</pub-id>
      <fpage>12630</fpage>
      <lpage>12641</lpage>
    </element-citation>
  </ref>
  <ref id="ref-robocasa2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nasiriany</surname><given-names>Soroush</given-names></name>
        <name><surname>Maddukuri</surname><given-names>Abhiram</given-names></name>
        <name><surname>Zhang</surname><given-names>Lance</given-names></name>
        <name><surname>Parikh</surname><given-names>Adeet</given-names></name>
        <name><surname>Lo</surname><given-names>Aaron</given-names></name>
        <name><surname>Joshi</surname><given-names>Abhishek</given-names></name>
        <name><surname>Mandlekar</surname><given-names>Ajay</given-names></name>
        <name><surname>Zhu</surname><given-names>Yuke</given-names></name>
      </person-group>
      <article-title>RoboCasa: Large-scale simulation of everyday tasks for generalist robots</article-title>
      <source>Robotics: Science and systems</source>
      <year iso-8601-date="2024">2024</year>
    </element-citation>
  </ref>
  <ref id="ref-schult24controlroom3d">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schult</surname><given-names>Jonas</given-names></name>
        <name><surname>Tsai</surname><given-names>Sam</given-names></name>
        <name><surname>Höllein</surname><given-names>Lukas</given-names></name>
        <name><surname>Wu</surname><given-names>Bichen</given-names></name>
        <name><surname>Wang</surname><given-names>Jialiang</given-names></name>
        <name><surname>Ma</surname><given-names>Chih-Yao</given-names></name>
        <name><surname>Li</surname><given-names>Kunpeng</given-names></name>
        <name><surname>Wang</surname><given-names>Xiaofang</given-names></name>
        <name><surname>Wimbauer</surname><given-names>Felix</given-names></name>
        <name><surname>He</surname><given-names>Zijian</given-names></name>
        <name><surname>Zhang</surname><given-names>Peizhao</given-names></name>
        <name><surname>Leibe</surname><given-names>Bastian</given-names></name>
        <name><surname>Vajda</surname><given-names>Peter</given-names></name>
        <name><surname>Hou</surname><given-names>Ji</given-names></name>
      </person-group>
      <article-title>ControlRoom3D: Room generation using semantic proxy rooms</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/cvpr52733.2024.00593</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hoellein2023text2room">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Höllein</surname><given-names>Lukas</given-names></name>
        <name><surname>Cao</surname><given-names>Ang</given-names></name>
        <name><surname>Owens</surname><given-names>Andrew</given-names></name>
        <name><surname>Johnson</surname><given-names>Justin</given-names></name>
        <name><surname>Nießner</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>Text2Room: Extracting textured 3D meshes from 2D text-to-image models</article-title>
      <source>Proceedings of the IEEE/CVF international conference on computer vision (ICCV)</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/iccv51070.2023.00727</pub-id>
      <fpage>7909</fpage>
      <lpage>7920</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yuan2024robopointvisionlanguagemodelspatial">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yuan</surname><given-names>Wentao</given-names></name>
        <name><surname>Duan</surname><given-names>Jiafei</given-names></name>
        <name><surname>Blukis</surname><given-names>Valts</given-names></name>
        <name><surname>Pumacay</surname><given-names>Wilbert</given-names></name>
        <name><surname>Krishna</surname><given-names>Ranjay</given-names></name>
        <name><surname>Murali</surname><given-names>Adithyavairavan</given-names></name>
        <name><surname>Mousavian</surname><given-names>Arsalan</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>RoboPoint: A vision-language model for spatial affordance prediction for robotics</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2406.10721</uri>
    </element-citation>
  </ref>
  <ref id="ref-fishman2022motionpolicynetworks">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Fishman</surname><given-names>Adam</given-names></name>
        <name><surname>Murali</surname><given-names>Adithyavairan</given-names></name>
        <name><surname>Eppner</surname><given-names>Clemens</given-names></name>
        <name><surname>Peele</surname><given-names>Bryan</given-names></name>
        <name><surname>Boots</surname><given-names>Byron</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>Motion policy networks</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2210.12209</uri>
    </element-citation>
  </ref>
  <ref id="ref-murali2023cabinet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Murali</surname><given-names>Adithyavairavan</given-names></name>
        <name><surname>Mousavian</surname><given-names>Arsalan</given-names></name>
        <name><surname>Eppner</surname><given-names>Clemens</given-names></name>
        <name><surname>Fishman</surname><given-names>Adam</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>CabiNet: Scaling neural collision detection for object rearrangement with procedural scene generation</article-title>
      <source>Proceedings of the IEEE international conference on robotics and automation (ICRA)</source>
      <year iso-8601-date="2023-05">2023</year><month>05</month>
      <uri>https://arxiv.org/abs/2304.09302</uri>
      <pub-id pub-id-type="doi">10.1109/icra48891.2023.10161528</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-dalal2023optimus">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dalal</surname><given-names>Murtaza</given-names></name>
        <name><surname>Mandlekar</surname><given-names>Ajay</given-names></name>
        <name><surname>Garrett</surname><given-names>Caelan</given-names></name>
        <name><surname>Handa</surname><given-names>Ankur</given-names></name>
        <name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>Imitating task and motion planning with visuomotor transformers</article-title>
      <source>Conference on Robot Learning</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-scenic2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fremont</surname><given-names>Daniel J.</given-names></name>
        <name><surname>Kim</surname><given-names>Edward</given-names></name>
        <name><surname>Dreossi</surname><given-names>Tommaso</given-names></name>
        <name><surname>Ghosh</surname><given-names>Shromona</given-names></name>
        <name><surname>Yue</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Sangiovanni-Vincentelli</surname><given-names>Alberto L.</given-names></name>
        <name><surname>Seshia</surname><given-names>Sanjit A.</given-names></name>
      </person-group>
      <article-title>Scenic: A language for scenario specification and data generation</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2020">2020</year>
      <volume>abs/2010.06580</volume>
      <uri>https://arxiv.org/abs/2010.06580</uri>
    </element-citation>
  </ref>
  <ref id="ref-yuan2023m2t2multitaskmaskedtransformer">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yuan</surname><given-names>Wentao</given-names></name>
        <name><surname>Murali</surname><given-names>Adithyavairavan</given-names></name>
        <name><surname>Mousavian</surname><given-names>Arsalan</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>M2T2: Multi-task masked transformer for object-centric pick and place</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2311.00926</uri>
    </element-citation>
  </ref>
  <ref id="ref-robotix2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Garcia-Garcia</surname><given-names>Alberto</given-names></name>
        <name><surname>Martinez-Gonzalez</surname><given-names>Pablo</given-names></name>
        <name><surname>Oprea</surname><given-names>Sergiu</given-names></name>
        <name><surname>Castro-Vargas</surname><given-names>John Alejandro</given-names></name>
        <name><surname>Orts-Escolano</surname><given-names>Sergio</given-names></name>
        <name><surname>Garcia-Rodriguez</surname><given-names>Jose</given-names></name>
        <name><surname>Jover-Alvarez</surname><given-names>Alvaro</given-names></name>
      </person-group>
      <article-title>The RobotriX: An eXtremely photorealistic and very-large-scale indoor dataset of sequences with robot trajectories and interactions</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1901.06514</uri>
      <pub-id pub-id-type="doi">10.1109/iros.2018.8594495</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-trimesh">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <string-name>Dawson-Haggerty et al.</string-name>
      </person-group>
      <article-title>Trimesh</article-title>
      <uri>https://trimesh.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-garrett2024simpler">
    <element-citation publication-type="patent">
      <person-group person-group-type="author">
        <name><surname>Garrett</surname><given-names>Caelan Reed</given-names></name>
        <name><surname>Ramos</surname><given-names>Fabio Tozeto</given-names></name>
        <name><surname>Akinola</surname><given-names>Iretiayo</given-names></name>
        <name><surname>Degirmenci</surname><given-names>Alperen</given-names></name>
        <name><surname>Eppner</surname><given-names>Clemens</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
        <name><surname>Hermans</surname><given-names>Tucker Ryer</given-names></name>
        <name><surname>Mandlekar</surname><given-names>Ajay Uday</given-names></name>
        <name><surname>Mousavian</surname><given-names>Arsalan</given-names></name>
        <name><surname>Narang</surname><given-names>Yashraj Shyam</given-names></name>
        <name><surname>O’Flaherty</surname><given-names>Rowland Wilde</given-names></name>
        <name><surname>Sundaralingam</surname><given-names>Balakumar</given-names></name>
        <name><surname>Yang</surname><given-names>Wei</given-names></name>
      </person-group>
      <article-title>Techniques for training machine learning models using robot simulation data</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://patentimages.storage.googleapis.com/98/5d/bb/6ac5fcbb5c0745/US20240338598A1.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-yang2024physcenephysicallyinteractable3d">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Yandan</given-names></name>
        <name><surname>Jia</surname><given-names>Baoxiong</given-names></name>
        <name><surname>Zhi</surname><given-names>Peiyuan</given-names></name>
        <name><surname>Huang</surname><given-names>Siyuan</given-names></name>
      </person-group>
      <article-title>PhyScene: Physically interactable 3D scene synthesis for embodied AI</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2404.09465</uri>
      <pub-id pub-id-type="doi">10.1109/cvpr52733.2024.01539</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
