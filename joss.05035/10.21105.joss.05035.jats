<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5035</article-id>
<article-id pub-id-type="doi">10.21105/joss.05035</article-id>
<title-group>
<article-title>AmpTorch: A Python package for scalable fingerprint-based
neural network training on multi-element systems with integrated
uncertainty quantification</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Shuaibi</surname>
<given-names>Muhammed</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3648-7749</contrib-id>
<name>
<surname>Hu</surname>
<given-names>Yuge</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lei</surname>
<given-names>Xiangyun</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Comer</surname>
<given-names>Benjamin M.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Adams</surname>
<given-names>Matt</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Paras</surname>
<given-names>Jacob</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Chen</surname>
<given-names>Rui Qi</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Musa</surname>
<given-names>Eric</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Musielewicz</surname>
<given-names>Joseph</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2855-9482</contrib-id>
<name>
<surname>Peterson</surname>
<given-names>Andrew A.</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8311-9581</contrib-id>
<name>
<surname>Medford</surname>
<given-names>Andrew J.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9401-4918</contrib-id>
<name>
<surname>Ulissi</surname>
<given-names>Zachary</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Chemical Engineering, Carnegie Mellon
University, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Chemical and Biomolecular Engineering,
Georgia Institute of Technology, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>School of Physics and School of Computer Science, Georgia
Institute of Technology, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Chemical Engineering, University of Michigan,
United States</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>School of Engineering, Brown University, United
States</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-06-07">
<day>7</day>
<month>6</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>87</issue>
<fpage>5035</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>machine learning interatomic potentials</kwd>
<kwd>neural networks</kwd>
<kwd>molecular dynamics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning (ML) force fields use ML models to predict the
  energy (and forces) given a chemical system defined by a set of atomic
  coordinates. Fingerprint-based and graph-based ML force fields are two
  major categories of atomistic machine-learning models.
  Fingerprint-based models convert the atomic coordinates into
  fixed-length “feature vectors” that are used as inputs to regression
  models such as neural networks or kernel ridge regression, while
  graph-based systems convert the coordinates into a molecular graph
  which is directly input into a deep learning model. Fingerprint-based
  approaches tend to require less training data and be more
  interpretable, while graph-based models are typically able to handle
  more complex chemical systems and are more accurate in the limit of
  large datasets. Fingerprint-based models are widely used due to their
  conceptual similarity to traditional force-fields, but most existing
  packages for fingerprint-based neural network force fields are limited
  to systems with relatively few (<inline-formula><alternatives>
  <tex-math><![CDATA[<5]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>)
  elements and relatively small (<inline-formula><alternatives>
  <tex-math><![CDATA[<10^6]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>&lt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>)
  datasets.</p>
  <p>This work introduces <monospace>AmpTorch</monospace>, a Python/C++
  package that leverages the Gaussian Multipole (GMP) fingerprinting
  scheme to build atomistic neural network models
  (<xref alt="Lei &amp; Medford, 2022" rid="ref-LeiU003A2022" ref-type="bibr">Lei
  &amp; Medford, 2022</xref>). It provides an efficient training routine
  scalable to <inline-formula><alternatives>
  <tex-math><![CDATA[\sim10^6]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>∼</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  training points, is compatible with many-element systems
  (<inline-formula><alternatives>
  <tex-math><![CDATA[50+]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>50</mml:mn><mml:mo>+</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  elements), and supports statistically rigorous uncertainty
  quantification (UQ) during the prediction step
  (<xref alt="Hu et al., 2022" rid="ref-HuU003A2022" ref-type="bibr">Hu
  et al., 2022</xref>). The fingerprint-based structure provides
  relatively simple models compared to graph-based alternatives, and the
  structure of the fingerprinting scheme allows the package to handle
  systems with an arbitrary number of elements and arbitrary boundary
  conditions (isolated, periodic, semi-periodic).</p>
  <p><monospace>AmpTorch</monospace> is a PyTorch adaptation of the
  Atomistic Machine-learning Package <monospace>AMP</monospace>
  (<xref alt="Khorshidi &amp; Peterson, 2016" rid="ref-KhorshidiU003A2016" ref-type="bibr">Khorshidi
  &amp; Peterson, 2016</xref>). It maintains the modular structure of
  <monospace>AMP</monospace>, with separate modules for generating
  fingerprints and training neural network models.
  <monospace>AmpTorch</monospace> supports standard symmetry function
  fingerprints and high-dimensional neural network potentials, but is
  able to scale to larger datasets and more complex chemical systems
  than <monospace>AMP</monospace> or other existing codes for
  feature-based ML potentials (for example, Atom-Centered Symmetry
  Functions
  (<xref alt="Behler, 2015" rid="ref-BehlerU003A2015" ref-type="bibr">Behler,
  2015</xref>) whose fingerprinting dimension scales with the number of
  elements). <monospace>AmpTorch</monospace> outsources traditional
  boilerplate trainer code to
  <ext-link ext-link-type="uri" xlink:href="https://skorch.readthedocs.io/en/stable/"><monospace>Skorch</monospace></ext-link>
  (<xref alt="Tietz et al., 2017" rid="ref-TietzU003A2017" ref-type="bibr">Tietz
  et al., 2017</xref>). <monospace>Skorch</monospace> serves as a
  PyTorch wrapper that allows users to easily modify traditional neural
  network training strategies. <monospace>AmpTorch</monospace>’s use of
  <monospace>Skorch</monospace> allows users to trivially modify
  training strategies, model architectures, and hyperparameters.</p>
  <p>The scalability of <monospace>AmpTorch</monospace> is a result of
  its fingerprinting scheme, software design, and database management.
  The GMP fingerprints have a fixed vector length regardless of the
  number of elements, and using Gaussian functions allows all necessary
  integrals to be computed analytically. The analytical solutions are
  coupled with a C++ implementation, ensuring the efficiency of the
  fingerprint calculation. The GMP fingerprints are also naturally
  compatible with the SingleNN
  (<xref alt="Liu &amp; Kitchin, 2020" rid="ref-LiuU003A2020" ref-type="bibr">Liu
  &amp; Kitchin, 2020</xref>) neural network structure, allowing for
  neural networks that have the same architecture and number of fitted
  parameters regardless of the number of elements present in the
  dataset.</p>
  <p>In addition, <monospace>AmpTorch</monospace> leverages the
  B-tree-based database management library,
  <ext-link ext-link-type="uri" xlink:href="http://www.lmdb.tech/doc/"><monospace>LMDB</monospace></ext-link>
  (<xref alt="Chu &amp; Hedenfalk, 2010" rid="ref-ChuU003A2010" ref-type="bibr">Chu
  &amp; Hedenfalk, 2010</xref>), to resolve possible memory issues when
  it comes to loading and training on large datasets. This allows
  <monospace>AmpTorch</monospace> to be trained on datasets that are too
  large to fit into temporary memory, although reading and writing data
  from <monospace>LMDB</monospace> is slower than using temporary
  memory.</p>
  <p><monospace>AmpTorch</monospace> also implements UQ as an optional
  feature during the prediction. Supported UQ methods include the
  ensemble method, dropout neural network method, and methods based on
  latent distances. In particular, <monospace>AmpTorch</monospace>
  includes an implementation of a highly scalable new approach that
  leverages distances in the latent space and the “conformal prediction”
  statistical technique to provide statistically rigorous error bars on
  complex pre-trained models.</p>
  <p><monospace>AmpTorch</monospace> is designed using standards from
  the <monospace>ASE</monospace> package for handling atomic structures
  (<xref alt="Hjorth Larsen et al., 2017" rid="ref-LarsenU003A2017" ref-type="bibr">Hjorth
  Larsen et al., 2017</xref>). It takes a list of
  <monospace>ase.Atoms</monospace> objects with energy (and forces) as
  input and output <monospace>ase.Calculator</monospace> object that can
  be used to compute the energy (and forces) with the trained model.
  This structure allows for easy integration with active learning
  packages such as <monospace>FINETUNA</monospace>
  (<xref alt="Musielewicz et al., 2022" rid="ref-MusielewiczU003A2022" ref-type="bibr">Musielewicz
  et al., 2022</xref>) and data visualization tools such as
  <monospace>ElectroLens</monospace>
  (<xref alt="Lei et al., 2019" rid="ref-LeiU003A2019" ref-type="bibr">Lei
  et al., 2019</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>There are numerous software packages for both constructing and
  applying ML force-fields. These include packages that are based on
  deep learning and graph convolutional models, kernel-based regression
  models, and neural networks with fixed feature vectors. Of these
  packages, only graph convolutional codes have been demonstrated for
  extremely large and complex datasets with
  <inline-formula><alternatives>
  <tex-math><![CDATA[>]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>&gt;</mml:mo></mml:math></alternatives></inline-formula>
  10 elements and <inline-formula><alternatives>
  <tex-math><![CDATA[>]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>&gt;</mml:mo></mml:math></alternatives></inline-formula>
  1M training points such as the OC20 dataset in
  <ext-link ext-link-type="uri" xlink:href="https://pubs.acs.org/doi/10.1021/acscatal.0c04525">Open
  Catalyst Project</ext-link>
  (<xref alt="Chanussot et al., 2021" rid="ref-ChanussotU003A2021" ref-type="bibr">Chanussot
  et al., 2021</xref>). However, at inference time, adapting these often
  complex architectures with 1-100M parameter models into atomistic
  simulation interfaces seeking extremely fast calls is a challenging
  task. On the other hand, all existing packages for training
  feature-based models are limited in both the number of elements they
  can handle (due to the poor scaling of feature vector size) and the
  number of training points (due to scaling of training kernel-based
  models and memory management issues in most codes). Thus,
  <monospace>AMPTorch</monospace> aims to fill this gap by developing a
  toolkit to train fingerprint-based neural network force fields on
  datasets of an arbitrary number of chemical elements. For this reason,
  we expect that the code will be widely used by researchers seeking to
  train ML force-field models for complex systems that can be integrated
  with existing atomistic simulation pipelines.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors are grateful for funding from the U.S. Department of
  Energy’s Basic Energy Science, Computational Chemical Sciences Program
  Office, under Award No. DE-SC0019441.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-LeiU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lei</surname><given-names>Xiangyun</given-names></name>
        <name><surname>Medford</surname><given-names>Andrew J.</given-names></name>
      </person-group>
      <article-title>A Universal Framework for Featurization of Atomistic Systems</article-title>
      <source>Journal of Physical Chemistry Letters</source>
      <year iso-8601-date="2022">2022</year>
      <volume>13</volume>
      <issue>34</issue>
      <uri>https://arxiv.org/abs/2102.02390</uri>
      <pub-id pub-id-type="doi">10.1021/acs.jpclett.2c02100</pub-id>
      <pub-id pub-id-type="pmid">35980312</pub-id>
      <fpage>7911</fpage>
      <lpage>7919</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HuU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hu</surname><given-names>Yuge</given-names></name>
        <name><surname>Musielewicz</surname><given-names>Joseph</given-names></name>
        <name><surname>Ulissi</surname><given-names>Zachary W.</given-names></name>
        <name><surname>Medford</surname><given-names>Andrew J.</given-names></name>
      </person-group>
      <article-title>Robust and scalable uncertainty estimation with conformal prediction for machine-learned interatomic potentials</article-title>
      <source>Machine Learning: Science and Technology</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <volume>3</volume>
      <issue>4</issue>
      <issn>2632-2153</issn>
      <uri>https://iopscience.iop.org/article/10.1088/2632-2153/aca7b1 https://iopscience.iop.org/article/10.1088/2632-2153/aca7b1/meta</uri>
      <pub-id pub-id-type="doi">10.1088/2632-2153/ACA7B1</pub-id>
      <fpage>045028</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-KhorshidiU003A2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Khorshidi</surname><given-names>Alireza</given-names></name>
        <name><surname>Peterson</surname><given-names>Andrew A.</given-names></name>
      </person-group>
      <article-title>Amp: A modular approach to machine learning in atomistic simulations</article-title>
      <source>Computer Physics Communications</source>
      <publisher-name>Elsevier B.V.</publisher-name>
      <year iso-8601-date="2016-10">2016</year><month>10</month>
      <volume>207</volume>
      <pub-id pub-id-type="doi">10.1016/j.cpc.2016.05.010</pub-id>
      <fpage>310</fpage>
      <lpage>324</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LeiU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lei</surname><given-names>Xiangyun</given-names></name>
        <name><surname>Hohman</surname><given-names>Fred</given-names></name>
        <name><surname>Chau</surname><given-names>Duen Horng Polo</given-names></name>
        <name><surname>Medford</surname><given-names>Andrew J.</given-names></name>
      </person-group>
      <article-title>ElectroLens: Understanding Atomistic Simulations Through Spatially-resolved Visualization of High-dimensional Features</article-title>
      <source>2019 IEEE Visualization Conference, VIS 2019</source>
      <publisher-name>Institute of Electrical; Electronics Engineers Inc.</publisher-name>
      <year iso-8601-date="2019-08">2019</year><month>08</month>
      <isbn>9781728149417</isbn>
      <uri>https://arxiv.org/abs/1908.08381v3</uri>
      <pub-id pub-id-type="doi">10.48550/arxiv.1908.08381</pub-id>
      <fpage>196</fpage>
      <lpage>200</lpage>
    </element-citation>
  </ref>
  <ref id="ref-MusielewiczU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Musielewicz</surname><given-names>Joseph</given-names></name>
        <name><surname>Wang</surname><given-names>Xiaoxiao</given-names></name>
        <name><surname>Tian</surname><given-names>Tian</given-names></name>
        <name><surname>Ulissi</surname><given-names>Zachary</given-names></name>
      </person-group>
      <article-title>FINETUNA: fine-tuning accelerated molecular simulations</article-title>
      <source>Machine Learning: Science and Technology</source>
      <year iso-8601-date="2022">2022</year>
      <volume>3</volume>
      <issue>3</issue>
      <uri>https://arxiv.org/abs/2205.01223</uri>
      <pub-id pub-id-type="doi">10.1088/2632-2153/ac8fe0</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-LiuU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Mingjie</given-names></name>
        <name><surname>Kitchin</surname><given-names>John R.</given-names></name>
      </person-group>
      <article-title>SingleNN: Modified Behler-Parrinello Neural Network with Shared Weights for Atomistic Simulations with Transferability</article-title>
      <source>Journal of Physical Chemistry C</source>
      <publisher-name>American Chemical Society</publisher-name>
      <year iso-8601-date="2020-08">2020</year><month>08</month>
      <volume>124</volume>
      <issue>32</issue>
      <pub-id pub-id-type="doi">10.1021/acs.jpcc.0c04225</pub-id>
      <fpage>17811</fpage>
      <lpage>17818</lpage>
    </element-citation>
  </ref>
  <ref id="ref-TietzU003A2017">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tietz</surname><given-names>Marian</given-names></name>
        <name><surname>Fan</surname><given-names>Thomas J.</given-names></name>
        <name><surname>Nouri</surname><given-names>Daniel</given-names></name>
        <name><surname>Bossan</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>skorch: A scikit-learn compatible neural network library that wraps PyTorch</article-title>
      <year iso-8601-date="2017-07">2017</year><month>07</month>
    </element-citation>
  </ref>
  <ref id="ref-ChuU003A2010">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chu</surname><given-names>Howard</given-names></name>
        <name><surname>Hedenfalk</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>Lightning Memory-Mapped Database Manager (LMDB)</article-title>
      <year iso-8601-date="2010">2010</year>
    </element-citation>
  </ref>
  <ref id="ref-BehlerU003A2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Behler</surname><given-names>Jörg</given-names></name>
      </person-group>
      <article-title>Constructing high-dimensional neural network potentials: A tutorial review</article-title>
      <source>International Journal of Quantum Chemistry</source>
      <year iso-8601-date="2015">2015</year>
      <volume>115</volume>
      <issue>16</issue>
      <pub-id pub-id-type="doi">10.1002/qua.24890</pub-id>
      <fpage>1032</fpage>
      <lpage>1050</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LarsenU003A2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hjorth Larsen</surname><given-names>Ask</given-names></name>
        <name><surname>JØrgen Mortensen</surname><given-names>Jens</given-names></name>
        <name><surname>Blomqvist</surname><given-names>Jakob</given-names></name>
        <name><surname>Castelli</surname><given-names>Ivano E.</given-names></name>
        <name><surname>Christensen</surname><given-names>Rune</given-names></name>
        <name><surname>Dułak</surname><given-names>Marcin</given-names></name>
        <name><surname>Friis</surname><given-names>Jesper</given-names></name>
        <name><surname>Groves</surname><given-names>Michael N.</given-names></name>
        <name><surname>Hammer</surname><given-names>BjØrk</given-names></name>
        <name><surname>Hargus</surname><given-names>Cory</given-names></name>
        <name><surname>Hermes</surname><given-names>Eric D.</given-names></name>
        <name><surname>Jennings</surname><given-names>Paul C.</given-names></name>
        <name><surname>Bjerre Jensen</surname><given-names>Peter</given-names></name>
        <name><surname>Kermode</surname><given-names>James</given-names></name>
        <name><surname>Kitchin</surname><given-names>John R.</given-names></name>
        <name><surname>Leonhard Kolsbjerg</surname><given-names>Esben</given-names></name>
        <name><surname>Kubal</surname><given-names>Joseph</given-names></name>
        <name><surname>Kaasbjerg</surname><given-names>Kristen</given-names></name>
        <name><surname>Lysgaard</surname><given-names>Steen</given-names></name>
        <name><surname>Bergmann Maronsson</surname><given-names>Jón</given-names></name>
        <name><surname>Maxson</surname><given-names>Tristan</given-names></name>
        <name><surname>Olsen</surname><given-names>Thomas</given-names></name>
        <name><surname>Pastewka</surname><given-names>Lars</given-names></name>
        <name><surname>Peterson</surname><given-names>Andrew</given-names></name>
        <name><surname>Rostgaard</surname><given-names>Carsten</given-names></name>
        <name><surname>SchiØtz</surname><given-names>Jakob</given-names></name>
        <name><surname>Schütt</surname><given-names>Ole</given-names></name>
        <name><surname>Strange</surname><given-names>Mikkel</given-names></name>
        <name><surname>Thygesen</surname><given-names>Kristian S.</given-names></name>
        <name><surname>Vegge</surname><given-names>Tejs</given-names></name>
        <name><surname>Vilhelmsen</surname><given-names>Lasse</given-names></name>
        <name><surname>Walter</surname><given-names>Michael</given-names></name>
        <name><surname>Zeng</surname><given-names>Zhenhua</given-names></name>
        <name><surname>Jacobsen</surname><given-names>Karsten W.</given-names></name>
      </person-group>
      <article-title>The atomic simulation environment—a Python library for working with atoms</article-title>
      <source>Journal of Physics: Condensed Matter</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2017-06">2017</year><month>06</month>
      <volume>29</volume>
      <issue>27</issue>
      <issn>0953-8984</issn>
      <uri>https://iopscience.iop.org/article/10.1088/1361-648X/aa680e https://iopscience.iop.org/article/10.1088/1361-648X/aa680e/meta</uri>
      <pub-id pub-id-type="doi">10.1088/1361-648X/AA680E</pub-id>
      <pub-id pub-id-type="pmid">28323250</pub-id>
      <fpage>273002</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ChanussotU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chanussot</surname><given-names>Lowik</given-names></name>
        <name><surname>Das</surname><given-names>Abhishek</given-names></name>
        <name><surname>Goyal</surname><given-names>Siddharth</given-names></name>
        <name><surname>Lavril</surname><given-names>Thibaut</given-names></name>
        <name><surname>Shuaibi</surname><given-names>Muhammed</given-names></name>
        <name><surname>Riviere</surname><given-names>Morgane</given-names></name>
        <name><surname>Tran</surname><given-names>Kevin</given-names></name>
        <name><surname>Heras-Domingo</surname><given-names>Javier</given-names></name>
        <name><surname>Ho</surname><given-names>Caleb</given-names></name>
        <name><surname>Hu</surname><given-names>Weihua</given-names></name>
        <name><surname>Palizhati</surname><given-names>Aini</given-names></name>
        <name><surname>Sriram</surname><given-names>Anuroop</given-names></name>
        <name><surname>Wood</surname><given-names>Brandon</given-names></name>
        <name><surname>Yoon</surname><given-names>Junwoong</given-names></name>
        <name><surname>Parikh</surname><given-names>Devi</given-names></name>
        <name><surname>Zitnick</surname><given-names>C. Lawrence</given-names></name>
        <name><surname>Ulissi</surname><given-names>Zachary</given-names></name>
      </person-group>
      <article-title>Open Catalyst 2020 (OC20) Dataset and Community Challenges</article-title>
      <source>ACS Catalysis</source>
      <year iso-8601-date="2021">2021</year>
      <volume>11</volume>
      <issue>10</issue>
      <uri>https://arxiv.org/abs/2010.09990</uri>
      <pub-id pub-id-type="doi">10.1021/acscatal.0c04525</pub-id>
      <fpage>6059</fpage>
      <lpage>6072</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
