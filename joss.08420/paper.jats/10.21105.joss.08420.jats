<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8420</article-id>
<article-id pub-id-type="doi">10.21105/joss.08420</article-id>
<title-group>
<article-title>voice: A Comprehensive R Package for Audio
Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5501-0877</contrib-id>
<name>
<surname>Zabala</surname>
<given-names>Filipe Jaeger</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7537-7289</contrib-id>
<name>
<surname>Salum</surname>
<given-names>Giovanni Abrahão</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Graduate Program of Psychiatry and Behavioral Sciences,
UFRGS, Brazil</institution>
<institution-id institution-id-type="ROR">041yk2d64</institution-id>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Child Mind Institute, New York, NY 10022, USA</institution>
<institution-id institution-id-type="ROR">01bfgxw09</institution-id>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-07-23">
<day>23</day>
<month>7</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>111</issue>
<fpage>8420</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>Python</kwd>
<kwd>voice</kwd>
<kwd>mood</kwd>
<kwd>emotion</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The <monospace>voice</monospace> package
  (<xref alt="Zabala, 2025" rid="ref-zabala2025voice" ref-type="bibr">Zabala,
  2025</xref>) for R
  (<xref alt="R Core Team, 2024" rid="ref-r2024r" ref-type="bibr">R Core
  Team, 2024</xref>) is a free, open-source toolkit designed to
  streamline audio analysis by integrating music theory and advanced
  computational techniques. It enables researchers to extract,
  summarize, and analyze voice data efficiently, supporting applications
  such as speech recognition, speaker identification, and mood
  inference. The package simplifies workflows through three core
  functions: <monospace>extract_features</monospace>,
  <monospace>tag</monospace>, and <monospace>diarize</monospace>. By
  bridging gaps in existing R tools, <monospace>voice</monospace> offers
  a unified solution for audio data analysis.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Tools like <monospace>reticulate</monospace>
  (<xref alt="Ushey et al., 2023" rid="ref-ushey2023reticulate" ref-type="bibr">Ushey
  et al., 2023</xref>) and <monospace>rpy2</monospace>
  (<xref alt="Gautier, 2025" rid="ref-gautier2025" ref-type="bibr">Gautier,
  2025</xref>) enable interoperability between R and Python, allowing
  users to leverage external libraries for audio processing. While R
  provides foundational packages like tuneR and
  <monospace>tuneR</monospace>
  (<xref alt="Ligges et al., 2023" rid="ref-tuner2023ligges" ref-type="bibr">Ligges
  et al., 2023</xref>), <monospace>seewave</monospace>
  (<xref alt="Sueur et al., 2008" rid="ref-sueur2008seewave" ref-type="bibr">Sueur
  et al., 2008</xref>) and <monospace>wrassp</monospace>
  (<xref alt="Winkelmann et al., 2024" rid="ref-wrassp2024winkelmann" ref-type="bibr">Winkelmann
  et al., 2024</xref>), Python’s ecosystem (e.g.,
  <monospace>Librosa</monospace>
  (<xref alt="McFee et al., 2015" rid="ref-mcfee2015librosa" ref-type="bibr">McFee
  et al., 2015</xref>), <monospace>pyannote-audio</monospace>
  (<xref alt="Bredin et al., 2019" rid="ref-bredin2019pyannote" ref-type="bibr">Bredin
  et al., 2019</xref>)) is more extensive, as evidenced by the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/andreimatveyeu/awesome-python-audio">Awesome
  Python Audio and Music</ext-link> collection by
  <ext-link ext-link-type="uri" xlink:href="https://github.com/andreimatveyeu">Andrei
  Matveyeu</ext-link>. However, R’s state-of-the-art time-series
  infrastructure
  (<ext-link ext-link-type="uri" xlink:href="https://cran.r-project.org/web/views/TimeSeries.html">CRAN
  Task View: Time Series Analysis</ext-link>), offers unique advantages
  for analyzing audio signals as temporal data. Our implementation
  combines these strengths, providing an R-centric workflow with
  optional Python integration for specialized tasks.</p>
  <p><monospace>voice</monospace> was designed with a user-friendly
  approach that makes it accessible to researchers in linguistics,
  psychology, and bioacoustics, where audio data remains underutilized.
  There are currently work fronts in these areas making use of
  <monospace>voice</monospace> functionalities. By simplifying the
  extraction and analysis of audio features, the package lowers the
  barrier to entry for researchers and expands the potential for audio
  data in scientific studies.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <sec id="core-functions">
    <title>Core Functions</title>
    <list list-type="order">
      <list-item>
        <p><bold><monospace>extract_features</monospace></bold>:
        Extracts standardized audio features from files (e.g.,
        <italic>F0</italic>, <italic>Formant Dispersion</italic>,
        <italic>Gain</italic>, <italic>MFCC</italic>), leveraging
        <monospace>wrassp</monospace> and <monospace>tuneR</monospace>
        while introducing new metrics to capture vocal tract
        characteristics.</p>
      </list-item>
      <list-item>
        <p><bold><monospace>tag</monospace></bold>:
        Attaches summarized audio features to datasets, supporting
        anonymization and privacy-aware analysis via a <italic>6-number
        summary</italic> (mean, median, standard deviation, coefficient
        of variation, interquartile range and median absolute
        deviation).</p>
      </list-item>
      <list-item>
        <p><bold><monospace>diarize</monospace></bold>:
        Identifies speaker segments using Python’s
        <monospace>pyannote-audio</monospace>
        (<xref alt="Bredin et al., 2019" rid="ref-bredin2019pyannote" ref-type="bibr">Bredin
        et al., 2019</xref>), generating RTTM files for transcription
        and analysis.</p>
      </list-item>
    </list>
  </sec>
  <sec id="novel-contributions">
    <title>Novel Contributions</title>
    <list list-type="bullet">
      <list-item>
        <p><bold>Formant Removals</bold>:
        Isolates fundamental frequency (F0) from formants, improving
        feature interpretability for classification tasks.</p>
      </list-item>
      <list-item>
        <p><bold>Integration of R and Python</bold>:
        Uses <monospace>reticulate</monospace>
        (<xref alt="Ushey et al., 2023" rid="ref-ushey2023reticulate" ref-type="bibr">Ushey
        et al., 2023</xref>) to combine R’s statistical power with
        Python’s tools.</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="example-applications">
  <title>Example Applications</title>
  <sec id="predicting-sex-from-voice">
    <title>Predicting Sex from Voice</title>
    <p>The package was tested on open datasets (AESDD
    (<xref alt="Vryzas, Kotsakis, et al., 2018" rid="ref-vryzas2018speech" ref-type="bibr">Vryzas,
    Kotsakis, et al., 2018</xref>;
    <xref alt="Vryzas, Matsiola, et al., 2018" rid="ref-vryzas2018subjective" ref-type="bibr">Vryzas,
    Matsiola, et al., 2018</xref>), CREMA-D
    (<xref alt="Cao et al., 2014" rid="ref-cao2014crema" ref-type="bibr">Cao
    et al., 2014</xref>), Mozilla Common Voice
    (<xref alt="Ardila et al., 2019" rid="ref-ardila2019common" ref-type="bibr">Ardila
    et al., 2019</xref>), RAVDESS
    (<xref alt="Livingstone &amp; Russo, 2018" rid="ref-livingstone2018ryerson" ref-type="bibr">Livingstone
    &amp; Russo, 2018</xref>) and VoxForge
    (<xref alt="VoxForge, 2023" rid="ref-voxforge2023" ref-type="bibr">VoxForge,
    2023</xref>)) to predict sex from voice features. Results showed
    high accuracy across multiple model classes (Binary Logistic
    (<xref alt="Cramer, 2002" rid="ref-cramer2002origins" ref-type="bibr">Cramer,
    2002</xref>), SVM
    (<xref alt="Vapnik, 2000" rid="ref-vapnik2000nature" ref-type="bibr">Vapnik,
    2000</xref>), Random Forest
    (<xref alt="Breiman, 2001" rid="ref-breiman2001random" ref-type="bibr">Breiman,
    2001</xref>), and BART
    (<xref alt="Sparapani et al., 2021" rid="ref-sparapani2021nonparametric" ref-type="bibr">Sparapani
    et al., 2021</xref>)), with formant removals ranking among the top
    predictive features.</p>
  </sec>
  <sec id="speaker-diarization">
    <title>Speaker Diarization</title>
    <p>The <monospace>diarize</monospace> function has been used
    successfully, and as a didactic example was applied to a LibriVox
    recording of
    <ext-link ext-link-type="uri" xlink:href="https://archive.org/details/adventuressherlockholmes_v4_1501_librivox"><italic>The
    Adventures of Sherlock Holmes</italic></ext-link> by Conan Doyle,
    successfully segmenting the audio into speaker turns. This
    demonstrates the package’s utility for applications in transcription
    and audio analysis.</p>
  </sec>
</sec>
<sec id="performance">
  <title>Performance</title>
  <p>The <monospace>voice</monospace> package efficiently processes
  audio files, with <monospace>extract_features</monospace> allowing
  parallelization and generating feature-rich data frames in seconds.
  The <monospace>diarize</monospace> function, while computationally
  intensive for long recordings, provides accurate segmentation and
  integrates seamlessly with R workflows.</p>
</sec>
<sec id="availability">
  <title>Availability</title>
  <p>The <monospace>voice</monospace> package is available on CRAN
  (<ext-link ext-link-type="uri" xlink:href="https://CRAN.R-project.org/package=voice">https://CRAN.R-project.org/package=voice</ext-link>)
  and GitHub
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/filipezabala/voice">https://github.com/filipezabala/voice</ext-link>).
  Documentation, including vignettes and examples, is provided to
  facilitate adoption.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>The authors gratefully acknowledge Renfei Mao for their technical
  support and guidance in implementing the <monospace>gm</monospace>
  library
  (<xref alt="Mao, 2025" rid="ref-mao2025gm" ref-type="bibr">Mao,
  2025</xref>).</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-vapnik2000nature">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Vapnik</surname><given-names>Vladimir</given-names></name>
      </person-group>
      <source>The Nature of Statistical Learning Theory</source>
      <publisher-name>Springer Science &amp; Business Media</publisher-name>
      <year iso-8601-date="2000">2000</year>
      <edition>2nd</edition>
      <uri>https://link.springer.com/book/10.1007/978-1-4757-3264-1</uri>
      <pub-id pub-id-type="doi">10.1007/978-1-4757-3264-1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-breiman2001random">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Breiman</surname><given-names>Leo</given-names></name>
      </person-group>
      <article-title>Random Forests</article-title>
      <source>Machine Learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2001">2001</year>
      <volume>45</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      <fpage>5</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cramer2002origins">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cramer</surname><given-names>Jan Salomon</given-names></name>
      </person-group>
      <article-title>The Origins of Logistic Regression</article-title>
      <publisher-name>Tinbergen Institute Working Paper</publisher-name>
      <year iso-8601-date="2002">2002</year>
      <uri>http://hdl.handle.net/10419/86100</uri>
      <pub-id pub-id-type="doi">10.2139/ssrn.360300</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-sueur2008seewave">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sueur</surname><given-names>J.</given-names></name>
        <name><surname>Aubin</surname><given-names>T.</given-names></name>
        <name><surname>Simonis</surname><given-names>C.</given-names></name>
      </person-group>
      <article-title>Seewave: A free modular tool for sound analysis and synthesis</article-title>
      <source>Bioacoustics</source>
      <year iso-8601-date="2008">2008</year>
      <volume>18</volume>
      <pub-id pub-id-type="doi">10.1080/09524622.2008.9753600</pub-id>
      <fpage>213</fpage>
      <lpage>226</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cao2014crema">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cao</surname><given-names>Houwei</given-names></name>
        <name><surname>Cooper</surname><given-names>David G</given-names></name>
        <name><surname>Keutmann</surname><given-names>Michael K</given-names></name>
        <name><surname>Gur</surname><given-names>Ruben C</given-names></name>
        <name><surname>Nenkova</surname><given-names>Ani</given-names></name>
        <name><surname>Verma</surname><given-names>Ragini</given-names></name>
      </person-group>
      <article-title>CREMA-D: Crowd-sourced emotional multimodal actors dataset</article-title>
      <source>IEEE transactions on affective computing</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>5</volume>
      <issue>4</issue>
      <uri>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/</uri>
      <pub-id pub-id-type="doi">10.1109/TAFFC.2014.2336244</pub-id>
      <fpage>377</fpage>
      <lpage>390</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mcfee2015librosa">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Raffel</surname><given-names>Colin</given-names></name>
        <name><surname>Liang</surname><given-names>Dawen</given-names></name>
        <name><surname>Ellis</surname><given-names>Daniel PW</given-names></name>
        <name><surname>McVicar</surname><given-names>Matt</given-names></name>
        <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
      </person-group>
      <article-title>Librosa: Audio and music signal analysis in python</article-title>
      <source>Proceedings of the 14th python in science conference</source>
      <year iso-8601-date="2015">2015</year>
      <volume>8</volume>
      <pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-003</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-vryzas2018speech">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vryzas</surname><given-names>Nikolaos</given-names></name>
        <name><surname>Kotsakis</surname><given-names>Rigas</given-names></name>
        <name><surname>Liatsou</surname><given-names>Aikaterini</given-names></name>
        <name><surname>Dimoulas</surname><given-names>Charalampos A</given-names></name>
        <name><surname>Kalliris</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>Speech emotion recognition for performance interaction</article-title>
      <source>Journal of the Audio Engineering Society</source>
      <publisher-name>Audio Engineering Society</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>66</volume>
      <issue>6</issue>
      <uri>https://www.researchgate.net/profile/Nikolaos-Vryzas/publication/326005164_Speech_Emotion_Recognition_for_Performance_Interaction/links/5b97e33b299bf14ad4ce3ee5/Speech-Emotion-Recognition-for-Performance-Interaction.pdf</uri>
      <pub-id pub-id-type="doi">10.17743/jaes.2018.0036</pub-id>
      <fpage>457</fpage>
      <lpage>467</lpage>
    </element-citation>
  </ref>
  <ref id="ref-vryzas2018subjective">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Vryzas</surname><given-names>Nikolaos</given-names></name>
        <name><surname>Matsiola</surname><given-names>Maria</given-names></name>
        <name><surname>Kotsakis</surname><given-names>Rigas</given-names></name>
        <name><surname>Dimoulas</surname><given-names>Charalampos</given-names></name>
        <name><surname>Kalliris</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>Subjective evaluation of a speech emotion recognition interaction framework</article-title>
      <source>Proceedings of the audio mostly 2018 on sound in immersion and emotion</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1145/3243274.3243294</pub-id>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-livingstone2018ryerson">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Livingstone</surname><given-names>Steven R.</given-names></name>
        <name><surname>Russo</surname><given-names>Frank A.</given-names></name>
      </person-group>
      <article-title>The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</article-title>
      <source>PloS one</source>
      <publisher-name>Public Library of Science San Francisco, CA USA</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>13</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0196391</pub-id>
      <fpage>e0196391</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-bredin2019pyannote">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bredin</surname><given-names>Hervé</given-names></name>
        <name><surname>Yin</surname><given-names>Ruiqing</given-names></name>
        <name><surname>Coria</surname><given-names>Juan Manuel</given-names></name>
        <name><surname>Gelly</surname><given-names>Gregory</given-names></name>
        <name><surname>Korshunov</surname><given-names>Pavel</given-names></name>
        <name><surname>Lavechin</surname><given-names>Marvin</given-names></name>
        <name><surname>Fustes</surname><given-names>Diego</given-names></name>
        <name><surname>Titeux</surname><given-names>Hadrien</given-names></name>
        <name><surname>Bouaziz</surname><given-names>Wassim</given-names></name>
        <name><surname>Gill</surname><given-names>Marie-Philippe</given-names></name>
      </person-group>
      <article-title>Pyannote.audio: Neural building blocks for speaker diarization</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1911.01255v1</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1911.01255</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ardila2019common">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ardila</surname><given-names>Rosana</given-names></name>
        <name><surname>Branson</surname><given-names>Megan</given-names></name>
        <name><surname>Davis</surname><given-names>Kelly</given-names></name>
        <name><surname>Henretty</surname><given-names>Michael</given-names></name>
        <name><surname>Kohler</surname><given-names>Michael</given-names></name>
        <name><surname>Meyer</surname><given-names>Josh</given-names></name>
        <name><surname>Morais</surname><given-names>Reuben</given-names></name>
        <name><surname>Saunders</surname><given-names>Lindsay</given-names></name>
        <name><surname>Tyers</surname><given-names>Francis M</given-names></name>
        <name><surname>Weber</surname><given-names>Gregor</given-names></name>
      </person-group>
      <article-title>Common voice: A massively-multilingual speech corpus</article-title>
      <source>arXiv preprint arXiv:1912.06670</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1912.06670</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.06670</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-sparapani2021nonparametric">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sparapani</surname><given-names>Rodney</given-names></name>
        <name><surname>Spanbauer</surname><given-names>Charles</given-names></name>
        <name><surname>McCulloch</surname><given-names>Robert</given-names></name>
      </person-group>
      <article-title>Nonparametric machine learning and efficient computation with Bayesian Additive Regression Trees: the BART R package</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2021">2021</year>
      <volume>97</volume>
      <issue>1</issue>
      <uri>https://www.jstatsoft.org/article/view/v097i01</uri>
      <pub-id pub-id-type="doi">10.18637/jss.v097.i01</pub-id>
      <fpage>1</fpage>
      <lpage>66</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tuner2023ligges">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ligges</surname><given-names>Uwe</given-names></name>
        <name><surname>Krey</surname><given-names>Sebastian</given-names></name>
        <name><surname>Mersmann</surname><given-names>Olaf</given-names></name>
        <name><surname>Schnackenberg</surname><given-names>Sarah</given-names></name>
      </person-group>
      <source>tuneR: Analysis of music and speech</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://CRAN.R-project.org/package=tuneR</uri>
      <pub-id pub-id-type="doi">10.32614/CRAN.package.tuneR</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ushey2023reticulate">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ushey</surname><given-names>Kevin</given-names></name>
        <name><surname>Allaire</surname><given-names>JJ</given-names></name>
        <name><surname>Tang</surname><given-names>Yuan</given-names></name>
      </person-group>
      <source>reticulate: Interface to ‘Python’</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://CRAN.R-project.org/package=reticulate</uri>
      <pub-id pub-id-type="doi">10.32614/CRAN.package.reticulate</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-voxforge2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>VoxForge</surname></name>
      </person-group>
      <article-title>VoxForge: An open speech dataset set up to collect transcribed speech</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>http://www.voxforge.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-r2024r">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>R Core Team</string-name>
      </person-group>
      <source>R: A language and environment for statistical computing</source>
      <publisher-name>R Foundation for Statistical Computing</publisher-name>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2024">2024</year>
      <uri>https://www.R-project.org/</uri>
      <pub-id pub-id-type="doi">10.1080/10618600.2000.10474900</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wrassp2024winkelmann">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Winkelmann</surname><given-names>Raphael</given-names></name>
        <name><surname>Bombien</surname><given-names>Lasse</given-names></name>
        <name><surname>Scheffers</surname><given-names>Michel</given-names></name>
        <name><surname>Jochim</surname><given-names>Markus</given-names></name>
      </person-group>
      <source>Wrassp: Interface to the ’ASSP’ library</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://CRAN.R-project.org/package=wrassp</uri>
      <pub-id pub-id-type="doi">10.32614/CRAN.package.wrassp</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mao2025gm">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Mao</surname><given-names>Renfei</given-names></name>
      </person-group>
      <source>Gm: Create music with ease</source>
      <year iso-8601-date="2025">2025</year>
      <uri>https://github.com/flujoo/gm</uri>
      <pub-id pub-id-type="doi">10.32614/cran.package.gm</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gautier2025">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gautier</surname><given-names>Laurent</given-names></name>
      </person-group>
      <article-title>rpy2</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://pypi.org/project/rpy2/</uri>
    </element-citation>
  </ref>
  <ref id="ref-zabala2025voice">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Zabala</surname><given-names>Filipe J.</given-names></name>
      </person-group>
      <source>Voice: Voice analysis, speaker recognition and mood inference via music theory</source>
      <year iso-8601-date="2025">2025</year>
      <uri>https://cran.r-project.org/package=voice</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
