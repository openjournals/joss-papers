<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250723222032-fdda2a9bff812ddd59566d18ee4a0c2487630844</doi_batch_id>
    <timestamp>20250723222032</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>111</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>voice: A Comprehensive R Package for Audio Analysis</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Filipe Jaeger</given_name>
            <surname>Zabala</surname>
            <affiliations>
              <institution><institution_name>Graduate Program of Psychiatry and Behavioral Sciences, UFRGS, Brazil</institution_name><institution_id type="ror">https://ror.org/041yk2d64</institution_id></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-5501-0877</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Giovanni Abrahão</given_name>
            <surname>Salum</surname>
            <affiliations>
              <institution><institution_name>Graduate Program of Psychiatry and Behavioral Sciences, UFRGS, Brazil</institution_name><institution_id type="ror">https://ror.org/041yk2d64</institution_id></institution>
              <institution><institution_name>Child Mind Institute, New York, NY 10022, USA</institution_name><institution_id type="ror">https://ror.org/01bfgxw09</institution_id></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-7537-7289</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>23</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8420</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08420</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.17605/OSF.IO/K2B73</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8420</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08420</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08420</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08420.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="vapnik2000nature">
            <volume_title>The Nature of Statistical Learning Theory</volume_title>
            <author>Vapnik</author>
            <doi>10.1007/978-1-4757-3264-1</doi>
            <cYear>2000</cYear>
            <unstructured_citation>Vapnik, V. (2000). The Nature of Statistical Learning Theory (2nd ed.). Springer Science &amp; Business Media. https://doi.org/10.1007/978-1-4757-3264-1</unstructured_citation>
          </citation>
          <citation key="breiman2001random">
            <article_title>Random Forests</article_title>
            <author>Breiman</author>
            <journal_title>Machine Learning</journal_title>
            <issue>1</issue>
            <volume>45</volume>
            <doi>10.1023/A:1010933404324</doi>
            <cYear>2001</cYear>
            <unstructured_citation>Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324</unstructured_citation>
          </citation>
          <citation key="cramer2002origins">
            <article_title>The Origins of Logistic Regression</article_title>
            <author>Cramer</author>
            <doi>10.2139/ssrn.360300</doi>
            <cYear>2002</cYear>
            <unstructured_citation>Cramer, J. S. (2002). The Origins of Logistic Regression. https://doi.org/10.2139/ssrn.360300</unstructured_citation>
          </citation>
          <citation key="sueur2008seewave">
            <article_title>Seewave: A free modular tool for sound analysis and synthesis</article_title>
            <author>Sueur</author>
            <journal_title>Bioacoustics</journal_title>
            <volume>18</volume>
            <doi>10.1080/09524622.2008.9753600</doi>
            <cYear>2008</cYear>
            <unstructured_citation>Sueur, J., Aubin, T., &amp; Simonis, C. (2008). Seewave: A free modular tool for sound analysis and synthesis. Bioacoustics, 18, 213–226. https://doi.org/10.1080/09524622.2008.9753600</unstructured_citation>
          </citation>
          <citation key="cao2014crema">
            <article_title>CREMA-D: Crowd-sourced emotional multimodal actors dataset</article_title>
            <author>Cao</author>
            <journal_title>IEEE transactions on affective computing</journal_title>
            <issue>4</issue>
            <volume>5</volume>
            <doi>10.1109/TAFFC.2014.2336244</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Cao, H., Cooper, D. G., Keutmann, M. K., Gur, R. C., Nenkova, A., &amp; Verma, R. (2014). CREMA-D: Crowd-sourced emotional multimodal actors dataset. IEEE Transactions on Affective Computing, 5(4), 377–390. https://doi.org/10.1109/TAFFC.2014.2336244</unstructured_citation>
          </citation>
          <citation key="mcfee2015librosa">
            <article_title>Librosa: Audio and music signal analysis in python</article_title>
            <author>McFee</author>
            <journal_title>Proceedings of the 14th python in science conference</journal_title>
            <volume>8</volume>
            <doi>10.25080/Majora-7b98e3ed-003</doi>
            <cYear>2015</cYear>
            <unstructured_citation>McFee, B., Raffel, C., Liang, D., Ellis, D. P., McVicar, M., Battenberg, E., &amp; Nieto, O. (2015). Librosa: Audio and music signal analysis in python. Proceedings of the 14th Python in Science Conference, 8. https://doi.org/10.25080/Majora-7b98e3ed-003</unstructured_citation>
          </citation>
          <citation key="vryzas2018speech">
            <article_title>Speech emotion recognition for performance interaction</article_title>
            <author>Vryzas</author>
            <journal_title>Journal of the Audio Engineering Society</journal_title>
            <issue>6</issue>
            <volume>66</volume>
            <doi>10.17743/jaes.2018.0036</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Vryzas, N., Kotsakis, R., Liatsou, A., Dimoulas, C. A., &amp; Kalliris, G. (2018). Speech emotion recognition for performance interaction. Journal of the Audio Engineering Society, 66(6), 457–467. https://doi.org/10.17743/jaes.2018.0036</unstructured_citation>
          </citation>
          <citation key="vryzas2018subjective">
            <article_title>Subjective evaluation of a speech emotion recognition interaction framework</article_title>
            <author>Vryzas</author>
            <journal_title>Proceedings of the audio mostly 2018 on sound in immersion and emotion</journal_title>
            <doi>10.1145/3243274.3243294</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Vryzas, N., Matsiola, M., Kotsakis, R., Dimoulas, C., &amp; Kalliris, G. (2018). Subjective evaluation of a speech emotion recognition interaction framework. In Proceedings of the audio mostly 2018 on sound in immersion and emotion (pp. 1–7). https://doi.org/10.1145/3243274.3243294</unstructured_citation>
          </citation>
          <citation key="livingstone2018ryerson">
            <article_title>The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</article_title>
            <author>Livingstone</author>
            <journal_title>PloS one</journal_title>
            <issue>5</issue>
            <volume>13</volume>
            <doi>10.1371/journal.pone.0196391</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Livingstone, S. R., &amp; Russo, F. A. (2018). The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PloS One, 13(5), e0196391. https://doi.org/10.1371/journal.pone.0196391</unstructured_citation>
          </citation>
          <citation key="bredin2019pyannote">
            <article_title>Pyannote.audio: Neural building blocks for speaker diarization</article_title>
            <author>Bredin</author>
            <doi>10.48550/arXiv.1911.01255</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Bredin, H., Yin, R., Coria, J. M., Gelly, G., Korshunov, P., Lavechin, M., Fustes, D., Titeux, H., Bouaziz, W., &amp; Gill, M.-P. (2019). Pyannote.audio: Neural building blocks for speaker diarization. https://doi.org/10.48550/arXiv.1911.01255</unstructured_citation>
          </citation>
          <citation key="ardila2019common">
            <article_title>Common voice: A massively-multilingual speech corpus</article_title>
            <author>Ardila</author>
            <journal_title>arXiv preprint arXiv:1912.06670</journal_title>
            <doi>10.48550/arXiv.1912.06670</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., &amp; Weber, G. (2019). Common voice: A massively-multilingual speech corpus. arXiv Preprint arXiv:1912.06670. https://doi.org/10.48550/arXiv.1912.06670</unstructured_citation>
          </citation>
          <citation key="sparapani2021nonparametric">
            <article_title>Nonparametric machine learning and efficient computation with bayesian additive regression trees: The BART r package</article_title>
            <author>Sparapani</author>
            <journal_title>Journal of Statistical Software</journal_title>
            <issue>1</issue>
            <volume>97</volume>
            <doi>10.18637/jss.v097.i01</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Sparapani, R., Spanbauer, C., &amp; McCulloch, R. (2021). Nonparametric machine learning and efficient computation with bayesian additive regression trees: The BART r package. Journal of Statistical Software, 97(1), 1–66. https://doi.org/10.18637/jss.v097.i01</unstructured_citation>
          </citation>
          <citation key="tuner2023ligges">
            <volume_title>tuneR: Analysis of music and speech</volume_title>
            <author>Ligges</author>
            <doi>10.32614/CRAN.package.tuneR</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Ligges, U., Krey, S., Mersmann, O., &amp; Schnackenberg, S. (2023). tuneR: Analysis of music and speech. https://doi.org/10.32614/CRAN.package.tuneR</unstructured_citation>
          </citation>
          <citation key="ushey2023reticulate">
            <volume_title>Reticulate: Interface to ’python’</volume_title>
            <author>Ushey</author>
            <doi>10.32614/CRAN.package.reticulate</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Ushey, K., Allaire, J., &amp; Tang, Y. (2023). Reticulate: Interface to ’python’. https://doi.org/10.32614/CRAN.package.reticulate</unstructured_citation>
          </citation>
          <citation key="voxforge2023">
            <article_title>VoxForge: An open speech dataset set up to collect transcribed speech</article_title>
            <author>VoxForge</author>
            <cYear>2023</cYear>
            <unstructured_citation>VoxForge. (2023). VoxForge: An open speech dataset set up to collect transcribed speech. http://www.voxforge.org/</unstructured_citation>
          </citation>
          <citation key="r2024r">
            <volume_title>R: A language and environment for statistical computing</volume_title>
            <author>R Core Team</author>
            <doi>10.1080/10618600.2000.10474900</doi>
            <cYear>2024</cYear>
            <unstructured_citation>R Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://doi.org/10.1080/10618600.2000.10474900</unstructured_citation>
          </citation>
          <citation key="wrassp2024winkelmann">
            <volume_title>Wrassp: Interface to the ’ASSP’ library</volume_title>
            <author>Winkelmann</author>
            <doi>10.32614/CRAN.package.wrassp</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Winkelmann, R., Bombien, L., Scheffers, M., &amp; Jochim, M. (2024). Wrassp: Interface to the ’ASSP’ library. https://doi.org/10.32614/CRAN.package.wrassp</unstructured_citation>
          </citation>
          <citation key="mao2025gm">
            <volume_title>Gm: Create music with ease</volume_title>
            <author>Mao</author>
            <doi>10.32614/cran.package.gm</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Mao, R. (2025). Gm: Create music with ease. https://doi.org/10.32614/cran.package.gm</unstructured_citation>
          </citation>
          <citation key="gautier2025">
            <article_title>rpy2</article_title>
            <author>Gautier</author>
            <cYear>2025</cYear>
            <unstructured_citation>Gautier, L. (2025). rpy2 (Version 3.6.1). https://pypi.org/project/rpy2/</unstructured_citation>
          </citation>
          <citation key="zabala2025voice">
            <volume_title>Voice: Voice analysis, speaker recognition and mood inference via music theory</volume_title>
            <author>Zabala</author>
            <cYear>2025</cYear>
            <unstructured_citation>Zabala, F. J. (2025). Voice: Voice analysis, speaker recognition and mood inference via music theory. https://cran.r-project.org/package=voice</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
