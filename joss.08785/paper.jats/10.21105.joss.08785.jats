<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8785</article-id>
<article-id pub-id-type="doi">10.21105/joss.08785</article-id>
<title-group>
<article-title>Fiats: Functional inference and training for
surrogates</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2344-868X</contrib-id>
<name>
<surname>Rouson</surname>
<given-names>Damian</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0724-9349</contrib-id>
<name>
<surname>Bonachea</surname>
<given-names>Dan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3205-2169</contrib-id>
<name>
<surname>Richardson</surname>
<given-names>Brad</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2882-594X</contrib-id>
<name>
<surname>Welsman</surname>
<given-names>Jordan A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0436-9118</contrib-id>
<name>
<surname>Bailey</surname>
<given-names>Jeremiah</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4077-3430</contrib-id>
<name>
<surname>Gutmann</surname>
<given-names>Ethan D</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2469-5284</contrib-id>
<name>
<surname>Torres</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7974-1853</contrib-id>
<name>
<surname>Rasmussen</surname>
<given-names>Katherine</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0008-0479-3948</contrib-id>
<name>
<surname>Dibba</surname>
<given-names>Baboucarr</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-3182-9296</contrib-id>
<name>
<surname>Zhang</surname>
<given-names>Yunhao</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-3846-6248</contrib-id>
<name>
<surname>Weaver</surname>
<given-names>Kareem</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3092-0903</contrib-id>
<name>
<surname>Bai</surname>
<given-names>Zhe</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3748-403X</contrib-id>
<name>
<surname>Nguyen</surname>
<given-names>Tan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Lawrence Berkeley National Laboratory, United
States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>NSF National Center for Atmospheric Research, United
States</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Northern New Mexico College, United States</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-06-20">
<day>20</day>
<month>6</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>116</issue>
<fpage>8785</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>artificial neural networks</kwd>
<kwd>high-performance computing</kwd>
<kwd>parallel programming</kwd>
<kwd>deep learning</kwd>
<kwd>Fortran</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><ext-link ext-link-type="uri" xlink:href="https://go.lbl.gov/fiats">Fiats</ext-link>
  provides a platform for research on the training and deployment of
  neural-network surrogate models for computational science. Fiats also
  supports exploring, advancing, and combining functional,
  object-oriented, and parallel programming patterns in Fortran 2023
  (<xref alt="Fortran Standards Committee JTC1/SC22/WG5, 2023" rid="ref-fortran2023" ref-type="bibr">Fortran
  Standards Committee JTC1/SC22/WG5, 2023</xref>). As such, the Fiats
  name has dual expansions: “Functional Inference And Training for
  Surrogates” or “Fortran Inference And Training for Science.” Fiats
  inference and training procedures are <monospace>pure</monospace> and
  therefore satisfy a language constraint imposed on procedure
  invocations inside Fortran’s parallel loop construct:
  <monospace>do concurrent</monospace>. Furthermore, the Fiats training
  procedures are built around a <monospace>do concurrent</monospace>
  parallel reduction. Several compilers can automatically parallelize
  <monospace>do concurrent</monospace> on Central Processing Units
  (CPUs) or Graphics Processing Units (GPUs). Fiats thus aims to achieve
  performance portability through standard language mechanisms.</p>
  <p>In addition to an <monospace>example</monospace> subdirectory with
  illustrative codes, the Fiats <monospace>demo/app</monospace>
  subdirectory contains three demonstration applications:</p>
  <list list-type="order">
    <list-item>
      <p>One trains a cloud-microphysics surrogate for the Lawrence
      Berkeley National Laboratory
      <ext-link ext-link-type="uri" xlink:href="https://go.lbl.gov/icar">fork</ext-link>
      of the Intermediate Complexity Atmospheric Research (ICAR)
      model.</p>
    </list-item>
    <list-item>
      <p>Another calculates input- and output-tensor statistics for
      ICAR’s physics-based microphysics models.</p>
    </list-item>
    <list-item>
      <p>A third performs batch inference using an aerosols surrogate
      for the Energy Exascale Earth Systems Model
      (<ext-link ext-link-type="uri" xlink:href="https://e3sm.org">E3SM</ext-link>).</p>
    </list-item>
  </list>
  <p>Ongoing research explores how Fiats can exploit multi-image
  execution, a set of Fortran features for Single-Program, Multiple-Data
  (SPMD) parallel programming with a Partitioned Global Address Space
  (PGAS)
  (<xref alt="Numrich, 2018" rid="ref-numrich2018co-arrays" ref-type="bibr">Numrich,
  2018</xref>), where the PGAS features center around “coarray”
  distributed data structures.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Developers of computational science software lack widespread
  support for adopting Fortran’s built-in parallel programming features.
  Those features take two forms: <monospace>do concurrent</monospace>
  for loop-level parallelism and multi-image execution for SPMD/PGAS
  parallelism in shared or distributed memory. Fiats addresses this
  problem by providing inference and training procedures that are
  compatible with both forms of parallel language features. Fiats thus
  facilitates studying deep learning for science and programming
  paradigms and patterns for deep learning in Fortran 2023. Fiats also
  provides a vehicle for contributors to ensure that Fortran supports
  the algorithms that are central to the emerging field of deep learning
  and to drive improvements in the Fortran language to best support this
  domain. The next section covers related work.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>At least six open-source software packages provide deep learning
  services to Fortran. Three provide Fortran application programming
  interfaces (APIs) that wrap C++ libraries:</p>
  <list list-type="bullet">
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/Cambridge-ICCS/fortran-tf-lib">Fortran-TF-Lib</ext-link>
      is a Fortran API for
      <ext-link ext-link-type="uri" xlink:href="https://tensorflow.org">TensorFlow</ext-link>,</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/Cambridge-ICCS/FTorch">FTorch</ext-link>
      (<xref alt="Atkinson et al., 2025" rid="ref-Atkinson-FTorch" ref-type="bibr">Atkinson
      et al., 2025</xref>) is a Fortran API for libtorch, the PyTorch
      back-end, and</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/NVIDIA/TorchFort">TorchFort</ext-link>
      is also a Fortran API for libtorch.</p>
    </list-item>
  </list>
  <p>As of this writing, recursive searches in the root directories of
  the these three projects find no <monospace>pure</monospace>
  procedures. Procedures are <monospace>pure</monospace> if declared as
  such or if declared <monospace>simple</monospace> or if declared
  <monospace>elemental</monospace> without the
  <monospace>impure</monospace> attribute. Because any procedure invoked
  within a <monospace>pure</monospace> procedure must also be
  <monospace>pure</monospace>, the absence of
  <monospace>pure</monospace> procedures precludes the use of these APIs
  anywhere in the call stack inside a
  <monospace>do concurrent</monospace> construct. Also, as APIs backed
  by C++ libraries, none use Fortran’s multi-image execution
  features.</p>
  <p>Three packages supporting deep learning in Fortran are themselves
  written in Fortran:</p>
  <list list-type="bullet">
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/nedtaylor/athena">Athena</ext-link>
      (<xref alt="Taylor, 2024" rid="ref-taylor2024athena" ref-type="bibr">Taylor,
      2024</xref>)</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://go.lbl.gov/fiats">Fiats</ext-link>
      (<xref alt="Rouson, Bai, Bonachea, Ergawy, et al., 2025" rid="ref-rouson2025automatically" ref-type="bibr">Rouson,
      Bai, Bonachea, Ergawy, et al., 2025</xref>)</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/modern-fortran/neural-fortran">neural-fortran</ext-link>
      (<xref alt="Curcic, 2019" rid="ref-curcic2019parallel" ref-type="bibr">Curcic,
      2019</xref>)</p>
    </list-item>
  </list>
  <p>Searching the Athena, Fiats, and neural-fortran
  <monospace>src</monospace> subdirectories finds that over half of the
  procedures in each are <monospace>pure</monospace>, including 75% of
  Fiats procedures. Included in these tallies are procedures explicitly
  marked as <monospace>pure</monospace> along with
  <monospace>simple</monospace> procedures and
  <monospace>elemental</monospace> procedures without the
  <monospace>impure</monospace> attribute. Athena, Fiats, and
  neural-fortran each employ <monospace>do concurrent</monospace>
  extensively. Only Fiats, however, leverages the locality specifiers
  introduced in Fortran 2018 and expanded in Fortran 2023 to include
  parallel reductions. These annotations make it more tractable for
  compilers to correctly parallelize code on processors or offload code
  to accelerators such as GPUs.</p>
  <p>Of the APIs and libraries discussed here, only neural-fortran and
  Fiats use multi-image features: neural-fortran in its core library and
  Fiats in a demonstration application. Both use multi-image features
  minimally, leaving considerable room for researching parallelization
  strategies.</p>
  <p>Each of the Fortran deep learning APIs and libraries discussed in
  this paper is actively developed except Fortran-TF-Lib.
  Fortran-TF-Lib’s most recent commit was in 2023 and no releases have
  been posted.</p>
</sec>
<sec id="recent-research-and-scholarly-publications">
  <title>Recent research and scholarly publications</title>
  <p>Fiats supports research in training surrogate models and
  parallelizing batch inference calculations for atmospheric sciences.
  This research recently generated two peer-reviewed papers described in
  this section. Four programs in the Fiats repository played significant
  roles in these papers:</p>
  <list list-type="order">
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/BerkeleyLab/fiats/blob/joss-line-references/example/concurrent-inferences.f90"><monospace>example/concurrent-inferences.f90</monospace></ext-link>,</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/BerkeleyLab/fiats/blob/joss-line-references/example/learn-saturated-mixing-ratio.F90"><monospace>example/learn-saturated-mixing-ratio.f90</monospace></ext-link>,</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/BerkeleyLab/fiats/blob/joss-line-references/demo/app/infer-aerosol.f90#L1"><monospace>app/demo/infer-aerosols.f90</monospace></ext-link>,
      and</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://github.com/BerkeleyLab/fiats/blob/joss-line-references/demo/app/train-cloud-microphysics.F90"><monospace>app/demo/train-cloud-microphysics.f90</monospace></ext-link>.</p>
    </list-item>
  </list>
  <p>Rouson, Bai, Bonachea, Ergawy, et al.
  (<xref alt="2025" rid="ref-rouson2025automatically" ref-type="bibr">2025</xref>)
  used program 1 to study automatically parallelizing batch inferences
  via <monospace>do concurrent</monospace>. Rouson, Bai, Bonachea,
  Dibba, et al.
  (<xref alt="2025" rid="ref-rouson2025cloud" ref-type="bibr">2025</xref>)
  used programs 2–4 to study neural-network training for cloud
  microphysics and inference for atmospheric aerosols. The derived types
  in the Unified Modeling Language (UML) class diagram in
  <xref alt="[fig:derived-types]" rid="figU003Aderived-types">[fig:derived-types]</xref>
  enabled these studies.</p>
  <p><xref alt="[fig:derived-types]" rid="figU003Aderived-types">[fig:derived-types]</xref>
  includes two of the
  <ext-link ext-link-type="uri" xlink:href="https://go.lbl.gov/julienne">Julienne</ext-link>
  correctness-checking framework’s derived types,
  <monospace>string_t</monospace> and <monospace>file_t</monospace>.
  These are included because other parts of the figure reference these
  types. The rightmost four types in
  <xref alt="[fig:derived-types]" rid="figU003Aderived-types">[fig:derived-types]</xref>
  exist primarily to support inference. The leftmost five support
  training. Because inference is considerably simpler, it makes sense to
  describe the right side of the diagram before the left side.</p>
  <p>The <monospace>concurrent-inferences</monospace> example program
  performs batch inference using the <monospace>string_t</monospace>,
  <monospace>file_t</monospace>, and
  <monospace>neural_network_t</monospace> types. From the bottom of the
  class hierarchy in
  <xref alt="[fig:derived-types]" rid="figU003Aderived-types">[fig:derived-types]</xref>,
  the <monospace>concurrent-inferences</monospace> program does the
  following:</p>
  <list list-type="order">
    <list-item>
      <p>Gets a <monospace>character</monospace> file name from the
      command line,</p>
    </list-item>
    <list-item>
      <p>Passes the name to a <monospace>string_t</monospace>
      constructor,</p>
    </list-item>
    <list-item>
      <p>Passes the resulting <monospace>string_t</monospace> object to
      a <monospace>file_t</monospace> constructor, and</p>
    </list-item>
    <list-item>
      <p>Passes the resulting <monospace>file_t</monospace> object to a
      <monospace>neural_network_t</monospace> constructor.</p>
    </list-item>
  </list>
  <p>The program then repeatedly invokes the
  <monospace>infer</monospace> type-bound procedure on each element of a
  three-dimensional (3D) array of <monospace>tensor_t</monospace>
  objects using OpenMP directives or
  <monospace>do concurrent</monospace> or an array statement. The array
  statement takes advantage of <monospace>infer</monospace> being
  <monospace>elemental</monospace>. The following line
  <monospace>example/concurrent-inferences.f90</monospace> at
  demonstrates neural-network construction from a file:</p>
  <preformat>   neural_network = neural_network_t(file_t(network_file_name)) </preformat>
  <p>In the same example, the following line demonstrates using the
  network for inference:</p>
  <preformat>   outputs(i,k,j) = neural_network%infer(inputs(i,k,j))</preformat>
  <p>The <monospace>infer-aerosols</monospace> program performs
  inferences by invoking <monospace>double precision</monospace>
  versions of the <monospace>infer</monospace> generic binding on an
  object of type <monospace>unmapped_network_t</monospace>, a
  parameterized derived type (PDT) that has a
  <monospace>kind</monospace> type parameter. To match the expected
  behavior of the aerosol model, which was trained in PyTorch, the
  <monospace>unmapped_network_t</monospace> implementation ensures the
  use of raw network input and output tensors without the normalizations
  and remappings that are performed by default for a
  <monospace>neural_network_t</monospace> object. The
  <monospace>double_precision_file_t</monospace> type controls the
  interpretation of the JSON network file: JSON does not distinguish
  between categories of numerical values such as
  <monospace>real</monospace>, <monospace>double precision</monospace>,
  or even <monospace>integer</monospace>, so something external to the
  file must determine the interpretation of the numbers in a JSON
  file.</p>
  <p>The <monospace>learn-saturated-mixing-ratio</monospace> and
  <monospace>train-cloud-microphysics</monospace> programs focus on
  using a <monospace>trainable_network_t</monospace> object for
  training. The former trains neural network surrogates for a
  thermodynamic function from ICAR: the saturated mixing ratio, a scalar
  function of temperature and pressure. The latter trains surrogates for
  the complete cloud microphysics models in ICAR – models implemented in
  thousands of lines of code. Whereas diagrammed relationships of
  <monospace>neural_network_t</monospace> reflect direct dependencies of
  only two types (<monospace>file_t</monospace> and
  <monospace>tensor_t</monospace>), even describing the basic behaviors
  of <monospace>trainable_network_t</monospace> requires showing
  dependencies on five types:</p>
  <list list-type="bullet">
    <list-item>
      <p>A <monospace>training_configuration_t</monospace> object, which
      holds hyperparameters such as the learning rate and choice of
      optimization algorithms,</p>
    </list-item>
    <list-item>
      <p>A <monospace>file_t</monospace> object representing a JSON
      input file from which the training configuration can alternatively
      be read inside the <monospace>trainable_network_t</monospace>
      constructor,</p>
    </list-item>
    <list-item>
      <p>A <monospace>mini_batch_t</monospace> object that stores an
      array of <monospace>input_output_pair</monospace> objects from the
      training data set,</p>
    </list-item>
    <list-item>
      <p>Two <monospace>tensor_map_t</monospace> objects storing the
      linear functions that map inputs to the training data range and
      map outputs from the training data range back to the application
      range, and</p>
    </list-item>
    <list-item>
      <p>A parent <monospace>neural_network_t</monospace> object storing
      the network architecture, including weights, biases, layer widths,
      etc.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Class diagram: derived types (named in bordered white
    boxes), type relationships (connecting lines), type extension (open
    triangles), composition (solid diamonds), or directional
    relationship (arrows). Read relationships as sentences wherein the
    type named at the base of an arrow is the subject followed by an
    annotation (in an unbordered gray box) followed by the type named at
    the arrow’s head as the object. Type extension reads with the type
    adjacent to the open triangle as the subject. Composition reads with
    the type adjacent to the closed diamond as the subject.
    <styled-content id="figU003Aderived-types"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="class-overview-rot.png" />
  </fig>
  <p>The <monospace>trainable_network_t</monospace> type stores a
  <monospace>workspace_t</monospace> (not shown) as a scratch-pad for
  training purposes. The workspace is not needed for inference. During
  each training step, a <monospace>trainable_network_t</monospace>
  object passes its <monospace>workspace_t</monospace> to a
  <monospace>learn</monospace> procedure binding (not shown) on its
  parent <monospace>neural_network_t</monospace>. Lines
  <ext-link ext-link-type="uri" xlink:href="https://github.com/BerkeleyLab/fiats/blob/joss-line-references/demo/app/train-cloud-microphysics.F90#L388-L396">388–396</ext-link>
  of <monospace>demo/app/train-cloud-microphysics.f90</monospace> at
  <monospace>git</monospace> tag
  <monospace>joss-line-references</monospace> demonstrate:</p>
  <list list-type="order">
    <list-item>
      <p>A loop over epochs,</p>
    </list-item>
    <list-item>
      <p>The shuffling of the <monospace>input_output_pair_t</monospace>
      objects at the beginning of each epoch,</p>
    </list-item>
    <list-item>
      <p>The grouping of <monospace>input_output_pair_t</monospace>
      objects into <monospace>mini_batch_t</monospace> objects, and</p>
    </list-item>
    <list-item>
      <p>The invocation of the <monospace>train</monospace> procedure
      for each mini-batch,</p>
    </list-item>
  </list>
  <p>where steps 2 and 3 express deep learning’s stochastic gradient
  descent algorithm.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This material is based upon work supported by the U.S. Department
  of Energy, Office of Science, Office of Advanced Scientific Computing
  Research and Office of Nuclear Physics, Scientific Discovery through
  Advanced Computing (SciDAC) Next-Generation Scientific Software
  Technologies (NGSST) programs under Contract No. DE-AC02-05CH11231.
  This material is also based on work supported by Laboratory Directed
  Research and Development (LDRD) funding from Lawrence Berkeley
  National Laboratory, provided by the Director, Office of Science, of
  the U.S. DOE under Contract No. DE-AC02-05CH11231. This manuscript has
  been authored by an author at Lawrence Berkeley National Laboratory
  under Contract No. DE-AC02-05CH11231 with the U.S. Department of
  Energy. The U.S. Government retains, and the publisher, by accepting
  the article for publication, acknowledges, that the U.S. Government
  retains a non-exclusive, paid-up, irrevocable, world-wide license to
  publish or reproduce the published form of this manuscript, or allow
  others to do so, for U.S. Government purposes.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-rouson2025cloud">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rouson</surname><given-names>Damian</given-names></name>
        <name><surname>Bai</surname><given-names>Zhe</given-names></name>
        <name><surname>Bonachea</surname><given-names>Dan</given-names></name>
        <name><surname>Dibba</surname><given-names>Baboucarr</given-names></name>
        <name><surname>Gutmann</surname><given-names>Ethan</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Katherine</given-names></name>
        <name><surname>Torres</surname><given-names>David</given-names></name>
        <name><surname>Welsman</surname><given-names>Jordan</given-names></name>
        <name><surname>Zhang</surname><given-names>Yunhao</given-names></name>
      </person-group>
      <article-title>Cloud microphysics training and aerosol inference with the Fiats deep learning library</article-title>
      <source>Improving scientific software conference (ISS)</source>
      <publisher-name>University Corporation for Atmospheric Research</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.25344/S4QS3J</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-rouson2025automatically">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rouson</surname><given-names>Damian</given-names></name>
        <name><surname>Bai</surname><given-names>Zhe</given-names></name>
        <name><surname>Bonachea</surname><given-names>Dan</given-names></name>
        <name><surname>Ergawy</surname><given-names>Kareem</given-names></name>
        <name><surname>Gutmann</surname><given-names>Ethan</given-names></name>
        <name><surname>Klemm</surname><given-names>Michael</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Katherine</given-names></name>
        <name><surname>Richardson</surname><given-names>Brad</given-names></name>
        <name><surname>Shende</surname><given-names>Sameer</given-names></name>
        <name><surname>Torres</surname><given-names>David</given-names></name>
        <name><surname>Zhang</surname><given-names>Yunhao</given-names></name>
      </person-group>
      <article-title>Automatically parallelizing batch inference on deep neural networks using Fiats and Fortran 2023 “do concurrent”</article-title>
      <source>5th international workshop on computational aspects of deep learning (CADL)</source>
      <publisher-name>IEEE; Springer Lecture Notes in Computer Science</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <pub-id pub-id-type="doi">10.25344/S4VG6T</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-curcic2019parallel">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Curcic</surname><given-names>Milan</given-names></name>
      </person-group>
      <article-title>A parallel Fortran framework for neural networks and deep learning</article-title>
      <source>ACM SIGPLAN fortran forum</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>38</volume>
      <pub-id pub-id-type="doi">10.1145/3323057.3323059</pub-id>
      <fpage>4</fpage>
      <lpage>21</lpage>
    </element-citation>
  </ref>
  <ref id="ref-taylor2024athena">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Taylor</surname><given-names>Ned Thaddeus</given-names></name>
      </person-group>
      <article-title>ATHENA: A Fortran package for neural networks</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>9</volume>
      <issue>99</issue>
      <uri>https://doi.org/10.21105/joss.06492</uri>
      <pub-id pub-id-type="doi">10.21105/joss.06492</pub-id>
      <fpage>6492</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-numrich2018co-arrays">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Numrich</surname><given-names>Robert W.</given-names></name>
      </person-group>
      <source>Parallel Programming with Co-arrays</source>
      <publisher-name>CRC Press</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1201/9780429437182</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-fortran2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>Fortran Standards Committee JTC1/SC22/WG5</string-name>
      </person-group>
      <source>Information technology – programming languages – Fortran, ISO/IEC 1539-1:2023</source>
      <publisher-name>International Organization for Standardization (ISO)</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-Atkinson-FTorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Atkinson</surname><given-names>Jack</given-names></name>
        <name><surname>Elafrou</surname><given-names>Athena</given-names></name>
        <name><surname>Kasoar</surname><given-names>Elliott</given-names></name>
        <name><surname>Wallwork</surname><given-names>Joseph G.</given-names></name>
        <name><surname>Meltzer</surname><given-names>Thomas</given-names></name>
        <name><surname>Clifford</surname><given-names>Simon</given-names></name>
        <name><surname>Orchard</surname><given-names>Dominic</given-names></name>
        <name><surname>Edsall</surname><given-names>Chris</given-names></name>
      </person-group>
      <article-title>FTorch: a library for coupling PyTorch models to Fortran</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <volume>10</volume>
      <issue>107</issue>
      <uri>https://doi.org/10.21105/joss.07602</uri>
      <pub-id pub-id-type="doi">10.21105/joss.07602</pub-id>
      <fpage>7602</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
