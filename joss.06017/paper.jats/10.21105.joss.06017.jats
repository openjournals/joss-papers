<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6017</article-id>
<article-id pub-id-type="doi">10.21105/joss.06017</article-id>
<title-group>
<article-title>SCAS dashboard: A tool to intuitively and interactively
analyze Slurm cluster usage</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-3995-709X</contrib-id>
<name>
<surname>Walzthoeni</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Singiali</surname>
<given-names>Bom Bahadur</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0510-4792</contrib-id>
<name>
<surname>Rayner</surname>
<given-names>N. William</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Casale</surname>
<given-names>Francesco Paolo</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-5"/>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0772-7267</contrib-id>
<name>
<surname>Feest</surname>
<given-names>Christoph</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2154-4552</contrib-id>
<name>
<surname>Marr</surname>
<given-names>Carsten</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7736-3059</contrib-id>
<name>
<surname>Wachsmann</surname>
<given-names>Alf</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Core Facility Genomics, Helmholtz Zentrum München - German
Research Center for Environmental Health, 85764 Neuherberg,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Digital Transformation &amp; IT, Helmholtz Munich,
Helmholtz Zentrum München - German Research Center for Environmental
Health, 85764 Neuherberg, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Institute of Translational Genomics, Helmholtz Zentrum
München - German Research Center for Environmental Health, 85764
Neuherberg, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Computational Health Center, Helmholtz Zentrum München -
German Research Center for Environmental Health, 85764 Neuherberg,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Helmholtz Pioneer Campus, Helmholtz Zentrum München -
German Research Center for Environmental Health, 85764 Neuherberg,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>School of Computation, Information and Technology,
Technical University of Munich, Munich, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Helmholtz AI, Helmholtz Zentrum München - German Research
Center for Environmental Health, 85764 Neuherberg, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-08-30">
<day>30</day>
<month>8</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>97</issue>
<fpage>6017</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Slurm</kwd>
<kwd>HPC</kwd>
<kwd>dashboard</kwd>
<kwd>python</kwd>
<kwd>R</kwd>
<kwd>shiny</kwd>
<kwd>containers</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Many organizations offer High Performance Computing (HPC)
  environments as a service, hosted on-premises or in the cloud. Compute
  jobs are commonly managed via Slurm
  (<xref alt="Yoo et al., 2003" rid="ref-slurm" ref-type="bibr">Yoo et
  al., 2003</xref>), but an intuitive, easy-to-use and interactive
  visualization has been lacking. To fill this gap, we developed a Slurm
  Cluster Admin Statistics (SCAS) dashboard. SCAS provides a means to
  analyze and visualize data of compute jobs and includes a feature to
  generate presentations for cluster users. It thus allows HPC
  stakeholders to easily analyze and identify bottlenecks of Slurm-based
  compute clusters in a timely fashion and provides decision-making
  support for managing cluster resources.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Slurm
  (<xref alt="Yoo et al., 2003" rid="ref-slurm" ref-type="bibr">Yoo et
  al., 2003</xref>) is an open-source cluster management and job
  scheduling system for Linux-based compute clusters and is widely used
  for High Performance Computing (HPC). It offers command line tools to
  export and analyze cluster use and various applications have been
  developed to monitor the current state of the cluster (e.g., live
  dashboards using Grafana
  (<xref alt="Dessalvi, 2021" rid="ref-grafanadb" ref-type="bibr">Dessalvi,
  2021</xref>)). A feature-rich tool for the analysis of cluster
  performance is Open XDMoD
  (<xref alt="Palmer et al., 2015" rid="ref-xdmod" ref-type="bibr">Palmer
  et al., 2015</xref>), which supports various schedulers and metrics.
  Open XDMoD uses 3rd-party software libraries that are not free for
  commercial use. Open OnDemand
  (<xref alt="Hudak et al., 2018" rid="ref-Hudak2018" ref-type="bibr">Hudak
  et al., 2018</xref>) allows users to access a HPC cluster using a web
  portal, it provides various apps to facilitate HPC usage and can
  integrate the Open XDMoD for usage statistics. Both Open XDMoD and
  Open OnDemand require continuous support and extensive configurations
  and therefore, intuitive, responsive, easy-to-install and easy-to-use
  applications that enable HPC administrators and managers to analyze
  and visualize cluster usage in detail and over time are highly
  complementary. This information is crucial to identify bottlenecks in
  compute clusters and make informed strategic decisions regarding their
  future development.</p>
  <p>To address this, we developed the Slurm Cluster Admin Statistics
  (SCAS) dashboard, a scalable and flexible dashboard application to
  analyze completed compute jobs on a Slurm-based cluster. The dashboard
  offers various statistics, visualizations, and insights to HPC
  stakeholders and cluster users. Additionally, we engineered the
  software to have a low-memory footprint and to be fast and responsive
  to user queries. The software stack is provided in an easy-to-use and
  easy-to-deploy manner using docker containers and a docker-compose
  implementation.</p>
</sec>
<sec id="description">
  <title>Description</title>
  <sec id="scas-dashboard-overview">
    <title>SCAS Dashboard overview</title>
    <p>The SCAS dashboard architecture consists of a nginx web server as
    a router (reverse proxy), a front end based on R-Shiny
    (<xref alt="Chang et al., 2023" rid="ref-shiny" ref-type="bibr">Chang
    et al., 2023</xref>;
    <xref alt="Chang &amp; Borges Ribeiro, 2021" rid="ref-shinydashboard" ref-type="bibr">Chang
    &amp; Borges Ribeiro, 2021</xref>;
    <xref alt="R Core Team, 2023" rid="ref-R" ref-type="bibr">R Core
    Team, 2023</xref>), a back end based on Python using the Django REST
    framework to provide an API, and a PostgreSQL database as back end
    (see <xref alt="[fig:fig1]" rid="figU003Afig1">[fig:fig1]</xref>).
    The dashboard is intended for HPC stakeholders and therefore
    includes secure user authentication. The front end is a
    user-friendly interface for filtering and visualizing the Slurm
    data. The back end provides an admin interface via Python Django
    Admin and a web API that is used by both the front end and a script
    for uploading new data. Additionally, the back end creates a daily
    index of the data, enabling the software to maintain a low memory
    footprint while being fast and responsive. Furthermore, a
    presentation can be generated automatically and viewed by various
    stakeholders, including the cluster users, via a web browser.</p>
    <fig>
      <caption><p>Architecture of the SCAS dashboard. The dashboard and
      the presentation are accessed by the user through a web browser.
      New data can be uploaded to the SCAS dashboard by executing a
      script that regularly fetches the latest data from a job
      submission node. On the server side, the architecture is organized
      into separate components (shown in dashed box): nginx (reverse
      proxy), SCAS-frontend, SCAS-backend and PostgresSQL database. A
      docker-compose implementation of the services is provided.
      <styled-content id="figU003Afig1"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="paper.jats/Figure1.png" />
    </fig>
  </sec>
  <sec id="scas-dashboard-workflow">
    <title>SCAS dashboard workflow</title>
    <p>Completed compute jobs and available node configurations are
    submitted to the SCAS-backend API with a script that utilizes the
    Slurm’s <italic>sacct</italic> tool. This script can be run as a
    daily or weekly <italic>cron</italic> job on a job submission node.
    The back end then generates the daily statistics that are stored in
    the database. This preprocessed indexed data enables the app to have
    a low memory footprint and high responsiveness, as no calculations
    are required when the data is fetched from the API. Upon filtering a
    date range in the front end, a request is sent to the back end that
    retrieves the data for the selected days and aggregates the
    statistics to generate the visualizations.</p>
  </sec>
  <sec id="frontend-dashboard-user-interface">
    <title>Frontend – dashboard user interface</title>
    <p><xref alt="[fig:fig2]" rid="figU003Afig2">[fig:fig2]</xref>
    displays some example views of the user interface. The date range,
    the cluster, and the partitions that should be analyzed can be
    selected from the menu
    (<xref alt="Figure 2a" rid="figU003Afig2">Figure 2a</xref>). Data
    tables and visualizations are then updated accordingly and displayed
    to the user.</p>
    <p>For the selected date range, the visualizations include:</p>
    <list list-type="bullet">
      <list-item>
        <p>Number of jobs, CPU and GPU hours per month
        (<xref alt="Figure 2b,c" rid="figU003Afig2">Figure
        2b,c</xref>)</p>
      </list-item>
      <list-item>
        <p>Memory and cores requested by users, displayed as contingency
        graphs</p>
      </list-item>
      <list-item>
        <p>Average job pending and runtimes per month and per day
        (<xref alt="Figure 2d,e,f" rid="figU003Afig2">Figure
        2d,e,f</xref>)</p>
      </list-item>
      <list-item>
        <p>Distribution of CPU hours used vs. the percentage of
        users</p>
      </list-item>
      <list-item>
        <p>Total cluster utilization per day and per month, individual
        node utilization per month, summaries of utilization per CPU/GPU
        or memory type of the nodes per month
        (<xref alt="Figure 2g" rid="figU003Afig2">Figure 2g</xref>)</p>
      </list-item>
    </list>
    <p>The data can also be downloaded for use in spreadsheet
    applications.</p>
  </sec>
  <sec id="frontend-automated-presentation">
    <title>Frontend – automated presentation</title>
    <p>For presenting key figures to the cluster users, a feature is
    available to generate a browser-based presentation in carousel mode.
    The presentation is auto-updated and customization settings are
    available via the admin interface.</p>
  </sec>
  <sec id="scas-dashboard---example-use-case">
    <title>SCAS dashboard - example use case</title>
    <p>To exemplify an analysis with the SCAS dashboard, we assumed that
    users reported longer pending times for GPU resources in recent
    months. We have simulated this case by increasing the number of GPU
    jobs (and their pending times) for GPU servers, with 16 GPUs, over a
    time frame of 1 year. As shown in
    <xref alt="Figure 2b,c" rid="figU003Afig2">Figure 2b,c</xref>, the
    increase in the number of GPU jobs and CPU hours for the GPU
    partition is visible and confirms the assumption. By inspecting the
    pending times per day
    (<xref alt="Figure 2d" rid="figU003Afig2">Figure 2d</xref>), there
    is a general, unbiased increase of the pending times visible for the
    last few months. From
    <xref alt="Figure 2e" rid="figU003Afig2">Figure 2e</xref> we can
    then see an increase of the pending times for the GPU partition for
    the previous 6 months.
    <xref alt="Figure 2f" rid="figU003Afig2">Figure 2f</xref> shows that
    the increase of the pending times is only seen for servers with
    &gt;10 GPUs, and the utilization of the nodes with 16 GPUs has
    increased while those with 2 and 4 GPUs were stable
    (<xref alt="Figure 2g" rid="figU003Afig2">Figure 2g</xref>). This
    analysis can be used to draw concrete conclusions, in this case, to
    either inform the users that resources are available if up to 4 GPUs
    are requested, or to make the decision to invest in new GPU servers
    to achieve shorter pending times and higher throughput.</p>
    <fig>
      <caption><p><bold>a</bold>. User interface of the SCAS Dashboard
      featuring navigation and the selection menu. The central panel
      displays statistics and graphics. <bold>b</bold>. Line plot
      showing the jobs run per month. <bold>c</bold>. Line plot showing
      the GPU hours per month. <bold>d</bold>. Heatmap plot showing the
      average daily pending times of the jobs. <bold>e</bold>. Line plot
      with the average jobs pending times per month. The positive error
      bars indicate the standard deviation. <bold>f</bold>. Line plot
      with the average jobs pending times per month separated by GPU
      categories. The positive error bars indicate the standard
      deviation. <bold>g</bold>. Line plot showing the utilization of
      nodes with different numbers of GPUs.
      <styled-content id="figU003Afig2"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="paper.jats/Figure2.png" />
    </fig>
  </sec>
</sec>
<sec id="conclusion-and-availability">
  <title>Conclusion and Availability</title>
  <p>The SCAS dashboard enables rapid and responsive analysis of
  Slurm-based cluster usage. This allows stakeholders: I) to identify
  current bottlenecks of CPU and GPU utilization, II) to make informed
  decisions to adapt SLURM parameters in the short term, and III) to
  support strategic decisions, all based on user needs. The SCAS
  dashboard, code, and the documentation are hosted on a publicly
  available GitHub repository
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/Bioinformatics-Munich/scas_dashboard">https://github.com/Bioinformatics-Munich/scas_dashboard</ext-link>).
  The repository also contains a docker-compose file for rapid
  deployment and testing of the software, as well as a program to
  generate test data for the dashboard.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge the Institute of Computational Biology
  (Prof. Dr. Dr. Fabian Theis) at Helmholtz Munich for supporting the
  development of the software. We thank Dr. Bastian Rieck, Helmholtz
  Munich, for valuable contributions and comments to the manuscript.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-grafanadb">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Dessalvi</surname><given-names>Matteo</given-names></name>
      </person-group>
      <article-title>SLURM Dashboard</article-title>
      <publisher-name>https://grafana.com/grafana/dashboards/4323</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-R">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>R Core Team</string-name>
      </person-group>
      <source>R: A language and environment for statistical computing</source>
      <publisher-name>R Foundation for Statistical Computing</publisher-name>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.R-project.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-shiny">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Chang</surname><given-names>Winston</given-names></name>
        <name><surname>Cheng</surname><given-names>Joe</given-names></name>
        <name><surname>Allaire</surname><given-names>JJ</given-names></name>
        <name><surname>Sievert</surname><given-names>Carson</given-names></name>
        <name><surname>Schloerke</surname><given-names>Barret</given-names></name>
        <name><surname>Xie</surname><given-names>Yihui</given-names></name>
        <name><surname>Allen</surname><given-names>Jeff</given-names></name>
        <name><surname>McPherson</surname><given-names>Jonathan</given-names></name>
        <name><surname>Dipert</surname><given-names>Alan</given-names></name>
        <name><surname>Borges</surname><given-names>Barbara</given-names></name>
      </person-group>
      <source>Shiny: Web application framework for r</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-shinydashboard">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Chang</surname><given-names>Winston</given-names></name>
        <name><surname>Borges Ribeiro</surname><given-names>Barbara</given-names></name>
      </person-group>
      <source>Shinydashboard: Create dashboards with ’shiny’</source>
      <year iso-8601-date="2021">2021</year>
      <uri>http://rstudio.github.io/shinydashboard/</uri>
    </element-citation>
  </ref>
  <ref id="ref-slurm">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yoo</surname><given-names>Andy B.</given-names></name>
        <name><surname>Jette</surname><given-names>Morris A.</given-names></name>
        <name><surname>Grondona</surname><given-names>Mark</given-names></name>
      </person-group>
      <article-title>SLURM: Simple linux utility for resource management</article-title>
      <source>Job scheduling strategies for parallel processing</source>
      <person-group person-group-type="editor">
        <name><surname>Feitelson</surname><given-names>Dror</given-names></name>
        <name><surname>Rudolph</surname><given-names>Larry</given-names></name>
        <name><surname>Schwiegelshohn</surname><given-names>Uwe</given-names></name>
      </person-group>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <publisher-loc>Berlin, Heidelberg</publisher-loc>
      <year iso-8601-date="2003">2003</year>
      <isbn>978-3-540-39727-4</isbn>
      <pub-id pub-id-type="doi">10.1007/10968987_3</pub-id>
      <fpage>44</fpage>
      <lpage>60</lpage>
    </element-citation>
  </ref>
  <ref id="ref-xdmod">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Palmer</surname><given-names>Jeffrey T.</given-names></name>
        <name><surname>Gallo</surname><given-names>Steven M.</given-names></name>
        <name><surname>Furlani</surname><given-names>Thomas R.</given-names></name>
        <name><surname>Jones</surname><given-names>Matthew D.</given-names></name>
        <name><surname>DeLeon</surname><given-names>Robert L.</given-names></name>
        <name><surname>White</surname><given-names>Joseph P.</given-names></name>
        <name><surname>Simakov</surname><given-names>Nikolay</given-names></name>
        <name><surname>Patra</surname><given-names>Abani K.</given-names></name>
        <name><surname>Sperhac</surname><given-names>Jeanette</given-names></name>
        <name><surname>Yearke</surname><given-names>Thomas</given-names></name>
        <name><surname>Rathsam</surname><given-names>Ryan</given-names></name>
        <name><surname>Innus</surname><given-names>Martins</given-names></name>
        <name><surname>Cornelius</surname><given-names>Cynthia D.</given-names></name>
        <name><surname>Browne</surname><given-names>James C.</given-names></name>
        <name><surname>Barth</surname><given-names>William L.</given-names></name>
        <name><surname>Evans</surname><given-names>Richard T.</given-names></name>
      </person-group>
      <article-title>Open XDMoD: A tool for the comprehensive management of high-performance computing resources</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <year iso-8601-date="2015">2015</year>
      <volume>17</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2015.68</pub-id>
      <fpage>52</fpage>
      <lpage>62</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Hudak2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hudak</surname><given-names>Dave</given-names></name>
        <name><surname>Johnson</surname><given-names>Doug</given-names></name>
        <name><surname>Chalker</surname><given-names>Alan</given-names></name>
        <name><surname>Nicklas</surname><given-names>Jeremy</given-names></name>
        <name><surname>Franz</surname><given-names>Eric</given-names></name>
        <name><surname>Dockendorf</surname><given-names>Trey</given-names></name>
        <name><surname>McMichael</surname><given-names>Brian L.</given-names></name>
      </person-group>
      <article-title>Open OnDemand: A web-based client portal for HPC centers</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>25</issue>
      <uri>https://doi.org/10.21105/joss.00622</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00622</pub-id>
      <fpage>622</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
