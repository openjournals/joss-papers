body = <<-EOF
<meta name="citation_title" content="edarf: <strong>E</strong>xploratory <strong>D</strong>ata <strong>A</strong>nalysis using <strong>R</strong>andom Forests">
<meta name="citation_author" content="M. Jones, Zachary"><meta name="citation_author" content="J. Linder, Fridolin">
<meta name="citation_publication_date" content="2016/10/23">
<meta name="citation_journal_title" content="The Journal of Open Source Software">
<meta name="citation_pdf_url" content="https://github.com/openjournals/joss-papers/blob/master/joss.00092/10.21105.joss.00092.pdf">
<div class="accepted-paper">
  <h1>edarf: <strong>E</strong>xploratory <strong>D</strong>ata <strong>A</strong>nalysis using <strong>R</strong>andom Forests</h1>
  <div class="columns links">
    <div class="column four-fifths" style="padding-bottom: 10px;">
      <strong>Authors</strong>
      <ul class="author-list">
            <li><a href="http://orcid.org/0000-0002-7523-0471" target="_blank">Zachary M. Jones</a></li>
            <li><a href="http://orcid.org/0000-0002-0499-0676" target="_blank">Fridolin J. Linder</a></li>
            </ul>
    </div>
    <div class="one-third column">
      <span class="repo">Repository:<br /><a href="http://github.com/zmjones/edarf/">Repository link &raquo;</a></span>
    </div>
    <div class="one-third column">
      <span class="paper">Paper:<br /><a href="https://github.com/openjournals/joss-papers/blob/master/joss.00092/10.21105.joss.00092.pdf">PDF link &raquo;</a></span>
    </div>
    <div class="one-third column">
      <span class="paper">Review:<br /><a href="https://github.com/openjournals/joss-reviews/issues/92">View review issue &raquo;</a></span>
    </div>

    <div class="one-third column" style="padding-top: 20px;">
      <span class="repo">DOI:<br /><a href="http://dx.doi.org/10.21105/joss.00092">http://dx.doi.org/10.21105/joss.00092</a></span>
    </div>
    <div class="one-third column" style="padding-top: 20px;">
      <span class="paper">Status badge:<br /><img src="http://joss.theoj.org/papers/10.21105/joss.00092/status.svg"></span>
    </div>
    <div class="one-third column" style="padding-top: 20px;">
      <span class="paper">Cite this paper:<br /><a href="http://www.doi2bib.org/#/doi/10.21105/joss.00092" target="_blank">doi2bib</a>
    </div>
  </div>
  <div class="paper-body">
  <h1 id="summary">Summary</h1>
  <p>This package contains functions useful for exploratory data analysis using random forests, which can be fit using the <a href="https://cran.r-project.org/package=randomForest">randomForest</a>, <a href="https://cran.r-project.org/package=randomForestSRC">randomForestSRC</a>, or <a href="https://cran.r-project.org/package=party">party</a> packages <span class="citation">(Liaw and Wiener 2002; Ishwaran and Kogalur 2013; Hothorn, Hornik, and Zeileis 2006)</span>. These functions can compute the partial dependence of covariates (individually or in combination) on the fitted forests' predictions, the permutation importance of covariates, as well as the distance between data points according to the fitted model.</p>
  <p>Random forests are an attractive method for social scientists. Random forests have only a few important tuning parameters and can be adapted to do classification, regression, and clustering. Many research tasks require interpretable models, and, although methods for interpreting random forests exist, in the most popular packages for fitting random forests these methods are available inconsistently. For example although there are methods for computing permutation importance in the <code>randomForest</code>, <code>randomForestSRC</code> and <code>party</code> packages, only <code>randomForest</code> can compute local importance. None of the packages can compute permutation importance for groups of covariates. Similarly partial dependence is only implemented in <code>randomForest</code>, and it has limited functionality compared to the functions provided herein. This software has been used in <span class="citation">Jones and Linder (2015)</span>; <span class="citation">Jones and Lupu (2016)</span>.</p>
  <div class="figure">
  <img src="https://cloud.githubusercontent.com/assets/4483/19628406/09155832-992b-11e6-93db-f638287468ea.png" width="800px" alt="The partial dependence of several covariates (each considered separately) on the probability that a convict voted in the 2012 presidential election, given that they had registered to do so. Data is from Gerber et al. (2015)." />
  <p class="caption">The partial dependence of several covariates (each considered separately) on the probability that a convict voted in the 2012 presidential election, given that they had registered to do so. Data is from <span class="citation">Gerber et al. (2015)</span>.</p>
  </div>
  <p>Partial dependence, as described by <span class="citation">Friedman (2001)</span>, estimates the marginal relationship between a subset of the covariates and the model's predictions by averaging over the marginal distribution of the compliment of this subset of the covariates. This approximation allows the display of the relationship between this subset of the covariates and the model's predictions even when there are many covariates which may interact. This functionality works with models fit by any of the aforementioned packages to any of the supported types of outcome variables. <code>partial_dependence</code> can be parallelized and also contains a number of additional parameters which allow the user to control this approximation. There is an associated plot function <code>plot_pd</code> which constructs plots for a wide variety of possible outputs from <code>partial_dependence</code> (e.g., when pairs of covariates are considered jointly, when each covariate is considered separately, when the outcome variable is categorical, etc).</p>
  <div class="figure">
  <img src="https://cloud.githubusercontent.com/assets/4483/19628404/0911fea8-992b-11e6-924a-5689bf69dab4.png" width="800px" alt="The permutation importance of the same covariates." />
  <p class="caption">The permutation importance of the same covariates.</p>
  </div>
  <p>Permutation importance estimates the importance of a covariate by randomly shuffling its values, breaking any dependence between said covariate and the outcome, and then computing the difference between the predictions made by the model with that covariate shuffled and the predictions made when the covariate was not shuffled. If the covariate was useful in generating predictions then the prediction errors will increase in expectation when the covariate is shuffled, whereas no such increase can be expected when the covariate has no influence. Although all three of the random forest packages provide at least one method of assessing variable importance, <code>variable_importance</code> provides a consistent way to compute permutation importance across all packages. <code>variable_importance</code> can also compute local importance. Rather than computing the average difference in prediction errors between the permuted and unpermuted data across all the training data (giving one number for each covariate), local importance computes the average change in the prediction error for each observation. This can be examined directly, or, in the case of a categorical outcome variable, can be aggregated to each class level. In the case of a continuous outcome variable the change in the prediction errors can be smoothed at different values of the outcome variable, giving a similar display. Lastly, <code>variable_importance</code> can operate on multiple variables simultaneously, giving the joint permutation importance of a set of covariates. This can be used to detect interactions by comparing the permutation importance of, for example, two covariates, by computing their joint permutation importance and comparing that to the sum of their individual permutation importance estimates. <code>plot_imp</code> provides a visualizations for all possible outputs.</p>
  <div class="figure">
  <img src="https://cloud.githubusercontent.com/assets/4483/19628405/091574f2-992b-11e6-8282-8955860f3d69.png" width="800px" alt="The proximity of the training data, colored according to the encouragement condition implemented by Gerber et al. (2015), with the shape of the points mapped to whether the individual had voted in the 2008 election and the size of the point mapped to the individual&#39;s age." />
  <p class="caption">The proximity of the training data, colored according to the encouragement condition implemented by <span class="citation">Gerber et al. (2015)</span>, with the shape of the points mapped to whether the individual had voted in the 2008 election and the size of the point mapped to the individual's age.</p>
  </div>
  <p>Due to the tree-structure of a random forest, there is a natural way to define a distance between data points: the proportion of times that the data points were in the same terminal node. This is called &quot;proximity&quot; and can be used do model-based clustering when the random forest was unsupervised, and can be used to visualize the model in the supervised case. Making generic the compuation of the proxmity matrix would require a consistent API for accessing information in the individual trees in the random forest, which does not exist, however, <code>extract_proximity</code> can extract a proximity matrix computed by one of the supported packages. These matrices are too high dimensional to be visualized directly, so <code>plot_prox</code> supports the visualization of two of the principal components of this matrix, as estimated by <code>prcomp</code> (included in the base distribution of <code>R</code>). <code>plot_prox</code> provides arguments which additionally allow the user to change the color, shape, and size of points according to auxillary information, such as the observed class label for categorical outcomes, or a covariates, which may aid the aforementioned visualization tasks.</p>
  <p></p>
  <h1 id="references" class="unnumbered">References</h1>
  <div id="refs" class="references">
  <div id="ref-friedman2001greedy">
  <p>Friedman, Jerome H. 2001. &#8220;Greedy Function Approximation: A Gradient Boosting Machine.&#8221; <em>The Annals of Statistics</em> 29 (5). The Institute of Mathematical Statistics: 1189&#8211;1232. doi:<a href="https://doi.org/10.1214/aos/1013203451">10.1214/aos/1013203451</a>.</p>
  </div>
  <div id="ref-gerber2014can">
  <p>Gerber, Alan S., Gregory A. Huber, Marc Meredith, Daniel R. Biggers, and David J. Hendry. 2015. &#8220;Can Incarcerated Felons Be (Re)integrated into the Political System? Results from a Field Experiment.&#8221; <em>American Journal of Political Science</em> 59 (4): 912&#8211;26. doi:<a href="https://doi.org/10.1111/ajps.12166">10.1111/ajps.12166</a>.</p>
  </div>
  <div id="ref-party">
  <p>Hothorn, Torsten, Kurt Hornik, and Achim Zeileis. 2006. &#8220;Unbiased Recursive Partitioning: A Conditional Inference Framework.&#8221; <em>Journal of Computational and Graphical Statistics</em> 15 (3). Taylor &amp; Francis: 651&#8211;74. doi:<a href="https://doi.org/10.1198/106186006X133933">10.1198/106186006X133933</a>.</p>
  </div>
  <div id="ref-randomForestSRC">
  <p>Ishwaran, H, and U Kogalur. 2013. &#8220;Random Forests for Survival, Regression and Classification (Rf-Src).&#8221; <em>URL Http://Cran.r-Project.org/Web/Packages/RandomForestSRC/. R Package Version</em> 1.</p>
  </div>
  <div id="ref-jones2015exploratory">
  <p>Jones, Zachary M., and Fridolin Linder. 2015. &#8220;Exploratory Data Analysis Using Random Forests.&#8221; In <em>Proceedings of the 73rd Annual Mpsa Conference</em>. <a href="http://zmjones.com/static/papers/rfss_manuscript.pdf" class="uri">http://zmjones.com/static/papers/rfss_manuscript.pdf</a>.</p>
  </div>
  <div id="ref-jones2016there">
  <p>Jones, Zachary M., and Yonatan Lupu. 2016. &#8220;Is There More Violence in the Middle?&#8221;</p>
  </div>
  <div id="ref-randomForest">
  <p>Liaw, Andy, and Matthew Wiener. 2002. &#8220;Classification and Regression by RandomForest.&#8221; <em>R News</em> 2 (3): 18&#8211;22. <a href="http://CRAN.R-project.org/doc/Rnews/" class="uri">http://CRAN.R-project.org/doc/Rnews/</a>.</p>
  </div>
  </div>
  </div>
</div>
EOF
