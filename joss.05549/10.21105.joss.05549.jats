<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5549</article-id>
<article-id pub-id-type="doi">10.21105/joss.05549</article-id>
<title-group>
<article-title>OmniTrax: A deep learning-driven multi-animal tracking
and pose-estimation add-on for Blender</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1012-6646</contrib-id>
<name>
<surname>Plum</surname>
<given-names>Fabian</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Imperial College London, Department of Bioengineering,
United Kingdom</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-03-06">
<day>6</day>
<month>3</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>95</issue>
<fpage>5549</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Blender</kwd>
<kwd>multi-object tracking</kwd>
<kwd>pose-estimation</kwd>
<kwd>deep learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>OmniTrax</monospace> is a deep learning-driven
  multi-animal tracking and pose-estimation Blender Add-on
  (<xref alt="Blender-Online-Community, 2022" rid="ref-Blender" ref-type="bibr">Blender-Online-Community,
  2022</xref>). <monospace>OmniTrax</monospace> provides an intuitive
  high-throughput tracking solution for large groups of freely moving
  subjects by leveraging recent advancements in deep-learning based
  detection
  (<xref alt="Bochkovskiy et al., 2020" rid="ref-YOLOv4" ref-type="bibr">Bochkovskiy
  et al., 2020</xref>;
  <xref alt="Redmon &amp; Farhadi, 2018" rid="ref-YOLOv3" ref-type="bibr">Redmon
  &amp; Farhadi, 2018</xref>) and computationally inexpensive
  buffer-and-recover tracking approaches. Combining automated tracking
  with the Blender-internal motion tracking pipeline allows to
  streamline the annotation and analysis process of large video files
  with hundreds of freely moving individuals. Additionally,
  <monospace>OmniTrax</monospace> integrates DeepLabCut-Live
  (<xref alt="Kane et al., 2020" rid="ref-Kane2020dlclive" ref-type="bibr">Kane
  et al., 2020</xref>) to enable running markerless pose-estimation on
  arbitrary numbers of animals. We leverage the existing DeepLabCut
  Model Zoo
  (<xref alt="Mathis et al., 2018" rid="ref-Mathisetal2018" ref-type="bibr">Mathis
  et al., 2018</xref>) as well as custom-trained detector and
  pose-estimator networks to facilitate large-scale behavioural studies
  of social animals.</p>
  <fig>
    <caption><p>OmniTrax
    user-interface.<styled-content id="figU003Ademo"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="media/eec8afc23dd1fa3760ad47b9ad66979f9500f781.jpg" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Deep learning-based computer vision approaches promise to transform
  the landscape of large-scale human and other animal behavioural
  research. The goal of <monospace>OmniTrax</monospace> is to provide an
  interactive inference pipeline that decreases the entry barrier for
  researchers who wish to streamline annotation and analysis processes
  using deep learning-driven computer vision tools.
  <monospace>OmniTrax</monospace> is designed to track and infer the
  pose of large numbers of freely moving animals. Unlike background
  subtraction, blob-detector, or optical-flow based approaches, common
  in multi-animal tracking
  (<xref alt="Kalafatic et al., 2001" rid="ref-Kalafatic2001" ref-type="bibr">Kalafatic
  et al., 2001</xref>;
  <xref alt="Pérez-Escudero et al., 2014" rid="ref-Perez-Escudero2014" ref-type="bibr">Pérez-Escudero
  et al., 2014</xref>;
  <xref alt="Walter &amp; Couzin, 2021" rid="ref-Walter2021" ref-type="bibr">Walter
  &amp; Couzin, 2021</xref>), the use of deep learning-based detectors
  allows for buffer-and-recover tracking in changing environments.
  <monospace>OmniTrax</monospace> automatically initiates new tracks
  when an animal is detected for the first time and terminates the track
  when the animal leaves the recording site or becomes occluded for
  prolonged periods. <monospace>OmniTrax</monospace> uses a tuneable
  Kalman-Filter and the Hungarian Method
  (<xref alt="Kuhn, 1955" rid="ref-Kuhn1955" ref-type="bibr">Kuhn,
  1955</xref>) for cost assignment to extrapolate the 2D trajectories of
  identified animals across frames, so that temporarily missing
  detections, e.g. due to occlusion, motion blur, or other changes in
  appearance, can be compensated.</p>
  <p>A key advantage of integrating such a pipeline into Blender is the
  seamless transition between automated tracking and iterative
  user-refinement. Additionally, Blender offers a number of video
  editing and compositing functions which make it possible to perform
  pre-processing of the imported video footage. This includes cropping,
  masking, or exposure adjustment, prior to running inference within the
  same environment, without relying on external software packages.</p>
  <p><monospace>OmniTrax</monospace> additionally offers markerless
  pose-estimation through DeepLabCut-Live
  (<xref alt="Kane et al., 2020" rid="ref-Kane2020dlclive" ref-type="bibr">Kane
  et al., 2020</xref>) which enables extracting kinematic parameters
  from virtually arbitrarily large groups of individuals. We are using
  <monospace>OmniTrax</monospace> in ongoing research monitoring
  foraging activities of various species of leafcutter ants, tracking
  the movements of thousands of individuals to extract path choice and
  changes to gait patterns.</p>
  <p>Through a library of neural networks trained on hand-annotated as
  well as synthetically generated samples of a number of digitised study
  organisms
  (<xref alt="Plum et al., 2023" rid="ref-Plumetal2023" ref-type="bibr">Plum
  et al., 2023</xref>;
  <xref alt="Plum &amp; Labonte, 2021" rid="ref-Plum2021" ref-type="bibr">Plum
  &amp; Labonte, 2021</xref>), we provide a range of out-of-the-box
  inference solutions and encourage the community to contribute to this
  emerging collection. Pre-trained detection and pose-estimation
  networks can be used within <monospace>OmniTrax</monospace> to
  accelerate the annotation and analysis process of large video data
  sets. The ease of use and focus on extendibility of
  <monospace>OmniTrax</monospace> will aid researchers in performing
  complex behavioural studies of social animals under laboratory as well
  as challenging field conditions.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This study was funded by the Imperial College’s President’s PhD
  Scholarship (to Fabian Plum) and is part of a project that has
  received funding from the European Research Council (ERC) under the
  European Union’s Horizon 2020 research and innovation programme (Grant
  agreement No. 851705, to David Labonte). The funders had no role in
  study design, data collection and analysis, decision to publish, or
  preparation of the manuscript.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Blender">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Blender-Online-Community</surname></name>
      </person-group>
      <article-title>Blender - a 3D modelling and rendering package</article-title>
      <publisher-name>Blender Foundation</publisher-name>
      <publisher-loc>Stichting Blender Foundation, Amsterdam</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <uri>http://www.blender.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-YOLOv4">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bochkovskiy</surname><given-names>Alexey</given-names></name>
        <name><surname>Wang</surname><given-names>Chien-Yao</given-names></name>
        <name><surname>Liao</surname><given-names>Hong-Yuan Mark</given-names></name>
      </person-group>
      <article-title>YOLOv4: Optimal speed and accuracy of object detection</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2004.10934</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2004.10934</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-YOLOv3">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Redmon</surname><given-names>Joseph</given-names></name>
        <name><surname>Farhadi</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>YOLOv3: An incremental improvement</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2018">2018</year>
      <volume>abs/1804.02767</volume>
      <uri>http://arxiv.org/abs/1804.02767</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1804.02767</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kuhn1955">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kuhn</surname><given-names>Harold W</given-names></name>
      </person-group>
      <article-title>The Hungarian method for the assignment problem</article-title>
      <source>Naval Research Logistics (NRL)</source>
      <year iso-8601-date="1955">1955</year>
      <volume>52</volume>
      <pub-id pub-id-type="doi">10.1002/nav.3800020109</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Mathisetal2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mathis</surname><given-names>Alexander</given-names></name>
        <name><surname>Mamidanna</surname><given-names>Pranav</given-names></name>
        <name><surname>Cury</surname><given-names>Kevin M.</given-names></name>
        <name><surname>Abe</surname><given-names>Taiga</given-names></name>
        <name><surname>Murthy</surname><given-names>Venkatesh N.</given-names></name>
        <name><surname>Mathis</surname><given-names>Mackenzie W.</given-names></name>
        <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>
      <source>Nature Neuroscience</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kane2020dlclive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kane</surname><given-names>Gary</given-names></name>
        <name><surname>Lopes</surname><given-names>Gonçalo</given-names></name>
        <name><surname>Sanders</surname><given-names>Jonny</given-names></name>
        <name><surname>Mathis</surname><given-names>Alexander</given-names></name>
        <name><surname>Mathis</surname><given-names>Mackenzie</given-names></name>
      </person-group>
      <article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title>
      <source>eLife</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Plum2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Plum</surname><given-names>Fabian</given-names></name>
        <name><surname>Labonte</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>scAnt  an open-source platform for the creation of 3D models of arthropods (and other small objects)</article-title>
      <source>PeerJ</source>
      <publisher-name>PeerJ</publisher-name>
      <year iso-8601-date="2021-04">2021</year><month>04</month>
      <volume>9</volume>
      <uri>https://doi.org/10.7717%2Fpeerj.11155</uri>
      <pub-id pub-id-type="doi">10.7717/peerj.11155</pub-id>
      <fpage>e11155</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Plumetal2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Plum</surname><given-names>Fabian</given-names></name>
        <name><surname>Bulla</surname><given-names>René</given-names></name>
        <name><surname>Beck</surname><given-names>Hendrik K</given-names></name>
        <name><surname>Imirzian</surname><given-names>Natalie</given-names></name>
        <name><surname>Labonte</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>replicAnt: A pipeline for generating annotated images of animals in complex environments using unreal engine</article-title>
      <source>Nature Communications</source>
      <year iso-8601-date="2023">2023</year>
      <volume>14</volume>
      <issn>2041-1723</issn>
      <uri>https://doi.org/10.1038/s41467-023-42898-9</uri>
      <pub-id pub-id-type="doi">10.1038/s41467-023-42898-9</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Kalafatic2001">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kalafatic</surname><given-names>Zoran</given-names></name>
        <name><surname>Ribaric</surname><given-names>Slobodan</given-names></name>
        <name><surname>Stanisavljevic</surname><given-names>Vladimir</given-names></name>
      </person-group>
      <article-title>A system for tracking laboratory animals based on optical flow and active contours</article-title>
      <source>Proceedings - 11th International Conference on Image Analysis and Processing, ICIAP 2001</source>
      <year iso-8601-date="2001">2001</year>
      <isbn>076951183X</isbn>
      <pub-id pub-id-type="doi">10.1109/ICIAP.2001.957031</pub-id>
      <fpage>334</fpage>
      <lpage>339</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Walter2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Walter</surname><given-names>Tristan</given-names></name>
        <name><surname>Couzin</surname><given-names>Iain D.</given-names></name>
      </person-group>
      <article-title>Trex, a fast multi-animal tracking system with markerless identi cation, and 2d estimation of posture and visual elds</article-title>
      <source>eLife</source>
      <year iso-8601-date="2021">2021</year>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.7554/eLife.64000</pub-id>
      <pub-id pub-id-type="pmid">33634789</pub-id>
      <fpage>1</fpage>
      <lpage>73</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Perez-Escudero2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pérez-Escudero</surname><given-names>Alfonso</given-names></name>
        <name><surname>Vicente-Page</surname><given-names>Julián</given-names></name>
        <name><surname>Hinz</surname><given-names>Robert C</given-names></name>
        <name><surname>Arganda</surname><given-names>Sara</given-names></name>
        <name><surname>De Polavieja</surname><given-names>Gonzalo G</given-names></name>
      </person-group>
      <article-title>IdTracker: Tracking individuals in a group by automatic identification of unmarked animals</article-title>
      <source>Nature Methods</source>
      <year iso-8601-date="2014">2014</year>
      <volume>11</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.1038/nmeth.2994</pub-id>
      <fpage>743</fpage>
      <lpage>748</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
