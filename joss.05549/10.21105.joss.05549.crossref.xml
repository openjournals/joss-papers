<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20240314T195636-93132f7f46f786b33c9da4f3c3dbf5a433922a62</doi_batch_id>
    <timestamp>20240314195636</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>03</month>
          <year>2024</year>
        </publication_date>
        <journal_volume>
          <volume>9</volume>
        </journal_volume>
        <issue>95</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>OmniTrax: A deep learning-driven multi-animal tracking
and pose-estimation add-on for Blender</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Fabian</given_name>
            <surname>Plum</surname>
            <ORCID>https://orcid.org/0000-0003-1012-6646</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>03</month>
          <day>14</day>
          <year>2024</year>
        </publication_date>
        <pages>
          <first_page>5549</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05549</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.10817891</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5549</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05549</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05549</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05549.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="Blender">
            <article_title>Blender - a 3D modelling and rendering
package</article_title>
            <author>Blender-Online-Community</author>
            <cYear>2022</cYear>
            <unstructured_citation>Blender-Online-Community. (2022).
Blender - a 3D modelling and rendering package. Blender Foundation.
http://www.blender.org</unstructured_citation>
          </citation>
          <citation key="YOLOv4">
            <article_title>YOLOv4: Optimal speed and accuracy of object
detection</article_title>
            <author>Bochkovskiy</author>
            <doi>10.48550/ARXIV.2004.10934</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Bochkovskiy, A., Wang, C.-Y., &amp;
Liao, H.-Y. M. (2020). YOLOv4: Optimal speed and accuracy of object
detection.
https://doi.org/10.48550/ARXIV.2004.10934</unstructured_citation>
          </citation>
          <citation key="YOLOv3">
            <article_title>YOLOv3: An incremental
improvement</article_title>
            <author>Redmon</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1804.02767</volume>
            <doi>10.48550/arXiv.1804.02767</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Redmon, J., &amp; Farhadi, A. (2018).
YOLOv3: An incremental improvement. CoRR, abs/1804.02767.
https://doi.org/10.48550/arXiv.1804.02767</unstructured_citation>
          </citation>
          <citation key="Kuhn1955">
            <article_title>The Hungarian method for the assignment
problem</article_title>
            <author>Kuhn</author>
            <journal_title>Naval Research Logistics
(NRL)</journal_title>
            <volume>52</volume>
            <doi>10.1002/nav.3800020109</doi>
            <cYear>1955</cYear>
            <unstructured_citation>Kuhn, H. W. (1955). The Hungarian
method for the assignment problem. Naval Research Logistics (NRL), 52.
https://doi.org/10.1002/nav.3800020109</unstructured_citation>
          </citation>
          <citation key="Mathisetal2018">
            <article_title>DeepLabCut: Markerless pose estimation of
user-defined body parts with deep learning</article_title>
            <author>Mathis</author>
            <journal_title>Nature Neuroscience</journal_title>
            <doi>10.1038/s41593-018-0209-y</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Mathis, A., Mamidanna, P., Cury, K.
M., Abe, T., Murthy, V. N., Mathis, M. W., &amp; Bethge, M. (2018).
DeepLabCut: Markerless pose estimation of user-defined body parts with
deep learning. Nature Neuroscience.
https://doi.org/10.1038/s41593-018-0209-y</unstructured_citation>
          </citation>
          <citation key="Kane2020dlclive">
            <article_title>Real-time, low-latency closed-loop feedback
using markerless posture tracking</article_title>
            <author>Kane</author>
            <journal_title>eLife</journal_title>
            <doi>10.7554/eLife.61909</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Kane, G., Lopes, G., Sanders, J.,
Mathis, A., &amp; Mathis, M. (2020). Real-time, low-latency closed-loop
feedback using markerless posture tracking. eLife.
https://doi.org/10.7554/eLife.61909</unstructured_citation>
          </citation>
          <citation key="Plum2021">
            <article_title>scAnt an open-source platform for the
creation of 3D models of arthropods (and other small
objects)</article_title>
            <author>Plum</author>
            <journal_title>PeerJ</journal_title>
            <volume>9</volume>
            <doi>10.7717/peerj.11155</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Plum, F., &amp; Labonte, D. (2021).
scAnt an open-source platform for the creation of 3D models of
arthropods (and other small objects). PeerJ, 9, e11155.
https://doi.org/10.7717/peerj.11155</unstructured_citation>
          </citation>
          <citation key="Plumetal2023">
            <article_title>replicAnt: A pipeline for generating
annotated images of animals in complex environments using unreal
engine</article_title>
            <author>Plum</author>
            <journal_title>Nature Communications</journal_title>
            <volume>14</volume>
            <doi>10.1038/s41467-023-42898-9</doi>
            <issn>2041-1723</issn>
            <cYear>2023</cYear>
            <unstructured_citation>Plum, F., Bulla, R., Beck, H. K.,
Imirzian, N., &amp; Labonte, D. (2023). replicAnt: A pipeline for
generating annotated images of animals in complex environments using
unreal engine. Nature Communications, 14.
https://doi.org/10.1038/s41467-023-42898-9</unstructured_citation>
          </citation>
          <citation key="Kalafatic2001">
            <article_title>A system for tracking laboratory animals
based on optical flow and active contours</article_title>
            <author>Kalafatic</author>
            <journal_title>Proceedings - 11th International Conference
on Image Analysis and Processing, ICIAP 2001</journal_title>
            <doi>10.1109/ICIAP.2001.957031</doi>
            <isbn>076951183X</isbn>
            <cYear>2001</cYear>
            <unstructured_citation>Kalafatic, Z., Ribaric, S., &amp;
Stanisavljevic, V. (2001). A system for tracking laboratory animals
based on optical flow and active contours. Proceedings - 11th
International Conference on Image Analysis and Processing, ICIAP 2001,
334–339.
https://doi.org/10.1109/ICIAP.2001.957031</unstructured_citation>
          </citation>
          <citation key="Walter2021">
            <article_title>Trex, a fast multi-animal tracking system
with markerless identi cation, and 2d estimation of posture and visual
elds</article_title>
            <author>Walter</author>
            <journal_title>eLife</journal_title>
            <volume>10</volume>
            <doi>10.7554/eLife.64000</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Walter, T., &amp; Couzin, I. D.
(2021). Trex, a fast multi-animal tracking system with markerless identi
cation, and 2d estimation of posture and visual elds. eLife, 10, 1–73.
https://doi.org/10.7554/eLife.64000</unstructured_citation>
          </citation>
          <citation key="Perez-Escudero2014">
            <article_title>IdTracker: Tracking individuals in a group by
automatic identification of unmarked animals</article_title>
            <author>Pérez-Escudero</author>
            <journal_title>Nature Methods</journal_title>
            <issue>7</issue>
            <volume>11</volume>
            <doi>10.1038/nmeth.2994</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Pérez-Escudero, A., Vicente-Page, J.,
Hinz, R. C., Arganda, S., &amp; De Polavieja, G. G. (2014). IdTracker:
Tracking individuals in a group by automatic identification of unmarked
animals. Nature Methods, 11(7), 743–748.
https://doi.org/10.1038/nmeth.2994</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
