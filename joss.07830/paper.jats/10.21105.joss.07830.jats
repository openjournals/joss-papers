<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7830</article-id>
<article-id pub-id-type="doi">10.21105/joss.07830</article-id>
<title-group>
<article-title>jaxKAN: A unified JAX framework for Kolmogorov-Arnold
Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-2352-8709</contrib-id>
<name>
<surname>Rigas</surname>
<given-names>Spyros</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5650-4206</contrib-id>
<name>
<surname>Papachristou</surname>
<given-names>Michalis</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Digital Industry Technologies, School of
Science, National and Kapodistrian University of Athens</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Physics, School of Science, National and
Kapodistrian University of Athens</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-01-08">
<day>8</day>
<month>1</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>107</issue>
<fpage>7830</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>JAX</kwd>
<kwd>Kolmogorov-Arnold Networks</kwd>
<kwd>Physics-Informed Neural Networks</kwd>
<kwd>PIKANs</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>jaxKAN</monospace> is a JAX-based library for building
  and training Kolmogorov–Arnold Networks (KANs)
  (<xref alt="Liu et al., 2024" rid="ref-LiuU003A2024" ref-type="bibr">Liu
  et al., 2024</xref>), built on Flax’s NNX
  (<xref alt="Heek et al., 2024" rid="ref-flax" ref-type="bibr">Heek et
  al., 2024</xref>) with Optax
  (<xref alt="DeepMind et al., 2020" rid="ref-optax" ref-type="bibr">DeepMind
  et al., 2020</xref>) for optimization. It provides a broad selection
  of layer implementations - from the original KAN design to more recent
  or efficient variants - and unifies them under a single interface.
  Beyond basic model instance definition and training,
  <monospace>jaxKAN</monospace> supports class-inherent adaptive
  training methods (e.g., grid updates) and provides utilities that
  address performance limitations in the original KAN framework. KANs
  from <monospace>jaxKAN</monospace> can be used in any setting where
  standard multilayer perceptrons (MLPs) would otherwise be employed as
  the underlying architecture, although the library includes specialized
  utilities for adaptive Partial Differential Equation (PDE) solving
  tasks, thus placing emphasis on scientific applications with
  Physics-Informed Kolmogorov-Arnold Networks (PIKANs)
  (<xref alt="Shukla et al., 2024" rid="ref-KarniadakisU003A2024" ref-type="bibr">Shukla
  et al., 2024</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The recent introduction of KANs
  (<xref alt="Liu et al., 2024" rid="ref-LiuU003A2024" ref-type="bibr">Liu
  et al., 2024</xref>) in machine learning provided an alternative to
  traditional MLPs by making the network activation functions trainable,
  through expansions in terms of basis functions - originally B-splines.
  This novel idea proved to be effective across diverse applications,
  from engineering to scientific computing. Consequently, KANs were
  extended to incorporate alternative basis functions, such as
  polynomial
  (<xref alt="Sidharth et al., 2024" rid="ref-SidharthU003A2024" ref-type="bibr">Sidharth
  et al., 2024</xref>) or trigonometric
  (<xref alt="Gist Noesis, 2024" rid="ref-fkan" ref-type="bibr">Gist
  Noesis, 2024</xref>) functions. Moreover, more efficient architectures
  were proposed to overcome performance bottlenecks in the original
  design
  (<xref alt="Blealtan, 2024" rid="ref-effkan" ref-type="bibr">Blealtan,
  2024</xref>). As a result, a growing body of research has explored new
  ways to build, refine and utilize KANs, yet a comprehensive,
  high-performance software framework that integrates these ideas under
  one roof has been lacking.</p>
  <p>Existing libraries only partially address this need. For instance,
  <monospace>pykan</monospace>
  (<xref alt="Liu, 2024" rid="ref-pykan" ref-type="bibr">Liu,
  2024</xref>), the original <monospace>PyTorch</monospace>-based
  (<xref alt="Paszke et al., 2019" rid="ref-paszkeU003A2019" ref-type="bibr">Paszke
  et al., 2019</xref>) KAN implementation, does not provide
  expandability to multiple layer types. Similarly, frameworks like
  <monospace>NeuroMANCER</monospace>
  (<xref alt="Drgona et al., 2023" rid="ref-neuromancer" ref-type="bibr">Drgona
  et al., 2023</xref>), while valuable for physics-informed
  applications, restrict KAN usage mainly to scientific computing tasks
  via the use of high-level APIs, thus limiting their applicability for
  other types of problems or low-level integration into novel deep
  learning architectures. Additionally, while decentralized repositories
  exist for individual KAN layer types, there is no unified approach to
  constructing and training KANs, making it challenging to combine them
  into hybrid architectures or incorporate them into broader workflows.
  This fragmented landscape perpetuates the tendency towards assembling
  custom, case-specific solutions by researchers and practitioners.</p>
  <p><monospace>jaxKAN</monospace> fills this gap by providing a
  JAX-native codebase that encompasses multiple KAN layer types, from
  the original B-spline-based approach to alternative variants. Through
  its unified interface, users can not only choose from various basis
  expansions but also utilize built-in adaptive routines - such as grid
  updates, adaptive state transitions, or basis order extension - to
  tackle problems where static representations underperform. Moreover,
  it delivers specialized utilities for the adaptive training of PIKANs,
  a field where KANs have proven to be equally or even more efficient
  than MLPs in several occasions
  (<xref alt="Rigas et al., 2024" rid="ref-RigasU003A2024" ref-type="bibr">Rigas
  et al., 2024</xref>;
  <xref alt="Shukla et al., 2024" rid="ref-KarniadakisU003A2024" ref-type="bibr">Shukla
  et al., 2024</xref>). At the time of writing,
  <monospace>jaxKAN</monospace> has been used in a series of academic
  works relevant to PDE problems
  (<xref alt="Howard, Jacob, Murphy, et al., 2024" rid="ref-Howard1U003A2024" ref-type="bibr">Howard,
  Jacob, Murphy, et al., 2024</xref>;
  <xref alt="Howard, Jacob, &amp; Stinis, 2024" rid="ref-Howard2U003A2024" ref-type="bibr">Howard,
  Jacob, &amp; Stinis, 2024</xref>;
  <xref alt="Jacob et al., 2024" rid="ref-JacobU003A2024" ref-type="bibr">Jacob
  et al., 2024</xref>;
  <xref alt="Rigas et al., 2024" rid="ref-RigasU003A2024" ref-type="bibr">Rigas
  et al., 2024</xref>), and has showcased significantly faster
  performance compared to the original <monospace>pykan</monospace>
  implementation
  (<xref alt="Rigas et al., 2024" rid="ref-RigasU003A2024" ref-type="bibr">Rigas
  et al., 2024</xref>).</p>
</sec>
<sec id="core-functionality">
  <title>Core Functionality</title>
  <p>In the following, a brief discussion on the core functionality of
  <monospace>jaxKAN</monospace> is provided.</p>
  <sec id="layer-selection">
    <title>Layer Selection</title>
    <p>In <monospace>jaxKAN</monospace>, KANs are built as instances of
    the <monospace>KAN</monospace> class, which comprises one or more
    Kolmogorov–Arnold layers. These layers are defined within the
    <monospace>jaxkan.layers</monospace> module, which currently
    includes five types:</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>base</monospace>: the original B-spline-based
        layer.</p>
      </list-item>
      <list-item>
        <p><monospace>spline</monospace>: the efficient variant of the
        original B-spline-based layer
        (<xref alt="Blealtan, 2024" rid="ref-effkan" ref-type="bibr">Blealtan,
        2024</xref>).</p>
      </list-item>
      <list-item>
        <p><monospace>cheby</monospace>: a layer using Chebyshev
        polynomials.</p>
      </list-item>
      <list-item>
        <p><monospace>mod-cheby</monospace>: a layer using Chebyshev
        polynomials without incorporating trigonometric functions for
        their calculation
        (<xref alt="Shukla et al., 2024" rid="ref-KarniadakisU003A2024" ref-type="bibr">Shukla
        et al., 2024</xref>).</p>
      </list-item>
      <list-item>
        <p><monospace>fourier</monospace>: a layer utilizing sines and
        cosines as basis functions.</p>
      </list-item>
    </list>
    <p>While these cover the most commonly used basis functions, the
    library will remain under active development, aiming to add further
    variants as research continues and identifies promising new
    options.</p>
  </sec>
  <sec id="network-methods">
    <title>Network Methods</title>
    <p>Each KAN instance provides three primary methods: initialization
    for customizing layer parameters, a forward pass for passing inputs
    through the network, and an <monospace>update_grids</monospace>
    method for in-place adjustments to each layer’s grid. In particular,
    the <monospace>update_grids</monospace> method can be called during
    training with a new grid size as its argument to extend the
    network’s current grid and subsequently adapt the extended grid to
    the training data. Although grid updates are naturally interpreted
    in spline-based layers, they also extend to other types: for
    Chebyshev layers, the method increases the degree of the polynomial
    basis functions, while in Fourier layers the method adds more terms
    to the Fourier sums. Notably, this method requires a technique to
    solve batched least-squares problems in parallel - an operation
    currently not supported natively by JAX libraries but implemented
    internally in <monospace>jaxKAN</monospace> for optimal
    performance.</p>
  </sec>
  <sec id="pikan-utilities">
    <title>PIKAN utilities</title>
    <p>Beyond its low-level KAN functionality,
    <monospace>jaxKAN</monospace> supplies dedicated utilities for
    PIKANs. Users wishing to fine-tune every detail can extend the
    <monospace>KAN</monospace> class to their own needs, as illustrated
    in the documentation’s tutorials. Alternatively, a higher-level
    <monospace>train_PIKAN</monospace> function automates the end-to-end
    adaptive training loop, allowing a PIKAN to solve a forward PDE
    problem with minimal overhead.</p>
  </sec>
</sec>
<sec id="case-study">
  <title>Case Study</title>
  <p>As a case study, the Allen-Cahn equation, defined by</p>
  <p><named-content id="eqU003AAC" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[
  \frac{\partial u}{\partial t} - D\frac{\partial^2 u}{\partial x^2} + 5 \left(u^3 - u\right) = 0,]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mfrac><mml:mrow><mml:msup><mml:mi>∂</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>5</mml:mn><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>u</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
  <p>for <inline-formula><alternatives>
  <tex-math><![CDATA[D = 10^{-3}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mi>−</mml:mi><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  in the <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega = [0,1]\times [-1, 1]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Ω</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>×</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>−</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  domain, subject to the boundary conditions</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[ u\left(t=0, x\right) = x^2 \cos\left(\pi x\right), ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>π</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[ u\left(t, x=-1\right) = u\left(t, x=1\right) = -1, ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>is solved, by training a network of <monospace>spline</monospace>
  layers for <inline-formula><alternatives>
  <tex-math><![CDATA[5\cdot 10^4]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  epochs.</p>
  <fig>
    <caption><p>Upper row: reference solution to
    <xref alt="Equation 1" rid="eqU003AAC">Equation 1</xref>. Middle
    row: approximation by vanilla PIKAN (left) and adaptive PIKAN
    (right). Lower row: absolute errors relative to the reference
    solution.<styled-content id="figU003AAC"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="AC.png" />
  </fig>
  <p>The reference solution to
  <xref alt="Equation 1" rid="eqU003AAC">Equation 1</xref> is depicted
  in the upper row of
  <xref alt="[fig:AC]" rid="figU003AAC">[fig:AC]</xref>; since the
  Allen-Cahn equation does not have an analytical solution, the
  reference solution used by Wu et al.
  (<xref alt="2023" rid="ref-WuU003A2023" ref-type="bibr">2023</xref>)
  is adopted herein. The plots in the middle row of
  <xref alt="[fig:AC]" rid="figU003AAC">[fig:AC]</xref> depict two
  approximate solutions, corresponding to the results obtained after
  training a vanilla PIKAN, i.e., a PIKAN trained without adopting
  adaptive techniques, (left) and an adaptively trained PIKAN (right).
  Finally, the lower row of
  <xref alt="[fig:AC]" rid="figU003AAC">[fig:AC]</xref> showcases the
  absolute error corresponding to each PIKAN’s approximation, relative
  to the reference. This example highlights the benefits of adaptive
  training, as the vanilla PIKAN fails to capture the details of the
  reference solution, especially in the <inline-formula><alternatives>
  <tex-math><![CDATA[t \geq 0.3]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  region.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-LiuU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Z.</given-names></name>
        <name><surname>Wang</surname><given-names>Y.</given-names></name>
        <name><surname>Vaidya</surname><given-names>S.</given-names></name>
        <name><surname>Ruehle</surname><given-names>F.</given-names></name>
        <name><surname>Halverson</surname><given-names>J.</given-names></name>
        <name><surname>Soljačić</surname><given-names>M.</given-names></name>
        <name><surname>Hou</surname><given-names>T. Y.</given-names></name>
        <name><surname>Tegmark</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>KAN: Kolmogorov-Arnold Networks</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2024-04">2024</year><month>04</month>
      <uri>https://arxiv.org/abs/2404.19756</uri>
    </element-citation>
  </ref>
  <ref id="ref-flax">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Heek</surname><given-names>J.</given-names></name>
        <name><surname>Levskaya</surname><given-names>A.</given-names></name>
        <name><surname>Oliver</surname><given-names>A.</given-names></name>
        <name><surname>Ritter</surname><given-names>M.</given-names></name>
        <name><surname>Rondepierre</surname><given-names>B.</given-names></name>
        <name><surname>Steiner</surname><given-names>A.</given-names></name>
        <name><surname>van Zee</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Flax: A neural network library and ecosystem for JAX</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/google/flax</uri>
    </element-citation>
  </ref>
  <ref id="ref-optax">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>DeepMind</surname></name>
        <name><surname>Babuschkin</surname><given-names>I.</given-names></name>
        <name><surname>Baumli</surname><given-names>K.</given-names></name>
        <name><surname>Bell</surname><given-names>A.</given-names></name>
        <name><surname>Bhupatiraju</surname><given-names>D.</given-names></name>
        <name><surname>Bruce</surname><given-names>J.</given-names></name>
        <name><surname>Buchlovsky</surname><given-names>P.</given-names></name>
        <name><surname>Budden</surname><given-names>D.</given-names></name>
        <name><surname>Cai</surname><given-names>T.</given-names></name>
        <name><surname>Clark</surname><given-names>A.</given-names></name>
        <name><surname>Danihelka</surname><given-names>I.</given-names></name>
        <name><surname>Dedieu</surname><given-names>A.</given-names></name>
        <name><surname>Fantacci</surname><given-names>C.</given-names></name>
        <name><surname>Godwin</surname><given-names>J.</given-names></name>
        <name><surname>Jones</surname><given-names>C.</given-names></name>
        <name><surname>Hemsley</surname><given-names>R.</given-names></name>
        <name><surname>Hennigan</surname><given-names>T.</given-names></name>
        <name><surname>Hessel</surname><given-names>M.</given-names></name>
        <name><surname>Hou</surname><given-names>S.</given-names></name>
        <name><surname>Kapturowski</surname><given-names>S.</given-names></name>
        <name><surname>Keck</surname><given-names>T.</given-names></name>
        <name><surname>Kemaev</surname><given-names>I.</given-names></name>
        <name><surname>King</surname><given-names>M.</given-names></name>
        <name><surname>Kunesch</surname><given-names>M.</given-names></name>
        <name><surname>Martens</surname><given-names>L.</given-names></name>
        <name><surname>Merzic</surname><given-names>H.</given-names></name>
        <name><surname>Mikulik</surname><given-names>V.</given-names></name>
        <name><surname>Norman</surname><given-names>T.</given-names></name>
        <name><surname>Papamakarios</surname><given-names>G.</given-names></name>
        <name><surname>Quan</surname><given-names>J.</given-names></name>
        <name><surname>Ring</surname><given-names>R.</given-names></name>
        <name><surname>Ruiz</surname><given-names>F.</given-names></name>
        <name><surname>Sanchez</surname><given-names>A.</given-names></name>
        <name><surname>Sartran</surname><given-names>L.</given-names></name>
        <name><surname>Schneider</surname><given-names>R.</given-names></name>
        <name><surname>Sezener</surname><given-names>E.</given-names></name>
        <name><surname>Spencer</surname><given-names>S.</given-names></name>
        <name><surname>Srinivasan</surname><given-names>S.</given-names></name>
        <name><surname>Stanojević</surname><given-names>M.</given-names></name>
        <name><surname>Stokowiec</surname><given-names>W.</given-names></name>
        <name><surname>Wang</surname><given-names>L.</given-names></name>
        <name><surname>Zhou</surname><given-names>G.</given-names></name>
        <name><surname>Viola</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>The DeepMind JAX Ecosystem</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/google-deepmind</uri>
    </element-citation>
  </ref>
  <ref id="ref-KarniadakisU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shukla</surname><given-names>K.</given-names></name>
        <name><surname>Toscano</surname><given-names>J. D.</given-names></name>
        <name><surname>Wang</surname><given-names>Z.</given-names></name>
        <name><surname>Zou</surname><given-names>Z.</given-names></name>
        <name><surname>Karniadakis</surname><given-names>G. E.</given-names></name>
      </person-group>
      <article-title>A comprehensive and FAIR comparison between MLP and KAN representations for differential equations and operator networks</article-title>
      <source>Computer Methods in Applied Mechanics and Engineering</source>
      <year iso-8601-date="2024">2024</year>
      <volume>431</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0045782524005462</uri>
      <pub-id pub-id-type="doi">10.1016/j.cma.2024.117290</pub-id>
      <fpage>117290</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-SidharthU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sidharth</surname><given-names>S. S.</given-names></name>
        <name><surname>Keerthana</surname><given-names>A. R.</given-names></name>
        <name><surname>Gokul</surname><given-names>R.</given-names></name>
        <name><surname>Anas</surname><given-names>K. P.</given-names></name>
      </person-group>
      <article-title>Chebyshev Polynomial-Based Kolmogorov-Arnold Networks: An Efficient Architecture for Nonlinear Function Approximation</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <uri>https://arxiv.org/abs/2405.07200</uri>
    </element-citation>
  </ref>
  <ref id="ref-fkan">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Gist Noesis</string-name>
      </person-group>
      <article-title>FourierKAN</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/GistNoesis/FourierKAN</uri>
    </element-citation>
  </ref>
  <ref id="ref-effkan">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Blealtan</string-name>
      </person-group>
      <article-title>An Efficient Implementation of Kolmogorov-Arnold Network</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/Blealtan/efficient-kan</uri>
    </element-citation>
  </ref>
  <ref id="ref-pykan">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Z.</given-names></name>
      </person-group>
      <article-title>Kolmogorov-Arnold Networks (KANs)</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/KindXiaoming/pykan</uri>
    </element-citation>
  </ref>
  <ref id="ref-paszkeU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>A.</given-names></name>
        <name><surname>Gross</surname><given-names>S.</given-names></name>
        <name><surname>Massa</surname><given-names>F.</given-names></name>
        <name><surname>A.</surname><given-names>Lerer</given-names></name>
        <name><surname>Bradbury</surname><given-names>J.</given-names></name>
        <name><surname>Chanan</surname><given-names>G.</given-names></name>
        <name><surname>Killeen</surname><given-names>T.</given-names></name>
        <name><surname>Lin</surname><given-names>Z.</given-names></name>
        <name><surname>Gimelshein</surname><given-names>N.</given-names></name>
        <name><surname>Antiga</surname><given-names>L.</given-names></name>
        <name><surname>Desmaison</surname><given-names>A.</given-names></name>
        <name><surname>Köpf</surname><given-names>A.</given-names></name>
        <name><surname>Yang</surname><given-names>E.</given-names></name>
        <name><surname>DeVito</surname><given-names>Z.</given-names></name>
        <name><surname>Raison</surname><given-names>M.</given-names></name>
        <name><surname>Tejani</surname><given-names>A.</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>S.</given-names></name>
        <name><surname>Steiner</surname><given-names>B.</given-names></name>
        <name><surname>Fang</surname><given-names>L.</given-names></name>
        <name><surname>Bai</surname><given-names>J.</given-names></name>
        <name><surname>Chintala</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2019-12">2019</year><month>12</month>
      <uri>https://arxiv.org/abs/1912.01703</uri>
    </element-citation>
  </ref>
  <ref id="ref-neuromancer">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Drgona</surname><given-names>J.</given-names></name>
        <name><surname>Tuor</surname><given-names>A.</given-names></name>
        <name><surname>Koch</surname><given-names>J.</given-names></name>
        <name><surname>Shapiro</surname><given-names>M.</given-names></name>
        <name><surname>Jacob</surname><given-names>B.</given-names></name>
        <name><surname>Vrabie</surname><given-names>D.</given-names></name>
      </person-group>
      <article-title>NeuroMANCER: Neural Modules with Adaptive Nonlinear Constraints and Efficient Regularizations</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/pnnl/neuromancer</uri>
    </element-citation>
  </ref>
  <ref id="ref-RigasU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rigas</surname><given-names>S.</given-names></name>
        <name><surname>Papachristou</surname><given-names>M.</given-names></name>
        <name><surname>Papadopoulos</surname><given-names>T.</given-names></name>
        <name><surname>Anagnostopoulos</surname><given-names>F.</given-names></name>
        <name><surname>Alexandridis</surname><given-names>G.</given-names></name>
      </person-group>
      <article-title>Adaptive Training of Grid-Dependent Physics-Informed Kolmogorov-Arnold Networks</article-title>
      <source>IEEE Access</source>
      <year iso-8601-date="2024">2024</year>
      <volume>12</volume>
      <uri>https://ieeexplore.ieee.org/document/10763509</uri>
      <pub-id pub-id-type="doi">10.1109/ACCESS.2024.3504962</pub-id>
      <fpage>176982</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Howard1U003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howard</surname><given-names>A. A.</given-names></name>
        <name><surname>Jacob</surname><given-names>B.</given-names></name>
        <name><surname>Murphy</surname><given-names>S. H.</given-names></name>
        <name><surname>Heinlein</surname><given-names>A.</given-names></name>
        <name><surname>Stinis</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Finite basis Kolmogorov-Arnold networks: domain decomposition for data-driven and physics-informed problems</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <uri>https://arxiv.org/abs/2406.19662</uri>
    </element-citation>
  </ref>
  <ref id="ref-Howard2U003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howard</surname><given-names>A. A.</given-names></name>
        <name><surname>Jacob</surname><given-names>B.</given-names></name>
        <name><surname>Stinis</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Multifidelity Kolmogorov-Arnold Networks</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2024-10">2024</year><month>10</month>
      <uri>https://arxiv.org/abs/2410.14764</uri>
    </element-citation>
  </ref>
  <ref id="ref-JacobU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jacob</surname><given-names>B.</given-names></name>
        <name><surname>Howard</surname><given-names>A. A.</given-names></name>
        <name><surname>Stinis</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>SPIKANs: Separable Physics-Informed Kolmogorov-Arnold Networks</article-title>
      <source>ArXiv e-prints</source>
      <year iso-8601-date="2024-11">2024</year><month>11</month>
      <uri>https://arxiv.org/abs/2411.06286</uri>
    </element-citation>
  </ref>
  <ref id="ref-WuU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>C.</given-names></name>
        <name><surname>Zhu</surname><given-names>M.</given-names></name>
        <name><surname>Tan</surname><given-names>Q.</given-names></name>
        <name><surname>Kartha</surname><given-names>Y.</given-names></name>
        <name><surname>Lu</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>A comprehensive study of non-adaptive and residual-based adaptive sampling for physics-informed neural networks</article-title>
      <source>Computer Methods in Applied Mechanics and Engineering</source>
      <year iso-8601-date="2023">2023</year>
      <volume>403</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S0045782522006260</uri>
      <pub-id pub-id-type="doi">10.1016/j.cma.2022.115671</pub-id>
      <fpage>115671</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
