<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5713</article-id>
<article-id pub-id-type="doi">10.21105/joss.05713</article-id>
<title-group>
<article-title>PyDGN: a Python Library for Flexible and Reproducible
Research on Deep Learning for Graphs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5181-2904</contrib-id>
<name>
<surname>Errica</surname>
<given-names>Federico</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5213-2468</contrib-id>
<name>
<surname>Bacciu</surname>
<given-names>Davide</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5764-5238</contrib-id>
<name>
<surname>Micheli</surname>
<given-names>Alessio</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>NEC Laboratories Europe, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Pisa, Italy</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-06-25">
<day>25</day>
<month>6</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>90</issue>
<fpage>5713</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Machine Learning</kwd>
<kwd>Graph Networks</kwd>
<kwd>Deep Learning for Graphs</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The use of standardized evaluation procedures is a key component in
  the Machine Learning (ML) field to determine whether new approaches
  grant real advantages over others. This is especially true for
  fast-growing research areas, where a substantial amount of literature
  relentlessly appears every day. In the graph machine learning field,
  some evaluation issues have already been brought to light and
  partially addressed, but a general-purpose library for rigorous
  evaluations and reproducible experiments is lacking. We therefore
  introduce a new Python library, called <monospace>PyDGN</monospace>,
  to provide users with a system that lets them focus on models’
  development while ensuring empirical rigor and reproducibility of
  their results.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>To date, the graph ML community
  (<xref alt="Bronstein et al., 2017" rid="ref-2017bronstein_geometric_2017" ref-type="bibr">Bronstein
  et al., 2017</xref>;
  <xref alt="Hamilton et al., 2017" rid="ref-2017hamilton_representation_2017" ref-type="bibr">Hamilton
  et al., 2017</xref>;
  <xref alt="Micheli, 2009" rid="ref-2009micheli_neural_2009" ref-type="bibr">Micheli,
  2009</xref>;
  <xref alt="Scarselli et al., 2009" rid="ref-2009scarselli_graph_2009" ref-type="bibr">Scarselli
  et al., 2009</xref>;
  <xref alt="Sperduti &amp; Starita, 1997" rid="ref-1997sperduti_supervised_1997" ref-type="bibr">Sperduti
  &amp; Starita, 1997</xref>;
  <xref alt="Wu et al., 2020" rid="ref-2020wu_comprehensive_2020" ref-type="bibr">Wu
  et al., 2020</xref>) has already developed benchmarking software to
  re-evaluate existing models on a fixed set of datasets
  (<xref alt="Errica et al., 2020" rid="ref-2020errica_fair_2020" ref-type="bibr">Errica
  et al., 2020</xref>;
  <xref alt="Hu et al., 2020" rid="ref-2020hu_open_2020" ref-type="bibr">Hu
  et al., 2020</xref>;
  <xref alt="Liu et al., 2021" rid="ref-2021liu_dig_2021" ref-type="bibr">Liu
  et al., 2021</xref>;
  <xref alt="Shchur et al., 2018" rid="ref-2018shchur_pitfalls_2018" ref-type="bibr">Shchur
  et al., 2018</xref>). In addition, existing libraries such as PyTorch
  Geometric (PyG)
  (<xref alt="Fey &amp; Lenssen, 2019" rid="ref-2019fey_fast_2019" ref-type="bibr">Fey
  &amp; Lenssen, 2019</xref>), Deep Graph Library (DGL)
  (<xref alt="Wang et al., 2019" rid="ref-2019wang_dgl_2019" ref-type="bibr">Wang
  et al., 2019</xref>), and Spektral
  (<xref alt="Grattarola &amp; Alippi, 2021" rid="ref-2021grattarola_graph_2021" ref-type="bibr">Grattarola
  &amp; Alippi, 2021</xref>) provide the building blocks of Deep Graph
  Networks (DGNs)
  (<xref alt="Bacciu et al., 2020" rid="ref-2020bacciu_gentle_2020" ref-type="bibr">Bacciu
  et al., 2020</xref>), also known as message-passing architectures
  (<xref alt="Gilmer et al., 2017" rid="ref-2017gilmer_neural_2017" ref-type="bibr">Gilmer
  et al., 2017</xref>), effectively acting as the backbone of most graph
  ML software packages. Other libraries, such as GraphGym
  (<xref alt="You et al., 2020" rid="ref-2020_you_design_2020" ref-type="bibr">You
  et al., 2020</xref>), allow the user to explore the design of existing
  DGNs by running hyper-parameter tuning experiments in parallel, but
  the customization is mostly limited to the models and does not allow,
  for instance, a modular extension of data splitting techniques,
  evaluation strategies, or experiments with a custom logic. This limits
  the ability of a researcher to use these libraries to carry out new
  and possibly original research.</p>
  <p>The community therefore lacks a software library that is
  specifically dedicated to ensuring reproducibility and replicability
  of experiments <italic>without</italic> compromising the flexibility
  required by our everyday research. To fill this gap, we have developed
  <monospace>PyDGN</monospace>, a Python library for Deep Graph Networks
  research. <monospace>PyDGN</monospace> builds upon
  <monospace>PyTorch</monospace>
  (<xref alt="Paszke et al., 2019" rid="ref-2019paszke_pytorch_2019" ref-type="bibr">Paszke
  et al., 2019</xref>) and <monospace>PyTorch Geometric</monospace>
  (PyG)
  (<xref alt="Fey &amp; Lenssen, 2019" rid="ref-2019fey_fast_2019" ref-type="bibr">Fey
  &amp; Lenssen, 2019</xref>) to handle graph-structured data and reuse
  efficient implementations of machine learning models. It exploits
  <ext-link ext-link-type="uri" xlink:href="https://www.ray.io/">Ray</ext-link>
  to run experiments in parallel (also on clusters of machines) and it
  supports
  <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/cuda-toolkit">GPU</ext-link>
  computation for faster executions. Our goal is to help practitioners
  and researchers to focus on the development of their models and to
  effortlessly evaluate them under fair, robust, and reproducible
  experimental conditions, thus mitigating empirical malpractices that
  often occur in the ML community
  (<xref alt="Errica et al., 2020" rid="ref-2020errica_fair_2020" ref-type="bibr">Errica
  et al., 2020</xref>;
  <xref alt="Lipton &amp; Steinhardt, 2018" rid="ref-2018lipton_troubling_2018" ref-type="bibr">Lipton
  &amp; Steinhardt, 2018</xref>;
  <xref alt="Shchur et al., 2018" rid="ref-2018shchur_pitfalls_2018" ref-type="bibr">Shchur
  et al., 2018</xref>). <monospace>PyDGN</monospace> has already been
  used in a number of research projects that have been published at
  top-tier venues, as listed in the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/diningphil/PyDGN">official
  GitHub repository</ext-link>.</p>
  <fig>
    <caption><p>PyDGN is logically organized into different modules that
    cover specific aspects of the entire evaluation’s pipeline, from
    data creation to a model’s risk
    assessment.<styled-content id="figU003Apydgn-structure"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/paper.png" />
  </fig>
  <p>We refer the reader to
  <xref alt="[fig:pydgn-structure]" rid="figU003Apydgn-structure">[fig:pydgn-structure]</xref>
  for a visual depiction of the main components. We remark that all
  modules, with the exception of the one responsible for evaluation (due
  to its standard behavior), are readily extensible and promote rapid
  prototyping through code reuse.</p>
  <sec id="how-to-use-it">
    <title>How to use it</title>
    <p>Users can easily prepare and launch their evaluations through
    <italic>configuration files</italic>, one for the data preparation
    and the second for the actual experiment. In the former, the user
    specifies: <bold>i)</bold> how to split the data; <bold>ii)</bold>
    the dataset to use; and <bold>iii)</bold> optional data
    transformations. In the second file, the user indicates:
    <bold>i)</bold> data and splits; <bold>ii)</bold> hardware devices
    and parallelism; <bold>iii)</bold> hyper-parameter configurations
    for model selection; <bold>iv)</bold> training-specific details like
    metrics and optimizer. Dedicated scripts prepare the data, its
    splits, launch the experiments and compute results. The
    <ext-link ext-link-type="uri" xlink:href="https://pydgn.readthedocs.io/">PyDGN
    documentation</ext-link> helps the user understand the main
    mechanisms through tutorials and examples.</p>
    <sec id="data-preparation">
      <title>Data preparation</title>
      <p>A recurrent issue in the evaluation of ML models is that they
      are compared using different data splits. The first step to
      reproducibility is thus the creation and retention of such splits:
      we provide code that partitions the data depending on the required
      graph/node/link prediction scenarios. Since splitting depends on
      the type of evaluation procedure, we cover
      <italic>hold-out</italic>, <italic>k-fold</italic>, and
      <italic>nested/double</italic> <italic>k-fold</italic> cross
      validation, which are the most common evaluation schemas in the ML
      literature.</p>
      <p>To create and use a dataset, we provide an interface to easily
      specify pre-processing as well as runtime processing of graphs; we
      extend the available dataset classes in <monospace>PyG</monospace>
      to achieve this goal. In addition, a data provider automatically
      retrieves the correct data subset (training/validation/test)
      during an experiment, making sure the user does not involuntarily
      leak test data into training/validation. An example on how to
      split a dataset to carry out a 10-fold cross validation with inner
      hold-out model selection is shown below:</p>
      <code language="yaml">splitter:
  root: DATA_SPLITS
  class_name: pydgn.data.splitter.Splitter
  args:
    n_outer_folds: 10
    n_inner_folds: 1
    seed: 42
    stratify: True
    shuffle: True
    inner_val_ratio: 0.1
    outer_val_ratio: 0.1
    test_ratio: 0.1</code>
    </sec>
    <sec id="evaluation-procedures">
      <title>Evaluation procedures</title>
      <p><monospace>PyDGN</monospace> is equipped with routines that
      remove the burden of performing model selection and risk
      assessment from the users. This reduces chances of empirical flaws
      and favors reproducible experiments. After model selection, the
      best configuration (with respect to the validation set) is
      re-trained and evaluated on the test set. In addition, a
      start-and-stop mechanism can resume execution of unfinished
      experiments when the whole evaluation is interrupted. These
      procedures are completely transparent to the user and handled in
      accordance to the data splits.</p>
    </sec>
    <sec id="experiment-templates">
      <title>Experiment templates</title>
      <p>We define an abstract interface for each experiment that
      consists of two methods, <italic>run_valid</italic> and
      <italic>run_test</italic>. The first is called during the model
      selection, and the second is called during risk assessment of the
      model. This should act as a reminder that the user cannot access
      the test data when performing a model-selection procedure (that
      is, <italic>run_valid</italic>), thus reducing the chances of test
      data leakage. Our library ships with two standard experiments, an
      end-to-end training on a single task and two-step training where
      first we compute unsupervised node/graph embeddings and then apply
      a supervised predictor on top of them to solve a downstream task.
      These two implementations cover most use cases and ensure that the
      different data splits are used in the correct way.</p>
    </sec>
    <sec id="implementing-models">
      <title>Implementing models</title>
      <p>To implement a DGN in our library, it is sufficient to adhere
      to a very simple interface that specifies initialization arguments
      and the type of output for the prediction step. The user wraps the
      interface around a <monospace>PyG</monospace> model to have it
      immediately working. This strategy allows the user to focus
      entirely on the development of the model regardless of all the
      code necessary to run the training pipeline, akin to what happens
      in
      (<xref alt="You et al., 2020" rid="ref-2020_you_design_2020" ref-type="bibr">You
      et al., 2020</xref>). The library will automatically provide the
      current hyper-parameter configuration to be evaluated to the model
      in the form of a dictionary <monospace>config</monospace>. To
      create a new model, the user simply has to subclass
      <monospace>ModuleInterface</monospace> and implement the two
      methods</p>
      <code language="python">class MyModel(pydgn.model.interface.ModelInterface):
    def __init__(self,
                 dim_node_features, dim_edge_features, dim_target,
                 readout_class, config: dict):
        ...

    def forward(self, data):
        ...</code>
      <p>where <monospace>data</monospace> is a PyG
      <monospace>Batch</monospace> object containing a batch of input
      graphs.</p>
    </sec>
    <sec id="training-via-publish-subscribe">
      <title>Training via publish-subscribe</title>
      <p>The interaction between all components of a training pipeline
      is perhaps the trickiest part to implement in any machine learning
      project. One false step, and nothing works. PyDGN’s training
      engine implements all boilerplate code regarding the training
      loop, and it relies on the publish-subscribe design pattern to
      trigger the execution of callbacks at specific points in the
      training procedure. Every metric, early stopping, scheduler,
      gradient clipper, optimizer, data fetcher, and stats plotter
      implements some of these callbacks; when a callback is triggered,
      a shared state object is passed as argument to allow communication
      between the different components of the training process. For
      instance, extending the MeanSquaredError loss to compute its
      logarithm is as easy as doing</p>
      <code language="python">class LogMSE(pydgn.training.callback.metric.MeanSquareError):
 
    @property
    def name(self):
        return &quot;LOG Mean Square Error&quot;

    def compute_metric(self, targets, predictions):
        mse_metric = super().compute_metric(targets, predictions)
        return torch.log(mse_metric)</code>
      <p>The new metric can be already referenced and used in the
      configuration file by referrint it as
      <monospace>my_metric_file.LogMeanSquareError</monospace>. It will
      also be automatically logged on Tensorboard by the
      <monospace>Plotter</monospace> callback if the latter is
      enabled.</p>
    </sec>
  </sec>
  <sec id="key-design-considerations">
    <title>Key Design Considerations</title>
    <p>Two important aspects make PyDGN a user-friendly choice for
    everyday research.</p>
    <p>First, PyDGN’s use of configuration files allows users to add new
    hyper-parameters without needing to modify any extra code. This
    feature simplifies the customization of a model while prototyping it
    to keep the user focused on the main task, that is, the definition
    and implementation of a model, ignoring all the boilerplate code
    used to forward the new hyper-parameters to the model’s class
    initializer.</p>
    <p>Additionally, the publish-subcribe design pattern keeps the
    codebase organized and flexible. Users can connect different
    components as needed using a shared state object, making it
    adaptable to various use cases. Most of the time, refining a
    training loop only requires subclassing the desired callbacks,
    simplifying the overall development process.</p>
    <p>In summary, PyDGN’s configuration files and event-based design
    pattern enhance its usability, making it a practical tool for
    researchers by simplifying customization and maintaining code
    flexibility. These features empower researchers to efficiently
    achieve their research goals.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from Antonio Carta, Danilo Numeroso,
  Daniele Castellana, Alessio Gravina, Francesco Landolfi, and support
  from Marco Podda during the genesis of this project. The authors would
  like to thank the reviewers Ido Ben-Yair and Sepand Haghighi for the
  constructive feedback that greatly improved the paper and the
  installation procedure.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-1997sperduti_supervised_1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sperduti</surname><given-names>Alessandro</given-names></name>
        <name><surname>Starita</surname><given-names>Antonina</given-names></name>
      </person-group>
      <article-title>Supervised neural networks for the classification of structures</article-title>
      <source>IEEE Transactions on Neural Networks</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="1997">1997</year>
      <volume>8</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/72.572108</pub-id>
      <fpage>714</fpage>
      <lpage>735</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2009scarselli_graph_2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Scarselli</surname><given-names>Franco</given-names></name>
        <name><surname>Gori</surname><given-names>Marco</given-names></name>
        <name><surname>Tsoi</surname><given-names>Ah Chung</given-names></name>
        <name><surname>Hagenbuchner</surname><given-names>Markus</given-names></name>
        <name><surname>Monfardini</surname><given-names>Gabriele</given-names></name>
      </person-group>
      <article-title>The graph neural network model</article-title>
      <source>IEEE Transactions on Neural Networks</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>20</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id>
      <fpage>61</fpage>
      <lpage>80</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2009micheli_neural_2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Micheli</surname><given-names>Alessio</given-names></name>
      </person-group>
      <article-title>Neural network for graphs: A contextual constructive approach</article-title>
      <source>IEEE Transactions on Neural Networks</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>20</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/TNN.2008.2010350</pub-id>
      <fpage>498</fpage>
      <lpage>511</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2017bronstein_geometric_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bronstein</surname><given-names>Michael M.</given-names></name>
        <name><surname>Bruna</surname><given-names>Joan</given-names></name>
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Szlam</surname><given-names>Arthur</given-names></name>
        <name><surname>Vandergheynst</surname><given-names>Pierre</given-names></name>
      </person-group>
      <article-title>Geometric deep learning: Going beyond Euclidean data</article-title>
      <source>IEEE Signal Processing Magazine</source>
      <year iso-8601-date="2017">2017</year>
      <volume>34</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1109/MSP.2017.2693418</pub-id>
      <fpage>25. 18</fpage>
      <lpage>42</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2017gilmer_neural_2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gilmer</surname><given-names>Justin</given-names></name>
        <name><surname>Schoenholz</surname><given-names>Samuel S</given-names></name>
        <name><surname>Riley</surname><given-names>Patrick F</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Dahl</surname><given-names>George E</given-names></name>
      </person-group>
      <article-title>Neural message passing for quantum chemistry</article-title>
      <source>Proceedings of the 34th International Conference on Machine Learning (ICML)</source>
      <year iso-8601-date="2017">2017</year>
      <fpage>1263</fpage>
      <lpage>1272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2017hamilton_representation_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hamilton</surname><given-names>William L.</given-names></name>
        <name><surname>Ying</surname><given-names>Rex</given-names></name>
        <name><surname>Leskovec</surname><given-names>Jure</given-names></name>
      </person-group>
      <article-title>Representation learning on graphs: Methods and applications</article-title>
      <source>IEEE Data Engineering Bulletin</source>
      <year iso-8601-date="2017">2017</year>
      <volume>40</volume>
      <issue>3</issue>
      <fpage>52</fpage>
      <lpage>74</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2018lipton_troubling_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lipton</surname><given-names>Zachary C</given-names></name>
        <name><surname>Steinhardt</surname><given-names>Jacob</given-names></name>
      </person-group>
      <article-title>Troubling trends in machine learning scholarship</article-title>
      <source>arXiv preprint arXiv:1807.03341</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-2018shchur_pitfalls_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shchur</surname><given-names>Oleksandr</given-names></name>
        <name><surname>Mumme</surname><given-names>Maximilian</given-names></name>
        <name><surname>Bojchevski</surname><given-names>Aleksandar</given-names></name>
        <name><surname>Günnemann</surname><given-names>Stephan</given-names></name>
      </person-group>
      <article-title>Pitfalls of graph neural network evaluation</article-title>
      <source>Workshop on Relational Representation Learning, Neural Information Processing Systems (NeurIPS)</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-2019fey_fast_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fey</surname><given-names>Matthias</given-names></name>
        <name><surname>Lenssen</surname><given-names>Jan Eric</given-names></name>
      </person-group>
      <article-title>Fast graph representation learning with PyTorch Geometric</article-title>
      <source>Representation Learning on Graphs and Manifolds Workshop, International Conference on Learning Representations (ICLR)</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-2019paszke_pytorch_2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Proceedings of the 33rd conference on neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-2020_you_design_2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>You</surname><given-names>Jiaxuan</given-names></name>
        <name><surname>Ying</surname><given-names>Zhitao</given-names></name>
        <name><surname>Leskovec</surname><given-names>Jure</given-names></name>
      </person-group>
      <article-title>Design space for graph neural networks</article-title>
      <source>Proceedings of the 34th conference on neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-2019wang_dgl_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Minjie</given-names></name>
        <name><surname>Zheng</surname><given-names>Da</given-names></name>
        <name><surname>Ye</surname><given-names>Zihao</given-names></name>
        <name><surname>Gan</surname><given-names>Quan</given-names></name>
        <name><surname>Li</surname><given-names>Mufei</given-names></name>
        <name><surname>Song</surname><given-names>Xiang</given-names></name>
        <name><surname>Zhou</surname><given-names>Jinjing</given-names></name>
        <name><surname>Ma</surname><given-names>Chao</given-names></name>
        <name><surname>Yu</surname><given-names>Lingfan</given-names></name>
        <name><surname>Gai</surname><given-names>Yu</given-names></name>
        <name><surname>Xiao</surname><given-names>Tianjun</given-names></name>
        <name><surname>He</surname><given-names>Tong</given-names></name>
        <name><surname>Karypis</surname><given-names>George</given-names></name>
        <name><surname>Li</surname><given-names>Jinyang</given-names></name>
        <name><surname>Zhang</surname><given-names>Zheng</given-names></name>
      </person-group>
      <article-title>Deep graph library: A graph-centric, highly-performant package for graph neural networks</article-title>
      <source>arXiv preprint arXiv:1909.01315</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-2020bacciu_gentle_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bacciu</surname><given-names>Davide</given-names></name>
        <name><surname>Errica</surname><given-names>Federico</given-names></name>
        <name><surname>Micheli</surname><given-names>Alessio</given-names></name>
        <name><surname>Podda</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>A gentle introduction to deep learning for graphs</article-title>
      <source>Neural Networks</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <volume>129</volume>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2020.06.006</pub-id>
      <fpage>203</fpage>
      <lpage>221</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2020errica_fair_2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Errica</surname><given-names>Federico</given-names></name>
        <name><surname>Podda</surname><given-names>Marco</given-names></name>
        <name><surname>Bacciu</surname><given-names>Davide</given-names></name>
        <name><surname>Micheli</surname><given-names>Alessio</given-names></name>
      </person-group>
      <article-title>A fair comparison of graph neural networks for graph classification</article-title>
      <source>8th International Conference on Learning Representations (ICLR)</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-2020hu_open_2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hu</surname><given-names>Weihua</given-names></name>
        <name><surname>Fey</surname><given-names>Matthias</given-names></name>
        <name><surname>Zitnik</surname><given-names>Marinka</given-names></name>
        <name><surname>Dong</surname><given-names>Yuxiao</given-names></name>
        <name><surname>Ren</surname><given-names>Hongyu</given-names></name>
        <name><surname>Liu</surname><given-names>Bowen</given-names></name>
        <name><surname>Catasta</surname><given-names>Michele</given-names></name>
        <name><surname>Leskovec</surname><given-names>Jure</given-names></name>
      </person-group>
      <article-title>Open graph benchmark: Datasets for machine learning on graphs</article-title>
      <source>Proceedings of the 34th conference on neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2020">2020</year>
      <fpage>22118</fpage>
      <lpage>22133</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2020wu_comprehensive_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Zonghan</given-names></name>
        <name><surname>Pan</surname><given-names>Shirui</given-names></name>
        <name><surname>Chen</surname><given-names>Fengwen</given-names></name>
        <name><surname>Long</surname><given-names>Guodong</given-names></name>
        <name><surname>Zhang</surname><given-names>Chengqi</given-names></name>
        <name><surname>Philip</surname><given-names>S Yu</given-names></name>
      </person-group>
      <article-title>A comprehensive survey on graph neural networks</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1109/TNNLS.2020.2978386</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-2021grattarola_graph_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grattarola</surname><given-names>Daniele</given-names></name>
        <name><surname>Alippi</surname><given-names>Cesare</given-names></name>
      </person-group>
      <article-title>Graph neural networks in TensorFlow and keras with spektral</article-title>
      <source>IEEE Computational Intelligence Magazine</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>16</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/MCI.2020.3039072</pub-id>
      <fpage>99</fpage>
      <lpage>106</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2021liu_dig_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Meng</given-names></name>
        <name><surname>Luo</surname><given-names>Youzhi</given-names></name>
        <name><surname>Wang</surname><given-names>Limei</given-names></name>
        <name><surname>Xie</surname><given-names>Yaochen</given-names></name>
        <name><surname>Yuan</surname><given-names>Hao</given-names></name>
        <name><surname>Gui</surname><given-names>Shurui</given-names></name>
        <name><surname>Yu</surname><given-names>Haiyang</given-names></name>
        <name><surname>Xu</surname><given-names>Zhao</given-names></name>
        <name><surname>Zhang</surname><given-names>Jingtun</given-names></name>
        <name><surname>Liu</surname><given-names>Yi</given-names></name>
        <name><surname>Yan</surname><given-names>Keqiang</given-names></name>
        <name><surname>Liu</surname><given-names>Haoran</given-names></name>
        <name><surname>Fu</surname><given-names>Cong</given-names></name>
        <name><surname>Oztekin</surname><given-names>Bora M</given-names></name>
        <name><surname>Zhang</surname><given-names>Xuan</given-names></name>
        <name><surname>Ji</surname><given-names>Shuiwang</given-names></name>
      </person-group>
      <article-title>DIG: A turnkey library for diving into graph deep learning research</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>240</issue>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
