<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8183</article-id>
<article-id pub-id-type="doi">10.21105/joss.08183</article-id>
<title-group>
<article-title>Turftopic: Topic Modelling with Contextual
Representations from Sentence Transformers</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9652-4498</contrib-id>
<name>
<surname>Kardos</surname>
<given-names>Márton</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8733-0966</contrib-id>
<name>
<surname>Enevoldsen</surname>
<given-names>Kenneth C.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9707-7121</contrib-id>
<name>
<surname>Kostkan</surname>
<given-names>Jan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8714-1911</contrib-id>
<name>
<surname>Kristensen-McLachlan</surname>
<given-names>Ross Deans</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9017-8088</contrib-id>
<name>
<surname>Rocca</surname>
<given-names>Roberta</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Center for Humanities Computing, Aarhus University,
Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Interacting Minds Center, Aarhus University,
Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Linguistics, Cognitive Science, and
Semiotics, Aarhus University, Denmark</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-06-25">
<day>25</day>
<month>6</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>111</issue>
<fpage>8183</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>topic modelling</kwd>
<kwd>sentence-transformers</kwd>
<kwd>embeddings</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Topic models are machine learning techniques that are able to
  discover themes in a set of documents. Turftopic is a topic modelling
  library including a number of recent developments in topic modelling
  that go beyond bag-of-words models and can understand text in context,
  utilizing representations from transformers. Turftopic focuses on ease
  of use, providing a unified interface for a number of different modern
  topic models, and boasting both model-specific and model-agnostic
  interpretation and visualization utilities. While the user is afforded
  great flexibility in model choice and customization, the library comes
  with reasonable defaults, so as not to needlessly overwhelm first-time
  users. In addition, Turftopic allows the user to: a) model topics as
  they change over time, b) learn topics on-line from a stream of texts,
  c) find hierarchical structure in topics, d) learning topics in
  multilingual texts and corpora. Users can utilize the power of large
  language models (LLMs) to give human-readable names to topics.
  Turftopic also comes with built-in utilities for generating topic
  descriptions based on key-phrases or lemmas rather than individual
  words.</p>
  <fig>
    <caption><p>An Overview of Turftopic’s Functionality</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="paper_banner.png" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>While a number of software packages have been developed for
  contextual topic modelling in recent years, including BERTopic
  (<xref alt="Grootendorst, 2022" rid="ref-bertopic_paper" ref-type="bibr">Grootendorst,
  2022</xref>), Top2Vec
  (<xref alt="Angelov, 2020" rid="ref-top2vec" ref-type="bibr">Angelov,
  2020</xref>), CTM
  (<xref alt="Bianchi, Terragni, &amp; Hovy, 2021" rid="ref-ctm" ref-type="bibr">Bianchi,
  Terragni, &amp; Hovy, 2021</xref>), these packages include
  implementations of one or two topic models, and most of the utilities
  they provide are model-specific. This has resulted in the unfortunate
  situation that practitioners need to switch between different
  libraries and adapt to their particularities in both interface and
  functionality. Some attempts have been made at creating unified
  packages for modern topic models, including STREAM
  (<xref alt="Thielmann et al., 2024" rid="ref-stream" ref-type="bibr">Thielmann
  et al., 2024</xref>) and TopMost
  (<xref alt="Wu, Pan, et al., 2024" rid="ref-topmost" ref-type="bibr">Wu,
  Pan, et al., 2024</xref>). These packages, however, have a focus on
  neural models and topic model evaluation, have abstract and highly
  specialized interfaces, and do not include some popular topic models.
  Additionally, while model interpretation is a fundamental aspect of
  topic modelling, the interpretation utilities provided in these
  libraries are fairly limited, especially in comparison with
  model-specific packages, like BERTopic.</p>
  <p>Turftopic unifies state-of-the-art contextual topic models under a
  superset of the <monospace>scikit-learn</monospace>
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) API, which many users may be familiar with, and
  can be readily included in <monospace>scikit-learn</monospace>
  workflows and pipelines. We focused on making Turftopic first and
  foremost an easy-to-use library that does not necessitate expert
  knowledge or excessive amounts of code to get started with, but gives
  great flexibility to power users. Furthermore, we included an
  extensive suite of pretty-printing and model-specific visualization
  utilities that aid users in interpreting their results. In addition,
  we provide native compatibility with
  <monospace>topicwizard</monospace>
  (<xref alt="Kardos et al., 2025" rid="ref-topicwizard" ref-type="bibr">Kardos
  et al., 2025</xref>), a model-agnostic topic model visualization
  library. The library also includes three topic models that, to our
  knowledge, only have implementations in Turftopic: KeyNMF
  (<xref alt="Kristensen-McLachlan et al., 2024" rid="ref-keynmf" ref-type="bibr">Kristensen-McLachlan
  et al., 2024</xref>), Semantic Signal Separation (S<sup>3</sup>)
  (<xref alt="Kardos et al., 2024" rid="ref-s3" ref-type="bibr">Kardos
  et al., 2024</xref>), and GMM, a Gaussian Mixture model of document
  representations with a soft-c-tf-idf term weighting scheme.</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p>Turftopic includes a wide array of contextual topic models from the
  literature, these include: FASTopic
  (<xref alt="Wu, Nguyen, et al., 2024" rid="ref-fastopic" ref-type="bibr">Wu,
  Nguyen, et al., 2024</xref>), Clustering models, such as BERTopic
  (<xref alt="Grootendorst, 2022" rid="ref-bertopic_paper" ref-type="bibr">Grootendorst,
  2022</xref>) and Top2Vec
  (<xref alt="Angelov, 2020" rid="ref-top2vec" ref-type="bibr">Angelov,
  2020</xref>), auto-encoding topic models, like CombinedTM
  (<xref alt="Bianchi, Terragni, &amp; Hovy, 2021" rid="ref-ctm" ref-type="bibr">Bianchi,
  Terragni, &amp; Hovy, 2021</xref>) and ZeroShotTM
  (<xref alt="Bianchi, Terragni, Hovy, Nozza, et al., 2021" rid="ref-zeroshot_tm" ref-type="bibr">Bianchi,
  Terragni, Hovy, Nozza, et al., 2021</xref>), KeyNMF
  (<xref alt="Kristensen-McLachlan et al., 2024" rid="ref-keynmf" ref-type="bibr">Kristensen-McLachlan
  et al., 2024</xref>), S<sup>3</sup>
  (<xref alt="Kardos et al., 2024" rid="ref-s3" ref-type="bibr">Kardos
  et al., 2024</xref>) and GMM. At the time of writing, these models are
  representative of the state of the art in contextual topic modelling
  and intend to expand on them in the future.</p>
  <fig>
    <caption><p>Components of a Topic Modelling Pipeline in
    Turftopic</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="https://x-tabdeveloping.github.io/turftopic/images/topic_modeling_pipeline.png" />
  </fig>
  <p>Each model in Turftopic has an <italic>encoder</italic> component,
  which is used for producing continuous document-representations
  (<xref alt="Reimers &amp; Gurevych, 2019" rid="ref-sentence_transformers" ref-type="bibr">Reimers
  &amp; Gurevych, 2019</xref>), and a <italic>vectorizer</italic>
  component, which extracts term counts in each documents, thereby
  dictating which terms will be considered in topics. The user has full
  control over what components should be used at different stages of the
  topic modelling process, thereby having fine-grained influence on the
  nature and quality of topics.</p>
  <p>The library comes loaded with numerous utilities to help users
  interpret their results, including <italic>pretty printing</italic>
  utilities for exploring topics, <italic>interactive
  visualizations</italic> partially powered by the
  <monospace>topicwizard</monospace>
  (<xref alt="Kardos et al., 2025" rid="ref-topicwizard" ref-type="bibr">Kardos
  et al., 2025</xref>) Python package, and <italic>automated topic
  naming</italic> with LLMs.</p>
  <p>To accommodate a variety of use cases, Turftopic can be used for
  <italic>dynamic</italic> topic modelling, where topics are expected to
  change over time. Turftopic is also capable of extracting topics at
  multiple levels of granularity, thereby uncovering
  <italic>hierarchical</italic> topic structures. Some models can also
  be fitted in an <italic>online</italic> fashion, where documents are
  accounted for as they come in batches. Turftopic also includes
  <italic>seeded</italic> topic modelling, where a seed phrase can be
  used to retrieve topics relevant to the specific research
  question.</p>
</sec>
<sec id="use-cases">
  <title>Use cases</title>
  <p>Topic models can be and have been utilized for numerous purposes in
  both academia and industry. They are a key tool in
  digital/computational humanities, mainly as an instrument of
  quantitative text analysis or <italic>distant reading</italic>
  (<xref alt="Nielbo et al., 2024" rid="ref-quantitative_text_analysis" ref-type="bibr">Nielbo
  et al., 2024</xref>), as topic models can pick up on macro-patterns in
  corpora, at times missed by close readers
  (<xref alt="JOCKERS, 2013" rid="ref-macroanalysis" ref-type="bibr">JOCKERS,
  2013</xref>), and might be able to provide a more impartial account of
  a corpus’s content. Topic models can also aid discourse analysis by
  facilitating exploratory data analysis, and quantitative modelling of
  information dynamics
  (<xref alt="Jacobs &amp; and, 2019" rid="ref-discourse_analysis" ref-type="bibr">Jacobs
  &amp; and, 2019</xref>). Industry analysts might make use of topic
  models for analyzing customer feedback
  (<xref alt="Nguyen &amp; Ho, 2023" rid="ref-hotel_sector" ref-type="bibr">Nguyen
  &amp; Ho, 2023</xref>) or social media data related to a company’s
  products
  (<xref alt="Huang et al., 2025" rid="ref-social_media_mining" ref-type="bibr">Huang
  et al., 2025</xref>).</p>
  <p>Since topic models learn topically informative representations of
  text, they can also be utilized for down-stream applications, such as
  content filtering, recommendation
  (<xref alt="Bergamaschi &amp; Po, 2015" rid="ref-content_recommendation" ref-type="bibr">Bergamaschi
  &amp; Po, 2015</xref>), unsupervised classification
  (<xref alt="Thielmann et al., 2023" rid="ref-unsupervised_classification" ref-type="bibr">Thielmann
  et al., 2023</xref>), information retrieval
  (<xref alt="Yi &amp; Allan, 2009" rid="ref-information_retrieval" ref-type="bibr">Yi
  &amp; Allan, 2009</xref>) and pre-training data curation
  (<xref alt="Peng et al., 2025" rid="ref-data_mixers" ref-type="bibr">Peng
  et al., 2025</xref>).</p>
  <p>The Turftopic framework has already been utilized by
  Kristensen-McLachlan et al.
  (<xref alt="2024" rid="ref-keynmf" ref-type="bibr">2024</xref>) for
  analyzing information dynamics in Chinese diaspora media, and is
  currently being used in multiple ongoing research projects, including
  one concerning the media coverage of the HPV vaccine in Denmark, and
  another studying Danish golden-age literature. We provide examples of
  correct usage and case studies as part of our documentation.</p>
</sec>
<sec id="target-audience">
  <title>Target Audience</title>
  <p>Turftopic’s utility has already been demonstrated for computational
  scholars in digital humanities, and political science, and we expect
  that it will be of utility to a diverse audience of researchers in
  social sciences, medicine, linguistics and legal studies. It can
  furthermore prove valuable to business analysts working with
  text-based data to generate qualitative insights.</p>
  <p>As the focus on pre-training data mixing techniques is on the rise,
  we expect that Turftopic will help facilitate foundational language
  model research. The library’s design, wide array of models, and
  flexibility are also aimed at enabling usage in more extended
  production pipelines for retrieval, filtering or content
  recommendation, and we thus expect the package to be a most valuable
  tool for the industry NLP practitioner.</p>
  <p>Turftopic is also an appropriate choice for educational purposes,
  providing instructors with a single, user-friendly framework for
  students to explore and compare alternative topic modelling
  approaches.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-s3">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kardos</surname><given-names>Márton</given-names></name>
        <name><surname>Kostkan</surname><given-names>Jan</given-names></name>
        <name><surname>Vermillet</surname><given-names>Arnault-Quentin</given-names></name>
        <name><surname>Nielbo</surname><given-names>Kristoffer</given-names></name>
        <name><surname>Enevoldsen</surname><given-names>Kenneth</given-names></name>
        <name><surname>Rocca</surname><given-names>Roberta</given-names></name>
      </person-group>
      <article-title>S^3 – semantic signal separation</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2406.09556</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2406.09556</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-keynmf">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kristensen-McLachlan</surname><given-names>Ross Deans</given-names></name>
        <name><surname>Hicke</surname><given-names>Rebecca Marie Matouschek</given-names></name>
        <name><surname>Kardos</surname><given-names>Márton</given-names></name>
        <name><surname>Thunø</surname><given-names>Mette</given-names></name>
      </person-group>
      <article-title>Context is key(NMF):: Modelling topical information dynamics in chinese diaspora media</article-title>
      <source>Proceedings of the computational humanities research conference 2024</source>
      <person-group person-group-type="editor">
        <name><surname>Haverals</surname><given-names>Wouter </given-names></name>
        <name><surname>Koolen</surname><given-names>Marijn </given-names></name>
        <name><surname>Thompson</surname><given-names>Laure </given-names></name>
      </person-group>
      <publisher-name>CEUR-WS</publisher-name>
      <publisher-loc>Germany</publisher-loc>
      <year iso-8601-date="2024-12">2024</year><month>12</month>
      <volume>3834</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2410.12791</pub-id>
      <fpage>829</fpage>
      <lpage>847</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bertopic_paper">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Grootendorst</surname><given-names>Maarten</given-names></name>
      </person-group>
      <article-title>BERTopic: Neural topic modeling with a class-based TF-IDF procedure</article-title>
      <source>arXiv preprint arXiv:2203.05794</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2203.05794</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-topmost">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Xiaobao</given-names></name>
        <name><surname>Pan</surname><given-names>Fengjun</given-names></name>
        <name><surname>Luu</surname><given-names>Anh Tuan</given-names></name>
      </person-group>
      <article-title>Towards the TopMost: A topic modeling system toolkit</article-title>
      <source>Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 3: System demonstrations)</source>
      <person-group person-group-type="editor">
        <name><surname>Cao</surname><given-names>Yixin</given-names></name>
        <name><surname>Feng</surname><given-names>Yang</given-names></name>
        <name><surname>Xiong</surname><given-names>Deyi</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Bangkok, Thailand</publisher-loc>
      <year iso-8601-date="2024-08">2024</year><month>08</month>
      <uri>https://aclanthology.org/2024.acl-demos.4/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2024.acl-demos.4</pub-id>
      <fpage>31</fpage>
      <lpage>41</lpage>
    </element-citation>
  </ref>
  <ref id="ref-quantitative_text_analysis">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nielbo</surname><given-names>Kristoffer L.</given-names></name>
        <name><surname>Karsdorp</surname><given-names>Folgert</given-names></name>
        <name><surname>Wevers</surname><given-names>Melvin</given-names></name>
        <name><surname>Lassche</surname><given-names>Alie</given-names></name>
        <name><surname>Baglini</surname><given-names>Rebekah B.</given-names></name>
        <name><surname>Kestemont</surname><given-names>Mike</given-names></name>
        <name><surname>Tahmasebi</surname><given-names>Nina</given-names></name>
      </person-group>
      <article-title>Quantitative text analysis</article-title>
      <source>Nature Reviews Methods Primers</source>
      <year iso-8601-date="2024-04">2024</year><month>04</month>
      <volume>4</volume>
      <issue>1</issue>
      <uri>https://www.nature.com/articles/s43586-024-00302-w#citeas</uri>
      <pub-id pub-id-type="doi">10.1038/s43586-024-00302-w</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-stream">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Thielmann</surname><given-names>Anton</given-names></name>
        <name><surname>Reuter</surname><given-names>Arik</given-names></name>
        <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
        <name><surname>Kant</surname><given-names>Gillian</given-names></name>
        <name><surname>Kumar</surname><given-names>Manish</given-names></name>
        <name><surname>Säfken</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>STREAM: Simplified topic retrieval, exploration, and analysis module</article-title>
      <source>Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 2: Short papers)</source>
      <person-group person-group-type="editor">
        <name><surname>Ku</surname><given-names>Lun-Wei</given-names></name>
        <name><surname>Martins</surname><given-names>Andre</given-names></name>
        <name><surname>Srikumar</surname><given-names>Vivek</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Bangkok, Thailand</publisher-loc>
      <year iso-8601-date="2024-08">2024</year><month>08</month>
      <uri>https://aclanthology.org/2024.acl-short.41/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2024.acl-short.41</pub-id>
      <fpage>435</fpage>
      <lpage>444</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ctm">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bianchi</surname><given-names>Federico</given-names></name>
        <name><surname>Terragni</surname><given-names>Silvia</given-names></name>
        <name><surname>Hovy</surname><given-names>Dirk</given-names></name>
      </person-group>
      <article-title>Pre-training is a hot topic: Contextualized document embeddings improve topic coherence</article-title>
      <source>Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 2: Short papers)</source>
      <person-group person-group-type="editor">
        <name><surname>Zong</surname><given-names>Chengqing</given-names></name>
        <name><surname>Xia</surname><given-names>Fei</given-names></name>
        <name><surname>Li</surname><given-names>Wenjie</given-names></name>
        <name><surname>Navigli</surname><given-names>Roberto</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021-08">2021</year><month>08</month>
      <uri>https://aclanthology.org/2021.acl-short.96</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2021.acl-short.96</pub-id>
      <fpage>759</fpage>
      <lpage>766</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zeroshot_tm">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bianchi</surname><given-names>Federico</given-names></name>
        <name><surname>Terragni</surname><given-names>Silvia</given-names></name>
        <name><surname>Hovy</surname><given-names>Dirk</given-names></name>
        <name><surname>Nozza</surname><given-names>Debora</given-names></name>
        <name><surname>Fersini</surname><given-names>Elisabetta</given-names></name>
      </person-group>
      <article-title>Cross-lingual contextualized topic models with zero-shot learning</article-title>
      <source>Proceedings of the 16th conference of the european chapter of the association for computational linguistics: Main volume</source>
      <person-group person-group-type="editor">
        <name><surname>Merlo</surname><given-names>Paola</given-names></name>
        <name><surname>Tiedemann</surname><given-names>Jorg</given-names></name>
        <name><surname>Tsarfaty</surname><given-names>Reut</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021-04">2021</year><month>04</month>
      <uri>https://aclanthology.org/2021.eacl-main.143</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2021.eacl-main.143</pub-id>
      <fpage>1676</fpage>
      <lpage>1683</lpage>
    </element-citation>
  </ref>
  <ref id="ref-top2vec">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Angelov</surname><given-names>Dimo</given-names></name>
      </person-group>
      <article-title>Top2Vec: Distributed representations of topics</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2008.09470</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2008.09470</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-scikit-learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fastopic">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Xiaobao</given-names></name>
        <name><surname>Nguyen</surname><given-names>Thong Thanh</given-names></name>
        <name><surname>Zhang</surname><given-names>Delvin Ce</given-names></name>
        <name><surname>Wang</surname><given-names>William Yang</given-names></name>
        <name><surname>Luu</surname><given-names>Anh Tuan</given-names></name>
      </person-group>
      <article-title>FASTopic: Pretrained transformer is a fast, adaptive, stable, and transferable topic model</article-title>
      <source>The thirty-eighth annual conference on neural information processing systems</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2405.17978</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-sentence_transformers">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Reimers</surname><given-names>Nils</given-names></name>
        <name><surname>Gurevych</surname><given-names>Iryna</given-names></name>
      </person-group>
      <article-title>Sentence-BERT: Sentence embeddings using Siamese BERT-networks</article-title>
      <source>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</source>
      <person-group person-group-type="editor">
        <name><surname>Inui</surname><given-names>Kentaro</given-names></name>
        <name><surname>Jiang</surname><given-names>Jing</given-names></name>
        <name><surname>Ng</surname><given-names>Vincent</given-names></name>
        <name><surname>Wan</surname><given-names>Xiaojun</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Hong Kong, China</publisher-loc>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <uri>https://aclanthology.org/D19-1410/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/D19-1410</pub-id>
      <fpage>3982</fpage>
      <lpage>3992</lpage>
    </element-citation>
  </ref>
  <ref id="ref-topicwizard">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kardos</surname><given-names>Márton</given-names></name>
        <name><surname>Enevoldsen</surname><given-names>Kenneth C.</given-names></name>
        <name><surname>Nielbo</surname><given-names>Kristoffer Laigaard</given-names></name>
      </person-group>
      <article-title>Topicwizard – a modern, model-agnostic framework for topic model visualization and interpretation</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://arxiv.org/abs/2505.13034</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2505.13034</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-discourse_analysis">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jacobs</surname><given-names>Thomas</given-names></name>
        <name><surname>and</surname><given-names>Robin Tschötschel</given-names></name>
      </person-group>
      <article-title>Topic models meet discourse analysis: A quantitative tool for a qualitative approach</article-title>
      <source>International Journal of Social Research Methodology</source>
      <publisher-name>Routledge</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>22</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1080/13645579.2019.1576317</pub-id>
      <fpage>469</fpage>
      <lpage>485</lpage>
    </element-citation>
  </ref>
  <ref id="ref-macroanalysis">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>JOCKERS</surname><given-names>MATTHEW L.</given-names></name>
      </person-group>
      <source>Macroanalysis: Digital methods and literary history</source>
      <publisher-name>University of Illinois Press</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-05-27">2025</year><month>05</month><day>27</day></date-in-citation>
      <isbn>9780252037528</isbn>
      <uri>http://www.jstor.org/stable/10.5406/j.ctt2jcc3m</uri>
    </element-citation>
  </ref>
  <ref id="ref-hotel_sector">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Van-Ho</given-names></name>
        <name><surname>Ho</surname><given-names>Thanh</given-names></name>
      </person-group>
      <article-title>Analysing online customer experience in hotel sector using dynamic topic modelling and net promoter score</article-title>
      <source>Journal of Hospitality and Tourism Technology</source>
      <year iso-8601-date="2023-02">2023</year><month>02</month>
      <volume>14</volume>
      <issue>2</issue>
      <uri>https://www.emerald.com/insight/content/doi/10.1108/jhtt-04-2021-0116/full/html</uri>
      <pub-id pub-id-type="doi">10.1108/jhtt-04-2021-0116</pub-id>
      <fpage>258</fpage>
      <lpage>277</lpage>
    </element-citation>
  </ref>
  <ref id="ref-social_media_mining">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Yinghui</given-names></name>
        <name><surname>Li</surname><given-names>Mei</given-names></name>
        <name><surname>Tsung</surname><given-names>Fugee</given-names></name>
        <name><surname>Chang</surname><given-names>Xiangyu</given-names></name>
      </person-group>
      <article-title>Mining social media data via supervised topic model: Can social media posts inform customer satisfaction?</article-title>
      <source>Decision Sciences</source>
      <year iso-8601-date="2025-01">2025</year><month>01</month>
      <uri>https://onlinelibrary.wiley.com/doi/full/10.1111/deci.12660</uri>
      <pub-id pub-id-type="doi">10.1111/deci.12660</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-content_recommendation">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bergamaschi</surname><given-names>Sonia</given-names></name>
        <name><surname>Po</surname><given-names>Laura</given-names></name>
      </person-group>
      <article-title>Comparing LDA and LSA topic models for content-based movie recommendation systems</article-title>
      <source>Web information systems and technologies</source>
      <person-group person-group-type="editor">
        <name><surname>Monfort</surname><given-names>Valérie</given-names></name>
        <name><surname>Krempels</surname><given-names>Karl-Heinz</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>978-3-319-27030-2</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-319-27030-2_16</pub-id>
      <fpage>247</fpage>
      <lpage>263</lpage>
    </element-citation>
  </ref>
  <ref id="ref-unsupervised_classification">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Thielmann</surname><given-names>Anton</given-names></name>
        <name><surname>Weisser</surname><given-names>Christoph</given-names></name>
        <name><surname>Krenz</surname><given-names>Astrid</given-names></name>
        <name><surname>and</surname><given-names>Benjamin Säfken</given-names></name>
      </person-group>
      <article-title>Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling</article-title>
      <source>Journal of Applied Statistics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>50</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1080/02664763.2021.1919063</pub-id>
      <fpage>574</fpage>
      <lpage>591</lpage>
    </element-citation>
  </ref>
  <ref id="ref-information_retrieval">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yi</surname><given-names>Xing</given-names></name>
        <name><surname>Allan</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>A comparative study of utilizing topic models for information retrieval</article-title>
      <source>Advances in information retrieval</source>
      <person-group person-group-type="editor">
        <name><surname>Boughanem</surname><given-names>Mohand</given-names></name>
        <name><surname>Berrut</surname><given-names>Catherine</given-names></name>
        <name><surname>Mothe</surname><given-names>Josiane</given-names></name>
        <name><surname>Soule-Dupuy</surname><given-names>Chantal</given-names></name>
      </person-group>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <publisher-loc>Berlin, Heidelberg</publisher-loc>
      <year iso-8601-date="2009">2009</year>
      <isbn>978-3-642-00958-7</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-642-00958-7_6</pub-id>
      <fpage>29</fpage>
      <lpage>41</lpage>
    </element-citation>
  </ref>
  <ref id="ref-data_mixers">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Peng</surname><given-names>Jiahui</given-names></name>
        <name><surname>Zhuang</surname><given-names>Xinlin</given-names></name>
        <name><surname>Jiantao</surname><given-names>Qiu</given-names></name>
        <name><surname>Ma</surname><given-names>Ren</given-names></name>
        <name><surname>Yu</surname><given-names>Jing</given-names></name>
        <name><surname>Bai</surname><given-names>Tianyi</given-names></name>
        <name><surname>He</surname><given-names>Conghui</given-names></name>
      </person-group>
      <article-title>Unsupervised topic models are data mixers for pre-training language models</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://arxiv.org/abs/2502.16802</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2502.16802</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
