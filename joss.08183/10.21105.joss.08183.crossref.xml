<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250701085215-0ba597b47b7b14078295c17715befa69ef4fa95e</doi_batch_id>
    <timestamp>20250701085215</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>111</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Turftopic: Topic Modelling with Contextual Representations from Sentence Transformers</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Márton</given_name>
            <surname>Kardos</surname>
            <affiliations>
              <institution><institution_name>Center for Humanities Computing, Aarhus University, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-9652-4498</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Kenneth C.</given_name>
            <surname>Enevoldsen</surname>
            <affiliations>
              <institution><institution_name>Center for Humanities Computing, Aarhus University, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-8733-0966</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Jan</given_name>
            <surname>Kostkan</surname>
            <affiliations>
              <institution><institution_name>Center for Humanities Computing, Aarhus University, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-9707-7121</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Ross Deans</given_name>
            <surname>Kristensen-McLachlan</surname>
            <affiliations>
              <institution><institution_name>Center for Humanities Computing, Aarhus University, Denmark</institution_name></institution>
              <institution><institution_name>Department of Linguistics, Cognitive Science, and Semiotics, Aarhus University, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-8714-1911</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Roberta</given_name>
            <surname>Rocca</surname>
            <affiliations>
              <institution><institution_name>Interacting Minds Center, Aarhus University, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-9017-8088</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>01</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8183</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08183</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.15688293</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8183</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08183</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08183</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08183.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="s3">
            <article_title>S^3 – semantic signal separation</article_title>
            <author>Kardos</author>
            <doi>10.48550/arXiv.2406.09556</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Kardos, M., Kostkan, J., Vermillet, A.-Q., Nielbo, K., Enevoldsen, K., &amp; Rocca, R. (2024). S^3 – semantic signal separation. https://doi.org/10.48550/arXiv.2406.09556</unstructured_citation>
          </citation>
          <citation key="keynmf">
            <article_title>Context is key(NMF):: Modelling topical information dynamics in chinese diaspora media</article_title>
            <author>Kristensen-McLachlan</author>
            <journal_title>Proceedings of the computational humanities research conference 2024</journal_title>
            <volume>3834</volume>
            <doi>10.48550/arXiv.2410.12791</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Kristensen-McLachlan, R. D., Hicke, R. M. M., Kardos, M., &amp; Thunø, M. (2024). Context is key(NMF):: Modelling topical information dynamics in chinese diaspora media. In W. Haverals, M. Koolen, &amp; L. Thompson (Eds.), Proceedings of the computational humanities research conference 2024 (Vol. 3834, pp. 829–847). CEUR-WS. https://doi.org/10.48550/arXiv.2410.12791</unstructured_citation>
          </citation>
          <citation key="bertopic_paper">
            <article_title>BERTopic: Neural topic modeling with a class-based TF-IDF procedure</article_title>
            <author>Grootendorst</author>
            <journal_title>arXiv preprint arXiv:2203.05794</journal_title>
            <doi>10.48550/arXiv.2203.05794</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv Preprint arXiv:2203.05794. https://doi.org/10.48550/arXiv.2203.05794</unstructured_citation>
          </citation>
          <citation key="topmost">
            <article_title>Towards the TopMost: A topic modeling system toolkit</article_title>
            <author>Wu</author>
            <journal_title>Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 3: System demonstrations)</journal_title>
            <doi>10.18653/v1/2024.acl-demos.4</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Wu, X., Pan, F., &amp; Luu, A. T. (2024). Towards the TopMost: A topic modeling system toolkit. In Y. Cao, Y. Feng, &amp; D. Xiong (Eds.), Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 3: System demonstrations) (pp. 31–41). Association for Computational Linguistics. https://doi.org/10.18653/v1/2024.acl-demos.4</unstructured_citation>
          </citation>
          <citation key="quantitative_text_analysis">
            <article_title>Quantitative text analysis</article_title>
            <author>Nielbo</author>
            <journal_title>Nature Reviews Methods Primers</journal_title>
            <issue>1</issue>
            <volume>4</volume>
            <doi>10.1038/s43586-024-00302-w</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Nielbo, K. L., Karsdorp, F., Wevers, M., Lassche, A., Baglini, R. B., Kestemont, M., &amp; Tahmasebi, N. (2024). Quantitative text analysis. Nature Reviews Methods Primers, 4(1). https://doi.org/10.1038/s43586-024-00302-w</unstructured_citation>
          </citation>
          <citation key="stream">
            <article_title>STREAM: Simplified topic retrieval, exploration, and analysis module</article_title>
            <author>Thielmann</author>
            <journal_title>Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 2: Short papers)</journal_title>
            <doi>10.18653/v1/2024.acl-short.41</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Thielmann, A., Reuter, A., Weisser, C., Kant, G., Kumar, M., &amp; Säfken, B. (2024). STREAM: Simplified topic retrieval, exploration, and analysis module. In L.-W. Ku, A. Martins, &amp; V. Srikumar (Eds.), Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 2: Short papers) (pp. 435–444). Association for Computational Linguistics. https://doi.org/10.18653/v1/2024.acl-short.41</unstructured_citation>
          </citation>
          <citation key="ctm">
            <article_title>Pre-training is a hot topic: Contextualized document embeddings improve topic coherence</article_title>
            <author>Bianchi</author>
            <journal_title>Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 2: Short papers)</journal_title>
            <doi>10.18653/v1/2021.acl-short.96</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Bianchi, F., Terragni, S., &amp; Hovy, D. (2021). Pre-training is a hot topic: Contextualized document embeddings improve topic coherence. In C. Zong, F. Xia, W. Li, &amp; R. Navigli (Eds.), Proceedings of the 59th annual meeting of the association for computational linguistics and the 11th international joint conference on natural language processing (volume 2: Short papers) (pp. 759–766). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.acl-short.96</unstructured_citation>
          </citation>
          <citation key="zeroshot_tm">
            <article_title>Cross-lingual contextualized topic models with zero-shot learning</article_title>
            <author>Bianchi</author>
            <journal_title>Proceedings of the 16th conference of the european chapter of the association for computational linguistics: Main volume</journal_title>
            <doi>10.18653/v1/2021.eacl-main.143</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Bianchi, F., Terragni, S., Hovy, D., Nozza, D., &amp; Fersini, E. (2021). Cross-lingual contextualized topic models with zero-shot learning. In P. Merlo, J. Tiedemann, &amp; R. Tsarfaty (Eds.), Proceedings of the 16th conference of the european chapter of the association for computational linguistics: Main volume (pp. 1676–1683). Association for Computational Linguistics. https://doi.org/10.18653/v1/2021.eacl-main.143</unstructured_citation>
          </citation>
          <citation key="top2vec">
            <article_title>Top2Vec: Distributed representations of topics</article_title>
            <author>Angelov</author>
            <doi>10.48550/arXiv.2008.09470</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Angelov, D. (2020). Top2Vec: Distributed representations of topics. https://doi.org/10.48550/arXiv.2008.09470</unstructured_citation>
          </citation>
          <citation key="scikit-learn">
            <article_title>Scikit-learn: Machine learning in Python</article_title>
            <author>Pedregosa</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <volume>12</volume>
            <cYear>2011</cYear>
            <unstructured_citation>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.</unstructured_citation>
          </citation>
          <citation key="fastopic">
            <article_title>FASTopic: Pretrained transformer is a fast, adaptive, stable, and transferable topic model</article_title>
            <author>Wu</author>
            <journal_title>The thirty-eighth annual conference on neural information processing systems</journal_title>
            <doi>10.48550/arXiv.2405.17978</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Wu, X., Nguyen, T. T., Zhang, D. C., Wang, W. Y., &amp; Luu, A. T. (2024). FASTopic: Pretrained transformer is a fast, adaptive, stable, and transferable topic model. The Thirty-Eighth Annual Conference on Neural Information Processing Systems. https://doi.org/10.48550/arXiv.2405.17978</unstructured_citation>
          </citation>
          <citation key="sentence_transformers">
            <article_title>Sentence-BERT: Sentence embeddings using Siamese BERT-networks</article_title>
            <author>Reimers</author>
            <journal_title>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</journal_title>
            <doi>10.18653/v1/D19-1410</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Reimers, N., &amp; Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In K. Inui, J. Jiang, V. Ng, &amp; X. Wan (Eds.), Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP) (pp. 3982–3992). Association for Computational Linguistics. https://doi.org/10.18653/v1/D19-1410</unstructured_citation>
          </citation>
          <citation key="topicwizard">
            <article_title>Topicwizard – a modern, model-agnostic framework for topic model visualization and interpretation</article_title>
            <author>Kardos</author>
            <doi>10.48550/arXiv.2505.13034</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Kardos, M., Enevoldsen, K. C., &amp; Nielbo, K. L. (2025). Topicwizard – a modern, model-agnostic framework for topic model visualization and interpretation. https://doi.org/10.48550/arXiv.2505.13034</unstructured_citation>
          </citation>
          <citation key="discourse_analysis">
            <article_title>Topic models meet discourse analysis: A quantitative tool for a qualitative approach</article_title>
            <author>Jacobs</author>
            <journal_title>International Journal of Social Research Methodology</journal_title>
            <issue>5</issue>
            <volume>22</volume>
            <doi>10.1080/13645579.2019.1576317</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Jacobs, T., &amp; and, R. T. (2019). Topic models meet discourse analysis: A quantitative tool for a qualitative approach. International Journal of Social Research Methodology, 22(5), 469–485. https://doi.org/10.1080/13645579.2019.1576317</unstructured_citation>
          </citation>
          <citation key="macroanalysis">
            <volume_title>Macroanalysis: Digital methods and literary history</volume_title>
            <author>JOCKERS</author>
            <isbn>9780252037528</isbn>
            <cYear>2013</cYear>
            <unstructured_citation>JOCKERS, M. L. (2013). Macroanalysis: Digital methods and literary history. University of Illinois Press. ISBN: 9780252037528</unstructured_citation>
          </citation>
          <citation key="hotel_sector">
            <article_title>Analysing online customer experience in hotel sector using dynamic topic modelling and net promoter score</article_title>
            <author>Nguyen</author>
            <journal_title>Journal of Hospitality and Tourism Technology</journal_title>
            <issue>2</issue>
            <volume>14</volume>
            <doi>10.1108/jhtt-04-2021-0116</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Nguyen, V.-H., &amp; Ho, T. (2023). Analysing online customer experience in hotel sector using dynamic topic modelling and net promoter score. Journal of Hospitality and Tourism Technology, 14(2), 258–277. https://doi.org/10.1108/jhtt-04-2021-0116</unstructured_citation>
          </citation>
          <citation key="social_media_mining">
            <article_title>Mining social media data via supervised topic model: Can social media posts inform customer satisfaction?</article_title>
            <author>Huang</author>
            <journal_title>Decision Sciences</journal_title>
            <doi>10.1111/deci.12660</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Huang, Y., Li, M., Tsung, F., &amp; Chang, X. (2025). Mining social media data via supervised topic model: Can social media posts inform customer satisfaction? Decision Sciences. https://doi.org/10.1111/deci.12660</unstructured_citation>
          </citation>
          <citation key="content_recommendation">
            <article_title>Comparing LDA and LSA topic models for content-based movie recommendation systems</article_title>
            <author>Bergamaschi</author>
            <journal_title>Web information systems and technologies</journal_title>
            <doi>10.1007/978-3-319-27030-2_16</doi>
            <isbn>978-3-319-27030-2</isbn>
            <cYear>2015</cYear>
            <unstructured_citation>Bergamaschi, S., &amp; Po, L. (2015). Comparing LDA and LSA topic models for content-based movie recommendation systems. In V. Monfort &amp; K.-H. Krempels (Eds.), Web information systems and technologies (pp. 247–263). Springer International Publishing. https://doi.org/10.1007/978-3-319-27030-2_16</unstructured_citation>
          </citation>
          <citation key="unsupervised_classification">
            <article_title>Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling</article_title>
            <author>Thielmann</author>
            <journal_title>Journal of Applied Statistics</journal_title>
            <issue>3</issue>
            <volume>50</volume>
            <doi>10.1080/02664763.2021.1919063</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Thielmann, A., Weisser, C., Krenz, A., &amp; and, B. S. (2023). Unsupervised document classification integrating web scraping, one-class SVM and LDA topic modelling. Journal of Applied Statistics, 50(3), 574–591. https://doi.org/10.1080/02664763.2021.1919063</unstructured_citation>
          </citation>
          <citation key="information_retrieval">
            <article_title>A comparative study of utilizing topic models for information retrieval</article_title>
            <author>Yi</author>
            <journal_title>Advances in information retrieval</journal_title>
            <doi>10.1007/978-3-642-00958-7_6</doi>
            <isbn>978-3-642-00958-7</isbn>
            <cYear>2009</cYear>
            <unstructured_citation>Yi, X., &amp; Allan, J. (2009). A comparative study of utilizing topic models for information retrieval. In M. Boughanem, C. Berrut, J. Mothe, &amp; C. Soule-Dupuy (Eds.), Advances in information retrieval (pp. 29–41). Springer Berlin Heidelberg. https://doi.org/10.1007/978-3-642-00958-7_6</unstructured_citation>
          </citation>
          <citation key="data_mixers">
            <article_title>Unsupervised topic models are data mixers for pre-training language models</article_title>
            <author>Peng</author>
            <doi>10.48550/arXiv.2502.16802</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Peng, J., Zhuang, X., Jiantao, Q., Ma, R., Yu, J., Bai, T., &amp; He, C. (2025). Unsupervised topic models are data mixers for pre-training language models. https://doi.org/10.48550/arXiv.2502.16802</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
