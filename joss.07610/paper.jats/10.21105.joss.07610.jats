<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7610</article-id>
<article-id pub-id-type="doi">10.21105/joss.07610</article-id>
<title-group>
<article-title>Parallel-CDM: Parallel Implementation of Continuum Damage
Mechanics Simulations using FEM and MATLAB</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-1659-9411</contrib-id>
<name>
<surname>Eldababy</surname>
<given-names>Habiba</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Saji</surname>
<given-names>Roshan Philip</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pantidis</surname>
<given-names>Panos</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mobasher</surname>
<given-names>Mostafa</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Mechanical Engineering Department, Tandon School of
Engineering, New York University, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Mechanical Engineering Department, New York University Abu
Dhabi, UAE</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Civil and Urban Engineering Department, New York University
Abu Dhabi, UAE</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>10</volume>
<issue>112</issue>
<fpage>7610</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>parallel computing</kwd>
<kwd>MATLAB</kwd>
<kwd>finite element method</kwd>
<kwd>continuum damage mechanics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Modeling fracture in materials and structures holds immense
  importance in our efforts to understand how materials fail and hence
  design more fracture-resistant structures. Among the several theories
  developed over the last decades, continuum damage mechanics (CDM)
  studies the behavior of cracks in materials and structures from the
  viewpoint of continuous stiffness degradation as the crack propagates
  inside the domain
  (<xref alt="Lemaitre, 2012" rid="ref-lemaitre2012" ref-type="bibr">Lemaitre,
  2012</xref>). CDM simulations are commonly implemented using the
  finite element method, however the associated computational cost is
  notoriously elevated. With parallel computing becoming increasingly
  widespread, it presents an efficient strategy for reducing the high
  computational cost associated with CDM simulations. This open source
  code utilizes parallelization techniques for MATLAB in order to
  significantly accelerate CDM simulations. Building upon previous work
  by the authors
  (<xref alt="Saji et al., 2024" rid="ref-saji2024" ref-type="bibr">Saji
  et al., 2024</xref>), we develop a parallel MATLAB code and
  demonstrate the additional efficiency over its serial counterpart. The
  code is geared for quasi-brittle materials, and it is implemented with
  two relevant damage models (Mazars’ model
  (<xref alt="Mazars, 1986" rid="ref-mazars1986" ref-type="bibr">Mazars,
  1986</xref>) and Geers’ model
  (<xref alt="Geers et al., 1998" rid="ref-geers1998" ref-type="bibr">Geers
  et al., 1998</xref>)). Both the unified arc-length (UAL) and
  Newton–Raphson solvers can be used.</p>
  <p>There are several FEM libraries with parallel capabilities publicly
  available, such as FEniCS
  (<xref alt="The FEniCS Project, 2024" rid="ref-fenics2024" ref-type="bibr">The
  FEniCS Project, 2024</xref>), OOFEM
  (<xref alt="Öhman et al., 2020" rid="ref-mikael_ohman_2020_4339630" ref-type="bibr">Öhman
  et al., 2020</xref>), Akantu
  (<xref alt="Richart et al., 2024" rid="ref-Richart2024" ref-type="bibr">Richart
  et al., 2024</xref>), OpenSees
  (<xref alt="Pacific Earthquake Engineering Research Center, 2025" rid="ref-opensees2025" ref-type="bibr">Pacific
  Earthquake Engineering Research Center, 2025</xref>), and deal.II
  (<xref alt="Arndt et al., 2021" rid="ref-Arndt_The_deal_II_finite_2021" ref-type="bibr">Arndt
  et al., 2021</xref>), but they use parallelization strategies such as
  domain decomposition and/or Message Passing Interface (MPI) and
  require coding expertise from the user. Our code is unique in its
  user-accessibility, given that it is written in MATLAB which many
  users are familiar with, while also implementing parallelization
  techniques for complex continuum damage mechanics simulations. Also,
  it features the implementation of a newly developed and robust Unified
  arc-length solver (UAL) developed in Saji et al.
  (<xref alt="2024" rid="ref-saji2024" ref-type="bibr">2024</xref>),
  which has demonstrated its superior performance against the
  force-controlled arc-length (FAL) and Newton–Raphson (NR) solvers both
  in terms of accuracy and time efficiency. For example, in a 1D bar
  problem modeled using the non-local gradient damage and a weakened
  region in the middle (modulus of elasticity is reduced by a factor of
  two), the UAL solver models the entire non-linear equilibrium path in
  0.98 seconds with 97 increments, while the FAL and NR solvers require
  one to two orders of magnitude more time and increments to trace the
  same path with a more relaxed convergence tolerance
  (<xref alt="Saji et al., 2024" rid="ref-saji2024" ref-type="bibr">Saji
  et al., 2024</xref>). The UAL solver can trace equilibrium paths with
  snap-backs, making it a suitable solver for a wide range of scenarios,
  which the NR solver cannot capture. Furthermore, the UAL solver is not
  constrained by the need for small step sizes and relaxed convergence
  tolerances that limit the usage of the FAL solver.</p>
</sec>
<sec id="methodology-and-software-implementation">
  <title>Methodology and Software Implementation</title>
  <p>A flowchart of the code’s general functionality is shown in
  <xref alt="[fig:general_flowchart]" rid="figU003Ageneral_flowchart">[fig:general_flowchart]</xref>.
  The function on which we focus is the
  <monospace>func_globalstiffness()</monospace> function, which
  calculates and assembles the global stiffness matrix, force vectors,
  and residual vector. The function also calculates the projection
  matrices required for contour plotting once the numerical analysis of
  each load increment is complete.</p>
  <p>In MATLAB, parallelization relies on the Parallel Computing
  Toolbox, in which the user can generate a parallel pool of workers.
  This parallelization is illustrated in
  <xref alt="[fig:gstiffness_flowchart]" rid="figU003Agstiffness_flowchart">[fig:gstiffness_flowchart]</xref>.
  In the “solver” mode of the code, the function will assemble global
  matrices, so we implement the single program multiple data (SPMD)
  construct in MATLAB since ordered execution and data sharing between
  workers is needed. In the “plotting properties” mode, we use “parfor”
  as the loop body is independent, and iterations can be executed in any
  order.</p>
  <fig>
    <caption><p>Flowchart of the general code
    structure<styled-content id="figU003Ageneral_flowchart"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="flowchart_general.png" />
  </fig>
  <fig>
    <caption><p>Flowchart of the parallel
    <monospace>func_globalstiffness()</monospace> function which
    includes solving and plotting properties
    routines<styled-content id="figU003Agstiffness_flowchart"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="flowchart_globalstiffness.png" />
  </fig>
</sec>
<sec id="testing-and-results">
  <title>Testing and Results</title>
  <p>A symmetric single notch tension (SSNT) problem is used to test the
  code’s effectiveness following Saji et al.
  (<xref alt="2024" rid="ref-saji2024" ref-type="bibr">2024</xref>) and
  Pantidis &amp; Mobasher
  (<xref alt="2023" rid="ref-pantidis2023" ref-type="bibr">2023</xref>).
  In
  <xref alt="[fig:threads_totalruntime_HPC]" rid="figU003Athreads_totalruntime_HPC">[fig:threads_totalruntime_HPC]</xref>
  we present the total runtime of a fine (10201 elements) mesh on the
  New York University Abu Dhabi (NYUAD) High Performance Computing (HPC)
  cluster, for serial and parallel implementation. From this figure, it
  is clear that the parallelization exhibits significant cost
  improvement, with a factor of three reduction in the total runtime of
  the code for a serial computation to parallel with 8, 16, or 32
  threads. Above 32 threads,
  <xref alt="[fig:threads_totalruntime_HPC]" rid="figU003Athreads_totalruntime_HPC">[fig:threads_totalruntime_HPC]</xref>
  reveals that additional parallel resources do not provide further
  improvement for the size of this problem. When pursuing faster
  runtimes with parallel computing, the overhead costs should be weighed
  against potential improvement in speed.</p>
  <fig>
    <caption><p>Total runtime using fine mesh on HPC with the UAL
    solver. The number of threads is represented on a logarithmic scale
    for clarity. Additional parallel resources reduce the runtime of a
    fine mesh on the HPC cluster.
    <styled-content id="figU003Athreads_totalruntime_HPC"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="threads_totalruntime_HPC.png" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was partially supported by the Sand Hazards and
  Opportunities for Resilience, Energy, and Sustainability (SHORES)
  Center, funded by Tamkeen under the NYUAD Research Institute. The
  authors would also like to acknowledge the support of the NYUAD Center
  for Research Computing for providing resources, services, and staff
  expertise.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-lemaitre2012">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Lemaitre</surname><given-names>J.</given-names></name>
      </person-group>
      <source>A course on damage mechanics</source>
      <publisher-name>Springer</publisher-name>
      <publisher-loc>Berlin</publisher-loc>
      <year iso-8601-date="2012">2012</year>
      <pub-id pub-id-type="doi">10.1007/978-3-642-18255-6</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-saji2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Saji</surname><given-names>R. P.</given-names></name>
        <name><surname>Pantidis</surname><given-names>P.</given-names></name>
        <name><surname>Mobasher</surname><given-names>M. E.</given-names></name>
      </person-group>
      <article-title>A new unified arc-length method for damage mechanics problems</article-title>
      <source>Computational Mechanics</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1007/s00466-024-02473-5</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mazars1986">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mazars</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>A description of micro-and macroscale damage of concrete structures</article-title>
      <source>Engineering Fracture Mechanics</source>
      <year iso-8601-date="1986">1986</year>
      <volume>25</volume>
      <issue>5-6</issue>
      <pub-id pub-id-type="doi">10.1016/0013-7944(86)90036-6</pub-id>
      <fpage>729</fpage>
      <lpage>737</lpage>
    </element-citation>
  </ref>
  <ref id="ref-geers1998">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Geers</surname><given-names>M. G. D.</given-names></name>
        <name><surname>Borst</surname><given-names>R. de</given-names></name>
        <name><surname>Brekelmans</surname><given-names>W. A. M.</given-names></name>
        <name><surname>Peerlings</surname><given-names>R. H. J.</given-names></name>
      </person-group>
      <article-title>Strain-based transient-gradient damage model for failure analyses</article-title>
      <source>Computer Methods in Applied Mechanics and Engineering</source>
      <year iso-8601-date="1998">1998</year>
      <pub-id pub-id-type="doi">10.1016/S0045-7825(98)80011-X</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-fenics2024">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <string-name>The FEniCS Project</string-name>
      </person-group>
      <article-title>FEniCS project</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://fenicsproject.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-mikael_ohman_2020_4339630">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Öhman</surname><given-names>Mikael</given-names></name>
        <name><surname>Patzak</surname><given-names>Borek</given-names></name>
        <name><surname>Brouzoulis</surname><given-names>Jim</given-names></name>
        <name><surname>Smilauer</surname><given-names>Vit</given-names></name>
        <name><surname>milanjirasek</surname></name>
        <name><surname>Grassl</surname><given-names>Peter</given-names></name>
        <name><surname>graspel</surname></name>
        <name><surname>feymark</surname></name>
        <name><surname>CarlSandstrom</surname></name>
        <name><surname>nitramkaroh</surname></name>
        <name><surname>MartinFagerstrom</surname></name>
        <name><surname>pedroskop</surname></name>
        <name><surname>Främby</surname><given-names>Johannes</given-names></name>
        <name><surname>eudoxos</surname></name>
        <name><surname>Sciegaj</surname><given-names>Adam</given-names></name>
        <name><surname>editaDvorakova</surname></name>
        <name><surname>Sulc</surname><given-names>Stanislav</given-names></name>
        <name><surname>Stránský</surname><given-names>Jan</given-names></name>
        <name><surname>karelmikes</surname></name>
        <name><surname>Carlsson</surname><given-names>Kristoffer</given-names></name>
        <name><surname>sancho-s</surname></name>
        <name><surname>vmonkey</surname></name>
      </person-group>
      <article-title>Oofem/oofem: OOFEM, version 2.5</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2020-12">2020</year><month>12</month>
      <uri>https://doi.org/10.5281/zenodo.4339630</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.4339630</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-akantu2024">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>EPFL LSMS</string-name>
      </person-group>
      <source>Akantu: A finite element library for complex multiphysics simulations</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://akantu.ch/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Richart2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Richart</surname><given-names>Nicolas</given-names></name>
        <name><surname>Anciaux</surname><given-names>Guillaume</given-names></name>
        <name><surname>Gallyamov</surname><given-names>Emil</given-names></name>
        <name><surname>Frérot</surname><given-names>Lucas</given-names></name>
        <name><surname>Kammer</surname><given-names>David</given-names></name>
        <name><surname>Pundir</surname><given-names>Mohit</given-names></name>
        <name><surname>Vocialta</surname><given-names>Marco</given-names></name>
        <name><surname>Ramos</surname><given-names>Aurelia Cuba</given-names></name>
        <name><surname>Corrado</surname><given-names>Mauro</given-names></name>
        <name><surname>Müller</surname><given-names>Philip</given-names></name>
        <name><surname>Barras</surname><given-names>Fabian</given-names></name>
        <name><surname>Zhang</surname><given-names>Shenghan</given-names></name>
        <name><surname>Ferry</surname><given-names>Roxane</given-names></name>
        <name><surname>Durussel</surname><given-names>Shad</given-names></name>
        <name><surname>Molinari</surname><given-names>Jean-François</given-names></name>
      </person-group>
      <article-title>Akantu: An HPC finite-element library for contact and dynamic fracture simulations</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>9</volume>
      <issue>94</issue>
      <uri>https://doi.org/10.21105/joss.05253</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05253</pub-id>
      <fpage>5253</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-opensees2025">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>Pacific Earthquake Engineering Research Center</string-name>
      </person-group>
      <source>OpenSees: Open system for earthquake engineering simulation</source>
      <year iso-8601-date="2025">2025</year>
      <uri>https://opensees.berkeley.edu/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Arndt_The_deal_II_finite_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arndt</surname><given-names>Daniel</given-names></name>
        <name><surname>Bangerth</surname><given-names>Wolfgang</given-names></name>
        <name><surname>Davydov</surname><given-names>Denis</given-names></name>
        <name><surname>Heister</surname><given-names>Timo</given-names></name>
        <name><surname>Heltai</surname><given-names>Luca</given-names></name>
        <name><surname>Kronbichler</surname><given-names>Martin</given-names></name>
        <name><surname>Maier</surname><given-names>Matthias</given-names></name>
        <name><surname>Pelteret</surname><given-names>Jean-Paul</given-names></name>
        <name><surname>Turcksin</surname><given-names>Bruno</given-names></name>
        <name><surname>Wells</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>The deal.II finite element library: Design, features, and insights</article-title>
      <source>Computers &amp; Mathematics with Applications</source>
      <year iso-8601-date="2021-01">2021</year><month>01</month>
      <volume>81</volume>
      <pub-id pub-id-type="doi">10.1016/j.camwa.2020.02.022</pub-id>
      <fpage>407</fpage>
      <lpage>422</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pantidis2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pantidis</surname><given-names>P.</given-names></name>
        <name><surname>Mobasher</surname><given-names>M. E.</given-names></name>
      </person-group>
      <article-title>Integrated finite element neural network (i-FENN) for non-local continuum damage mechanics</article-title>
      <source>Computational Methods in Applied Mechanics and Engineering</source>
      <year iso-8601-date="2023">2023</year>
      <volume>404</volume>
      <pub-id pub-id-type="doi">10.1016/j.cma.2022.115766</pub-id>
      <fpage>115766</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
