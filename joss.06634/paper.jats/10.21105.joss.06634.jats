<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6634</article-id>
<article-id pub-id-type="doi">10.21105/joss.06634</article-id>
<title-group>
<article-title>Soundata: Reproducible use of audio
datasets</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4506-6639</contrib-id>
<name>
<surname>Fuentes</surname>
<given-names>Magdalena</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3450-3194</contrib-id>
<name>
<surname>Plaja-Roglans</surname>
<given-names>Genís</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2827-8955</contrib-id>
<name>
<surname>Cortès-Sebastià</surname>
<given-names>Guillem</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0004-3770-8317</contrib-id>
<name>
<surname>Khandelwal</surname>
<given-names>Tanmay</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2563-075X</contrib-id>
<name>
<surname>Miron</surname>
<given-names>Marius</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1395-2345</contrib-id>
<name>
<surname>Serra</surname>
<given-names>Xavier</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8561-5204</contrib-id>
<name>
<surname>Bello</surname>
<given-names>Juan Pablo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6345-4593</contrib-id>
<name>
<surname>Salamon</surname>
<given-names>Justin</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>New York University, New York, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Universitat Pompeu Fabra, Barcelona, Spain</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Earth Species Project, Barcelona, Spain</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Adobe Research, San Francisco, United States</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-11-30">
<day>30</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>98</issue>
<fpage>6634</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>audio</kwd>
<kwd>dataset</kwd>
<kwd>urban-sound</kwd>
<kwd>environmental-sound</kwd>
<kwd>bioacoustics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Soundata</monospace> is an open-source Python library
  for working with audio datasets in a programmatic and standardized
  way. It removes the need for writing custom loaders and improves
  reproducibility by providing tools to validate data against a
  canonical version. It speeds up research pipelines by allowing users
  to quickly download a dataset, validate that the dataset is complete
  and correct, and load it into memory in a standardized and
  reproducible way. It is designed to work with bioacoustics,
  environmental, urban, and spatial sound datasets; to be easy to use
  and easy to contribute to; and to increase reproducibility and
  standardize the usage of sound datasets in a flexible way.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>As research pipelines become increasingly complex, it is key that
  their different components are reproducible. In recent years, the
  research community has made considerable efforts towards
  standardization and reproducibility, with modelling and evaluation
  libraries
  (<xref alt="Abadi et al., 2016" rid="ref-tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>;
  <xref alt="Mesaros et al., 2016" rid="ref-mesaros2016metrics" ref-type="bibr">Mesaros
  et al., 2016</xref>;
  <xref alt="Pedregosa et al., 2011" rid="ref-pedregosa2011scikit" ref-type="bibr">Pedregosa
  et al., 2011</xref>), open sourcing models
  (<xref alt="Ravanelli et al., 2021" rid="ref-speechbrain" ref-type="bibr">Ravanelli
  et al., 2021</xref>;
  <xref alt="Zinemanas et al., 2020" rid="ref-zinemanas2020dcase" ref-type="bibr">Zinemanas
  et al., 2020</xref>), and data dissemination using resources such as
  <ext-link ext-link-type="uri" xlink:href="https://zenodo.org">Zenodo</ext-link>.
  However, discrepancies in the local version of the data and different
  practices in loading and parsing datasets can lead to considerable
  differences in performance results, which is misleading when comparing
  methods
  (<xref alt="Bittner et al., 2019" rid="ref-bittner2019mirdata" ref-type="bibr">Bittner
  et al., 2019</xref>). In addition, it is extremely inefficient to
  develop pipelines from scratch for loading and parsing a dataset for
  each researcher or team each time, and this increases the chances of
  bugs and differences that hinder reproducibility.</p>
  <p><monospace>Soundata</monospace> is based on and inspired by
  <monospace>Mirdata</monospace>
  (<xref alt="Bittner et al., 2019" rid="ref-bittner2019mirdata" ref-type="bibr">Bittner
  et al., 2019</xref>), the popular library for working with Music
  Information Research (MIR) datasets, and shares its goals and vision.
  However, in MIR, the aforementioned issues are exacerbated due to the
  intrinsic commercial nature of music data, since it is very difficult
  to get licenses to distribute music recordings openly. Since musical
  datasets are extremely complex compared to other audio datasets, using
  the same software package for handling music and other audio datasets
  would lead to a very complex, hard-to-manage repository, which would
  be difficult to scale. Instead, we introduce
  <monospace>Soundata</monospace> as a separate effort that specifically
  addresses the annotation types and formats required by communities
  like DCASE<xref ref-type="fn" rid="fn1">1</xref>, which work with
  bioacoustics, environmental, urban, and spatial sound datasets.</p>
  <p>There are other libraries that handle datasets like
  <monospace>Tensorflow</monospace>
  (<xref alt="Abadi et al., 2016" rid="ref-tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>) or <monospace>Tensorflow Datasets</monospace>
  (<xref alt="TensorFlow, 2019" rid="ref-tensorflow_datasets" ref-type="bibr">TensorFlow,
  2019</xref>), <monospace>DCASE-models</monospace>
  (<xref alt="Zinemanas et al., 2020" rid="ref-zinemanas2020dcase" ref-type="bibr">Zinemanas
  et al., 2020</xref>), and <monospace>HuggingFace Datasets</monospace>
  (<xref alt="Lhoest et al., 2021" rid="ref-lhoest2021datasets" ref-type="bibr">Lhoest
  et al., 2021</xref>). But none of them serves as a stand-alone library
  that can easily be plugged into different work pipelines, with
  different modeling software. Having a community-centric, open-source,
  audio-specialized library allows us greater flexibility to incorporate
  more audio-specific API functionalities and align our priorities with
  those of the audio community.</p>
  <p><monospace>Soundata</monospace> follows these design
  principles:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Easy to use</bold>: Simplifies audio research pipelines
      considerably by having plug-and-play datasets in a standardized
      format.</p>
    </list-item>
    <list-item>
      <p><bold>Easy to contribute to:</bold> Users do not need to go
      through all the source code to contribute.
      <monospace>Soundata</monospace> provides extensive documentation
      explaining how to contribute a new loader.</p>
    </list-item>
    <list-item>
      <p><bold>Increase reproducibility:</bold> Provides a common
      framework for users to compare and validate their data. It also
      allows them to easily propagate dataset updates or fixes to the
      audio community, ensuring that methods are still comparable and
      users have the same up-to-date dataset versions. On that note,
      <monospace>Soundata</monospace> is designed to handle multiple
      versions of the same dataset, allowing transparent access to all
      versions of the dataset.</p>
    </list-item>
    <list-item>
      <p><bold>Standardize usage of sound datasets:</bold> Standardizes
      common attributes of sound datasets such as audio or tags to
      simplify audio research pipelines, while preserving the
      idiosyncrasies of each dataset (e.g., if a dataset has
      ‘non-standard’ attributes, we include them as well).</p>
    </list-item>
  </list>
</sec>
<sec id="design-choices">
  <title>Design Choices</title>
  <p><monospace>Soundata</monospace> has three main components, depicted
  in
  <xref alt="[fig:soundata_overview]" rid="figU003Asoundata_overview">[fig:soundata_overview]</xref>:
  a <monospace>core</monospace> module that implements the generic
  functions used by all the data loaders (e.g., Dataset), a
  <monospace>utils</monospace> module with the main utility functions
  such as downloading and validating the data or converting to
  <monospace>JAMS</monospace><xref ref-type="fn" rid="fn2">2</xref>
  format, and the dataset <monospace>loaders</monospace> containing
  dataset-specific code to load and parse each dataset in a standardized
  way. Following this design, when a new dataset requires a new
  functionality, it is added to the core module so it can be used for
  similar loaders added later on.</p>
  <fig>
    <caption><p><monospace>Soundata</monospace>’s main
    components.<styled-content id="figU003Asoundata_overview"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="architecture.png" />
  </fig>
</sec>
<sec id="annotation-types">
  <title>Annotation Types</title>
  <p>Annotation types in <monospace>Soundata</monospace> (see
  <xref alt="[fig:annotations]" rid="figU003Aannotations">[fig:annotations]</xref>)
  ensure compatibility with existing evaluation libraries from the DCASE
  community such as <monospace>sed_eval</monospace>, and are convertible
  to the <monospace>JAMS</monospace> format. These annotation types
  allow <monospace>Soundata</monospace> to support a wide range of audio
  research tasks, as shown in
  <xref alt="[fig:tasks]" rid="figU003Atasks">[fig:tasks]</xref>. It
  currently includes three annotation types:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Tags</bold>: String labels with associated confidence
      values, spanning the full duration of the audio clip.</p>
    </list-item>
    <list-item>
      <p><bold>Events</bold>: These annotations are for sound events
      with defined start times, end times, labels, and (optionally)
      confidence values.</p>
    </list-item>
    <list-item>
      <p><bold>Spatial Events</bold>: Spatial Events extends Events
      introducing additional attributes such as geographical coordinates
      (latitude, longitude), altitude, direction (azimuth and
      elevation), and distance from reference points.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Annotation types included in
    <monospace>Soundata</monospace>.<styled-content id="figU003Aannotations"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="annotation_types.png" />
  </fig>
  <fig>
    <caption><p>Audio tasks supported by <monospace>Soundata</monospace>
    as of today.
    <styled-content id="figU003Atasks"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="annotations_tasks.pdf" />
  </fig>
  <sec id="example-usage">
    <title>Example usage
    <styled-content id="secU003Aexample_usage"></styled-content></title>
    <p><monospace>Soundata</monospace> is designed to be user-friendly,
    so that users can start working with audio datasets right away after
    following a few steps, as summarized in
    <xref alt="[fig:supported_datasets]" rid="figU003Asupported_datasets">[fig:supported_datasets]</xref>.</p>
    <fig>
      <caption><p>How to work with any supported dataset in
      <monospace>Soundata</monospace>.<styled-content id="figU003Asupported_datasets"></styled-content></p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="download.pdf" />
    </fig>
    <p>Once the dataset is downloaded and validated,
    <monospace>Soundata</monospace> can be integrated into an audio
    research pipeline easily. The code in
    <xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>
    shows an example of how to get any SED dataset into a deep learning
    pipeline using <monospace>Soundata</monospace> and
    <monospace>Tensorflow</monospace>.</p>
    <fig>
      <caption><p>Soundata usage example. It shows an example of how to
      get any SED dataset into a deep learning
      pipeline.<styled-content id="figU003Aexample"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="example.png" />
    </fig>
  </sec>
  <sec id="contributing">
    <title>Contributing
    <styled-content id="secU003Acontributing"></styled-content></title>
    <p>Contribution to <monospace>Soundata</monospace> is highly
    encouraged. To facilitate the process,
    <monospace>Soundata</monospace> provides an exhaustive contributing
    guide<xref ref-type="fn" rid="fn3">3</xref> available in the
    documentation with all the necessary information on how to
    contribute. The most common contribution in
    <monospace>Soundata</monospace> is the creation of new dataset
    loaders, as they play a crucial role in advancing
    <monospace>Soundata</monospace>’s objective of accommodating as many
    datasets as possible.
    <xref alt="[fig:contributing]" rid="figU003Acontributing">[fig:contributing]</xref>
    summarizes the process of creating a new loader.</p>
    <fig>
      <caption><p>Steps for contributing a dataset loader to
      <monospace>Soundata</monospace>.
      <styled-content id="figU003Acontributing"></styled-content></p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="contributing.pdf" />
    </fig>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We extend our sincere gratitude to all the contributors who have
  been invaluable in the development of this library. We deeply
  appreciate contributions and look forward to continued collaboration
  and growth.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-lhoest2021datasets">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lhoest</surname><given-names>Quentin</given-names></name>
        <name><surname>Moral</surname><given-names>Albert Villanova del</given-names></name>
        <name><surname>Jernite</surname><given-names>Yacine</given-names></name>
        <name><surname>Thakur</surname><given-names>Abhishek</given-names></name>
        <name><surname>Platen</surname><given-names>Patrick von</given-names></name>
        <name><surname>Patil</surname><given-names>Suraj</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Drame</surname><given-names>Mariama</given-names></name>
        <name><surname>Plu</surname><given-names>Julien</given-names></name>
        <name><surname>Tunstall</surname><given-names>Lewis</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Datasets: A community library for natural language processing</article-title>
      <source>arXiv preprint arXiv:2109.02846</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://doi.org/10.48550/arXiv.2109.02846</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2109.02846</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bittner2019mirdata">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bittner</surname><given-names>Rachel M</given-names></name>
        <name><surname>Fuentes</surname><given-names>Magdalena</given-names></name>
        <name><surname>Rubinstein</surname><given-names>David</given-names></name>
        <name><surname>Jansson</surname><given-names>Andreas</given-names></name>
        <name><surname>Choi</surname><given-names>Keunwoo</given-names></name>
        <name><surname>Kell</surname><given-names>Thor</given-names></name>
      </person-group>
      <article-title>Mirdata: Software for reproducible usage of datasets</article-title>
      <source>ISMIR</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://doi.org/10.5281/zenodo.3527750</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.3527750</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pedregosa2011scikit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Michel</surname><given-names>Vincent</given-names></name>
        <name><surname>Thirion</surname><given-names>Bertrand</given-names></name>
        <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
        <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
        <name><surname>Weiss</surname><given-names>Ron</given-names></name>
        <name><surname>Dubourg</surname><given-names>Vincent</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of machine learning research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <issue>Oct</issue>
      <uri>https://doi.org/10.48550/arXiv.1201.0490</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1201.0490</pub-id>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martin</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek G.</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow: A system for large-scale machine learning</article-title>
      <source>12th USENIX symposium on operating systems design and implementation (OSDI 16)</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://doi.org/10.48550/arXiv.1605.08695</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1605.08695</pub-id>
      <fpage>265</fpage>
      <lpage>283</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow_datasets">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>TensorFlow</surname></name>
      </person-group>
      <article-title>TensorFlow Datasets: A collection of ready-to-use datasets</article-title>
      <publisher-name>https://www.tensorflow.org/datasets</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-zinemanas2020dcase">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zinemanas</surname><given-names>Pablo</given-names></name>
        <name><surname>Hounie</surname><given-names>Ignacio</given-names></name>
        <name><surname>Cancela</surname><given-names>Pablo</given-names></name>
        <name><surname>Font Corbera</surname><given-names>Frederic</given-names></name>
        <name><surname>Rocamora</surname><given-names>Martı́n</given-names></name>
        <name><surname>Serra</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>DCASE-models: A Python library for computational environmental sound analysis using deep-learning models</article-title>
      <source>Proceedings of the fifth workshop on detection and classification of acoustic scenes and events (DCASE 2020)</source>
      <person-group person-group-type="editor">
        <name><surname>Ono</surname><given-names>Nobutaka</given-names></name>
        <name><surname>Harada</surname><given-names>Noboru</given-names></name>
        <name><surname>Kawaguchi</surname><given-names>Yohei</given-names></name>
        <name><surname>Mesaros</surname><given-names>Annamaria</given-names></name>
        <name><surname>Imoto</surname><given-names>Keisuke</given-names></name>
        <name><surname>Koizumi</surname><given-names>Yuma</given-names></name>
        <name><surname>Komatsu</surname><given-names>Tatsuya</given-names></name>
      </person-group>
      <publisher-name>Detection; Classication of Acoustic Scenes; Events (DCASE)</publisher-name>
      <year iso-8601-date="2020-11">2020</year><month>11</month>
      <uri>https://doi.org/10.5281/zenodo.4061782</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.4061782</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mesaros2016metrics">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mesaros</surname><given-names>Annamaria</given-names></name>
        <name><surname>Heittola</surname><given-names>Toni</given-names></name>
        <name><surname>Virtanen</surname><given-names>Tuomas</given-names></name>
      </person-group>
      <article-title>Metrics for polyphonic sound event detection</article-title>
      <source>Applied Sciences</source>
      <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>6</volume>
      <issue>6</issue>
      <uri>https://doi.org/10.3390/app6060162</uri>
      <pub-id pub-id-type="doi">10.3390/app6060162</pub-id>
      <fpage>162</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-speechbrain">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ravanelli</surname><given-names>Mirco</given-names></name>
        <name><surname>Parcollet</surname><given-names>Titouan</given-names></name>
        <name><surname>Plantinga</surname><given-names>Peter</given-names></name>
        <name><surname>Rouhe</surname><given-names>Aku</given-names></name>
        <name><surname>Cornell</surname><given-names>Samuele</given-names></name>
        <name><surname>Lugosch</surname><given-names>Loren</given-names></name>
        <name><surname>Subakan</surname><given-names>Cem</given-names></name>
        <name><surname>Dawalatabad</surname><given-names>Nauman</given-names></name>
        <name><surname>Heba</surname><given-names>Abdelwahab</given-names></name>
        <name><surname>Zhong</surname><given-names>Jianyuan</given-names></name>
        <name><surname>Chou</surname><given-names>Ju-Chieh</given-names></name>
        <name><surname>Yeh</surname><given-names>Sung-Lin</given-names></name>
        <name><surname>Fu</surname><given-names>Szu-Wei</given-names></name>
        <name><surname>Liao</surname><given-names>Chien-Feng</given-names></name>
        <name><surname>Rastorgueva</surname><given-names>Elena</given-names></name>
        <name><surname>Grondin</surname><given-names>François</given-names></name>
        <name><surname>Aris</surname><given-names>William</given-names></name>
        <name><surname>Na</surname><given-names>Hwidong</given-names></name>
        <name><surname>Gao</surname><given-names>Yan</given-names></name>
        <name><surname>Mori</surname><given-names>Renato De</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>SpeechBrain: A general-purpose speech toolkit</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://doi.org/10.48550/arXiv.2106.04624</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2106.04624</pub-id>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://dcase.community/">https://dcase.community/</ext-link></p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/marl/jams">https://github.com/marl/jams</ext-link></p>
  </fn>
  <fn id="fn3">
    <label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://soundata.readthedocs.io/en/latest/source/contributing.html">https://soundata.readthedocs.io/en/latest/source/contributing.html</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
