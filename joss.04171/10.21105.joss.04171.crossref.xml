<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20220425T114705-66332b36ac5e7616d164a22204b96d3d932d49ee</doi_batch_id>
    <timestamp>20220425114705</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>04</month>
          <year>2022</year>
        </publication_date>
        <journal_volume>
          <volume>7</volume>
        </journal_volume>
        <issue>72</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>hessQuik: Fast Hessian computation of composite
functions</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Elizabeth</given_name>
            <surname>Newman</surname>
            <ORCID>https://orcid.org/0000-0002-6309-7706</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Lars</given_name>
            <surname>Ruthotto</surname>
            <ORCID>https://orcid.org/0000-0003-0803-3299</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>04</month>
          <day>25</day>
          <year>2022</year>
        </publication_date>
        <pages>
          <first_page>4171</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.04171</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.6478757</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/4171</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.04171</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.04171</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.04171.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="amos2017input">
            <article_title>Input convex neural networks</article_title>
            <author>Amos</author>
            <cYear>2017</cYear>
            <unstructured_citation>Amos, B., Xu, L., &amp; Kolter, J. Z.
(2017). Input convex neural networks.
https://arxiv.org/abs/1609.07152</unstructured_citation>
          </citation>
          <citation key="ruthotto2021introduction">
            <article_title>An introduction to deep generative
modeling</article_title>
            <author>Ruthotto</author>
            <doi>10.1002/gamm.202100008</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Ruthotto, L., &amp; Haber, E. (2021).
An introduction to deep generative modeling.
https://doi.org/10.1002/gamm.202100008</unstructured_citation>
          </citation>
          <citation key="Raissi:2019hv">
            <article_title>Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving
nonlinear partial differential equations</article_title>
            <author>Raissi</author>
            <journal_title>Journal of Computational Physics,
Elsevier</journal_title>
            <volume>378</volume>
            <doi>10.1016/j.jcp.2018.10.045</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Raissi, M., Perdikaris, P., &amp;
Karniadakis, G. (2019). Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving
nonlinear partial differential equations. Journal of Computational
Physics, Elsevier, 378, 686–707.
https://doi.org/10.1016/j.jcp.2018.10.045</unstructured_citation>
          </citation>
          <citation key="huang2021convex">
            <article_title>Convex potential flows: Universal probability
distributions with optimal transport and convex
optimization</article_title>
            <author>Huang</author>
            <journal_title>International conference on learning
representations</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Huang, C.-W., Chen, R. T. Q.,
Tsirigotis, C., &amp; Courville, A. (2021). Convex potential flows:
Universal probability distributions with optimal transport and convex
optimization. International Conference on Learning Representations.
https://openreview.net/forum?id=te7PVH1sPxJ</unstructured_citation>
          </citation>
          <citation key="He2016:deep">
            <article_title>Deep residual learning for image
recognition</article_title>
            <author>He</author>
            <journal_title>2016 IEEE conference on computer vision and
pattern recognition (CVPR)</journal_title>
            <doi>10.1109/CVPR.2016.90</doi>
            <cYear>2016</cYear>
            <unstructured_citation>He, K., Zhang, X., Ren, S., &amp;
Sun, J. (2016). Deep residual learning for image recognition. 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), 770–778.
https://doi.org/10.1109/CVPR.2016.90</unstructured_citation>
          </citation>
          <citation key="papernot2016limitations">
            <article_title>The limitations of deep learning in
adversarial settings</article_title>
            <author>Papernot</author>
            <journal_title>2016 IEEE european symposium on security and
privacy (EuroS p)</journal_title>
            <doi>10.1109/EuroSP.2016.36</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Papernot, N., McDaniel, P., Jha, S.,
Fredrikson, M., Celik, Z. B., &amp; Swami, A. (2016). The limitations of
deep learning in adversarial settings. 2016 IEEE European Symposium on
Security and Privacy (EuroS p), 372–387.
https://doi.org/10.1109/EuroSP.2016.36</unstructured_citation>
          </citation>
          <citation key="googleColab">
            <article_title>Google Colaboratory</article_title>
            <author>Bisong</author>
            <journal_title>Building machine learning and deep learning
models on Google Cloud Platform: A comprehensive guide for
beginners</journal_title>
            <doi>10.1007/978-1-4842-4470-8_7</doi>
            <isbn>978-1-4842-4470-8</isbn>
            <cYear>2019</cYear>
            <unstructured_citation>Bisong, E. (2019). Google
Colaboratory. In Building machine learning and deep learning models on
Google Cloud Platform: A comprehensive guide for beginners (pp. 59–64).
Apress.
https://doi.org/10.1007/978-1-4842-4470-8_7</unstructured_citation>
          </citation>
          <citation key="pytorch">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E.
Fox, &amp; R. Garnett (Eds.), Advances in neural information processing
systems 32 (pp. 8024–8035). Curran Associates, Inc.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
          <citation key="olearyroseberry2020illposedness">
            <article_title>Ill-posedness and optimization geometry for
nonlinear neural network training</article_title>
            <author>O’Leary-Roseberry</author>
            <cYear>2020</cYear>
            <unstructured_citation>O’Leary-Roseberry, T., &amp; Ghattas,
O. (2020). Ill-posedness and optimization geometry for nonlinear neural
network training.
https://arxiv.org/abs/2002.02882</unstructured_citation>
          </citation>
          <citation key="haberDerivative">
            <volume_title>Computational methods in geophysical
electromagnetics</volume_title>
            <author>Haber</author>
            <doi>10.1137/1.9781611973808</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Haber, E. (2014). Computational
methods in geophysical electromagnetics. Society for Industrial; Applied
Mathematics.
https://doi.org/10.1137/1.9781611973808</unstructured_citation>
          </citation>
          <citation key="Ruthotto9183">
            <article_title>A machine learning framework for solving
high-dimensional mean field game and mean field control
problems</article_title>
            <author>Ruthotto</author>
            <journal_title>Proceedings of the National Academy of
Sciences</journal_title>
            <issue>17</issue>
            <volume>117</volume>
            <doi>10.1073/pnas.1922204117</doi>
            <issn>0027-8424</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Ruthotto, L., Osher, S. J., Li, W.,
Nurbekyan, L., &amp; Fung, S. W. (2020). A machine learning framework
for solving high-dimensional mean field game and mean field control
problems. Proceedings of the National Academy of Sciences, 117(17),
9183–9193.
https://doi.org/10.1073/pnas.1922204117</unstructured_citation>
          </citation>
          <citation key="KoldaBader09">
            <article_title>Tensor decompositions and
applications</article_title>
            <author>Kolda</author>
            <journal_title>SIAM Review</journal_title>
            <issue>3</issue>
            <volume>51</volume>
            <doi>10.1137/07070111X</doi>
            <cYear>2009</cYear>
            <unstructured_citation>Kolda, T. G., &amp; Bader, B. W.
(2009). Tensor decompositions and applications. SIAM Review, 51(3),
455–500. https://doi.org/10.1137/07070111X</unstructured_citation>
          </citation>
          <citation key="E2017">
            <article_title>A proposal on machine learning via dynamical
systems</article_title>
            <author>E</author>
            <journal_title>Communications in Mathematics and
Statistics</journal_title>
            <issue>1</issue>
            <volume>5</volume>
            <doi>10.1007/s40304-017-0103-z</doi>
            <issn>2194-671X</issn>
            <cYear>2017</cYear>
            <unstructured_citation>E, W. (2017). A proposal on machine
learning via dynamical systems. Communications in Mathematics and
Statistics, 5(1), 1–11.
https://doi.org/10.1007/s40304-017-0103-z</unstructured_citation>
          </citation>
          <citation key="HaberRuthotto2017">
            <article_title>Stable architectures for deep neural
networks</article_title>
            <author>Haber</author>
            <journal_title>Inverse Problems</journal_title>
            <issue>1</issue>
            <volume>34</volume>
            <doi>10.1088/1361-6420/aa9a90</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Haber, E., &amp; Ruthotto, L. (2017).
Stable architectures for deep neural networks. Inverse Problems, 34(1),
014004. https://doi.org/10.1088/1361-6420/aa9a90</unstructured_citation>
          </citation>
          <citation key="Goodfellow-et-al-2016">
            <volume_title>Deep learning</volume_title>
            <author>Goodfellow</author>
            <cYear>2016</cYear>
            <unstructured_citation>Goodfellow, I., Bengio, Y., &amp;
Courville, A. (2016). Deep learning. MIT Press.</unstructured_citation>
          </citation>
          <citation key="Anirudh9741">
            <article_title>Improved surrogates in inertial confinement
fusion with manifold and cycle consistencies</article_title>
            <author>Anirudh</author>
            <journal_title>Proceedings of the National Academy of
Sciences</journal_title>
            <issue>18</issue>
            <volume>117</volume>
            <doi>10.1073/pnas.1916634117</doi>
            <issn>0027-8424</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Anirudh, R., Thiagarajan, J. J.,
Bremer, P.-T., &amp; Spears, B. K. (2020). Improved surrogates in
inertial confinement fusion with manifold and cycle consistencies.
Proceedings of the National Academy of Sciences, 117(18), 9741–9746.
https://doi.org/10.1073/pnas.1916634117</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
