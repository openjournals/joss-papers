<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5352</article-id>
<article-id pub-id-type="doi">10.21105/joss.05352</article-id>
<title-group>
<article-title>Physics-Informed Neural networks for Advanced
modeling</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8833-6833</contrib-id>
<name>
<surname>Coscia</surname>
<given-names>Dario</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2369-4493</contrib-id>
<name>
<surname>Ivagnes</surname>
<given-names>Anna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3107-9738</contrib-id>
<name>
<surname>Demo</surname>
<given-names>Nicola</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0810-8812</contrib-id>
<name>
<surname>Rozza</surname>
<given-names>Gianluigi</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>SISSA, International School of Advanced Studies, Via
Bonomea 265, Trieste, Italy</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-03-15">
<day>15</day>
<month>3</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>87</issue>
<fpage>5352</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>deep learning</kwd>
<kwd>physics-informed neural networks</kwd>
<kwd>scientific machine learning</kwd>
<kwd>differential equations.</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="introduction">
  <title>Introduction</title>
  <p>Artificial Intelligence (AI) strategies are massively emerging in
  several fields of academia and industrial research Wang &amp; Liao
  (<xref alt="2004" rid="ref-Wang_2005" ref-type="bibr">2004</xref>) due
  to the growing disposal of data, as well as the great improvement in
  computational resources. In the area of applied mathematics and
  simulations, AI strategies are being used to solve problems where
  classical methods fail
  (<xref alt="Cuomo et al., 2022" rid="ref-pinns" ref-type="bibr">Cuomo
  et al., 2022</xref>). However, the amount of data required to analyze
  complex systems is often insufficient to make AI predictions reliable
  and robust. Physics-informed neural networks (PINNs) have been
  formulated
  (<xref alt="Raissi et al., 2019" rid="ref-RAISSI2019686" ref-type="bibr">Raissi
  et al., 2019</xref>) to overcome the issues of missing data, by
  incorporating the physical knowledge into the neural network training.
  Thus, PINNs aim to approximate any differential equation by solving a
  minimization problem in an unsupervised learning setting, learning the
  unknown field in order to preserve the imposed constraints (boundaries
  and physical residuals). Formally, we consider the general form of a
  differential equation, which typically presents the most challenging
  issues from a numerical point of view: <disp-formula><alternatives>
  <tex-math><![CDATA[\begin{split}
      \mathcal{F}(\pmb{u}(\pmb{z});\alpha)&=\pmb{f}(\pmb{z}) \quad \pmb{z} \in \Omega,\\
      \mathcal{B}(\pmb{u}(\pmb{z}))&=\pmb{g}(\pmb{z}) \quad \pmb{z} \in \partial\Omega,
  \end{split}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>ℱ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐮</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐳</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>𝐟</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐳</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="1.0em"></mml:mspace><mml:mi>𝐳</mml:mi><mml:mo>∈</mml:mo><mml:mi>Ω</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>ℬ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐮</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐳</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>𝐠</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐳</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="1.0em"></mml:mspace><mml:mi>𝐳</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>Ω</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega\subset\mathbb{R}^d]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Ω</mml:mi><mml:mo>⊂</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is the domain and <inline-formula><alternatives>
  <tex-math><![CDATA[\partial\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∂</mml:mi><mml:mi>Ω</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  the boundaries of the latter. In particular,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\pmb{z}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐳</mml:mi></mml:math></alternatives></inline-formula>
  indicates the spatio-temporal coordinates vector,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\pmb{u}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐮</mml:mi></mml:math></alternatives></inline-formula>
  the unknown field, <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  the physical parameters, <inline-formula><alternatives>
  <tex-math><![CDATA[\pmb{f}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐟</mml:mi></mml:math></alternatives></inline-formula>
  the forcing term, and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{F}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ℱ</mml:mi></mml:math></alternatives></inline-formula>
  the differential operator. In addition, <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{B}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ℬ</mml:mi></mml:math></alternatives></inline-formula>
  identifies the operator indicating arbitrary initial or boundary
  conditions and <inline-formula><alternatives>
  <tex-math><![CDATA[\pmb{g}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐠</mml:mi></mml:math></alternatives></inline-formula>
  the boundary function. The PINN’s objective is to find a solution to
  the problem, which is done by approximating the true solution
  <inline-formula><alternatives>
  <tex-math><![CDATA[\pmb{u}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐮</mml:mi></mml:math></alternatives></inline-formula>
  with a neural network <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{\pmb{u}}_{\theta} : \Omega \rightarrow \mathbb{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mover><mml:mi>𝐮</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mi>θ</mml:mi></mml:msub><mml:mo>:</mml:mo><mml:mi>Ω</mml:mi><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  with <inline-formula><alternatives>
  <tex-math><![CDATA[\theta]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>θ</mml:mi></mml:math></alternatives></inline-formula>
  network’s parameters. Such a model is trained to find the optimal
  parameters <inline-formula><alternatives>
  <tex-math><![CDATA[\theta^*]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
  whose minimizing the physical loss function depending on the physical
  conditions <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{L}_{\mathcal{F}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>ℱ</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  boundary conditions <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{L}_{\mathcal{B}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>ℬ</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  and, if available, real data <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{L}_{\textrm{data}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ℒ</mml:mi><mml:mtext mathvariant="normal">data</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\theta^* = \underset{\theta}{\mathrm{argmin}} \mathcal{L} =
      \underset{\theta}{\mathrm{argmin}} (\mathcal{L}_{\mathcal{F}} + \mathcal{L}_{\mathcal{B}} + \mathcal{L}_{\text{data}}).]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:mi>ℒ</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mi>θ</mml:mi></mml:munder><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>ℱ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mi>ℬ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ℒ</mml:mi><mml:mtext mathvariant="normal">data</mml:mtext></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>The PINNs framework is completely general and applicable to
  different types of ordinary differential equations (ODEs), or partial
  differential equations (PDEs). Nevertheless, the loss function
  strictly depends on the problem chosen to be solved, since different
  operators or boundary conditions lead to different losses, increasing
  the difficulty to write a general and portable code for different
  problems.</p>
  <fig>
    <caption><p>PINA
    logo.<styled-content id="logo"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/pina_logo.png" />
  </fig>
  <p><bold>PINA</bold>, <italic>Physics-Informed Neural networks for
  Advanced modeling</italic>, is a Python library built using PyTorch
  that provides a user-friendly API to formalize a large variety of
  physical problems and solve it using PINNs easily.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>PINA is an open-source Python library that provides an intuitive
  interface for the approximated resolution of Ordinary Differential
  Equations and Partial Differential Equations using a deep learning
  paradigm, in particular via PINNs. The gain of popularity for PINNs in
  recent years, and the evolution of open-source frameworks, such as
  TensorFlow, Keras, and PyTorch, led to the development of several
  libraries, whose focus is the exploitation of PINNs to approximately
  solve ODEs and PDEs. We here mention some PyTorch-based libraries,
  <monospace>NeuroDiffEq</monospace>
  (<xref alt="Chen et al., 2020" rid="ref-chen2020neurodiffeq" ref-type="bibr">Chen
  et al., 2020</xref>), <monospace>IDRLNet</monospace>
  (<xref alt="Peng et al., 2021" rid="ref-peng2021idrlnet" ref-type="bibr">Peng
  et al., 2021</xref>), NVIDIA <monospace>Modulus</monospace>
  (<xref alt="NVIDIA Modulus, 2023" rid="ref-modulussym" ref-type="bibr"><italic>NVIDIA
  Modulus</italic>, 2023</xref>), and some TensorFlow-based libraries,
  such as <monospace>DeepXDE</monospace>
  (<xref alt="Lu et al., 2021" rid="ref-lu2021deepxde" ref-type="bibr">Lu
  et al., 2021</xref>), <monospace>TensorDiffEq</monospace>
  (<xref alt="McClenny et al., 2021" rid="ref-mcclenny2021tensordiffeq" ref-type="bibr">McClenny
  et al., 2021</xref>), <monospace>SciANN</monospace>
  (<xref alt="Haghighat &amp; Juanes, 2021" rid="ref-haghighat2021sciann" ref-type="bibr">Haghighat
  &amp; Juanes, 2021</xref>) (which is both TensorFlow and Keras-based),
  <monospace>PyDEns</monospace>
  (<xref alt="Koryagin et al., 2019" rid="ref-koryagin2019pydens" ref-type="bibr">Koryagin
  et al., 2019</xref>), <monospace>Elvet</monospace>
  (<xref alt="Araz et al., 2021" rid="ref-araz2021elvet" ref-type="bibr">Araz
  et al., 2021</xref>), <monospace>NVIDIA SimNet</monospace>
  (<xref alt="Hennigh et al., 2021" rid="ref-hennigh2021nvidia" ref-type="bibr">Hennigh
  et al., 2021</xref>). Among all these frameworks, PINA wants to emerge
  for its easiness of usage, allowing the users to quickly formulate the
  problem at hand and solve it, resulting in an intuitive framework
  designed by researchers for researchers.</p>
  <p>Built over PyTorch — in order to inherit the
  <monospace>autograd</monospace> module and all the other features
  already implemented — PINA provides indeed documented API to explain
  usage and capabilities of the different classes. We have built several
  abstract interfaces not only for better structure of the source code
  but especially to give the final user an easy entry point to implement
  their own extensions, like new loss functions, new training
  procedures, and so on. This aspect, together with the capability to
  use all the PyTorch models, makes it possible to incorporate almost
  any existing architecture into the PINA framework. We have decided to
  build it on top of PyTorch in order to exploit the
  <monospace>autograd</monospace> module, as well as all the other
  features implemented in this framework. The final outcome is then a
  library with incremental complexity, capable of being used by the new
  users to perform the first investigation using PINNs, but also as a
  core framework to actively develop new features to improve the
  discussed methodology.</p>
  <p>The high-level structure of the package is depicted in our
  <ext-link ext-link-type="uri" xlink:href="https://github.com/mathLab/PINA/tree/master/readme/API_color.png">API</ext-link>;
  the approximated solution of a differential equation can be
  implemented using PINA in a few lines of code thanks to the intuitive
  and user-friendly interface. Besides the user-friendly interface, PINA
  also offers several examples and tutorials, aiming to guide new users
  toward an easy exploration of the software features. The online
  documentation is released at
  <ext-link ext-link-type="uri" xlink:href="https://mathlab.github.io/PINA/">https://mathlab.github.io/PINA/</ext-link>,
  while the robustness of the package is continuously monitored by unit
  tests.</p>
  <p>PINA workflow is characterized by 3 main steps: the problem
  formulation, the model definition, i.e., the structure of the neural
  network used, and the training, eventually followed by the data
  visualization.</p>
  <sec id="problem-definition-in-pina">
    <title>Problem definition in PINA</title>
    <p>The first step is the formalization of the problem. The problem
    definition in the PINA framework is inherited from one or more
    problem classes (at the moment the available classes are
    <monospace>SpatialProblem</monospace>,
    <monospace>TimeDependentProblem</monospace>,
    <monospace>ParametricProblem</monospace>), depending on the nature
    of the problem treated. The user has to include in the problem
    formulation the following components:</p>
    <list list-type="bullet">
      <list-item>
        <p>the information about the domain, i.e., the spatial and
        temporal variables, the parameters of the problem (if any), with
        the corresponding range of variation;</p>
      </list-item>
      <list-item>
        <p>the output variables, i.e., the unknowns of the problem;</p>
      </list-item>
      <list-item>
        <p>the conditions that the neural network has to satisfy, i.e.,
        the differential equations, the boundary and initial
        conditions.</p>
      </list-item>
    </list>
    <p>We highlight that in PINA we abandoned the classical division
    between physical loss, boundary loss, and data loss: all these terms
    are encapsulated within the <monospace>Condition</monospace> class,
    in order to keep the framework as general as possible. The users can
    indeed define all the constraints the unknown field needs to
    satisfy, avoiding any forced structure in the formulation and
    allowing them to mix heterogeneous constraints — e.g., data values,
    differential boundary conditions. Moreover PINA already implements
    functions to easily compute the diffential operations (gradient,
    divergence, laplacian) over the output(s) of interest, aiming to
    make the problem definition an easy task for the users.</p>
  </sec>
  <sec id="model-definition-in-pina">
    <title>Model definition in PINA</title>
    <p>The second fundamental step is the definition of the model of the
    neural network employed to find the approximated solution to the
    differential problem in question. In PINA, the user has the
    possibility to use either a custom <monospace>torch</monospace>
    network model, or to exploit one of the built-in models such as
    <monospace>FeedForward</monospace>,
    <monospace>MultiFeedForward</monospace> and
    <monospace>DeepONet</monospace>, defining their characteristics
    during instantiation — i.e., number of layers, number of neurons,
    activation functions. The list of the built-in models will be
    extended in the next release of the library.</p>
  </sec>
  <sec id="training-in-pina">
    <title>Training in PINA</title>
    <p>In the last step, the actual training of the model in order to
    solve the problem at hand is computed. In this phase, the residuals
    of the conditions (expressed in the problem) are minimized in order
    to provide the target approximation. The sampling points where the
    physical residuals are evaluated can be passed by the user, or
    automatically sampled from the original domain using one of the
    available sampling techniques. The training is then computed for a
    certain amount of epochs, or until reaching the user-defined loss
    threshold. Once the model is ready to be inferred, the user can save
    it onto a binary file for future reusing, by inheriting the PyTorch
    functionality. The user can also evaluate the (trained) model for
    any new input, or just use it together with the
    <monospace>Plotter</monospace> in order to render the predicted
    output variables.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank our colleagues and research partners who contributed in
  the former and current developments of PINA library. This work was
  partially funded by European Union Funding for Research and Innovation
  — Horizon 2020 Program — in the framework of European Research Council
  Executive Agency: H2020 ERC CoG 2015 AROMA-CFD project 681447,
  “Advanced Reduced Order Methods with Applications in Computational
  Fluid Dynamics,” P.I. Professor Gianluigi Rozza.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-deng2014deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Deng</surname><given-names>Li</given-names></name>
        <name><surname>Yu</surname><given-names>Dong</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Deep learning: Methods and applications</article-title>
      <source>Foundations and trends in signal processing</source>
      <publisher-name>Now Publishers, Inc.</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>7</volume>
      <issue>3–4</issue>
      <pub-id pub-id-type="doi">10.1561/9781601988157</pub-id>
      <fpage>197</fpage>
      <lpage>387</lpage>
    </element-citation>
  </ref>
  <ref id="ref-modulussym">
    <element-citation>
      <article-title>NVIDIA Modulus</article-title>
      <publisher-name>https://github.com/NVIDIA/modulus</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-Wang_2005">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>D H</given-names></name>
        <name><surname>Liao</surname><given-names>W H</given-names></name>
      </person-group>
      <article-title>Modeling and control of magnetorheological fluid dampers using neural networks</article-title>
      <source>Smart Materials and Structures</source>
      <year iso-8601-date="2004-12">2004</year><month>12</month>
      <volume>14</volume>
      <issue>1</issue>
      <uri>https://dx.doi.org/10.1088/0964-1726/14/1/011</uri>
      <pub-id pub-id-type="doi">10.1088/0964-1726/14/1/011</pub-id>
      <fpage>111</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-RAISSI2019686">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Raissi</surname><given-names>M.</given-names></name>
        <name><surname>Perdikaris</surname><given-names>P.</given-names></name>
        <name><surname>Karniadakis</surname><given-names>G. E.</given-names></name>
      </person-group>
      <article-title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>
      <source>Journal of Computational Physics</source>
      <year iso-8601-date="2019">2019</year>
      <volume>378</volume>
      <issn>0021-9991</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0021999118307125</uri>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2018.10.045</pub-id>
      <fpage>686</fpage>
      <lpage>707</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pinns">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Cuomo</surname><given-names>Salvatore</given-names></name>
        <name><surname>Cola</surname><given-names>Vincenzo Schiano di</given-names></name>
        <name><surname>Giampaolo</surname><given-names>Fabio</given-names></name>
        <name><surname>Rozza</surname><given-names>Gianluigi</given-names></name>
        <name><surname>Raissi</surname><given-names>Maziar</given-names></name>
        <name><surname>Piccialli</surname><given-names>Francesco</given-names></name>
      </person-group>
      <article-title>Scientific machine learning through physics-informed neural networks: Where we are and what’s next</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2201.05624</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2201.05624</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chen2020neurodiffeq">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Feiyu</given-names></name>
        <name><surname>Sondak</surname><given-names>David</given-names></name>
        <name><surname>Protopapas</surname><given-names>Pavlos</given-names></name>
        <name><surname>Mattheakis</surname><given-names>Marios</given-names></name>
        <name><surname>Liu</surname><given-names>Shuheng</given-names></name>
        <name><surname>Agarwal</surname><given-names>Devansh</given-names></name>
        <name><surname>Di Giovanni</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>Neurodiffeq: A Python package for solving differential equations with neural networks</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>46</issue>
      <pub-id pub-id-type="doi">10.21105/joss.01931</pub-id>
      <fpage>1931</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lu2021deepxde">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Lu</given-names></name>
        <name><surname>Meng</surname><given-names>Xuhui</given-names></name>
        <name><surname>Mao</surname><given-names>Zhiping</given-names></name>
        <name><surname>Karniadakis</surname><given-names>George Em</given-names></name>
      </person-group>
      <article-title>DeepXDE: A deep learning library for solving differential equations</article-title>
      <source>SIAM Review</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>63</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1137/19m1274067</pub-id>
      <fpage>208</fpage>
      <lpage>228</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mcclenny2021tensordiffeq">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McClenny</surname><given-names>Levi D</given-names></name>
        <name><surname>Haile</surname><given-names>Mulugeta A</given-names></name>
        <name><surname>Braga-Neto</surname><given-names>Ulisses M</given-names></name>
      </person-group>
      <article-title>TensorDiffEq: Scalable multi-GPU forward and inverse solvers for physics informed neural networks</article-title>
      <source>arXiv preprint arXiv:2103.16034</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2103.16034</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-peng2021idrlnet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peng</surname><given-names>Wei</given-names></name>
        <name><surname>Zhang</surname><given-names>Jun</given-names></name>
        <name><surname>Zhou</surname><given-names>Weien</given-names></name>
        <name><surname>Zhao</surname><given-names>Xiaoyu</given-names></name>
        <name><surname>Yao</surname><given-names>Wen</given-names></name>
        <name><surname>Chen</surname><given-names>Xiaoqian</given-names></name>
      </person-group>
      <article-title>IDRLnet: A physics-informed neural network library</article-title>
      <source>arXiv preprint arXiv:2107.04320</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2107.04320</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hennigh2021nvidia">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hennigh</surname><given-names>Oliver</given-names></name>
        <name><surname>Narasimhan</surname><given-names>Susheela</given-names></name>
        <name><surname>Nabian</surname><given-names>Mohammad Amin</given-names></name>
        <name><surname>Subramaniam</surname><given-names>Akshay</given-names></name>
        <name><surname>Tangsali</surname><given-names>Kaustubh</given-names></name>
        <name><surname>Fang</surname><given-names>Zhiwei</given-names></name>
        <name><surname>Rietmann</surname><given-names>Max</given-names></name>
        <name><surname>Byeon</surname><given-names>Wonmin</given-names></name>
        <name><surname>Choudhry</surname><given-names>Sanjay</given-names></name>
      </person-group>
      <article-title>NVIDIA SimNet™: An AI-accelerated multi-physics simulation framework</article-title>
      <source>International conference on computational science</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-77977-1_36</pub-id>
      <fpage>447</fpage>
      <lpage>461</lpage>
    </element-citation>
  </ref>
  <ref id="ref-haghighat2021sciann">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Haghighat</surname><given-names>Ehsan</given-names></name>
        <name><surname>Juanes</surname><given-names>Ruben</given-names></name>
      </person-group>
      <article-title>Sciann: A Keras/TensorFlow wrapper for scientific computations and physics-informed deep learning using artificial neural networks</article-title>
      <source>Computer Methods in Applied Mechanics and Engineering</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>373</volume>
      <pub-id pub-id-type="doi">10.1016/j.cma.2020.113552</pub-id>
      <fpage>113552</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-koryagin2019pydens">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Koryagin</surname><given-names>Alexander</given-names></name>
        <name><surname>Khudorozkov</surname><given-names>Roman</given-names></name>
        <name><surname>Tsimfer</surname><given-names>Sergey</given-names></name>
      </person-group>
      <article-title>PyDEns: A Python framework for solving differential equations with neural networks</article-title>
      <source>arXiv preprint arXiv:1909.11544</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1909.11544</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-araz2021elvet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Araz</surname><given-names>Jack Y</given-names></name>
        <name><surname>Criado</surname><given-names>Juan Carlos</given-names></name>
        <name><surname>Spannowsky</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Elvet – a neural network-based differential equation and variational problem solver</article-title>
      <source>arXiv preprint arXiv:2103.14575</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2103.14575</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
