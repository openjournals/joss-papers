<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6554</article-id>
<article-id pub-id-type="doi">10.21105/joss.06554</article-id>
<title-group>
<article-title>InvertibleNetworks.jl: A Julia package for scalable
normalizing flows</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Orozco</surname>
<given-names>Rafael</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Witte</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Louboutin</surname>
<given-names>Mathias</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Siahkoohi</surname>
<given-names>Ali</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Rizzuti</surname>
<given-names>Gabrio</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Peters</surname>
<given-names>Bas</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Herrmann</surname>
<given-names>Felix J.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Georgia Institute of Technology (GT), USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Microsoft Research, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Devito Codes, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Rice University, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Shearwater GeoServices, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Computational Geosciences Inc, Canada</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-11-28">
<day>28</day>
<month>11</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6554</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Julia</kwd>
<kwd>inverse problems</kwd>
<kwd>Bayesian inference</kwd>
<kwd>imaging</kwd>
<kwd>normalizing flows</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Normalizing flows is a density estimation method that provides
  efficient exact likelihood estimation and sampling
  (<xref alt="Dinh et al., 2014" rid="ref-dinh2014nice" ref-type="bibr">Dinh
  et al., 2014</xref>) from high-dimensional distributions. This method
  depends on the use of the change of variables formula, which requires
  an invertible transform. Thus normalizing flow architectures are built
  to be invertible by design
  (<xref alt="Dinh et al., 2014" rid="ref-dinh2014nice" ref-type="bibr">Dinh
  et al., 2014</xref>). In theory, the invertibility of architectures
  constrains the expressiveness, but the use of coupling layers allows
  normalizing flows to exploit the power of arbitrary neural networks,
  which do not need to be invertible,
  (<xref alt="Dinh et al., 2016" rid="ref-dinh2016density" ref-type="bibr">Dinh
  et al., 2016</xref>) and layer invertibility means that, if properly
  implemented, many layers can be stacked to increase expressiveness
  without creating a training memory bottleneck.</p>
  <p>The package we present, InvertibleNetworks.jl, is a pure Julia
  (<xref alt="Bezanson et al., 2017" rid="ref-bezanson2017julia" ref-type="bibr">Bezanson
  et al., 2017</xref>) implementation of normalizing flows. We have
  implemented many relevant neural network layers, including GLOW 1x1
  invertible convolutions
  (<xref alt="Kingma &amp; Dhariwal, 2018" rid="ref-kingma2018glow" ref-type="bibr">Kingma
  &amp; Dhariwal, 2018</xref>), affine/additive coupling layers
  (<xref alt="Dinh et al., 2014" rid="ref-dinh2014nice" ref-type="bibr">Dinh
  et al., 2014</xref>), Haar wavelet multiscale transforms
  (<xref alt="Haar, 1909" rid="ref-haar1909theorie" ref-type="bibr">Haar,
  1909</xref>), and Hierarchical invertible neural transport (HINT)
  (<xref alt="Kruse et al., 2021" rid="ref-kruse2021hint" ref-type="bibr">Kruse
  et al., 2021</xref>), among others. These modular layers can be easily
  composed and modified to create different types of normalizing flows.
  As starting points, we have implemented RealNVP, GLOW, HINT,
  Hyperbolic networks
  (<xref alt="Lensink et al., 2022" rid="ref-lensink2022fully" ref-type="bibr">Lensink
  et al., 2022</xref>) and their conditional counterparts for users to
  quickly implement their individual applications.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>This software package focuses on memory efficiency. The promise of
  neural networks is in learning high-dimensional distributions from
  data thus normalizing flow packages should allow application to large
  dimensional inputs such as images or 3D volumes. Interestingly, the
  invertibility of normalizing flows naturally alleviates memory
  concerns since intermediate network activations can be recomputed
  instead of saved in memory. This greatly reduces the memory needed
  during backpropagation. The problem is that directly implementing
  normalizing flows in automatic differentiation frameworks such as
  PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>) will not automatically exploit this
  invertibility. The available packages for normalizing flows such as
  Bijectors.jl
  (<xref alt="Fjelde et al., 2020" rid="ref-fjelde2020bijectors" ref-type="bibr">Fjelde
  et al., 2020</xref>), NormalizingFlows.jl
  (<xref alt="Zuheng Xu &amp; contributors, 2023" rid="ref-NormalizingFlows.jl" ref-type="bibr">Zuheng
  Xu &amp; contributors, 2023</xref>), nflows
  (<xref alt="Durkan et al., 2020" rid="ref-nflows" ref-type="bibr">Durkan
  et al., 2020</xref>), normflows
  (<xref alt="Stimper et al., 2023" rid="ref-stimper2023normflows" ref-type="bibr">Stimper
  et al., 2023</xref>), and FrEIA
  (<xref alt="Ardizzone et al., 2018-2022" rid="ref-freia" ref-type="bibr">Ardizzone
  et al., 2018-2022</xref>) are built depending on automatic
  differentiation frameworks and thus do not exploit invertibility for
  memory efficiently.</p>
  <p>We chose to write this package in Julia since it was built with a
  commitment to facilitate interoperability with other packages for
  workflows in scientific machine learning
  (<xref alt="Louboutin et al., 2022" rid="ref-louboutin2022accelerating" ref-type="bibr">Louboutin
  et al., 2022</xref>). The interoperability was facilitated by the
  multiple dispatch system of Julia. Our goal is to provide solutions
  for imaging problems with high degrees of freedom, where computational
  speed is crucial. We have found that this software significantly
  benefits from Julia’s Just-In-Time compilation technology.</p>
</sec>
<sec id="memory-efficiency">
  <title>Memory efficiency</title>
  <p>By implementing gradients by hand, instead of depending completely
  on automatic differentiation, our layers are capable of scaling to
  large inputs. By scaling, we mean that these codes are not prone to
  out-of-memory errors when training on GPU accelerators. Indeed,
  previous literature has described memory problems when using
  normalizing flows as their invertibility requires the latent code to
  maintain the same dimensionality as the input
  (<xref alt="Khorashadizadeh et al., 2023" rid="ref-khorashadizadeh2023conditional" ref-type="bibr">Khorashadizadeh
  et al., 2023</xref>).</p>
  <fig>
    <caption><p>Our InvertibleNetworks.jl package provides memory frugal
    implementations of normalizing flows. Here, we compare our
    implementation of GLOW with an equivalent implementation in a
    PyTorch package. Using a 40GB A100 GPU, the PyTorch package can not
    train on image sizes larger than 480x480, while our package can
    handle sizes larger than 1024x1024.
    <styled-content id="figU003Amemory"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./figs/mem_used_new.png" />
  </fig>
  <p>In
  <xref alt="[fig:memory]" rid="figU003Amemory">[fig:memory]</xref>, we
  show the relation between input size and the memory required for a
  gradient calculation in a PyTorch normalizing flow package (normflows
  (<xref alt="Stimper et al., 2023" rid="ref-stimper2023normflows" ref-type="bibr">Stimper
  et al., 2023</xref>)) as compared to our package. The two tests were
  run with identical normalizing flow architectures. We note that the
  PyTorch implementation quickly increases the memory load and throws an
  out-of-memory error on the 40-GB A100 GPU at a spatial image size of
  480x480, while our InvertibleNetworks.jl implementation still has not
  run out of memory at spatial size 1024x1024. Note that this is in the
  context of a typical learning routine, so the images include 3
  channels (RGB) and we used a batch size of 8.</p>
  <fig>
    <caption><p>Due to the invertibility of the normalizing flow
    architecture, the memory consumption does not increase as we
    increase the depth of the network. Our package properly exploits
    this property and thus shows constant memory consumption, whereas
    the PyTorch package does not.
    <styled-content id="figU003Amemory-depth"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./figs/mem_used_new_depth.png" />
  </fig>
  <p>Since traditional normalizing flow architectures need to be
  invertible they might be less expressive then their non-invertible
  counterparts. In order to increase their expressiveness, practitioners
  stack many invertible layers to increase the overall expressive power.
  Increasing the depth of a neural network would in most cases increase
  the memory consumption of the network but in this case, since
  normalizing flows are invertible, the memory consumption does not
  increase. Our package displays this phenomenon as shown in
  <xref alt="[fig:memory-depth]" rid="figU003Amemory-depth">[fig:memory-depth]</xref>
  while the PyTorch (normflows) package, which has been implemented with
  automatic differentiation, does not display this constant memory
  phenomenon.</p>
</sec>
<sec id="ease-of-use">
  <title>Ease of use</title>
  <p>Although the normalizing flow layer gradients are hand-written, the
  package is fully compatibly with ChainRules
  (<xref alt="White et al., 2023" rid="ref-frames_white_2023_10100624" ref-type="bibr">White
  et al., 2023</xref>) in order to integrate with automatic
  differentiation frameworks in Julia such as Zygote
  (<xref alt="Innes et al., 2019" rid="ref-innes2019differentiable" ref-type="bibr">Innes
  et al., 2019</xref>). This integration allows users to add arbitrary
  neural networks which will be differentiated by automatic
  differentiation while the memory bottleneck created by normalizing
  flow gradients will be dealt with by InvertibleNetworks.jl. The
  typical use case for this combination are the summary networks used in
  amortized variational inference such as BayesFlow
  (<xref alt="Radev et al., 2020" rid="ref-radev2020bayesflow" ref-type="bibr">Radev
  et al., 2020</xref>), which is also implemented in our package.</p>
  <p>All implemented layers are tested for invertibility and correctness
  of their gradients with continuous integration testing via GitHub
  actions. There are many examples for layers, networks and application
  workflows allowing new users to quickly build networks for a variety
  of applications. The ease of use is demonstrated by the publications
  that made use of the package.</p>
  <p>Many publications have used InvertibleNetworks.jl for diverse
  applications including change point detection,
  (<xref alt="Peters, 2022" rid="ref-peters2022point" ref-type="bibr">Peters,
  2022</xref>), acoustic data denoising
  (<xref alt="Kumar et al., 2021" rid="ref-kumar2021enabling" ref-type="bibr">Kumar
  et al., 2021</xref>), seismic imaging
  (<xref alt="Alemohammad et al., 2023" rid="ref-alemohammad2023self" ref-type="bibr">Alemohammad
  et al., 2023</xref>;
  <xref alt="Louboutin et al., 2023" rid="ref-louboutin2023learned" ref-type="bibr">Louboutin
  et al., 2023</xref>;
  <xref alt="Rizzuti et al., 2020" rid="ref-rizzuti2020parameterizing" ref-type="bibr">Rizzuti
  et al., 2020</xref>;
  <xref alt="Siahkoohi et al., 2021" rid="ref-siahkoohi2021preconditioned" ref-type="bibr">Siahkoohi
  et al., 2021</xref>,
  <xref alt="2022" rid="ref-siahkoohi2022wave" ref-type="bibr">2022</xref>,
  <xref alt="2023" rid="ref-siahkoohi2023reliable" ref-type="bibr">2023</xref>),
  fluid flow dynamics
  (<xref alt="Yin et al., 2023" rid="ref-yin2023solving" ref-type="bibr">Yin
  et al., 2023</xref>), medical imaging
  (<xref alt="Orozco, Siahkoohi, Rizzuti, et al., 2023" rid="ref-orozco2023adjoint" ref-type="bibr">Orozco,
  Siahkoohi, Rizzuti, et al., 2023</xref>;
  <xref alt="Orozco et al., 2021" rid="ref-orozco2021photoacoustic" ref-type="bibr">Orozco
  et al., 2021</xref>;
  <xref alt="Orozco, Louboutin, et al., 2023" rid="ref-orozco2023amortized" ref-type="bibr">Orozco,
  Louboutin, et al., 2023</xref>;
  <xref alt="Orozco, Siahkoohi, Louboutin, et al., 2023" rid="ref-orozco2023refining" ref-type="bibr">Orozco,
  Siahkoohi, Louboutin, et al., 2023</xref>), and monitoring CO2 for
  combating climate change
  (<xref alt="Gahlot et al., 2023" rid="ref-gahlot2023inference" ref-type="bibr">Gahlot
  et al., 2023</xref>).</p>
</sec>
<sec id="future-work">
  <title>Future work</title>
  <p>The neural network primitives (convolutions, non-linearities,
  pooling etc) are implemented in NNlib.jl abstractions, thus support
  for AMD, Intel, and Apple GPUs can be trivially extended. Also, while
  our package can currently handle 3D inputs and has been used on large
  volume-based medical imaging
  (<xref alt="Orozco et al., 2022" rid="ref-orozco2022memory" ref-type="bibr">Orozco
  et al., 2022</xref>), there are interesting avenues of research
  regarding the “channel explosion” seen in invertible down and
  upsampling used in invertible networks
  (<xref alt="Peters et al., 2019" rid="ref-peters2019symmetric" ref-type="bibr">Peters
  et al., 2019</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The development of this package was carried out with the support of
  Georgia Research Alliance and partners of the ML4Seismic Center. This
  was also supported in part by the US National Science Foundation grant
  OAC 2203821 and the Department of Energy grant No. DE-SC0021515.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-louboutin2022accelerating">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Witte</surname><given-names>Philipp</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Yin</surname><given-names>Ziyi</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Accelerating innovation with software abstractions for scalable computational geophysics</article-title>
      <source>Second international meeting for applied geoscience &amp; energy</source>
      <publisher-name>Society of Exploration Geophysicists; American Association of Petroleum …</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1190/image2022-3750561.1</pub-id>
      <fpage>1482</fpage>
      <lpage>1486</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NormalizingFlows.jl">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zuheng Xu</surname><given-names>Tor Erlend Fjelde</given-names><suffix>Xianda Sun</suffix></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>NormalizingFlows.jl</article-title>
      <year iso-8601-date="2023-06">2023</year><month>06</month>
      <uri>https://github.com/TuringLang/NormalizingFlows.jl</uri>
    </element-citation>
  </ref>
  <ref id="ref-fjelde2020bijectors">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Fjelde</surname><given-names>Tor Erlend</given-names></name>
        <name><surname>Xu</surname><given-names>Kai</given-names></name>
        <name><surname>Tarek</surname><given-names>Mohamed</given-names></name>
        <name><surname>Yalburgi</surname><given-names>Sharan</given-names></name>
        <name><surname>Ge</surname><given-names>Hong</given-names></name>
      </person-group>
      <article-title>Bijectors. Jl: Flexible transformations for probability distributions</article-title>
      <source>Symposium on advances in approximate Bayesian inference</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <fpage>1</fpage>
      <lpage>17</lpage>
    </element-citation>
  </ref>
  <ref id="ref-alemohammad2023self">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alemohammad</surname><given-names>Sina</given-names></name>
        <name><surname>Casco-Rodriguez</surname><given-names>Josue</given-names></name>
        <name><surname>Luzi</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Humayun</surname><given-names>Ahmed Imtiaz</given-names></name>
        <name><surname>Babaei</surname><given-names>Hossein</given-names></name>
        <name><surname>LeJeune</surname><given-names>Daniel</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Baraniuk</surname><given-names>Richard G</given-names></name>
      </person-group>
      <article-title>Self-consuming generative models go mad</article-title>
      <source>arXiv preprint arXiv:2307.01850</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.52591/lxai202312101</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-peters2022point">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peters</surname><given-names>Bas</given-names></name>
      </person-group>
      <article-title>Point-to-set distance functions for output-constrained neural networks.</article-title>
      <source>Journal of Applied &amp; Numerical Optimization</source>
      <year iso-8601-date="2022">2022</year>
      <volume>4</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.23952/jano.4.2022.2.05</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bezanson2017julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM review</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>59</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1137/141000671</uri>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-peters2019symmetric">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peters</surname><given-names>Bas</given-names></name>
        <name><surname>Haber</surname><given-names>Eldad</given-names></name>
        <name><surname>Lensink</surname><given-names>Keegan</given-names></name>
      </person-group>
      <article-title>Symmetric block-low-rank layers for fully reversible multilevel neural networks</article-title>
      <source>arXiv preprint arXiv:1912.12137</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.12137</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-orozco2022memory">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Memory efficient invertible neural networks for 3D photoacoustic imaging</article-title>
      <source>arXiv preprint arXiv:2204.11850</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2204.11850</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-radev2020bayesflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T</given-names></name>
        <name><surname>Mertens</surname><given-names>Ulf K</given-names></name>
        <name><surname>Voss</surname><given-names>Andreas</given-names></name>
        <name><surname>Ardizzone</surname><given-names>Lynton</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
      </person-group>
      <article-title>BayesFlow: Learning complex stochastic models with invertible neural networks</article-title>
      <source>IEEE transactions on neural networks and learning systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1109/tnnls.2020.3042395</pub-id>
      <fpage>1452</fpage>
      <lpage>1466</lpage>
    </element-citation>
  </ref>
  <ref id="ref-innes2019differentiable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Innes</surname><given-names>Mike</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Fischer</surname><given-names>Keno</given-names></name>
        <name><surname>Rackauckas</surname><given-names>Chris</given-names></name>
        <name><surname>Saba</surname><given-names>Elliot</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B</given-names></name>
        <name><surname>Tebbutt</surname><given-names>Will</given-names></name>
      </person-group>
      <article-title>A differentiable programming system to bridge machine learning and scientific computing</article-title>
      <source>arXiv preprint arXiv:1907.07587</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1907.07587</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-frames_white_2023_10100624">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>White</surname><given-names>Frames</given-names></name>
        <name><surname>Abbott</surname><given-names>Michael</given-names></name>
        <name><surname>Zgubic</surname><given-names>Miha</given-names></name>
        <name><surname>Revels</surname><given-names>Jarrett</given-names></name>
        <name><surname>Axen</surname><given-names>Seth</given-names></name>
        <name><surname>Arslan</surname><given-names>Alex</given-names></name>
        <name><surname>Schaub</surname><given-names>Simeon</given-names></name>
        <name><surname>Robinson</surname><given-names>Nick</given-names></name>
        <name><surname>Ma</surname><given-names>Yingbo</given-names></name>
        <name><surname>Sam</surname></name>
        <name><surname>Dhingra</surname><given-names>Gaurav</given-names></name>
        <name><surname>Tebbutt</surname><given-names>Will</given-names></name>
        <name><surname>Widmann</surname><given-names>David</given-names></name>
        <name><surname>Heim</surname><given-names>Niklas</given-names></name>
        <name><surname>Schmitz</surname><given-names>Niklas</given-names></name>
        <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
        <name><surname>Lucibello</surname><given-names>Carlo</given-names></name>
        <name><surname>Fischer</surname><given-names>Keno</given-names></name>
        <name><surname>Heintzmann</surname><given-names>Rainer</given-names></name>
        <name><surname>frankschae</surname></name>
        <name><surname>Noack</surname><given-names>Andreas</given-names></name>
        <name><surname>Robson</surname><given-names>Alex</given-names></name>
        <name><surname>cossio</surname></name>
        <name><surname>Ling</surname><given-names>Jerry</given-names></name>
        <name><surname>mattBrzezinski</surname></name>
        <name><surname>Finnegan</surname><given-names>Rory</given-names></name>
        <name><surname>Zhabinski</surname><given-names>Andrei</given-names></name>
        <name><surname>Wennberg</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>JuliaDiff/ChainRules.jl: v1.58.0</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023-11">2023</year><month>11</month>
      <uri>https://doi.org/10.5281/zenodo.10100624</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.10100624</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-freia">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Ardizzone</surname><given-names>Lynton</given-names></name>
        <name><surname>Bungert</surname><given-names>Till</given-names></name>
        <name><surname>Draxler</surname><given-names>Felix</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Kruse</surname><given-names>Jakob</given-names></name>
        <name><surname>Schmier</surname><given-names>Robert</given-names></name>
        <name><surname>Sorrenson</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Framework for easily invertible architectures (FrEIA)</article-title>
      <uri>https://github.com/vislearn/FrEIA</uri>
    </element-citation>
  </ref>
  <ref id="ref-nflows">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        <name><surname>Bekasov</surname><given-names>Artur</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>nflows: Normalizing flows in PyTorch</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2020-11">2020</year><month>11</month>
      <uri>https://doi.org/10.5281/zenodo.4296287</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.4296287</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-haar1909theorie">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Haar</surname><given-names>Alfred</given-names></name>
      </person-group>
      <source>Zur theorie der orthogonalen funktionensysteme</source>
      <publisher-name>Georg-August-Universitat, Gottingen.</publisher-name>
      <year iso-8601-date="1909">1909</year>
      <pub-id pub-id-type="doi">10.1007/bf01456927</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-dinh2014nice">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dinh</surname><given-names>Laurent</given-names></name>
        <name><surname>Krueger</surname><given-names>David</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>Nice: Non-linear independent components estimation</article-title>
      <source>arXiv preprint arXiv:1410.8516</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1410.8516</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-dinh2016density">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dinh</surname><given-names>Laurent</given-names></name>
        <name><surname>Sohl-Dickstein</surname><given-names>Jascha</given-names></name>
        <name><surname>Bengio</surname><given-names>Samy</given-names></name>
      </person-group>
      <article-title>Density estimation using real nvp</article-title>
      <source>arXiv preprint arXiv:1605.08803</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1605.08803</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lensink2022fully">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lensink</surname><given-names>Keegan</given-names></name>
        <name><surname>Peters</surname><given-names>Bas</given-names></name>
        <name><surname>Haber</surname><given-names>Eldad</given-names></name>
      </person-group>
      <article-title>Fully hyperbolic convolutional neural networks</article-title>
      <source>Research in the Mathematical Sciences</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>9</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1007/s40687-022-00343-1</pub-id>
      <fpage>60</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kruse2021hint">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kruse</surname><given-names>Jakob</given-names></name>
        <name><surname>Detommaso</surname><given-names>Gianluca</given-names></name>
        <name><surname>Köthe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Scheichl</surname><given-names>Robert</given-names></name>
      </person-group>
      <article-title>HINT: Hierarchical invertible neural transport for density estimation and Bayesian inference</article-title>
      <source>Proceedings of the AAAI conference on artificial intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>35</volume>
      <pub-id pub-id-type="doi">10.1609/aaai.v35i9.16997</pub-id>
      <fpage>8191</fpage>
      <lpage>8199</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kingma2018glow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Durk P</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
      </person-group>
      <article-title>Glow: Generative flow with invertible 1x1 convolutions</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
    </element-citation>
  </ref>
  <ref id="ref-stimper2023normflows">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Liu</surname><given-names>David</given-names></name>
        <name><surname>Campbell</surname><given-names>Andrew</given-names></name>
        <name><surname>Berenz</surname><given-names>Vincent</given-names></name>
        <name><surname>Ryll</surname><given-names>Lukas</given-names></name>
        <name><surname>Schölkopf</surname><given-names>Bernhard</given-names></name>
        <name><surname>Hernández-Lobato</surname><given-names>José Miguel</given-names></name>
      </person-group>
      <article-title>Normflows: A PyTorch package for normalizing flows</article-title>
      <source>arXiv preprint arXiv:2302.12014</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.21105/joss.05361</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kumar2021enabling">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kumar</surname><given-names>Rajiv</given-names></name>
        <name><surname>Kotsi</surname><given-names>Maria</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Malcolm</surname><given-names>Alison</given-names></name>
      </person-group>
      <article-title>Enabling uncertainty quantification for seismic data preprocessing using normalizing flows (NF)—an interpolation example</article-title>
      <source>First international meeting for applied geoscience &amp; energy</source>
      <publisher-name>Society of Exploration Geophysicists</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1190/segam2021-3583705.1</pub-id>
      <fpage>1515</fpage>
      <lpage>1519</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rizzuti2020parameterizing">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Witte</surname><given-names>Philipp A</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Parameterizing uncertainty by deep invertible networks: An application to reservoir characterization</article-title>
      <source>SEG international exposition and annual meeting</source>
      <publisher-name>SEG</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1190/segam2020-3428150.1</pub-id>
      <fpage>D031S057R006</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-siahkoohi2021preconditioned">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Witte</surname><given-names>Philipp A</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Preconditioned training of normalizing flows for variational inference in inverse problems</article-title>
      <source>arXiv preprint arXiv:2101.03709</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2101.03709</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-siahkoohi2022wave">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Wave-equation-based inversion with amortized variational Bayesian inference</article-title>
      <source>arXiv preprint arXiv:2203.15881</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2203.15881</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-orozco2023refining">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Refining amortized posterior approximations using gradient-based summary statistics</article-title>
      <source>arXiv preprint arXiv:2305.08733</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2305.08733</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-louboutin2023learned">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Yin</surname><given-names>Ziyi</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Grady</surname><given-names>Thomas J</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Witte</surname><given-names>Philipp A</given-names></name>
        <name><surname>Møyner</surname><given-names>Olav</given-names></name>
        <name><surname>Gorman</surname><given-names>Gerard J</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Learned multiphysics inversion with differentiable programming and machine learning</article-title>
      <source>The Leading Edge</source>
      <publisher-name>Society of Exploration Geophysicists</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>42</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.1190/tle42070474.1</pub-id>
      <fpage>474</fpage>
      <lpage>486</lpage>
    </element-citation>
  </ref>
  <ref id="ref-orozco2021photoacoustic">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Leeuwen</surname><given-names>Tristan van</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix Johan</given-names></name>
      </person-group>
      <article-title>Photoacoustic imaging with conditional priors from normalizing flows</article-title>
      <source>NeurIPS 2021 workshop on deep learning and inverse problems</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-siahkoohi2023reliable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Reliable amortized variational inference with physics-based latent distribution correction</article-title>
      <source>Geophysics</source>
      <publisher-name>Society of Exploration Geophysicists</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>88</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1190/geo2022-0472.1</pub-id>
      <fpage>R297</fpage>
      <lpage>R322</lpage>
    </element-citation>
  </ref>
  <ref id="ref-yin2023solving">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yin</surname><given-names>Ziyi</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Solving multiphysics-based inverse problems with learned surrogates and constraints</article-title>
      <source>Advanced Modeling and Simulation in Engineering Sciences</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>10</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1186/s40323-023-00252-0</pub-id>
      <fpage>14</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gahlot2023inference">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gahlot</surname><given-names>Abhinav Prakash</given-names></name>
        <name><surname>Erdinc</surname><given-names>Huseyin Tuna</given-names></name>
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Yin</surname><given-names>Ziyi</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Inference of CO2 flow patterns–a feasibility study</article-title>
      <source>arXiv preprint arXiv:2311.00290</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2311.00290</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-orozco2023amortized">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Louboutin</surname><given-names>Mathias</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Leeuwen</surname><given-names>Tristan van</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix</given-names></name>
      </person-group>
      <article-title>Amortized normalizing flows for transcranial ultrasound with uncertainty quantification</article-title>
      <source>arXiv preprint arXiv:2303.03478</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2303.03478</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-orozco2023adjoint">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Orozco</surname><given-names>Rafael</given-names></name>
        <name><surname>Siahkoohi</surname><given-names>Ali</given-names></name>
        <name><surname>Rizzuti</surname><given-names>Gabrio</given-names></name>
        <name><surname>Leeuwen</surname><given-names>Tristan van</given-names></name>
        <name><surname>Herrmann</surname><given-names>Felix J</given-names></name>
      </person-group>
      <article-title>Adjoint operators enable fast and amortized machine learning based Bayesian uncertainty quantification</article-title>
      <source>Medical imaging 2023: Image processing</source>
      <publisher-name>SPIE</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>12464</volume>
      <pub-id pub-id-type="doi">10.1117/12.2651691</pub-id>
      <fpage>357</fpage>
      <lpage>367</lpage>
    </element-citation>
  </ref>
  <ref id="ref-khorashadizadeh2023conditional">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Khorashadizadeh</surname><given-names>AmirEhsan</given-names></name>
        <name><surname>Kothari</surname><given-names>Konik</given-names></name>
        <name><surname>Salsi</surname><given-names>Leonardo</given-names></name>
        <name><surname>Harandi</surname><given-names>Ali Aghababaei</given-names></name>
        <name><surname>Hoop</surname><given-names>Maarten de</given-names></name>
        <name><surname>Dokmanić</surname><given-names>Ivan</given-names></name>
      </person-group>
      <article-title>Conditional injective flows for Bayesian imaging</article-title>
      <source>IEEE Transactions on Computational Imaging</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.1109/tci.2023.3248949</pub-id>
      <fpage>224</fpage>
      <lpage>237</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
