<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4101</article-id>
<article-id pub-id-type="doi">10.21105/joss.04101</article-id>
<title-group>
<article-title>TorchMetrics - Measuring Reproducibility in
PyTorch</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-8133-682X</contrib-id>
<name>
<surname>Detlefsen</surname>
<given-names>Nicki Skafte</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7437-824X</contrib-id>
<name>
<surname>Borovec</surname>
<given-names>Jiri</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-0512-3053</contrib-id>
<name>
<surname>Schock</surname>
<given-names>Justus</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jha</surname>
<given-names>Ananya Harsh</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Koker</surname>
<given-names>Teddy</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Liello</surname>
<given-names>Luca Di</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Stancl</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Quan</surname>
<given-names>Changsheng</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Grechkin</surname>
<given-names>Maxim</given-names>
</name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Falcon</surname>
<given-names>William</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-8"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Grid AI Labs</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Technical University of Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University Hospital Düsseldorf</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>University of Trento</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Charles University</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Zhejiang University</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Independent Researcher</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>New York University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-12-08">
<day>8</day>
<month>12</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>70</issue>
<fpage>4101</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>deep learning</kwd>
<kwd>pytorch</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>A main problem with reproducing machine learning publications is
  the variance of metric implementations across papers. A lack of
  standardization leads to different behavior in mechanisms such as
  checkpointing, learning rate schedulers or early stopping, that will
  influence the reported results. For example, a complex metric such as
  Fréchet inception distance (FID) for synthetic image quality
  evaluation
  (<xref alt="Heusel et al., 2017" rid="ref-fid" ref-type="bibr">Heusel
  et al., 2017</xref>) will differ based on the specific interpolation
  method used.</p>
  <p>There have been a few attempts at tackling the reproducibility
  issues. Papers With Code
  (<xref alt="Papers with Code, n.d." rid="ref-papers_with_code" ref-type="bibr"><italic>Papers
  with Code</italic>, n.d.</xref>) links research code with its
  corresponding paper. Similarly, arXiv
  (<xref alt="Arxiv, n.d." rid="ref-arxiv" ref-type="bibr"><italic>Arxiv</italic>,
  n.d.</xref>) recently added a code and data section that links both
  official and community code to papers. However, these methods rely on
  the paper code to be made publicly accessible which is not always
  possible. Our approach is to provide the de-facto reference
  implementation for metrics. This approach enables proprietary work to
  still be comparable as long as they’ve used our reference
  implementations.</p>
  <p>We introduce TorchMetrics, a general-purpose metrics package that
  covers a wide variety of tasks and domains used in the machine
  learning community. TorchMetrics provides standard classification and
  regression metrics; and domain-specific metrics for audio, computer
  vision, natural language processing, and information retrieval. Our
  process for adding a new metric is as follows, first we integrate a
  well-tested and established third-party library. Once we’ve verified
  the implementations and written tests for them, we re-implement them
  in native PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>) to enable hardware acceleration and remove any
  bottlenecks in inter-device transfer.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Currently, there is no standard, widely-adopted metrics library for
  native PyTorch. Some native PyTorch libraries support domain-specific
  metrics such as Transformers
  (<xref alt="Wolf et al., 2020" rid="ref-transformers" ref-type="bibr">Wolf
  et al., 2020</xref>) for calculating NLP-specific metrics. However, no
  library exists that covers multiple domains. PyTorch users, therefore,
  often rely on non-PyTorch packages such as Scikit-learn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit_learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) for computing even simple metrics such as
  accuracy, F1, or AUROC metrics.</p>
  <p>However, while Scikit-learn is considered the gold standard for
  computing metrics in regression and classification, it relies on the
  core assumption that all predictions and targets are available
  simultaneously. This contradicts the typical workflow in a modern deep
  learning training/evaluation loop where data comes in batches.
  Therefore, the metric needs to be calculated in an online fashion. It
  is important to note that, in general, it is not possible to calculate
  a global metric as its average or sum of the metric calculated per
  batch.</p>
  <p>TorchMetrics solves this problem by introducing stateful metrics
  that can calculate metric values on a stream of data alongside the
  classical functional and stateless metrics provided by other packages
  like Scikit-learn. We do this with an effortless
  <monospace>update</monospace> and <monospace>compute</monospace>
  interface, well known from packages such as Keras
  (<xref alt="Chollet &amp; others, 2015" rid="ref-keras" ref-type="bibr">Chollet
  &amp; others, 2015</xref>). The <monospace>update</monospace> function
  takes in a batch of predictions and targets and updates the internal
  state. For example, for a metric such as accuracy, the internal states
  are simply the number of correctly classified samples and the total
  observed number of samples. When all batches have been passed to the
  <monospace>update</monospace> method, the
  <monospace>compute</monospace> method can get the accumulated accuracy
  over all the batches. In addition to <monospace>update</monospace> and
  <monospace>compute</monospace>, each metric also has a
  <monospace>forward</monospace> method (as any other
  <monospace>torch.nn.Module</monospace>) that can be used to both get
  the metric on the current batch of data and accumulate global state.
  This enables the user to get fine-grained info about the metric on the
  individual batch and the global metric of how well their model is
  doing.</p>
  <code language="python"># Minimal example showcasing the TorchMetrics interface
import torch
from torch import tensor, Tensor
# base class all modular metrics inherit from
from torchmetrics import Metric

class Accuracy(Metric):
    def __init__(self):
        super().__init__()
        # `self.add_state` defines the states of the metric
        #  that should be accumulated and will automatically
        #  be synchronized between devices
        self.add_state(&quot;correct&quot;, default=tensor(0), dist_reduce_fx=&quot;sum&quot;)
        self.add_state(&quot;total&quot;, default=tensor(0), dist_reduce_fx=&quot;sum&quot;)

    def update(self, preds: Tensor, target: Tensor) -&gt; None:
        # update takes `preds` and `target` and accumulate the current
        # stream of data into the global states for later
        self.correct += torch.sum(preds == target)
        self.total += target.numel()

    def compute(self) -&gt; Tensor:
        # compute takes the accumulated states
        # and returns the final metric value
        return self.correct / self.total</code>
  <p>Another core feature of TorchMetrics is its ability to scale to
  multiple devices seamlessly. Modern deep learning models are often
  trained on hundreds of devices such as GPUs or TPUs (see Zhai et al.
  (<xref alt="2021" rid="ref-large_example1" ref-type="bibr">2021</xref>);
  Liu et al.
  (<xref alt="2019" rid="ref-large_example2" ref-type="bibr">2019</xref>)
  for examples). This scale introduces the need to synchronize metrics
  across machines to get the correct value during training and
  evaluation. In distributed environments, TorchMetrics automatically
  accumulates across devices before reporting the calculated metric to
  the user.</p>
  <p>In addition to stateful metrics (called modular metrics in
  TorchMetrics), we also support a functional interface that works
  similar to Scikit-learn. This interface provides simple Python
  functions that take as input PyTorch Tensors and return the
  corresponding metric as a PyTorch Tensor. These can be used when
  metrics are evaluated on single devices, and no accumulation is
  needed, making them very fast to compute.</p>
  <p>TorchMetrics exhibits high test coverage on the various
  configurations, including all three major OS platforms (Linux, macOS,
  and Windows), and various Python, CUDA, and PyTorch versions. We test
  both minimum and latest package requirements for all combinations of
  OS and Python versions and include additional tests for each PyTorch
  version from 1.3 up to future development versions. On every pull
  request and merge to master, we run a full test suite. All standard
  tests run on CPU. In addition, we run all tests on a multi-GPU setting
  which reflects realistic Deep Learning workloads. For usability, we
  have auto-generated HTML documentation (hosted at
  <ext-link ext-link-type="uri" xlink:href="https://torchmetrics.readthedocs.io/en/stable/">readthedocs</ext-link>)
  from the source code which updates in real-time with new merged pull
  requests.</p>
  <p>TorchMetrics is released under the Apache 2.0 license. The source
  code is available at https://github.com/Lightning-AI/metrics.</p>
</sec>
<sec id="acknowledgement">
  <title>Acknowledgement</title>
  <p>The TorchMetrics team thanks Thomas Chaton, Ethan Harris, Carlos
  Mocholí, Sean Narenthiran, Adrian Wälchli, and Ananth Subramaniam for
  contributing ideas, participating in discussions on API design, and
  completing Pull Request reviews. We also thank all of our open-source
  contributors for reporting and resolving issues with this package. We
  are grateful to the PyTorch Lightning team for their ongoing and
  dedicated support of this project, and Grid.ai for providing computing
  resources and cloud credits needed to run our Continuos
  Integrations.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-fid">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Heusel</surname><given-names>Martin</given-names></name>
        <name><surname>Ramsauer</surname><given-names>Hubert</given-names></name>
        <name><surname>Unterthiner</surname><given-names>Thomas</given-names></name>
        <name><surname>Nessler</surname><given-names>Bernhard</given-names></name>
        <name><surname>Hochreiter</surname><given-names>Sepp</given-names></name>
      </person-group>
      <article-title>GANs trained by a two time-scale update rule converge to a local Nash equilibrium</article-title>
      <source>Advances in neural information processing systems</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
      <uri>https://proceedings.neurips.cc/paper/2017/hash/8a1d694707eb0fefe65871369074926d-Abstract.html</uri>
      <fpage>6629</fpage>
      <lpage>6640</lpage>
    </element-citation>
  </ref>
  <ref id="ref-papers_with_code">
    <element-citation>
      <article-title>Papers with code</article-title>
      <publisher-name>https://paperswithcode.com/</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-arxiv">
    <element-citation>
      <article-title>Arxiv</article-title>
      <publisher-name>https://arxiv.org/</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-transformers">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>Rémi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>Davison</surname><given-names>Joe</given-names></name>
        <name><surname>Shleifer</surname><given-names>Sam</given-names></name>
        <name><surname>Platen</surname><given-names>Patrick von</given-names></name>
        <name><surname>Ma</surname><given-names>Clara</given-names></name>
        <name><surname>Jernite</surname><given-names>Yacine</given-names></name>
        <name><surname>Plu</surname><given-names>Julien</given-names></name>
        <name><surname>Xu</surname><given-names>Canwen</given-names></name>
        <name><surname>Scao</surname><given-names>Teven Le</given-names></name>
        <name><surname>Gugger</surname><given-names>Sylvain</given-names></name>
        <name><surname>Drame</surname><given-names>Mariama</given-names></name>
        <name><surname>Lhoest</surname><given-names>Quentin</given-names></name>
        <name><surname>Rush</surname><given-names>Alexander M.</given-names></name>
      </person-group>
      <article-title>Transformers: State-of-the-art natural language processing</article-title>
      <source>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <uri>https://www.aclweb.org/anthology/2020.emnlp-demos.6</uri>
      <fpage>38</fpage>
      <lpage>45</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scikit_learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-keras">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Keras</article-title>
      <publisher-name>https://github.com/fchollet/keras; GitHub</publisher-name>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-large_example1">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zhai</surname><given-names>Xiaohua</given-names></name>
        <name><surname>Kolesnikov</surname><given-names>Alexander</given-names></name>
        <name><surname>Houlsby</surname><given-names>Neil</given-names></name>
        <name><surname>Beyer</surname><given-names>Lucas</given-names></name>
      </person-group>
      <article-title>Scaling vision transformers</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2106.04560</uri>
    </element-citation>
  </ref>
  <ref id="ref-large_example2">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Yinhan</given-names></name>
        <name><surname>Ott</surname><given-names>Myle</given-names></name>
        <name><surname>Goyal</surname><given-names>Naman</given-names></name>
        <name><surname>Du</surname><given-names>Jingfei</given-names></name>
        <name><surname>Joshi</surname><given-names>Mandar</given-names></name>
        <name><surname>Chen</surname><given-names>Danqi</given-names></name>
        <name><surname>Levy</surname><given-names>Omer</given-names></name>
        <name><surname>Lewis</surname><given-names>Mike</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
        <name><surname>Stoyanov</surname><given-names>Veselin</given-names></name>
      </person-group>
      <article-title>RoBERTa: A robustly optimized BERT pretraining approach</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1907.11692</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
