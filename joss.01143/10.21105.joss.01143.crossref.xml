<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>9533ae8ff394df17f1c44a9e00c3c44a</doi_batch_id>
    <timestamp>20191117211304</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>http://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2019</year>
        </publication_date>
        <journal_volume>
          <volume>4</volume>
        </journal_volume>
        <issue>33</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>ArviZ a unified library for exploratory analysis of Bayesian models in Python</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Ravin</given_name>
            <surname>Kumar</surname>
            <ORCID>http://orcid.org/0000-0003-0501-6098</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Colin</given_name>
            <surname>Carroll</surname>
            <ORCID>http://orcid.org/0000-0001-6977-0861</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Ari</given_name>
            <surname>Hartikainen</surname>
            <ORCID>http://orcid.org/0000-0002-4569-569X</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Osvaldo</given_name>
            <surname>Martin</surname>
            <ORCID>http://orcid.org/0000-0001-7419-8978</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>15</day>
          <year>2019</year>
        </publication_date>
        <pages>
          <first_page>1143</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.01143</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">https://doi.org/10.5281/zenodo.2540945</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/1143</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.01143</doi>
          <resource>http://joss.theoj.org/papers/10.21105/joss.01143</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.01143/10.21105.joss.01143.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <unstructured_citation>arXiv, arxiv, 1709.01449, stat, Visualization in Bayesian Workflow, Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers., arXiv:1709.01449 [stat], Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew, sep, 2017, Statistics - Methodology,Statistics - Applications, /home/osvaldo/Zotero/storage/KJYIRJWV/Gabry et al. - 2017 - Visualization in Bayesian workflow.pdf;/home/osvaldo/Zotero/storage/985JKW2Q/1709.html, Comment: 17 pages, 11 Figures. Includes supplementary material, 9</unstructured_citation>
          </citation>
          <citation key="ref2">
            <unstructured_citation>arXiv, arxiv, 1507.04544, Practical Bayesian Model Evaluation Using Leave-One-out Cross-Validation and WAIC, Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparing of predictive errors between two models. We implement the computations in an R package called ’loo’ and demonstrate using models fit with the Bayesian inference package Stan., arXiv:1507.04544 [stat], Vehtari, Aki and Gelman, Andrew and Gabry, Jonah, jul, 2015, Statistics - Computation,Statistics - Methodology, 7</unstructured_citation>
          </citation>
          <citation key="ref3">
            <unstructured_citation>A Widely Applicable Bayesian Information Criterion, 14, arXiv, arxiv, 1208.6338, A statistical model or a learning machine is called regular if the map taking a parameter to a probability distribution is one-to-one and if its Fisher information matrix is always positive definite. If otherwise, it is called singular. In regular statistical models, the Bayes free energy, which is defined by the minus logarithm of Bayes marginal likelihood, can be asymptotically approximated by the Schwarz Bayes information criterion (BIC), whereas in singular models such approximation does not hold. Recently, it was proved that the Bayes free energy of a singular model is asymptotically given by a generalized formula using a birational invariant, the real log canonical threshold (RLCT), instead of half the number of parameters in BIC. Theoretical values of RLCTs in several statistical models are now being discovered based on algebraic geometrical methodology. However, it has been difficult to estimate the Bayes free energy using only training samples, because an RLCT depends on an unknown true distribution. In the present paper, we define a widely applicable Bayesian information criterion (WBIC) by the average log likelihood function over the posterior distribution with the inverse temperature 1/log n, where n is the number of training samples. We mathematically prove that WBIC has the same asymptotic expansion as the Bayes free energy, even if a statistical model is singular for or  unrealizable by a statistical model. Since WBIC can be numerically calculated without any information about a true distribution, it is a generalized version of BIC onto singular statistical models., Journal of Machine Learning Research, Watanabe, Sumio, mar, 2013, 867-897, /home/osvaldo/Zotero/storage/424KD4K3/Watanabe - 2013 - A Widely Applicable Bayesian Information Criterion.pdf, 3</unstructured_citation>
          </citation>
          <citation key="ref4">
            <doi>10.5334/jors.148</doi>
          </citation>
          <citation key="ref5">
            <doi>10.1109/38.56302</doi>
          </citation>
          <citation key="ref6">
            <doi>10.1063/1.4823180</doi>
          </citation>
          <citation key="ref7">
            <unstructured_citation>Probabilistic Programming, http://probabilistic-programming.org/wiki/Home, http://probabilistic-programming.org/wiki/Home, Daniel Roy, 2015, /home/osvaldo/Zotero/storage/6UZ84C6Q/Home.html</unstructured_citation>
          </citation>
          <citation key="ref8">
            <unstructured_citation>Boca Raton, 1 edition, Bayesian Programming, 978-1-4398-8032-6, Probability as an Alternative to Boolean LogicWhile logic is the mathematical foundation of rational reasoning and the fundamental principle of computing, it is restricted to problems where information is both complete and certain. However, many real-world problems, from financial investments to email filtering, are incomplete or uncertain in nature. Probability theory and Bayesian computing together provide an alternative framework to deal with incomplete and uncertain data.  Decision-Making Tools and Methods for Incomplete and Uncertain DataEmphasizing probability as an alternative to Boolean logic, Bayesian Programming covers new methods to build probabilistic programs for real-world applications. Written by the team who designed and implemented an efficient probabilistic inference engine to interpret Bayesian programs, the book offers many Python examples that are also available on a supplementary website together with an interpreter that allows readers to experiment with this new approach to programming. Principles and Modeling Only requiring a basic foundation in mathematics, the first two parts of the book present a new methodology for building subjective probabilistic models. The authors introduce the principles of Bayesian programming and discuss good practices for probabilistic modeling. Numerous simple examples highlight the application of Bayesian modeling in different fields. Formalism and AlgorithmsThe third part synthesizes existing work on Bayesian inference algorithms since an efficient Bayesian inference engine is needed to automate the probabilistic calculus in Bayesian programs. Many bibliographic references are included for readers who would like more details on the formalism of Bayesian programming, the main probabilistic models, general purpose algorithms for Bayesian inference, and learning problems. FAQsAlong with a glossary, the fourth part contains answers to frequently asked questions. The authors compare Bayesian programming and possibility theories, discuss the computational complexity of Bayesian inference, cover the irreducibility of incompleteness, and address the subjectivist versus objectivist epistemology of probability.  The First Steps toward a Bayesian ComputerA new modeling methodology, new inference algorithms, new programming languages, and new hardware are all needed to create a complete Bayesian computing framework. Focusing on the methodology and algorithms, this book describes the first steps toward reaching that goal. It encourages readers to explore emerging areas, such as bio-inspired computing, and develop new programming languages and hardware architectures., English, Chapman and Hall/CRC, Bessiere, Pierre and Mazer, Emmanuel and Ahuactzin, Juan Manuel and Mekhnacha, Kamel, dec, 2013, 12</unstructured_citation>
          </citation>
          <citation key="ref9">
            <doi>10.1038/nature14541</doi>
          </citation>
          <citation key="ref10">
            <unstructured_citation>Reading, Mass, 1 edition, Exploratory Data Analysis, 978-0-201-07616-5, The approach in this introductory book is that of informal study of the data. Methods range from plotting picture-drawing techniques to rather elaborate numerical summaries. Several of the methods are the original creations of the author, and all can be carried out either with pencil or aided by hand-held calculator., English, Pearson, Tukey, John W., 1977</unstructured_citation>
          </citation>
          <citation key="ref11">
            <unstructured_citation>arXiv, arxiv, 1709.01449, stat, Visualization in Bayesian Workflow, Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workflow and it is indispensable when drawing inferences from the types of modern, high-dimensional models that are used by applied researchers., arXiv:1709.01449 [stat], Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew, sep, 2017, Statistics - Methodology,Statistics - Applications, /home/osvaldo/Zotero/storage/KJYIRJWV/Gabry et al. - 2017 - Visualization in Bayesian workflow.pdf;/home/osvaldo/Zotero/storage/985JKW2Q/1709.html, Comment: 17 pages, 11 Figures. Includes supplementary material, 9</unstructured_citation>
          </citation>
          <citation key="ref12">
            <doi>10.1111/j.1751-5823.2003.tb00203.x</doi>
          </citation>
          <citation key="ref13">
            <doi>10.1002/9781118150702.ch1</doi>
          </citation>
          <citation key="ref14">
            <unstructured_citation>Pyro: Deep Universal Probabilistic Programming, Journal of Machine Learning Research, Bingham, Eli and Chen, Jonathan P. and Jankowiak, Martin and Obermeyer, Fritz and Pradhan, Neeraj and Karaletsos, Theofanis and Singh, Rohit and Szerlip, Paul and Horsfall, Paul and Goodman, Noah D., 2018, arXiv, arxiv, 1810.09538</unstructured_citation>
          </citation>
          <citation key="ref15">
            <doi>10.1086/670067</doi>
          </citation>
          <citation key="ref16">
            <doi>10.7717/peerj-cs.55</doi>
          </citation>
          <citation key="ref17">
            <doi>10.18637/jss.v076.i01</doi>
          </citation>
          <citation key="ref18">
            <unstructured_citation>arXiv, arxiv, 1610.09787, cs, stat, Edward: A Library for Probabilistic Modeling, Inference, and Criticism, Edward, Probabilistic modeling is a powerful approach for analyzing empirical information. We describe Edward, a library for probabilistic modeling. Edward’s design reflects an iterative process pioneered by George Box: build a model of a phenomenon, make inferences about the model given data, and criticize the model’s fit to the data. Edward supports a broad class of probabilistic models, efficient algorithms for inference, and many techniques for model criticism. The library builds on top of TensorFlow to support distributed training and hardware such as GPUs. Edward enables the development of complex probabilistic models and their algorithms at a massive scale., arXiv:1610.09787 [cs, stat], Tran, Dustin and Kucukelbir, Alp and Dieng, Adji B. and Rudolph, Maja and Liang, Dawen and Blei, David M., oct, 2016, Statistics - Computation,Computer Science - Artificial Intelligence,Statistics - Machine Learning,Statistics - Applications,Computer Science - Programming Languages, /home/osvaldo/Zotero/storage/PD4M3ZKL/Tran et al. - 2016 - Edward A library for probabilistic modeling, infe.pdf;/home/osvaldo/Zotero/storage/LX37VDW6/1610.html, 10</unstructured_citation>
          </citation>
          <citation key="ref19">
            <unstructured_citation>arXiv, arxiv, 1701.03757, cs, stat, Deep Probabilistic Programming, We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations—random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow., arXiv:1701.03757 [cs, stat], Tran, Dustin and Hoffman, Matthew D. and Saurous, Rif A. and Brevdo, Eugene and Murphy, Kevin and Blei, David M., jan, 2017, Statistics - Computation,Computer Science - Artificial Intelligence,Statistics - Machine Learning,Computer Science - Machine Learning,Computer Science - Programming Languages, /home/osvaldo/Zotero/storage/7EJVRYHP/Tran et al. - 2017 - Deep Probabilistic Programming.pdf;/home/osvaldo/Zotero/storage/FBQ79439/1701.html, Comment: Appears in International Conference on Learning Representations, 2017. A companion webpage for this paper is available at http://edwardlib.org/iclr2017, 1</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
