<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5776</article-id>
<article-id pub-id-type="doi">10.21105/joss.05776</article-id>
<title-group>
<article-title>BlackBIRDS: Black-Box Inference foR Differentiable
Simulators</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5055-9863</contrib-id>
<name>
<surname>Quera-Bofarull</surname>
<given-names>Arnau</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8304-8450</contrib-id>
<name>
<surname>Dyer</surname>
<given-names>Joel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-2"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2082-734X</contrib-id>
<name>
<surname>Calinescu</surname>
<given-names>Anisoara</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7871-073X</contrib-id>
<name>
<surname>Farmer</surname>
<given-names>J. Doyne</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9329-8410</contrib-id>
<name>
<surname>Wooldridge</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Computer Science, University of Oxford,
UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute for New Economic Thinking, University of Oxford,
UK</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Mathematical Institute, University of Oxford,
UK</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Santa Fe Institute, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
<corresp id="cor-2">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-07-14">
<day>14</day>
<month>7</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>89</issue>
<fpage>5776</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bayesian inference</kwd>
<kwd>differentiable simulators</kwd>
<kwd>variational inference</kwd>
<kwd>Markov chain Monte Carlo</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>BlackBIRDS</monospace> is a Python package consisting of
  generically applicable, black-box inference methods for differentiable
  simulation models. It facilitates both (a) the differentiable
  implementation of simulation models by providing a common
  object-oriented framework for their implementation in
  <monospace>PyTorch</monospace>
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>), and (b) the use of a variety of
  gradient-assisted inference procedures for these simulation models,
  allowing researchers to easily exploit the differentiable nature of
  their simulator in parameter estimation tasks. The package consists of
  both Bayesian and non-Bayesian inference methods, and relies on
  well-supported software libraries (e.g.,
  <monospace>normflows</monospace>,
  <xref alt="Stimper et al., 2023" rid="ref-normflows" ref-type="bibr">Stimper
  et al., 2023</xref>) to provide this broad functionality.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Across scientific disciplines and application domains, simulation
  is used extensively as a means to studying complex mathematical models
  of real-world systems. A simulation-based approach to modelling such
  systems provides the modeller with significant benefits, permitting
  them to specify their model in the way that they believe most
  faithfully represents the true data-generating process and relieving
  them from concerns regarding the mathematical tractability of the
  resulting model. However, this additional flexibility comes at a
  price: the resulting model can be too complex to easily perform
  optimisation and inference tasks on the corresponding simulator, which
  in many cases necessitates the use of approximate, simulation-based
  inference and optimisation methods to perform these tasks
  inexactly.</p>
  <p>The complicated and black-box nature of many simulation models can
  present a significant barrier to the successful deployment of these
  simulation-based inference and optimisation techniques. Consequently,
  there has been increasing interest within various scientific
  communities in constructing <italic>differentiable</italic> simulation
  models (see e.g.,
  <xref alt="Baydin et al., 2020" rid="ref-hep" ref-type="bibr">Baydin
  et al., 2020</xref>;
  <xref alt="Chopra et al., 2023" rid="ref-gradabm" ref-type="bibr">Chopra
  et al., 2023</xref>): simulation models for which the gradient of the
  model output with respect to the model‚Äôs input parameters can be
  easily obtained. The primary motivation for this is that access to
  this additional information, which captures the sensitivity of the
  output of the simulator to changes in the input, can enable the use of
  more efficient simulation-based optimisation and inference procedures,
  helping to reduce the total runtime of such algorithms, their overall
  consumption of valuable computational resources, and their concomitant
  financial and environmental costs.</p>
  <p>To this end, <monospace>BlackBIRDS</monospace> was designed to
  provide researchers with easy access to a set of parameter inference
  methods that exploit the gradient information provided by
  differentiable simulators. The package provides support for a variety
  of approaches to gradient-assisted parameter inference, including:</p>
  <list list-type="bullet">
    <list-item>
      <p>Simulated Minimum Distance
      (<xref alt="Franke, 2009" rid="ref-smm" ref-type="bibr">Franke,
      2009</xref>; SMD, see e.g.,
      <xref alt="Gourieroux et al., 1993" rid="ref-ii" ref-type="bibr">Gourieroux
      et al., 1993</xref>), in which parameter point estimates
      <inline-formula><alternatives>
      <tex-math><![CDATA[\hat{\boldsymbol{\theta}}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>ùõâ</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
      are obtained as</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\hat{\boldsymbol{\theta}} 
          = 
          \arg \min_{\boldsymbol{\theta} \in \boldsymbol{\Theta}} {
            \ell(\boldsymbol{\theta}, \mathbf{y})
          },]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>ùõâ</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>ùõâ</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>ùöØ</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùõâ</mml:mi></mml:math></alternatives></inline-formula>
      are the simulator‚Äôs parameters, which take values in some set
      <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\Theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùöØ</mml:mi></mml:math></alternatives></inline-formula>,
      and <inline-formula><alternatives>
      <tex-math><![CDATA[\ell]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>
      is a loss function capturing the compatibility between the
      observed data <inline-formula><alternatives>
      <tex-math><![CDATA[\mathbf{y}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùê≤</mml:mi></mml:math></alternatives></inline-formula>
      and the simulator‚Äôs behaviour at parameter vector
      <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùõâ</mml:mi></mml:math></alternatives></inline-formula>;</p>
    </list-item>
    <list-item>
      <p>Markov chain Monte Carlo (MCMC), in which samples from a
      parameter posterior</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\pi(\boldsymbol{\theta} \mid \mathbf{y}) \propto e^{-\ell(\boldsymbol{\theta}, \mathbf{y})} \pi(\boldsymbol{\theta}),]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>œÄ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo>‚à£</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>‚àù</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>‚àí</mml:mo><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>œÄ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>corresponding to a choice of loss function
      <inline-formula><alternatives>
      <tex-math><![CDATA[\ell]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>
      and a prior density <inline-formula><alternatives>
      <tex-math><![CDATA[\pi]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>œÄ</mml:mi></mml:math></alternatives></inline-formula>
      over <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\Theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùöØ</mml:mi></mml:math></alternatives></inline-formula>
      are generated by executing a Markov chain on
      <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\Theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùöØ</mml:mi></mml:math></alternatives></inline-formula>.
      Currently, support is provided for Metropolis-adjusted Langevin
      Dynamics
      (<xref alt="Roberts &amp; Tweedie, 1996" rid="ref-mala" ref-type="bibr">Roberts
      &amp; Tweedie, 1996</xref>), although nothing prevents the
      inclusion of additional gradient-assisted MCMC algorithms such as
      Hamiltonian Monte Carlo
      (<xref alt="Duane et al., 1987" rid="ref-hmc" ref-type="bibr">Duane
      et al., 1987</xref>);</p>
    </list-item>
    <list-item>
      <p>Variational Inference (VI), in which a parameteric
      approximation <inline-formula><alternatives>
      <tex-math><![CDATA[q^*]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>q</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
      to the intractable posterior is obtained by solving the following
      optimisation problem over a variational family
      <inline-formula><alternatives>
      <tex-math><![CDATA[\mathcal{Q}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùí¨</mml:mi></mml:math></alternatives></inline-formula>
      <disp-formula><alternatives>
      <tex-math><![CDATA[q^* = \arg\min_{q \in \mathcal{Q}} {
                \mathbb{E}_{q}\left[ -\ell(\boldsymbol{\theta}, \mathbf{y})\right]
                + \mathbb{E}_{q}\left[\log\frac{q(\boldsymbol{\theta})}{\pi(\boldsymbol{\theta})}\right]
        },]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>q</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mo>arg</mml:mo><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>q</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>ùí¨</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:msub><mml:mi>ùîº</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mo>‚àí</mml:mo><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo>,</mml:mo><mml:mi>ùê≤</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>ùîº</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mo>log</mml:mo><mml:mfrac><mml:mrow><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>œÄ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùõâ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[\ell]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>
      is defined as above and <inline-formula><alternatives>
      <tex-math><![CDATA[\pi]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>œÄ</mml:mi></mml:math></alternatives></inline-formula>
      is a prior density over <inline-formula><alternatives>
      <tex-math><![CDATA[\boldsymbol{\Theta}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùöØ</mml:mi></mml:math></alternatives></inline-formula>.</p>
    </list-item>
  </list>
  <p>The package is written such that the user is free to specify their
  choice of <inline-formula><alternatives>
  <tex-math><![CDATA[\ell]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\pi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>œÄ</mml:mi></mml:math></alternatives></inline-formula>
  (in the case of Bayesian methods), under the constraint that both
  choices are differentiable with respect to
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\theta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùõâ</mml:mi></mml:math></alternatives></inline-formula>.
  This allows the user to target a wide variety of parameter point
  estimators, and both classical and generalised (see e.g.,
  <xref alt="Bissiri et al., 2016" rid="ref-bissiri" ref-type="bibr">Bissiri
  et al., 2016</xref>;
  <xref alt="Knoblauch et al., 2022" rid="ref-gvi" ref-type="bibr">Knoblauch
  et al., 2022</xref>) posteriors. We provide a number of
  <ext-link ext-link-type="uri" xlink:href="https://www.arnau.ai/blackbirds/">tutorials</ext-link>
  demonstrating (a) how to implement a simulator in a differentiable
  framework in PyTorch and (b) how to apply the different parameter
  inference methods supported by <monospace>BlackBIRDS</monospace> to
  these differentiable simulators. Our package provides the user with
  flexible posterior density estimators with the use of normalising
  flows, and has already been used in scientific research to calibrate
  differentiable simulators
  (<xref alt="Quera-Bofarull, Chopra, et al., 2023" rid="ref-ai4abm" ref-type="bibr">Quera-Bofarull,
  Chopra, et al., 2023</xref>;
  <xref alt="Quera-Bofarull, Dyer, et al., 2023" rid="ref-dae" ref-type="bibr">Quera-Bofarull,
  Dyer, et al., 2023</xref>).</p>
  <sec id="related-software">
    <title>Related software</title>
    <p><monospace>BlackBIRDS</monospace> offers complementary
    functionality to a number of existing Python packages.
    <monospace>sbi</monospace>
    (<xref alt="Tejero-Cantero et al., 2020" rid="ref-sbi" ref-type="bibr">Tejero-Cantero
    et al., 2020</xref>) is a package offering PyTorch-based
    implementations of numerous simulation-based inference algorithms,
    including those based on the use of MCMC and neural conditional
    density estimators. Our package differs significantly, however: in
    contrast to <monospace>sbi</monospace>,
    <monospace>BlackBIRDS</monospace> provides support for both Bayesian
    and non-Bayesian inference methods, and permits the researcher to
    exploit gradients of the simulator, loss function, and/or posterior
    density with respect to parameters <inline-formula><alternatives>
    <tex-math><![CDATA[\mathbf{\theta}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùõâ</mml:mi></mml:math></alternatives></inline-formula>
    during inference tasks. The same comparison applies to the the
    <monospace>BayesFlow</monospace> package
    (<xref alt="Radev et al., 2023" rid="ref-radev2023bayesflow" ref-type="bibr">Radev
    et al., 2023</xref>). <monospace>black-it</monospace>
    (<xref alt="Benedetti et al., 2022" rid="ref-blit" ref-type="bibr">Benedetti
    et al., 2022</xref>) is a further recent Python package that
    collects some recently developed parameter estimation methods from
    the agent-based modelling community; the focus of this package is,
    however, on non-Bayesian methods, and the package does not currently
    support the exploitation of simulator gradients.
    <monospace>PyVBMC</monospace>
    (<xref alt="Huggins et al., 2023" rid="ref-pyvbmc" ref-type="bibr">Huggins
    et al., 2023</xref>) provides a Python implementation of the
    Variational Bayesian Monte Carlo algorithm using Gaussian processes,
    but differs from our package in that it does not exploit simulator
    gradients and is focused on Bayesian inference alone. Additional
    older packages (e.g.,
    <xref alt="Dutta et al., 2021" rid="ref-abcpy" ref-type="bibr">Dutta
    et al., 2021</xref>;
    <xref alt="Sch√§lte et al., 2022" rid="ref-pyabc" ref-type="bibr">Sch√§lte
    et al., 2022</xref>) also focus on approximate Bayesian inference
    methods for non-differentiable simulators. Beyond this, we are
    unaware of other mature software projects in Python that support
    parameter inference in the specific case of differentiable
    simulation models.</p>
  </sec>
</sec>
<sec id="features">
  <title>Features</title>
  <list list-type="bullet">
    <list-item>
      <p>User-friendly and flexible API: SMD only requires the loss
      function <inline-formula><alternatives>
      <tex-math><![CDATA[\ell]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>
      and the optimiser to use, while MCMC (resp. VI) requires only the
      loss <inline-formula><alternatives>
      <tex-math><![CDATA[\ell]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>‚Ñì</mml:mo></mml:math></alternatives></inline-formula>,
      the prior density <inline-formula><alternatives>
      <tex-math><![CDATA[\pi]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>œÄ</mml:mi></mml:math></alternatives></inline-formula>,
      and the MCMC method (resp. posterior approximator) to be
      specified. However, additional arguments can be provided to
      straightforwardly customise hyperparameters of the different
      methods.</p>
    </list-item>
    <list-item>
      <p>Multi-GPU parallelisation support with MPI.</p>
    </list-item>
    <list-item>
      <p>Support for both forward-mode and reverse-mode
      auto-differentiation.</p>
    </list-item>
    <list-item>
      <p>Continuous integration and unit tests.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This research was supported by a UKRI AI World Leading Researcher
  Fellowship awarded to Wooldridge (grant EP/W002949/1). M. Wooldridge
  and A. Calinescu acknowledge funding from Trustworthy AI - Integrating
  Learning, Optimisation and Reasoning
  (<ext-link ext-link-type="uri" xlink:href="https://tailor-network.eu/">TAILOR</ext-link>),
  a project funded by European Union Horizon2020 research and innovation
  program under Grant Agreement 952215.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>K√∂pf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zach</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Proceedings of the 33rd international conference on neural information processing systems</source>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <publisher-loc>Red Hook, NY, USA</publisher-loc>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-normflows">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stimper</surname><given-names>Vincent</given-names></name>
        <name><surname>Liu</surname><given-names>David</given-names></name>
        <name><surname>Campbell</surname><given-names>Andrew</given-names></name>
        <name><surname>Berenz</surname><given-names>Vincent</given-names></name>
        <name><surname>Ryll</surname><given-names>Lukas</given-names></name>
        <name><surname>Sch√∂lkopf</surname><given-names>Bernhard</given-names></name>
        <name><surname>Hern√°ndez-Lobato</surname><given-names>Jos√© Miguel</given-names></name>
      </person-group>
      <article-title>Normflows: A PyTorch package for normalizing flows</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>86</issue>
      <uri>https://doi.org/10.21105/joss.05361</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05361</pub-id>
      <fpage>5361</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gradabm">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chopra</surname><given-names>Ayush</given-names></name>
        <name><surname>Rodr√≠guez</surname><given-names>Alexander</given-names></name>
        <name><surname>Subramanian</surname><given-names>Jayakumar</given-names></name>
        <name><surname>Quera-Bofarull</surname><given-names>Arnau</given-names></name>
        <name><surname>Krishnamurthy</surname><given-names>Balaji</given-names></name>
        <name><surname>Prakash</surname><given-names>B. Aditya</given-names></name>
        <name><surname>Raskar</surname><given-names>Ramesh</given-names></name>
      </person-group>
      <article-title>Differentiable agent-based epidemiology</article-title>
      <source>Proceedings of the 2023 international conference on autonomous agents and multiagent systems</source>
      <publisher-name>International Foundation for Autonomous Agents and Multiagent Systems</publisher-name>
      <publisher-loc>Richland, SC</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <isbn>978-1-4503-9432-1</isbn>
      <fpage>1848</fpage>
      <lpage>1857</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baydin</surname><given-names>Atƒ±lƒ±m G√ºnes</given-names></name>
        <name><surname>NYU</surname><given-names>Kyle Cranmer</given-names></name>
        <name><surname>Feickert</surname><given-names>Matthew</given-names></name>
        <name><surname>Gray</surname><given-names>Lindsey</given-names></name>
        <name><surname>Heinrich</surname><given-names>Lukas</given-names></name>
        <name><surname>NYU</surname><given-names>Alexander Held</given-names></name>
        <name><surname>Neubauer</surname><given-names>Andrew Melo Vanderbilt Mark</given-names></name>
        <name><surname>Pearkes</surname><given-names>Jannicke</given-names></name>
        <name><surname>Simpson</surname><given-names>Nathan</given-names></name>
        <name><surname>Smith</surname><given-names>Nick</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Differentiable programming in high-energy physics</article-title>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-ii">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gourieroux</surname><given-names>Christian</given-names></name>
        <name><surname>Monfort</surname><given-names>Alain</given-names></name>
        <name><surname>Renault</surname><given-names>Eric</given-names></name>
      </person-group>
      <article-title>Indirect inference</article-title>
      <source>Journal of applied econometrics</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1993">1993</year>
      <volume>8</volume>
      <issue>S1</issue>
      <fpage>S85</fpage>
      <lpage>S118</lpage>
    </element-citation>
  </ref>
  <ref id="ref-smm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Franke</surname><given-names>Reiner</given-names></name>
      </person-group>
      <article-title>Applying the method of simulated moments to estimate a small agent-based asset pricing model</article-title>
      <source>Journal of Empirical Finance</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>16</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1016/j.jempfin.2009.06.006</pub-id>
      <fpage>804</fpage>
      <lpage>815</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mala">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roberts</surname><given-names>Gareth O</given-names></name>
        <name><surname>Tweedie</surname><given-names>Richard L</given-names></name>
      </person-group>
      <article-title>Exponential convergence of Langevin distributions and their discrete approximations</article-title>
      <source>Bernoulli</source>
      <publisher-name>JSTOR</publisher-name>
      <year iso-8601-date="1996">1996</year>
      <pub-id pub-id-type="doi">10.2307/3318418</pub-id>
      <fpage>341</fpage>
      <lpage>363</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hmc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Duane</surname><given-names>Simon</given-names></name>
        <name><surname>Kennedy</surname><given-names>Anthony D</given-names></name>
        <name><surname>Pendleton</surname><given-names>Brian J</given-names></name>
        <name><surname>Roweth</surname><given-names>Duncan</given-names></name>
      </person-group>
      <article-title>Hybrid Monte Carlo</article-title>
      <source>Physics letters B</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="1987">1987</year>
      <volume>195</volume>
      <issue>2</issue>
      <fpage>216</fpage>
      <lpage>222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bissiri">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bissiri</surname><given-names>Pier Giovanni</given-names></name>
        <name><surname>Holmes</surname><given-names>Chris</given-names></name>
        <name><surname>Walker</surname><given-names>Stephen G</given-names></name>
      </person-group>
      <article-title>A general framework for updating belief distributions</article-title>
      <source>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</source>
      <publisher-name>Wiley-Blackwell</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>78</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1111/rssb.12158</pub-id>
      <fpage>1103</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gvi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Knoblauch</surname><given-names>Jeremias</given-names></name>
        <name><surname>Jewson</surname><given-names>Jack</given-names></name>
        <name><surname>Damoulas</surname><given-names>Theodoros</given-names></name>
      </person-group>
      <article-title>An optimization-centric view on Bayes‚Äô rule: Reviewing and generalizing variational inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>132</issue>
      <fpage>1</fpage>
      <lpage>109</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ai4abm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Quera-Bofarull</surname><given-names>Arnau</given-names></name>
        <name><surname>Chopra</surname><given-names>Ayush</given-names></name>
        <name><surname>Calinescu</surname><given-names>Anisoara</given-names></name>
        <name><surname>Wooldridge</surname><given-names>Michael</given-names></name>
        <name><surname>Dyer</surname><given-names>Joel</given-names></name>
      </person-group>
      <article-title>Bayesian calibration of differentiable agent-based models</article-title>
      <source>ICLR Workshop on AI for Agent-based Modelling</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-dae">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Quera-Bofarull</surname><given-names>Arnau</given-names></name>
        <name><surname>Dyer</surname><given-names>Joel</given-names></name>
        <name><surname>Calinescu</surname><given-names>Anisoara</given-names></name>
        <name><surname>Wooldridge</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Some challenges of calibrating differentiable agent-based models</article-title>
      <source>ICML Differentiable Almost Everything Workshop</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-sbi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tejero-Cantero</surname><given-names>Alvaro</given-names></name>
        <name><surname>Boelts</surname><given-names>Jan</given-names></name>
        <name><surname>Deistler</surname><given-names>Michael</given-names></name>
        <name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name>
        <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        <name><surname>Gon√ßalves</surname><given-names>Pedro J.</given-names></name>
        <name><surname>Greenberg</surname><given-names>David S.</given-names></name>
        <name><surname>Macke</surname><given-names>Jakob H.</given-names></name>
      </person-group>
      <article-title>Sbi: A toolkit for simulation-based inference</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>52</issue>
      <uri>https://doi.org/10.21105/joss.02505</uri>
      <pub-id pub-id-type="doi">10.21105/joss.02505</pub-id>
      <fpage>2505</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-blit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Benedetti</surname><given-names>Marco</given-names></name>
        <name><surname>Catapano</surname><given-names>Gennaro</given-names></name>
        <name><surname>Sclavis</surname><given-names>Francesco De</given-names></name>
        <name><surname>Favorito</surname><given-names>Marco</given-names></name>
        <name><surname>Glielmo</surname><given-names>Aldo</given-names></name>
        <name><surname>Magnanimi</surname><given-names>Davide</given-names></name>
        <name><surname>Muci</surname><given-names>Antonio</given-names></name>
      </person-group>
      <article-title>Black-it: A ready-to-use and easy-to-extend calibration kit for agent-based models</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>79</issue>
      <uri>https://doi.org/10.21105/joss.04622</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04622</pub-id>
      <fpage>4622</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-pyvbmc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huggins</surname><given-names>Bobby</given-names></name>
        <name><surname>Li</surname><given-names>Chengkun</given-names></name>
        <name><surname>Tobaben</surname><given-names>Marlon</given-names></name>
        <name><surname>Aarnos</surname><given-names>Mikko J.</given-names></name>
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>PyVBMC: Efficient Bayesian inference in python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>86</issue>
      <uri>https://doi.org/10.21105/joss.05428</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05428</pub-id>
      <fpage>5428</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-abcpy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dutta</surname><given-names>Ritabrata</given-names></name>
        <name><surname>Schoengens</surname><given-names>Marcel</given-names></name>
        <name><surname>Pacchiardi</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Ummadisingu</surname><given-names>Avinash</given-names></name>
        <name><surname>Widmer</surname><given-names>Nicole</given-names></name>
        <name><surname>K√ºnzli</surname><given-names>Pierre</given-names></name>
        <name><surname>Onnela</surname><given-names>Jukka-Pekka</given-names></name>
        <name><surname>Mira</surname><given-names>Antonietta</given-names></name>
      </person-group>
      <article-title>ABCpy: A high-performance computing perspective to approximate Bayesian computation</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2021">2021</year>
      <volume>100</volume>
      <issue>7</issue>
      <uri>https://www.jstatsoft.org/index.php/jss/article/view/v100i07</uri>
      <pub-id pub-id-type="doi">10.18637/jss.v100.i07</pub-id>
      <fpage>1</fpage>
      <lpage>38</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pyabc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sch√§lte</surname><given-names>Yannik</given-names></name>
        <name><surname>Klinger</surname><given-names>Emmanuel</given-names></name>
        <name><surname>Alamoudi</surname><given-names>Emad</given-names></name>
        <name><surname>Hasenauer</surname><given-names>Jan</given-names></name>
      </person-group>
      <article-title>pyABC: Efficient and robust easy-to-use approximate Bayesian computation</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>74</issue>
      <uri>https://doi.org/10.21105/joss.04304</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04304</pub-id>
      <fpage>4304</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-radev2023bayesflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Radev</surname><given-names>Stefan T.</given-names></name>
        <name><surname>Schmitt</surname><given-names>Marvin</given-names></name>
        <name><surname>Schumacher</surname><given-names>Lukas</given-names></name>
        <name><surname>Elsem√ºller</surname><given-names>Lasse</given-names></name>
        <name><surname>Pratz</surname><given-names>Valentin</given-names></name>
        <name><surname>Sch√§lte</surname><given-names>Yannik</given-names></name>
        <name><surname>K√∂the</surname><given-names>Ullrich</given-names></name>
        <name><surname>B√ºrkner</surname><given-names>Paul-Christian</given-names></name>
      </person-group>
      <article-title>BayesFlow: Amortized Bayesian Workflows With Neural Networks</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2023-09">2023</year><month>09</month>
      <volume>8</volume>
      <issue>89</issue>
      <uri>https://joss.theoj.org/papers/10.21105/joss.05702</uri>
      <pub-id pub-id-type="doi">10.21105/joss.05702</pub-id>
      <fpage>5702</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
