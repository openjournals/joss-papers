<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7226</article-id>
<article-id pub-id-type="doi">10.21105/joss.07226</article-id>
<title-group>
<article-title>DeepRiver: A Deep Learning Library for Data
Streams</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9363-4728</contrib-id>
<name>
<surname>Kulbach</surname>
<given-names>Cedric</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2886-1219</contrib-id>
<name>
<surname>Cazzonelli</surname>
<given-names>Lucas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7583-753X</contrib-id>
<name>
<surname>Ngo</surname>
<given-names>Hoang-Anh</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-2"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1464-4520</contrib-id>
<name>
<surname>Halford</surname>
<given-names>Max</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0092-3572</contrib-id>
<name>
<surname>Mastelini</surname>
<given-names>Saulo Martiello</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>FZI Research Center for Information Technology, Karlsruhe,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>AI Institute, University of Waikato, Hamilton, New
Zealand</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Carbonfact, Paris, France</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Institute of Mathematics and Computer Science, University
of São Paulo, São Carlos, Brazil</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
<corresp id="cor-2">* E-mail: <email></email></corresp>
</author-notes>
<volume>10</volume>
<issue>105</issue>
<fpage>7226</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning algorithms enhance decision-making efficiency by
  leveraging available data. However, as data evolves over time, it
  becomes crucial to adapt machine learning (ML) systems incrementally
  to accommodate new data patterns. This adaptation is achieved through
  online learning or continuous ML technologies. Although deep learning
  technologies have demonstrated outstanding performance on predefined
  datasets, their application to online, streaming, and continuous
  learning scenarios has been limited.</p>
  <p><monospace>DeepRiver</monospace> is a Python package for deep
  learning on data streams. Built on top of <monospace>River</monospace>
  (<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>) and PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>), it offers a unified API for both supervised and
  unsupervised learning. Additionally, it provides a suite of tools for
  preprocessing data streams and evaluating deep learning models.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In today’s rapidly evolving landscape, machine learning (ML)
  algorithms play a pivotal role in shaping decision-making processes
  based on available data. These algorithms, while accelerating
  analysis, require continuous adaptation to dynamic data structures, as
  patterns may evolve rapidly. To address this imperative, adopting
  online learning and continuous ML technologies becomes paramount.
  While deep learning technologies have demonstrated exceptional
  performance on static, predefined datasets, their application to
  dynamic and continuously evolving data streams remains underexplored.
  The absence of widespread integration of deep learning into online,
  streaming, and continuous learning scenarios hampers the full
  potential of these advanced algorithms in real-time decision-making
  (<xref alt="Kulbach et al., 2024" rid="ref-kulbach2024retrospectivetutorialopportunitieschallenges" ref-type="bibr">Kulbach
  et al., 2024</xref>). The emergence of the
  <monospace>DeepRiver</monospace> Python package fills a critical void
  in the field of deep learning on data streams. Leveraging the
  capabilities of <monospace>River</monospace>
  (<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>) and PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>), <monospace>DeepRiver</monospace> offers a
  unified API for both supervised and unsupervised learning, providing a
  seamless bridge between cutting-edge deep learning techniques and the
  challenges posed by dynamic data streams. Moreover, the package equips
  practitioners with essential tools for data stream preprocessing and
  the evaluation of deep learning models in dynamic, real-time
  environments. Such functionality has been applied to Streaming Anomaly
  Detection
  (<xref alt="Cazzonelli &amp; Kulbach, 2022" rid="ref-cazzonelli2022detecting" ref-type="bibr">Cazzonelli
  &amp; Kulbach, 2022</xref>). As the demand for effective and efficient
  adaptation of machine learning systems to evolving data structures
  continues to grow, the integration of <monospace>DeepRiver</monospace>
  into the landscape becomes crucial. This package stands as a valuable
  asset, unlocking the potential for deep learning technologies to excel
  in online, streaming, and continuous learning scenarios. The need for
  such advancements is evident in the quest to harness the full power of
  machine learning in dynamically changing environments, ensuring our
  decision-making processes remain accurate, relevant, and agile in the
  face of evolving data landscapes.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>Online machine learning involves updating models incrementally as
  new data arrives, rather than retraining models from scratch. Several
  frameworks and libraries have been developed to support this
  paradigm:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>scikit-multiflow</monospace>
      (<xref alt="Montiel et al., 2018" rid="ref-JMLRU003Av19U003A18-251" ref-type="bibr">Montiel
      et al., 2018</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Python-based Library: Inspired by the Java-based MOA
          framework, designed for streaming data and online learning in
          Python.</p>
        </list-item>
        <list-item>
          <p>Key Features:</p>
          <list list-type="bullet">
            <list-item>
              <p>Supports algorithms like Hoeffding Trees, online
              bagging, and boosting.</p>
            </list-item>
            <list-item>
              <p>Includes concept drift detection (e.g., ADWIN,
              Page-Hinkley) to adapt to changing data distributions.</p>
            </list-item>
            <list-item>
              <p>Stream generators and evaluators for real-time data
              simulation and model assessment.</p>
            </list-item>
          </list>
        </list-item>
        <list-item>
          <p>Limitations: Focuses mainly on traditional machine learning
          methods, with limited support for deep learning
          architectures.</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><monospace>creme</monospace>
      (<xref alt="Halford et al., 2020" rid="ref-creme" ref-type="bibr">Halford
      et al., 2020</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Lightweight Online Learning: Specialized in incremental
          learning where models are updated per instance, leading to
          efficient, low-latency model training.</p>
        </list-item>
        <list-item>
          <p>Provides a unified API with a broad range of online
          learning algorithms, making it the go-to library for streaming
          data analysis in Python.</p>
        </list-item>
        <list-item>
          <p>Limitations: Primarily supports feature-based models with
          limited capabilities for deep neural networks.</p>
        </list-item>
      </list>
    </list-item>
  </list>
  <p>In 2020, <monospace>creme</monospace> merged with
  <monospace>scikit-multiflow</monospace> to create
  <monospace>River</monospace>, combining the strengths of both
  frameworks.</p>
  <list list-type="bullet">
    <list-item>
      <p>Massive Online Analysis (MOA)
      (<xref alt="Bifet et al., 2010" rid="ref-JMLRU003Av11U003Abifet10a" ref-type="bibr">Bifet
      et al., 2010</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Java-based Pioneer: One of the earliest frameworks
          dedicated to stream mining and online learning, widely used in
          academic research.</p>
        </list-item>
        <list-item>
          <p>Key Features:</p>
          <list list-type="bullet">
            <list-item>
              <p>Introduces foundational algorithms like Hoeffding
              Trees, Adaptive Random Forest (ARF), and several drift
              detection techniques (e.g., DDM, EDDM).</p>
            </list-item>
            <list-item>
              <p>Excellent scalability for handling high-throughput data
              streams in real-time.</p>
            </list-item>
            <list-item>
              <p>Strong focus on concept drift adaptation, making it
              robust in non-stationary environments.</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>capyMOA
      (<xref alt="CapyMOA Developers, 2024" rid="ref-capymoaCapyMOAx2024" ref-type="bibr">CapyMOA
      Developers, 2024</xref>)</p>
      <list list-type="bullet">
        <list-item>
          <p>Python Interface for MOA: capyMOA serves as a bridge
          between the Java-based MOA framework and Python, allowing
          users to leverage MOA’s powerful streaming algorithms within
          Python workflows.</p>
        </list-item>
        <list-item>
          <p>Key Features:</p>
          <list list-type="bullet">
            <list-item>
              <p>Enables access to MOA’s core functionalities (e.g.,
              Hoeffding Trees, Adaptive Random Forest) from Python.</p>
            </list-item>
            <list-item>
              <p>Facilitates hybrid workflows by integrating MOA’s Java
              algorithms with Python’s machine learning libraries.</p>
            </list-item>
            <list-item>
              <p>Useful for Python developers looking to use MOA’s
              advanced stream mining capabilities without switching
              ecosystems.</p>
            </list-item>
          </list>
        </list-item>
      </list>
    </list-item>
  </list>
  <p><monospace>scikit-multiflow</monospace> and
  <monospace>creme</monospace> (<monospace>River</monospace>) focus on
  efficient online learning in Python, mainly for traditional machine
  learning algorithms. MOA offers extensive tools for stream mining but
  lacks deep learning support and Python compatibility. While capyMOA
  provides Python accessibility to MOA, capyMOA is limited by the
  underlying Java infrastructure and lacks a natural integration with
  PyTorch’s deep learning ecosystem.</p>
  <p><monospace>DeepRiver</monospace> differentiates itself by
  integrating deep learning capabilities directly into streaming data
  workflows, enabling continuous learning for neural network models.
  This addresses a critical gap left by existing frameworks, which are
  predominantly focused on non-deep learning models.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p><monospace>DeepRiver</monospace> enables the usage of deep learning
  models for data streams. This means that deep learning models need to
  adapt to changes within the evolving data stream
  (<xref alt="Bayram et al., 2022" rid="ref-bayram2022concept" ref-type="bibr">Bayram
  et al., 2022</xref>;
  <xref alt="Lu et al., 2018" rid="ref-lu2018learning" ref-type="bibr">Lu
  et al., 2018</xref>) e.g. the number of classes might change over
  time. In addition to the integration of PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
  et al., 2017</xref>) into <monospace>River</monospace>
  (<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
  et al., 2021</xref>), this package offers additional data stream
  specific functionalities such as class incremental learning or
  specific optimizers for data streams.</p>
  <sec id="compatibility">
    <title>Compatibility</title>
    <p><monospace>DeepRiver</monospace> is built on the unified
    application programming interface (API) of
    <monospace>River</monospace>(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
    et al., 2021</xref>) that seamlessly integrates both supervised and
    unsupervised learning techniques. Additionally, it incorporates
    PyTorch’s
    (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
    et al., 2017</xref>) extensive functionality for deep learning such
    as using GPU acceleration and a broad range of architectures. This
    unified approach simplifies the development process and facilitates
    a cohesive workflow for practitioners working with dynamic data
    streams. Leveraging the capabilities of the well-established
    <monospace>River</monospace>(<xref alt="Montiel et al., 2021" rid="ref-montiel2021river" ref-type="bibr">Montiel
    et al., 2021</xref>) library and the powerful
    PyTorch(<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
    et al., 2017</xref>) framework, <monospace>DeepRiver</monospace>
    combines the strengths of these technologies to deliver a robust and
    flexible platform for deep learning on data streams. This foundation
    ensures reliability, scalability, and compatibility with
    state-of-the-art machine learning methodologies, with comprehensive
    <ext-link ext-link-type="uri" xlink:href="https://online-ml.github.io/deep-river/">documentation</ext-link>
    guiding users through the installation, implementation, and
    customization processes. Additionally, a supportive community
    ensures that all <monospace>DeepRiver</monospace>’s users have
    access to resources, discussions, and assistance, fostering a
    collaborative environment for continuous improvement and knowledge
    sharing.</p>
  </sec>
  <sec id="adaptivity">
    <title>Adaptivity</title>
    <p><monospace>DeepRiver</monospace> is specifically designed to
    cater to the requirements of online learning scenarios. It enables
    continuous adaptation to evolving data by supporting incremental
    updates and learning from new observations in real time, a critical
    feature for applications where data arrives sequentially. Moreover,
    it allows the model to dynamically adjust to changes in the number
    of classes over time for classification tasks. It equips
    practitioners with tools for evaluating the performance of deep
    learning models on data streams. This feature is crucial for
    ensuring the reliability and effectiveness of models in real-time
    applications, enabling users to monitor and fine-tune their models
    as the data evolves.</p>
  </sec>
</sec>
<sec id="architecture">
  <title>Architecture</title>
  <p>The <monospace>DeepRiver</monospace> library is structured around
  various types of estimators for anomaly detection, classification, and
  regression. In anomaly detection, the base class
  <monospace>AnomalyScaler</monospace> has derived classes
  <monospace>AnomalyMeanScaler</monospace>,
  <monospace>AnomalyMinMaxScaler</monospace>, and
  <monospace>AnomalyStandardScaler</monospace>. Additionally, the
  <monospace>Autoencoder</monospace> class, which inherits from
  <monospace>DeepEstimator</monospace>, has a specialized subclass
  called <monospace>ProbabilityWeightedAutoencoder</monospace>. The
  <monospace>RollingAutoencoder</monospace> class inherits from
  <monospace>RollingDeepEstimator</monospace>.</p>
  <p>For classification, the base class
  <monospace>Classifier</monospace> inherits from
  <monospace>DeepEstimator</monospace>. Derived from
  <monospace>Classifier</monospace> are specific classes like
  <monospace>LogisticRegression</monospace> and
  <monospace>MultiLayerPerceptron</monospace>. The
  <monospace>RollingClassifier</monospace> class inherits from both
  <monospace>RollingDeepEstimator</monospace> and
  <monospace>Classifier</monospace>.</p>
  <p>In regression, the base class <monospace>Regressor</monospace>
  inherits from <monospace>DeepEstimator</monospace>. Specific
  regression classes like <monospace>LinearRegression</monospace> and
  <monospace>MultiLayerPerceptron</monospace> inherit from
  <monospace>Regressor</monospace>. The
  <monospace>MultiTargetRegressor</monospace> also inherits from
  <monospace>DeepEstimator</monospace>. The
  <monospace>RollingRegressor</monospace> class inherits from both
  <monospace>RollingDeepEstimator</monospace> and
  <monospace>Regressor</monospace>.</p>
  <fig>
    <caption><p>Architecture of
    DeepRiver<styled-content id="figU003AArchitecture"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="classes.png" />
  </fig>
  <p>Overall, the library is organized to provide a flexible and
  hierarchical framework for different types of machine learning tasks,
  with a clear inheritance structure connecting more specific
  implementations to their base classes.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Hoang-Anh Ngo received an External Study Awards from the AI
  Institute, University of Waikato, Hamilton, New Zealand for research
  on online machine learning under the supervision of Prof. Albert
  Bifet.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-montiel2021river">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Montiel</surname><given-names>Jacob</given-names></name>
        <name><surname>Halford</surname><given-names>Max</given-names></name>
        <name><surname>Mastelini</surname><given-names>Saulo Martiello</given-names></name>
        <name><surname>Bolmier</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Sourty</surname><given-names>Raphael</given-names></name>
        <name><surname>Vaysse</surname><given-names>Robin</given-names></name>
        <name><surname>Zouitine</surname><given-names>Adil</given-names></name>
        <name><surname>Gomes</surname><given-names>Heitor Murilo</given-names></name>
        <name><surname>Read</surname><given-names>Jesse</given-names></name>
        <name><surname>Abdessalem</surname><given-names>Talel</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
      </person-group>
      <article-title>River: Machine learning for streaming data in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>110</issue>
      <uri>http://jmlr.org/papers/v22/20-1380.html</uri>
      <fpage>1</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kulbach2024retrospectivetutorialopportunitieschallenges">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kulbach</surname><given-names>Cedric</given-names></name>
        <name><surname>Cazzonelli</surname><given-names>Lucas</given-names></name>
        <name><surname>Ngo</surname><given-names>Hoang-Anh</given-names></name>
        <name><surname>Le-Nguyen</surname><given-names>Minh-Huong</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
      </person-group>
      <article-title>A retrospective of the tutorial on opportunities and challenges of online deep learning</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2405.17222</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2405.17222</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-cazzonelli2022detecting">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Cazzonelli</surname><given-names>Lucas</given-names></name>
        <name><surname>Kulbach</surname><given-names>Cedric</given-names></name>
      </person-group>
      <article-title>Detecting anomalies with autoencoders on data streams</article-title>
      <source>Joint european conference on machine learning and knowledge discovery in databases</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1007/978-3-031-26387-3_16</pub-id>
      <fpage>258</fpage>
      <lpage>274</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2017automatic">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zach</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
      </person-group>
      <article-title>Automatic differentiation in PyTorch</article-title>
      <year iso-8601-date="2017">2017</year>
      <uri>https://api.semanticscholar.org/CorpusID:40027675</uri>
    </element-citation>
  </ref>
  <ref id="ref-bayram2022concept">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bayram</surname><given-names>Firas</given-names></name>
        <name><surname>Ahmed</surname><given-names>Bestoun S.</given-names></name>
        <name><surname>Kassler</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>From concept drift to model degradation: An overview on performance-aware drift detectors</article-title>
      <source>Knowledge-Based Systems</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>245</volume>
      <pub-id pub-id-type="doi">10.1016/j.knosys.2022.108632</pub-id>
      <fpage>108632</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lu2018learning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Jie</given-names></name>
        <name><surname>Liu</surname><given-names>Anjin</given-names></name>
        <name><surname>Dong</surname><given-names>Fan</given-names></name>
        <name><surname>Gu</surname><given-names>Feng</given-names></name>
        <name><surname>Gama</surname><given-names>Joao</given-names></name>
        <name><surname>Zhang</surname><given-names>Guangquan</given-names></name>
      </person-group>
      <article-title>Learning under concept drift: A review</article-title>
      <source>IEEE transactions on knowledge and data engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1109/TKDE.2018.2876857</pub-id>
      <fpage>2346</fpage>
      <lpage>2363</lpage>
    </element-citation>
  </ref>
  <ref id="ref-JMLRU003Av11U003Abifet10a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
        <name><surname>Holmes</surname><given-names>Geoff</given-names></name>
        <name><surname>Kirkby</surname><given-names>Richard</given-names></name>
        <name><surname>Pfahringer</surname><given-names>Bernhard</given-names></name>
      </person-group>
      <article-title>MOA: Massive online analysis</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2010">2010</year>
      <volume>11</volume>
      <issue>52</issue>
      <uri>http://jmlr.org/papers/v11/bifet10a.html</uri>
      <fpage>1601</fpage>
      <lpage>1604</lpage>
    </element-citation>
  </ref>
  <ref id="ref-JMLRU003Av19U003A18-251">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Montiel</surname><given-names>Jacob</given-names></name>
        <name><surname>Read</surname><given-names>Jesse</given-names></name>
        <name><surname>Bifet</surname><given-names>Albert</given-names></name>
        <name><surname>Abdessalem</surname><given-names>Talel</given-names></name>
      </person-group>
      <article-title>Scikit-multiflow: A multi-output streaming framework</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2018">2018</year>
      <volume>19</volume>
      <issue>72</issue>
      <uri>http://jmlr.org/papers/v19/18-251.html</uri>
      <fpage>1</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
  <ref id="ref-creme">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Halford</surname><given-names>Max</given-names></name>
        <name><surname>Bolmier</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Sourty</surname><given-names>Raphael</given-names></name>
        <name><surname>Vaysse</surname><given-names>Robin</given-names></name>
        <name><surname>Zouitine</surname><given-names>Adil</given-names></name>
      </person-group>
      <article-title>creme, a Python library for online machine learning</article-title>
      <year iso-8601-date="2020-06-10">2020</year><month>06</month><day>10</day>
      <uri>https://github.com/MaxHalford/creme</uri>
    </element-citation>
  </ref>
  <ref id="ref-capymoaCapyMOAx2024">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>CapyMOA Developers</string-name>
      </person-group>
      <article-title>CapyMOA — capymoa.org</article-title>
      <publisher-name>https://capymoa.org</publisher-name>
      <year iso-8601-date="2024">2024</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
