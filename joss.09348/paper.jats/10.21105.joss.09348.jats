<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9348</article-id>
<article-id pub-id-type="doi">10.21105/joss.09348</article-id>
<title-group>
<article-title>KeemenaPreprocessing.jl: Unicode-Robust Cleaning,
Multi-Level Tokenisation and Streaming Offset Bundling for Julia
NLP</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0026-5725</contrib-id>
<name>
<surname>Mantzaris</surname>
<given-names>Alexander V.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Statistics and Data Science, University of
Central Florida (UCF), USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-07-07">
<day>7</day>
<month>7</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>118</issue>
<fpage>9348</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Julia</kwd>
<kwd>NLP</kwd>
<kwd>Text Processing</kwd>
<kwd>Tokenization</kwd>
<kwd>Corpus Cleaning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>KeemenaPreprocessing.jl begins where raw text first enters a
  research workflow, applying a carefully chosen set of cleaning
  operations that work well for most corpora yet remain fully
  customisable. By default the toolkit lower-cases characters, folds
  accents, removes control glyphs, normalises whitespace, and replaces
  URLs, e-mails, and numbers by sentinel tokens; each rule may be
  toggled individually through an optional
  <monospace>PreprocessConfiguration</monospace>, so users can disable
  lower-casing for case-sensitive tasks or preserve digits for OCR
  evaluation without rewriting the pipeline.</p>
  <p>After cleaning, the same configuration drives tokenisation. Keemena
  ships byte-, character-, and word-level tokenisers and will seamlessly
  wrap a user-supplied function—allowing, for instance, a spaCy
  segmentation pass when language-specific heuristics are required
  (<xref alt="Honnibal et al., 2020" rid="ref-Honnibal_spaCy_Industrial-strength_Natural_2020" ref-type="bibr">Honnibal
  et al., 2020</xref>).
  Multiple segmentation levels can be recorded in one sweep (byte,
  character, word, sentence, paragraph, document), so downstream code
  can choose the granularity it needs without re-running preprocessing.
  Each token stream is accompanied by dense offset vectors: words are
  anchored to their byte and character positions, sentences and
  paragraphs are delimited explicitly, and a cross-alignment table keeps
  byte <inline-formula><alternatives>
  <tex-math><![CDATA[\leftrightarrow]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>↔</mml:mo></mml:math></alternatives></inline-formula>
  char <inline-formula><alternatives>
  <tex-math><![CDATA[\leftrightarrow]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>↔</mml:mo></mml:math></alternatives></inline-formula>
  word mappings exact. This design guarantees that every higher-level
  span can be traced unambiguously back to the source bytes, a property
  indispensable for annotation projection and reversible data
  augmentation.</p>
  <p>All artefacts—clean strings, token-ids, offset vectors, vocabulary
  statistics, and alignment tables are consolidated into a single
  <monospace>PreprocessBundle</monospace>. For Julia workflows, the
  bundle can be saved or loaded with one function call using JLD2 (a
  Julia-native serialization format). This persistence mechanism is
  independent of any particular model: the bundle exposes the token-id
  stream, vocabulary, offsets, and alignment tables needed by embedding
  and language-model training code, including word2vec-style pipelines
  (<xref alt="Mikolov et al., 2013" rid="ref-mikolov2013efficient" ref-type="bibr">Mikolov
  et al., 2013</xref>), and these same arrays can be exported to other
  interchange formats when interoperability is required. For modest
  datasets, the entire pipeline executes in a single statement; for
  web-scale corpora, KeemenaPreprocessing’s streaming mode processes
  fixed-size token chunks in constant memory while still accumulating
  global frequency tables. Thus, whether invoked with default settings
  for a quick experiment or finely tuned for production,
  KeemenaPreprocessing.jl offers a cohesive, Julia-native path from raw
  text to analysis-ready data
  (<xref alt="Bezanson et al., 2017" rid="ref-julia" ref-type="bibr">Bezanson
  et al., 2017</xref>). Many of these design principles are present in
  classic NLP preprocessing practices
  (<xref alt="Bird et al., 2009" rid="ref-bird2009natural" ref-type="bibr">Bird
  et al., 2009</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Modern NLP and language-modeling experiments depend on
  preprocessing that is reliable, reproducible, and auditable: changes
  in cleaning rules, tokenisation boundaries, or vocabulary construction
  can change model behavior and evaluation. Some ecosystems provide
  full-featured NLP toolkits (e.g. spaCy
  (<xref alt="Honnibal et al., 2020" rid="ref-Honnibal_spaCy_Industrial-strength_Natural_2020" ref-type="bibr">Honnibal
  et al., 2020</xref>), Stanford CoreNLP
  (<xref alt="Manning et al., 2014" rid="ref-manning2014stanford" ref-type="bibr">Manning
  et al., 2014</xref>), and Gensim
  (<xref alt="Řehůřek &amp; Sojka, 2010" rid="ref-vrehuuvrek2010software" ref-type="bibr">Řehůřek
  &amp; Sojka, 2010</xref>)), but these are primarily developed in and
  for Python/Java and are commonly used as end-to-end NLP pipelines
  rather than as a lightweight preprocessing step that produces a stable
  output type for downstream Julia modeling.</p>
  <p>Within Julia, existing packages such as WordTokenizers.jl
  (<xref alt="Kaushal et al., 2020" rid="ref-kaushal2020wordtokenizers" ref-type="bibr">Kaushal
  et al., 2020</xref>) provide fast tokenisation primitives, but many
  research workflows require additional infrastructure that is typically
  reimplemented per project: (i) a deterministic vocabulary and token-id
  representation, (ii) multi-level offsets and span traceability back to
  the raw text, and (iii) predictable memory behavior for corpora that
  cannot be loaded into RAM in one piece.</p>
  <p>KeemenaPreprocessing.jl fills this gap by focusing narrowly on
  corpus preprocessing as an explicit, reproducible artifact-building
  stage. It is intended for researchers and practitioners who preprocess
  large corpora for training or evaluating ML/NLP models and who need
  stable alignment across tokenisation levels
  (byte/char/word/sentence/paragraph/document). It is
  <italic>not</italic> intended to be a general NLP toolkit (tagging,
  parsing, NER, etc), nor a collection of tokenizer implementations;
  instead, it emphasizes a stable data model, deterministic
  preprocessing, and loose interoperability via user-supplied
  callables.</p>
  <p>Concretely, KeemenaPreprocessing provides:</p>
  <list list-type="bullet">
    <list-item>
      <p>A streaming, two-pass preprocessing workflow that supports
      corpora larger than available RAM by processing fixed-size token
      chunks.</p>
    </list-item>
    <list-item>
      <p>Deterministic vocabulary construction with user-defined special
      tokens, producing stable token-id streams suitable for downstream
      modeling.</p>
    </list-item>
    <list-item>
      <p>Dense offset tables and cross-level alignment maps that
      preserve exact traceability between bytes, characters, and
      higher-level tokenisation units, enabling robust span alignment
      and evaluation.</p>
    </list-item>
    <list-item>
      <p>A compact <monospace>PreprocessBundle</monospace> interface
      that can be saved and loaded for long-running experiments while
      remaining a plain Julia object for direct use in numerical and
      modeling code.</p>
    </list-item>
  </list>
  <p>These design choices support Julia-native modeling pipelines while
  keeping the preprocessing step transparent, testable, and
  reproducible—principles that underlie many established NLP workflows
  (<xref alt="Bird et al., 2009" rid="ref-bird2009natural" ref-type="bibr">Bird
  et al., 2009</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Thanks to the Julia community for their continued support of
  open-source scientific computing.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM Review</source>
      <publisher-name>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>59</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1137%2F141000671</uri>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Honnibal_spaCy_Industrial-strength_Natural_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Honnibal</surname><given-names>Matthew</given-names></name>
        <name><surname>Montani</surname><given-names>Ines</given-names></name>
        <name><surname>Van Landeghem</surname><given-names>Sofie</given-names></name>
        <name><surname>Boyd</surname><given-names>Adriane</given-names></name>
      </person-group>
      <article-title>spaCy: Industrial-strength Natural Language Processing in Python</article-title>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.1212303</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-vrehuuvrek2010software">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Řehůřek</surname><given-names>Radim</given-names></name>
        <name><surname>Sojka</surname><given-names>Petr</given-names></name>
      </person-group>
      <article-title>Software framework for topic modelling with large corpora</article-title>
      <source>Proceedings of the LREC 2010 workshop on new challenges for NLP frameworks</source>
      <publisher-name>ELRA</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <uri>http://is.muni.cz/publication/884893/en</uri>
      <pub-id pub-id-type="doi">10.13140/2.1.2393.1847</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kaushal2020wordtokenizers">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kaushal</surname><given-names>Ayush</given-names></name>
        <name><surname>White</surname><given-names>Lyndon</given-names></name>
        <name><surname>Innes</surname><given-names>Mike</given-names></name>
        <name><surname>Kumar</surname><given-names>Rohit</given-names></name>
      </person-group>
      <article-title>WordTokenizers.jl: Basic tools for tokenizing natural language in Julia</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>46</issue>
      <uri>https://doi.org/10.21105/joss.01956</uri>
      <pub-id pub-id-type="doi">10.21105/joss.01956</pub-id>
      <fpage>1956</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-bird2009natural">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Bird</surname><given-names>Steven</given-names></name>
        <name><surname>Klein</surname><given-names>Ewan</given-names></name>
        <name><surname>Loper</surname><given-names>Edward</given-names></name>
      </person-group>
      <source>Natural language processing with Python: Analyzing text with the natural language toolkit</source>
      <publisher-name>&quot;O’Reilly Media, Inc.&quot;</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <isbn>978-0-596-51649-9</isbn>
      <uri>https://dl.acm.org/doi/10.5555/1717171</uri>
    </element-citation>
  </ref>
  <ref id="ref-mikolov2013efficient">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
        <name><surname>Chen</surname><given-names>Kai</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>Efficient estimation of word representations in vector space</article-title>
      <source>arXiv preprint arXiv:1301.3781</source>
      <year iso-8601-date="2013">2013</year>
      <volume>3781</volume>
      <uri>https://doi.org/10.48550/arXiv.1301.3781</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1301.3781</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-manning2014stanford">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Manning</surname><given-names>Christopher D</given-names></name>
        <name><surname>Surdeanu</surname><given-names>Mihai</given-names></name>
        <name><surname>Bauer</surname><given-names>John</given-names></name>
        <name><surname>Finkel</surname><given-names>Jenny Rose</given-names></name>
        <name><surname>Bethard</surname><given-names>Steven</given-names></name>
        <name><surname>McClosky</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>The stanford CoreNLP natural language processing toolkit</article-title>
      <source>Proceedings of 52nd annual meeting of the association for computational linguistics: System demonstrations</source>
      <year iso-8601-date="2014">2014</year>
      <uri>https://doi.org/10.3115/v1/P14-5010</uri>
      <pub-id pub-id-type="doi">10.3115/v1/P14-5010</pub-id>
      <fpage>55</fpage>
      <lpage>60</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
