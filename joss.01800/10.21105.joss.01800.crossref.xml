<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>1341b4d2fe16d97f5824c495835ad1aa</doi_batch_id>
    <timestamp>20200120210753</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2020</year>
        </publication_date>
        <journal_volume>
          <volume>5</volume>
        </journal_volume>
        <issue>45</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Distant Viewing Toolkit: A Python Package for the Analysis of Visual Culture</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Taylor</given_name>
            <surname>Arnold</surname>
            <ORCID>http://orcid.org/0000-0003-0576-0669</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Lauren</given_name>
            <surname>Tilton</surname>
            <ORCID>http://orcid.org/0000-0003-4629-8888</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>20</day>
          <year>2020</year>
        </publication_date>
        <pages>
          <first_page>1800</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.01800</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.5281/zenodo.3614034”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/1800</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.01800</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.01800</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.01800/10.21105.joss.01800.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <unstructured_citation>Tensorflow: A system for large-scale machine learning, Abadi, Martı́n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael, 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16), 265–283, 2016</unstructured_citation>
          </citation>
          <citation key="ref2">
            <doi>10.1093/digitalsh/fqz013</doi>
          </citation>
          <citation key="ref3">
            <doi>10.22148/16.043</doi>
          </citation>
          <citation key="ref4">
            <unstructured_citation>Task-driven programming pedagogy in the digital humanities, Birnbaum, David J and Langmead, Alison, New directions for computing education, 63–85, 2017, Springer</unstructured_citation>
          </citation>
          <citation key="ref5">
            <unstructured_citation>Television: Critical methods and applications, Butler, Jeremy G, 2012, Routledge</unstructured_citation>
          </citation>
          <citation key="ref6">
            <unstructured_citation>Vggface2: A dataset for recognising faces across pose and age, Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M and Zisserman, Andrew, 2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018), 67–74, 2018</unstructured_citation>
          </citation>
          <citation key="ref7">
            <unstructured_citation>PixPlot, Duhaime, Douglas, 2019, ://github.com/YaleDHLab/pix-plot, Accessed: 2020-01-15</unstructured_citation>
          </citation>
          <citation key="ref8">
            <unstructured_citation>Two-frame motion estimation based on polynomial expansion, Farnebäck, Gunnar, Scandinavian conference on Image analysis, 363–370, 2003</unstructured_citation>
          </citation>
          <citation key="ref9">
            <unstructured_citation>Building a Crowdsourcing Platform for the Analysis of Film Colors, Flueckiger, Barbara and Halter, Gaudenz, Moving Image: The Journal of the Association of Moving Image Archivists, 18, 1, 80–83, 2018</unstructured_citation>
          </citation>
          <citation key="ref10">
            <unstructured_citation>Face detection with the faster R-CNN, Jiang, Huaizu and Learned-Miller, Erik, 2017 12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017), 650–657, 2017</unstructured_citation>
          </citation>
          <citation key="ref11">
            <unstructured_citation>Color image (dis) similarity assessment and grouping based on dominant colors, Karasek, Jan and Burget, Radim and Uher, Vaclav and Masek, Jan and Dutta, Malay Kishore, 2015 38th International Conference on Telecommunications and Signal Processing (TSP), 756–759, 2015</unstructured_citation>
          </citation>
          <citation key="ref12">
            <unstructured_citation>Recurrent RetinaNet: A Video Object Detection Model Based on Focal Loss, Li, Xiaobo and Zhao, Haohua and Zhang, Liqing, International Conference on Neural Information Processing, 499–508, 2018, Springer</unstructured_citation>
          </citation>
          <citation key="ref13">
            <unstructured_citation>Video shot boundary detection: a review, Pal, Gautam and Rudrapaul, Dwijen and Acharjee, Suvojit and Ray, Ruben and Chakraborty, Sayan and Dey, Nilanjan, Emerging ICT for Bridging the Future-Proceedings of the 49th Annual Convention of the Computer Society of India CSI Volume 2, 119–127, 2015, Springer</unstructured_citation>
          </citation>
          <citation key="ref14">
            <unstructured_citation>The Hitchhiker’s Guide to Python: Best Practices for Development, Reitz, Kenneth and Schlusser, Tanya, 2016, O’Reilly Media, Inc.</unstructured_citation>
          </citation>
          <citation key="ref15">
            <unstructured_citation>Using JavaScript static checkers on GitHub systems: A first evaluation, Santos, Adriano L and Valente, Marco Tulio and Figueiredo, Eduardo, Proccedings of the 3rd Workshop on Software Visualization, Evolution and Maintenance (VEM), 33–40, 2015</unstructured_citation>
          </citation>
          <citation key="ref16">
            <unstructured_citation>Facenet: A unified embedding for face recognition and clustering, Schroff, Florian and Kalenichenko, Dmitry and Philbin, James, Proceedings of the IEEE conference on computer vision and pattern recognition, 815–823, 2015</unstructured_citation>
          </citation>
          <citation key="ref17">
            <unstructured_citation>Inception-v4, inception-resnet and the impact of residual connections on learning, Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A, Thirty-First AAAI Conference on Artificial Intelligence, 2017</unstructured_citation>
          </citation>
          <citation key="ref18">
            <unstructured_citation>Code of conduct in open source projects, Tourani, Parastou and Adams, Bram and Serebrenik, Alexander, 2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER), 24–33, 2017</unstructured_citation>
          </citation>
          <citation key="ref19">
            <unstructured_citation>The visual digital turn: Using neural networks to study historical images, Wevers, Melvin and Smits, Thomas, Digital Scholarship in the Humanities, 2019</unstructured_citation>
          </citation>
          <citation key="ref20">
            <unstructured_citation>Best practices for scientific computing, Wilson, Greg and Aruliah, Dhavide A and Brown, C Titus and Hong, Neil P Chue and Davis, Matt and Guy, Richard T and Haddock, Steven HD and Huff, Kathryn D and Mitchell, Ian M and Plumbley, Mark D, PLoS biology, 12, 1, 2014, Public Library of Science</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
