<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6143</article-id>
<article-id pub-id-type="doi">10.21105/joss.06143</article-id>
<title-group>
<article-title>PerMetrics: A Framework of Performance Metrics for
Machine Learning Models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9994-8747</contrib-id>
<name>
<surname>Thieu</surname>
<given-names>Nguyen Van</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Faculty of Computer Science, Phenikaa University, Yen
Nghia, Ha Dong, Hanoi, 12116, Vietnam.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-08-08">
<day>8</day>
<month>8</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>95</issue>
<fpage>6143</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>model assessment tools</kwd>
<kwd>performance metrics</kwd>
<kwd>classification validation metrics</kwd>
<kwd>regression evaluation criteria</kwd>
<kwd>clustering criterion indices</kwd>
<kwd>machine learning metrics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Performance metrics are pivotal in machine learning field,
  especially for tasks like regression, classification, and clustering
  (<xref alt="Saura, 2021" rid="ref-saura_using_2021" ref-type="bibr">Saura,
  2021</xref>). They offer quantitative measures to assess the accuracy
  and efficacy of models, aiding researchers and practitioners in
  evaluating, contrasting, and enhancing algorithms and models. In
  regression tasks, where continuous predictions are made, metrics such
  as mean squared error (MSE), root mean square error (RMSE), and
  Coefficient of Determination (COD)
  (<xref alt="Nguyen et al., 2018" rid="ref-nguyen2018resource" ref-type="bibr">Nguyen
  et al., 2018</xref>;
  <xref alt="Nguyen, Nguyen, &amp; Nguyen, 2019" rid="ref-nguyen2019building" ref-type="bibr">Nguyen,
  Nguyen, &amp; Nguyen, 2019</xref>) can reveal how well models capture
  data patterns. In classification tasks, metrics such as accuracy,
  precision, recall, F1-score, and AUC-ROC
  (<xref alt="Luque et al., 2019" rid="ref-luque_impact_2019" ref-type="bibr">Luque
  et al., 2019</xref>) assess a model’s ability to classify instances
  correctly, detect false results, and gauge overall predictive
  performance. Clustering tasks aim to discover inherent patterns and
  structures within unlabeled data by grouping similar instances
  together. Metrics like Silhouette coefficient, Davies-Bouldin index,
  and Calinski-Harabasz index
  (<xref alt="Nainggolan et al., 2019" rid="ref-nainggolan_improved_2019" ref-type="bibr">Nainggolan
  et al., 2019</xref>) measure clustering quality, helping evaluate how
  well algorithms capture data distribution and assign instances to
  clusters. In general, performance metrics serve multiple purposes.
  They enable researchers to compare different models and algorithms
  (<xref alt="Ahmed et al., 2021" rid="ref-ahmed2021comprehensive" ref-type="bibr">Ahmed
  et al., 2021</xref>), identify strengths and weaknesses
  (<xref alt="Nguyen, Nguyen, Nguyen, &amp; Nguyen, 2019" rid="ref-Nguyen2019" ref-type="bibr">Nguyen,
  Nguyen, Nguyen, &amp; Nguyen, 2019</xref>), and make informed
  decisions about model selection and parameter tuning
  (<xref alt="Nguyen, Hoang, et al., 2020" rid="ref-nguyen2020new" ref-type="bibr">Nguyen,
  Hoang, et al., 2020</xref>). Moreover, it also plays a crucial role in
  the iterative process of model development and improvement. By
  quantifying the model’s performance, metrics guide the optimization
  process
  (<xref alt="Van Thieu, Deb Barma, et al., 2023" rid="ref-thieu_groundwater_2023" ref-type="bibr">Van
  Thieu, Deb Barma, et al., 2023</xref>), allowing researchers to
  fine-tune algorithms, explore feature engineering techniques
  (<xref alt="Nguyen et al., 2021" rid="ref-nguyen2021multi" ref-type="bibr">Nguyen
  et al., 2021</xref>), and address issues such as overfitting,
  underfitting, and bias
  (<xref alt="Nguyen, Nguyen, et al., 2020" rid="ref-nguyen2020eo" ref-type="bibr">Nguyen,
  Nguyen, et al., 2020</xref>). This paper introduces a Python framework
  named <bold>PerMetrics</bold> (PERformance METRICS), designed to offer
  comprehensive performance metrics for machine learning models. The
  library, packaged as <monospace>permetrics</monospace>, is open-source
  and written in Python. It provides a wide number of metrics to enable
  users to evaluate their models effectively.
  <monospace>permetrics</monospace> is hosted on GitHub and is under
  continuous development and maintenance by the dedicated team. The
  framework is accompanied by comprehensive documentation, examples, and
  test cases, facilitating easy comprehension and integration into
  users’ workflows.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><bold>PerMetrics</bold> is a Python project developed in the field
  of performance assessment and machine learning. To the best of our
  knowledge, it is the first open-source framework that contributes a
  significant number of metrics, totaling 111 methods, for three
  fundamental problems: regression, classification, and clustering. This
  library relies exclusively on only two well-known third-party Python
  scientific computing packages: <monospace>NumPy</monospace>
  (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
  et al., 2020</xref>) and <monospace>SciPy</monospace>
  (<xref alt="Virtanen et al., 2020" rid="ref-virtanen2020scipy" ref-type="bibr">Virtanen
  et al., 2020</xref>). The modules of <monospace>permetrics</monospace>
  are extensively documented, and the automatically generated API
  provides a complete and up-to-date description of both the
  object-oriented and functional implementations underlying the
  framework.</p>
  <p>To gain a better understanding of the necessity of
  <bold>PerMetrics</bold> library, this section will compare it to
  several notable libraries currently are available. Most notably,
  <monospace>Scikit-Learn</monospace>
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit_learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>), which also encompasses an assortment of metrics
  for regression, classification, and clustering problems. Nevertheless,
  a few classification metrics present in
  <monospace>Scikit-Learn</monospace> lack support for multiple outputs,
  such as the Matthews correlation coefficient (MCC) and Hinge loss.
  Furthermore, critical metrics such as RMSE, mean absolute percentage
  error (MAPE), Nash-Sutcliffe efficiency (NSE), and Kling-Gupta
  efficiency (KGE) are absent. <monospace>permetrics</monospace>
  addresses these deficiencies. Additionally,
  <monospace>Scikit-Learn</monospace> is deficient in various vital
  clustering metrics, including but not limited to Ball Hall index,
  Banfeld Raftery index, sum of squared error, Duda Hart index, and
  Hartigan index
  (<xref alt="Van Thieu, Oliva, et al., 2023" rid="ref-van2023metacluster" ref-type="bibr">Van
  Thieu, Oliva, et al., 2023</xref>).</p>
  <p>Another popular package is <monospace>Metrics</monospace>
  (<xref alt="Hamner, 2015" rid="ref-benhamner" ref-type="bibr">Hamner,
  2015</xref>). It provides a variety of metrics for different
  programming languages such as Python, MATLAB, R, and Haskell. However,
  the development team has ceased activity since 2015. They offer a
  limited number of metrics because they focused on creating a single
  set of metrics for multiple programming languages. Additionally, the
  metrics are not packaged as a complete library but rather exist as
  repository code on GitHub.</p>
  <p><monospace>TorchMetrics</monospace>
  (<xref alt="Nicki Skafte Detlefsen et al., 2022" rid="ref-torchmetrics" ref-type="bibr">Nicki
  Skafte Detlefsen et al., 2022</xref>) is a widely recognized framework
  for performance metrics developed for PyTorch users. The library
  includes over 100 metrics, covering various domains such as
  regression, classification, audio, detection, and text. However,
  <monospace>TorchMetrics</monospace> does not provide metrics
  specifically for clustering tasks. Although it offers a substantial
  number of metrics, it falls short compared to
  <monospace>permetrics</monospace>. Moreover, it relies heavily on
  other major libraries such as <monospace>NumPy</monospace>,
  <monospace>Torch</monospace>,
  <monospace>Typing-extensions</monospace>,
  <monospace>Packaging</monospace>, and
  <monospace>Lightning-utilities</monospace>. Additionally, using this
  library may not be easy for beginners in Python programming, as it
  requires a deep understanding of the Torch library to utilize
  <monospace>TorchMetrics</monospace> effectively.</p>
  <p>Other popular libraries such as <monospace>TensorFlow</monospace>
  (<xref alt="Abadi et al., 2016" rid="ref-abadi2016tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>), <monospace>Keras</monospace>
  (<xref alt="Chollet, 2017" rid="ref-chollet2017xception" ref-type="bibr">Chollet,
  2017</xref>), <monospace>CatBoost</monospace>
  (<xref alt="Prokhorenkova et al., 2018" rid="ref-prokhorenkova2018catboost" ref-type="bibr">Prokhorenkova
  et al., 2018</xref>), and <monospace>MxNet</monospace>
  (<xref alt="Chen et al., 2015" rid="ref-chen2015mxnet" ref-type="bibr">Chen
  et al., 2015</xref>) also contain modules dedicated to metrics.
  However, the issue with these libraries is that their metric modules
  are specific to each respective one. It is challenging to combine
  metric modules from different libraries with each other. If it is
  possible to combine them, it often requires installing numerous
  related libraries. Furthermore, the metric modules within each library
  are tailored to users who are familiar with that specific one,
  requiring users to learn multiple libraries, syntax structures, and
  necessary commands associated with each framework to use them in
  combination. These are significant obstacles when using metrics from
  such libraries.</p>
  <p>All the aforementioned challenges are addressed by our
  <bold>PerMetrics</bold> library. It not only offers a simple and
  concise syntax and usage but also does not require any knowledge of
  other major libraries such as <monospace>TensorFlow</monospace>,
  <monospace>Keras</monospace>, or <monospace>PyTorch</monospace>.
  Additionally, it can be seamlessly integrated with any computational
  or machine learning library. In the future, we plan to expand
  <monospace>permetrics</monospace> to include other domains such as
  text metrics, audio metrics, detection metrics, and image metrics.</p>
</sec>
<sec id="available-methods">
  <title>Available Methods</title>
  <p>At the time of publication, <monospace>PerMetrics</monospace>
  provides three types of performance metrics include regression,
  classification, and clustering metrics. We listed all methods of each
  type below.</p>
  <table-wrap>
    <table>
      <colgroup>
        <col width="19%" />
        <col width="9%" />
        <col width="14%" />
        <col width="58%" />
      </colgroup>
      <thead>
        <tr>
          <th><bold>Problem</bold></th>
          <th><bold>ID</bold></th>
          <th><bold>Metric</bold></th>
          <th><bold>Metric Fullname</bold></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Regression</td>
          <td>1</td>
          <td>EVS</td>
          <td>Explained Variance Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>2</td>
          <td>ME</td>
          <td>Max Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>3</td>
          <td>MBE</td>
          <td>Mean Bias Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>4</td>
          <td>MAE</td>
          <td>Mean Absolute Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>5</td>
          <td>MSE</td>
          <td>Mean Squared Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>6</td>
          <td>RMSE</td>
          <td>Root Mean Squared Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>7</td>
          <td>MSLE</td>
          <td>Mean Squared Log Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>8</td>
          <td>MedAE</td>
          <td>Median Absolute Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>9</td>
          <td>MRE / MRB</td>
          <td>Mean Relative Error / Mean Relative Bias</td>
        </tr>
        <tr>
          <td>****</td>
          <td>10</td>
          <td>MPE</td>
          <td>Mean Percentage Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>11</td>
          <td>MAPE</td>
          <td>Mean Absolute Percentage Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>12</td>
          <td>SMAPE</td>
          <td>Symmetric Mean Absolute Percentage Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>13</td>
          <td>MAAPE</td>
          <td>Mean Arctangent Absolute Percentage Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>14</td>
          <td>MASE</td>
          <td>Mean Absolute Scaled Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>15</td>
          <td>NSE</td>
          <td>Nash-Sutcliffe Efficiency Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>16</td>
          <td>NNSE</td>
          <td>Normalized Nash-Sutcliffe Efficiency Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>17</td>
          <td>WI</td>
          <td>Willmott Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>18</td>
          <td>R / PCC</td>
          <td>Pearson’s Correlation Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>19</td>
          <td>AR / APCC</td>
          <td>Absolute Pearson’s Correlation Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>20</td>
          <td>RSQ/R2S</td>
          <td>(Pearson’s Correlation Index) ^ 2</td>
        </tr>
        <tr>
          <td>****</td>
          <td>21</td>
          <td>R2 / COD</td>
          <td>Coefficient of Determination</td>
        </tr>
        <tr>
          <td>****</td>
          <td>22</td>
          <td>AR2 / ACOD</td>
          <td>Adjusted Coefficient of Determination</td>
        </tr>
        <tr>
          <td>****</td>
          <td>23</td>
          <td>CI</td>
          <td>Confidence Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>24</td>
          <td>DRV</td>
          <td>Deviation of Runoff Volume</td>
        </tr>
        <tr>
          <td>****</td>
          <td>25</td>
          <td>KGE</td>
          <td>Kling-Gupta Efficiency</td>
        </tr>
        <tr>
          <td>****</td>
          <td>26</td>
          <td>GINI</td>
          <td>Gini Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>27</td>
          <td>GINI_WIKI</td>
          <td>Gini Coefficient on Wikipage</td>
        </tr>
        <tr>
          <td>****</td>
          <td>28</td>
          <td>PCD</td>
          <td>Prediction of Change in Direction</td>
        </tr>
        <tr>
          <td>****</td>
          <td>29</td>
          <td>CE</td>
          <td>Cross Entropy</td>
        </tr>
        <tr>
          <td>****</td>
          <td>30</td>
          <td>KLD</td>
          <td>Kullback Leibler Divergence</td>
        </tr>
        <tr>
          <td>****</td>
          <td>31</td>
          <td>JSD</td>
          <td>Jensen Shannon Divergence</td>
        </tr>
        <tr>
          <td>****</td>
          <td>32</td>
          <td>VAF</td>
          <td>Variance Accounted For</td>
        </tr>
        <tr>
          <td>****</td>
          <td>33</td>
          <td>RAE</td>
          <td>Relative Absolute Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>34</td>
          <td>A10</td>
          <td>A10 Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>35</td>
          <td>A20</td>
          <td>A20 Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>36</td>
          <td>A30</td>
          <td>A30 Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>37</td>
          <td>NRMSE</td>
          <td>Normalized Root Mean Square Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>38</td>
          <td>RSE</td>
          <td>Residual Standard Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>39</td>
          <td>RE / RB</td>
          <td>Relative Error / Relative Bias</td>
        </tr>
        <tr>
          <td>****</td>
          <td>40</td>
          <td>AE</td>
          <td>Absolute Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>41</td>
          <td>SE</td>
          <td>Squared Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>42</td>
          <td>SLE</td>
          <td>Squared Log Error</td>
        </tr>
        <tr>
          <td>****</td>
          <td>43</td>
          <td>COV</td>
          <td>Covariance</td>
        </tr>
        <tr>
          <td>****</td>
          <td>44</td>
          <td>COR</td>
          <td>Correlation</td>
        </tr>
        <tr>
          <td>****</td>
          <td>45</td>
          <td>EC</td>
          <td>Efficiency Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>46</td>
          <td>OI</td>
          <td>Overall Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>47</td>
          <td>CRM</td>
          <td>Coefficient of Residual Mass</td>
        </tr>
        <tr>
          <td>–</td>
          <td>–</td>
          <td>–</td>
          <td>–</td>
        </tr>
        <tr>
          <td>Classification</td>
          <td>1</td>
          <td>PS</td>
          <td>Precision Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>2</td>
          <td>NPV</td>
          <td>Negative Predictive Value</td>
        </tr>
        <tr>
          <td>****</td>
          <td>3</td>
          <td>RS</td>
          <td>Recall Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>4</td>
          <td>AS</td>
          <td>Accuracy Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>5</td>
          <td>F1S</td>
          <td>F1 Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>6</td>
          <td>F2S</td>
          <td>F2 Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>7</td>
          <td>FBS</td>
          <td>F-Beta Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>8</td>
          <td>SS</td>
          <td>Specificity Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>9</td>
          <td>MCC</td>
          <td>Matthews Correlation Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>10</td>
          <td>HS</td>
          <td>Hamming Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>11</td>
          <td>CKS</td>
          <td>Cohen’s kappa score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>12</td>
          <td>JSI</td>
          <td>Jaccard Similarity Coefficient</td>
        </tr>
        <tr>
          <td>****</td>
          <td>13</td>
          <td>GMS</td>
          <td>Geometric Mean Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>14</td>
          <td>ROC-AUC</td>
          <td>ROC-AUC</td>
        </tr>
        <tr>
          <td>****</td>
          <td>15</td>
          <td>LS</td>
          <td>Lift Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>16</td>
          <td>GINI</td>
          <td>GINI Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>17</td>
          <td>CEL</td>
          <td>Cross Entropy Loss</td>
        </tr>
        <tr>
          <td>****</td>
          <td>18</td>
          <td>HL</td>
          <td>Hinge Loss</td>
        </tr>
        <tr>
          <td>****</td>
          <td>19</td>
          <td>KLDL</td>
          <td>Kullback Leibler Divergence Loss</td>
        </tr>
        <tr>
          <td>****</td>
          <td>20</td>
          <td>BSL</td>
          <td>Brier Score Loss</td>
        </tr>
        <tr>
          <td>–</td>
          <td>–</td>
          <td>–</td>
          <td>–</td>
        </tr>
        <tr>
          <td>Clustering</td>
          <td>1</td>
          <td>BHI</td>
          <td>Ball Hall Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>2</td>
          <td>XBI</td>
          <td>Xie Beni Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>3</td>
          <td>DBI</td>
          <td>Davies Bouldin Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>4</td>
          <td>BRI</td>
          <td>Banfeld Raftery Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>5</td>
          <td>KDI</td>
          <td>Ksq Detw Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>6</td>
          <td>DRI</td>
          <td>Det Ratio Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>7</td>
          <td>DI</td>
          <td>Dunn Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>8</td>
          <td>CHI</td>
          <td>Calinski Harabasz Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>9</td>
          <td>LDRI</td>
          <td>Log Det Ratio Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>10</td>
          <td>LSRI</td>
          <td>Log SS Ratio Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>11</td>
          <td>SI</td>
          <td>Silhouette Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>12</td>
          <td>SSEI</td>
          <td>Sum of Squared Error Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>13</td>
          <td>MSEI</td>
          <td>Mean Squared Error Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>14</td>
          <td>DHI</td>
          <td>Duda-Hart Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>15</td>
          <td>BI</td>
          <td>Beale Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>16</td>
          <td>RSI</td>
          <td>R-squared Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>17</td>
          <td>DBCVI</td>
          <td>Density-based Clustering Validation Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>18</td>
          <td>HI</td>
          <td>Hartigan Index</td>
        </tr>
        <tr>
          <td>****</td>
          <td>19</td>
          <td>MIS</td>
          <td>Mutual Info Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>20</td>
          <td>NMIS</td>
          <td>Normalized Mutual Info Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>21</td>
          <td>RaS</td>
          <td>Rand Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>22</td>
          <td>ARS</td>
          <td>Adjusted Rand Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>23</td>
          <td>FMS</td>
          <td>Fowlkes Mallows Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>24</td>
          <td>HS</td>
          <td>Homogeneity Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>25</td>
          <td>CS</td>
          <td>Completeness Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>26</td>
          <td>VMS</td>
          <td>V-Measure Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>27</td>
          <td>PrS</td>
          <td>Precision Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>28</td>
          <td>ReS</td>
          <td>Recall Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>29</td>
          <td>FmS</td>
          <td>F-Measure Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>30</td>
          <td>CDS</td>
          <td>Czekanowski Dice Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>31</td>
          <td>HGS</td>
          <td>Hubert Gamma Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>32</td>
          <td>JS</td>
          <td>Jaccard Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>33</td>
          <td>KS</td>
          <td>Kulczynski Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>34</td>
          <td>MNS</td>
          <td>Mc Nemar Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>35</td>
          <td>PhS</td>
          <td>Phi Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>36</td>
          <td>RTS</td>
          <td>Rogers Tanimoto Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>37</td>
          <td>RRS</td>
          <td>Russel Rao Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>38</td>
          <td>SS1S</td>
          <td>Sokal Sneath1 Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>39</td>
          <td>SS2S</td>
          <td>Sokal Sneath2 Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>40</td>
          <td>PuS</td>
          <td>Purity Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>41</td>
          <td>ES</td>
          <td>Entropy Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>42</td>
          <td>TS</td>
          <td>Tau Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>43</td>
          <td>GAS</td>
          <td>Gamma Score</td>
        </tr>
        <tr>
          <td>****</td>
          <td>44</td>
          <td>GPS</td>
          <td>Gplus Score</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
<sec id="installation-and-simple-example">
  <title>Installation and Simple Example</title>
  <p><bold>PerMetrics</bold> is
  <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/permetrics/">published</ext-link>
  to the Python Packaging Index (PyPI) and can be installed via pip</p>
  <code language="bash">pip install permetrics</code>
  <p>Below are a few fundamental examples illustrating the usage of the
  <monospace>permetrics</monospace> library. We have prepared a folder
  <monospace>examples</monospace> in Github repository that contains
  these examples and more advances one. Furthermore, to gain a
  comprehensive understanding of our library, we recommend reading the
  documentation available at the following
  <ext-link ext-link-type="uri" xlink:href="https://permetrics.readthedocs.io/">link</ext-link>.</p>
  <sec id="regression-metrics">
    <title>Regression Metrics</title>
    <code language="python">import numpy as np
from permetrics import RegressionMetric

y_true = np.array([3, -0.5, 2, 7, 5, 6])
y_pred = np.array([2.5, 0.0, 2, 8, 5, 6])

evaluator = RegressionMetric(y_true=y_true, y_pred=y_pred)

print(evaluator.mean_squared_error())
print(evaluator.median_absolute_error())
print(evaluator.MAPE())</code>
  </sec>
  <sec id="classification-metrics">
    <title>Classification Metrics</title>
    <code language="python">from permetrics import ClassificationMetric

## For integer labels or categorical labels
y_true = [0, 1, 0, 0, 1, 0]
y_pred = [0, 1, 0, 0, 0, 1]
# y_true = [&quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;bird&quot;, &quot;bird&quot;, &quot;bird&quot;]
# y_pred = [&quot;ant&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;cat&quot;, &quot;ant&quot;, &quot;cat&quot;, &quot;bird&quot;, &quot;ant&quot;]

evaluator = ClassificationMetric(y_true, y_pred)

print(evaluator.f1_score())
print(evaluator.F1S(average=&quot;micro&quot;))
print(evaluator.f1_score(average=&quot;macro&quot;))</code>
  </sec>
  <sec id="clustering-metrics">
    <title>Clustering Metrics</title>
    <code language="python">import numpy as np
from permetrics import ClusteringMetric
from sklearn.datasets import make_blobs

# Generate sample data
X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)
y_pred = np.random.randint(0, 4, size=300)

evaluator = ClusteringMetric(y_true=y_true, y_pred=y_pred, X=X)

# Call specific function inside evaluator, each function has 2 names 
#   (fullname and short name)

## 1. Internal metrics: Need X and y_pred and has function's suffix as `index`
print(evaluator.ball_hall_index(X=X, y_pred=y_pred))
print(evaluator.CHI(X=X, y_pred=y_pred))

## 2. External metrics: Need y_true and y_pred and has function's suffix as `score`
print(evaluator.adjusted_rand_score(y_true=y_true, y_pred=y_pred))
print(evaluator.completeness_score(y_true=y_true, y_pred=y_pred))</code>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We express our sincere thanks to the individuals who have enhanced
  our software through their valuable issue reports and insightful
  feedback.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-saura_using_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Saura</surname><given-names>Jose Ramon</given-names></name>
      </person-group>
      <article-title>Using Data Sciences in Digital Marketing: Framework, methods, and performance metrics</article-title>
      <source>Journal of Innovation &amp; Knowledge</source>
      <year iso-8601-date="2021-04">2021</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-08-08">2023</year><month>08</month><day>08</day></date-in-citation>
      <volume>6</volume>
      <issue>2</issue>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S2444569X20300329</uri>
      <pub-id pub-id-type="doi">10.1016/j.jik.2020.08.001</pub-id>
      <fpage>92</fpage>
      <lpage>102</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2019building">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Giang</given-names></name>
      </person-group>
      <article-title>Building resource auto-scaler with functional-link neural network and adaptive bacterial foraging optimization</article-title>
      <source>International conference on theory and applications of models of computation</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://doi.org/10.1007/978-3-030-14812-6_31</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-14812-6_31</pub-id>
      <fpage>501</fpage>
      <lpage>517</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2018resource">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Tran</surname><given-names>Nhuan</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Giang</given-names></name>
      </person-group>
      <article-title>A resource usage prediction system using functional-link and genetic algorithm neural network for multivariate cloud metrics</article-title>
      <source>2018 IEEE 11th conference on service-oriented computing and applications (SOCA)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://doi.org/10.1109/SOCA.2018.00014</uri>
      <pub-id pub-id-type="doi">10.1109/SOCA.2018.00014</pub-id>
      <fpage>49</fpage>
      <lpage>56</lpage>
    </element-citation>
  </ref>
  <ref id="ref-luque_impact_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Luque</surname><given-names>Amalia</given-names></name>
        <name><surname>Carrasco</surname><given-names>Alejandro</given-names></name>
        <name><surname>Martín</surname><given-names>Alejandro</given-names></name>
        <name><surname>De Las Heras</surname><given-names>Ana</given-names></name>
      </person-group>
      <article-title>The impact of class imbalance in classification performance metrics based on the binary confusion matrix</article-title>
      <source>Pattern Recognition</source>
      <year iso-8601-date="2019-07">2019</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-08-08">2023</year><month>08</month><day>08</day></date-in-citation>
      <volume>91</volume>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S0031320319300950</uri>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2019.02.023</pub-id>
      <fpage>216</fpage>
      <lpage>231</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nainggolan_improved_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nainggolan</surname><given-names>Rena</given-names></name>
        <name><surname>Perangin-angin</surname><given-names>Resianta</given-names></name>
        <name><surname>Simarmata</surname><given-names>Emma</given-names></name>
        <name><surname>Tarigan</surname><given-names>Astuti Feriani</given-names></name>
      </person-group>
      <article-title>Improved the Performance of the K-Means Cluster Using the Sum of Squared Error (SSE) optimized by using the Elbow Method</article-title>
      <source>Journal of Physics: Conference Series</source>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-08-08">2023</year><month>08</month><day>08</day></date-in-citation>
      <volume>1361</volume>
      <issue>1</issue>
      <issn>1742-6588</issn>
      <uri>https://iopscience.iop.org/article/10.1088/1742-6596/1361/1/012015</uri>
      <pub-id pub-id-type="doi">10.1088/1742-6596/1361/1/012015</pub-id>
      <fpage>012015</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ahmed2021comprehensive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ahmed</surname><given-names>Ali Najah</given-names></name>
        <name><surname>Van Lam</surname><given-names>To</given-names></name>
        <name><surname>Hung</surname><given-names>Nguyen Duy</given-names></name>
        <name><surname>Van Thieu</surname><given-names>Nguyen</given-names></name>
        <name><surname>Kisi</surname><given-names>Ozgur</given-names></name>
        <name><surname>El-Shafie</surname><given-names>Ahmed</given-names></name>
      </person-group>
      <article-title>A comprehensive comparison of recent developed meta-heuristic algorithms for streamflow time series forecasting problem</article-title>
      <source>Applied Soft Computing</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>105</volume>
      <pub-id pub-id-type="doi">10.1016/j.asoc.2021.107282</pub-id>
      <fpage>107282</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Nguyen2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Nguyen</surname><given-names>Tu</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Giang</given-names></name>
      </person-group>
      <article-title>Efficient time-series forecasting using neural network and opposition-based coral reefs optimization</article-title>
      <source>International Journal of Computational Intelligence Systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>12</volume>
      <issn>1875-6883</issn>
      <uri>https://doi.org/10.2991/ijcis.d.190930.003</uri>
      <pub-id pub-id-type="doi">10.2991/ijcis.d.190930.003</pub-id>
      <fpage>1144</fpage>
      <lpage>1161</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2020new">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Hoang</surname><given-names>Bao</given-names></name>
        <name><surname>Nguyen</surname><given-names>Giang</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
      </person-group>
      <article-title>A new workload prediction model using extreme learning machine and enhanced tug of war optimization</article-title>
      <source>Procedia Computer Science</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>170</volume>
      <uri>https://doi.org/10.1016/j.procs.2020.03.063</uri>
      <pub-id pub-id-type="doi">10.1016/j.procs.2020.03.063</pub-id>
      <fpage>362</fpage>
      <lpage>369</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2020eo">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Nguyen</surname><given-names>Giang</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
      </person-group>
      <article-title>EO-CNN: An enhanced CNN model trained by equilibrium optimization for traffic transportation prediction</article-title>
      <source>Procedia Computer Science</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>176</volume>
      <uri>https://doi.org/10.1016/j.procs.2020.09.075</uri>
      <pub-id pub-id-type="doi">10.1016/j.procs.2020.09.075</pub-id>
      <fpage>800</fpage>
      <lpage>809</lpage>
    </element-citation>
  </ref>
  <ref id="ref-thieu_groundwater_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Van Thieu</surname><given-names>Nguyen</given-names></name>
        <name><surname>Deb Barma</surname><given-names>Surajit</given-names></name>
        <name><surname>Van Lam</surname><given-names>To</given-names></name>
        <name><surname>Kisi</surname><given-names>Ozgur</given-names></name>
        <name><surname>Mahesha</surname><given-names>Amai</given-names></name>
      </person-group>
      <article-title>Groundwater level modeling using Augmented Artificial Ecosystem Optimization</article-title>
      <source>Journal of Hydrology</source>
      <year iso-8601-date="2023-02">2023</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-08-08">2023</year><month>08</month><day>08</day></date-in-citation>
      <volume>617</volume>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S0022169422016043</uri>
      <pub-id pub-id-type="doi">10.1016/j.jhydrol.2022.129034</pub-id>
      <fpage>129034</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-nguyen2021multi">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Thieu</given-names></name>
        <name><surname>Nguyen</surname><given-names>Thang</given-names></name>
        <name><surname>Vu</surname><given-names>Quoc-Hien</given-names></name>
        <name><surname>Huynh</surname><given-names>Thi Thanh Binh</given-names></name>
        <name><surname>Nguyen</surname><given-names>Binh Minh</given-names></name>
      </person-group>
      <article-title>Multi-objective sparrow search optimization for task scheduling in fog-cloud-blockchain systems</article-title>
      <source>2021 IEEE international conference on services computing (SCC)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1109/scc53864.2021.00065</pub-id>
      <fpage>450</fpage>
      <lpage>455</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris2020array">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R</given-names></name>
        <name><surname>Millman</surname><given-names>K Jarrod</given-names></name>
        <name><surname>Van Der Walt</surname><given-names>Stéfan J</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <uri>https://doi.org/10.1038/s41586-020-2649-2</uri>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-virtanen2020scipy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in python</article-title>
      <source>Nature methods</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scikit_learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.1201.0490</pub-id>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-benhamner">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hamner</surname><given-names>Ben</given-names></name>
      </person-group>
      <article-title>Metrics</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://github.com/benhamner/Metrics</uri>
    </element-citation>
  </ref>
  <ref id="ref-torchmetrics">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <string-name>Nicki Skafte Detlefsen</string-name>
        <string-name>Jiri Borovec</string-name>
        <string-name>Justus Schock</string-name>
        <string-name>Ananya Harsh</string-name>
        <string-name>Teddy Koker</string-name>
        <string-name>Luca Di Liello</string-name>
        <string-name>Daniel Stancl</string-name>
        <string-name>Changsheng Quan</string-name>
        <string-name>Maxim Grechkin</string-name>
        <string-name>William Falcon</string-name>
      </person-group>
      <article-title>TorchMetrics - Measuring Reproducibility in PyTorch</article-title>
      <year iso-8601-date="2022-02">2022</year><month>02</month>
      <uri>https://github.com/Lightning-AI/torchmetrics</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04101</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-abadi2016tensorflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Tensorflow: Large-scale machine learning on heterogeneous distributed systems</article-title>
      <source>arXiv preprint arXiv:1603.04467</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://doi.org/10.48550/arXiv.1603.04467</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1603.04467</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chollet2017xception">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
      </person-group>
      <article-title>Xception: Deep learning with depthwise separable convolutions</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2017">2017</year>
      <uri>https://keras.io</uri>
      <pub-id pub-id-type="doi">10.1109/CVPR.2017.195</pub-id>
      <fpage>1251</fpage>
      <lpage>1258</lpage>
    </element-citation>
  </ref>
  <ref id="ref-prokhorenkova2018catboost">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Prokhorenkova</surname><given-names>Liudmila</given-names></name>
        <name><surname>Gusev</surname><given-names>Gleb</given-names></name>
        <name><surname>Vorobev</surname><given-names>Aleksandr</given-names></name>
        <name><surname>Dorogush</surname><given-names>Anna Veronika</given-names></name>
        <name><surname>Gulin</surname><given-names>Andrey</given-names></name>
      </person-group>
      <article-title>CatBoost: Unbiased boosting with categorical features</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <uri>https://catboost.ai</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1706.09516</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chen2015mxnet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Tianqi</given-names></name>
        <name><surname>Li</surname><given-names>Mu</given-names></name>
        <name><surname>Li</surname><given-names>Yutian</given-names></name>
        <name><surname>Lin</surname><given-names>Min</given-names></name>
        <name><surname>Wang</surname><given-names>Naiyan</given-names></name>
        <name><surname>Wang</surname><given-names>Minjie</given-names></name>
        <name><surname>Xiao</surname><given-names>Tianjun</given-names></name>
        <name><surname>Xu</surname><given-names>Bing</given-names></name>
        <name><surname>Zhang</surname><given-names>Chiyuan</given-names></name>
        <name><surname>Zhang</surname><given-names>Zheng</given-names></name>
      </person-group>
      <article-title>Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</article-title>
      <source>arXiv preprint arXiv:1512.01274</source>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1512.01274</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-van2023metacluster">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Van Thieu</surname><given-names>Nguyen</given-names></name>
        <name><surname>Oliva</surname><given-names>Diego</given-names></name>
        <name><surname>Pérez-Cisneros</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>MetaCluster: An open-source python library for metaheuristic-based clustering problems</article-title>
      <source>SoftwareX</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>24</volume>
      <pub-id pub-id-type="doi">10.1016/j.softx.2023.101597</pub-id>
      <fpage>101597</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
