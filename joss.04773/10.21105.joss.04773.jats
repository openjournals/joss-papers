<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4773</article-id>
<article-id pub-id-type="doi">10.21105/joss.04773</article-id>
<title-group>
<article-title><monospace>enetLTS</monospace>: Robust and Sparse Methods
for High Dimensional Linear, Binary, and Multinomial
Regression</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5958-7366</contrib-id>
<name>
<surname>KURNAZ</surname>
<given-names>Fatma Sevinc</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8014-4682</contrib-id>
<name>
<surname>FILZMOSER</surname>
<given-names>Peter</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Statistics, Yildiz Technical University,
Istanbul, Turkey</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institute of Statistics and Mathematical Methods in
Economics, TU Wien, Vienna, Austria</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-05-24">
<day>24</day>
<month>5</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>82</issue>
<fpage>4773</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>Robust regression</kwd>
<kwd>Elastic net</kwd>
<kwd>outlier detection</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>enetLTS</monospace> is an <monospace>R</monospace>
  package
  (<xref alt="R-Development-Core-Team, 2021" rid="ref-R21" ref-type="bibr">R-Development-Core-Team,
  2021</xref>) that provides a fully robust version of the elastic net
  estimator for high dimensional linear, binary, and multinomial
  regression. The elastic net penalization provides intrinsic variable
  selection and coefficient estimates for highly correlated variables,
  in particular for high dimensional low sample size data sets, and it
  has been extended to generalized linear regression models
  (<xref alt="Friedman et al., 2010" rid="ref-Friedman10" ref-type="bibr">Friedman
  et al., 2010</xref>). Combining these advantages with trimming
  outliers yields the robust solutions. The main idea of the algorithm
  is to search for outlier-free subsets on which the classical elastic
  net estimator can be applied. Outlier-free subsets are determined by
  trimming the penalized log-likelihood function for the considered
  regression model. The algorithm starts with 500 elemental subsets only
  for one combination of the elastic net parameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
  and takes the <italic>warm start</italic> strategy for subsequent
  combinations in order to save the computation time. The final
  reweighting step is added to improve the statistical efficiency of the
  proposed estimators. From this point of view, the enet-LTS estimator
  can be seen as a trimmed version of the elastic net regression
  estimator for linear, binary, and multinomial regression
  (<xref alt="Friedman et al., 2010" rid="ref-Friedman10" ref-type="bibr">Friedman
  et al., 2010</xref>). Selecting model with the optimal tuning
  parameters is done via cross-validation, and various plots are
  available to illustrate model selection and to evaluate the final
  model estimates.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>A number of new robust linear regression methods have been
  developed during the last decade in the context of high dimensional
  data, such as Alfons et al.
  (<xref alt="2013" rid="ref-Alfons13" ref-type="bibr">2013</xref>);
  Alfons
  (<xref alt="2021" rid="ref-Alfons21R" ref-type="bibr">2021</xref>);
  Kepplinger et al.
  (<xref alt="2021" rid="ref-Keplinger21R" ref-type="bibr">2021</xref>).
  However, to the best of our knowledge, robust logistic (both binary
  and multinomial) regression for high dimensional data is not available
  elsewhere. The package <monospace>enetLTS</monospace> therefore
  provides researchers with access to robust solutions and variable
  selection at the same time with high-dimensional linear and logistic
  regression data. It has already been used in several benchmark studies
  in the statistical literature, e.g.
  (<xref alt="Insolia et al., 2021" rid="ref-Insolia21b" ref-type="bibr">Insolia
  et al., 2021</xref>,
  <xref alt="2022" rid="ref-Insolia21a" ref-type="bibr">2022</xref>;
  <xref alt="Monti &amp; Filzmoser, 2021" rid="ref-Monti21" ref-type="bibr">Monti
  &amp; Filzmoser, 2021</xref>), as well as in empirical research, e.g.
  (<xref alt="Jensch et al., 2022" rid="ref-Jensch22" ref-type="bibr">Jensch
  et al., 2022</xref>;
  <xref alt="Segaert et al., 2018" rid="ref-Segaert18" ref-type="bibr">Segaert
  et al., 2018</xref>).</p>
</sec>
<sec id="example-robust-and-sparse-linear-regression-familygaussian">
  <title>Example: Robust and Sparse Linear Regression
  (<monospace>family=&quot;gaussian&quot;</monospace>)</title>
  <p>We have considered the
  <ext-link ext-link-type="uri" xlink:href="https://discover.nci.nih.gov/cellminer/">NCI-60
  cancer cell panel</ext-link> data
  (<xref alt="Reinhold et al., 2012" rid="ref-Reinhold12" ref-type="bibr">Reinhold
  et al., 2012</xref>) in order to provide an example for the
  <monospace>enetLTS</monospace> model. The NCI-60 data set includes 60
  human cancer cell lines with nine cancer types, which are breast,
  central nervous system, colon, leukemia, lung, melanoma, ovarian,
  prostate and renal cancers. In this example, we regress the protein
  expression on gene expression data. Using the Affymetrix HG-U133A chip
  and normalizing with the GCRMA method, the number of predictors is
  obtained as 22,283. One observation with missing values is omitted.
  This data set is available in the package
  <monospace>robustHD</monospace>.</p>
  <p>As in Alfons
  (<xref alt="2021" rid="ref-Alfons21R" ref-type="bibr">2021</xref>) we
  determine the response variable with one of the protein expressions
  which is 92th protein. Out of the gene expressions of the 22,283 genes
  for predictors, we have considered the gene expressions of the 100
  genes that have the highest (robustly estimated) correlations with the
  response variable. The code lines for loading and re-organizing the
  response variable and the predictors is as follows:</p>
  <code language="r script"># load data
library(&quot;robustHD&quot;)
data(&quot;nci60&quot;)  # contains matrices 'protein' and 'gene'

# define response variable
y &lt;- protein[, 92]
# screen most correlated predictor variables
correlations &lt;- apply(gene, 2, corHuber, y)
keep &lt;- partialOrder(abs(correlations), 100, decreasing = TRUE)
X &lt;- gene[, keep]</code>
  <p>The package <monospace>enetLTS</monospace> can either be installed
  from <monospace>CRAN</monospace> or directly from
  <monospace>Github</monospace>. The main function is
  <monospace>enetLTS()</monospace>, and the default
  <monospace>family</monospace> option is
  <monospace>gaussian</monospace>, which corresponds to linear
  regression.</p>
  <code language="r script"># install and load package
install.packages(&quot;enetLTS&quot;)
# alternatively install package from Github
# library(devtools)
# install_github(&quot;fatmasevinck/enetLTS&quot;,force=TRUE)
library(enetLTS)
# fit the model for family=&quot;gaussian&quot;
set.seed(1)
fit.gaussian &lt;- enetLTS(X, y, crit.plot=TRUE)
## [1] &quot;optimal model: lambda = 0.1043 alpha = 0.8&quot;

fit.gaussian</code>
  <code language="r script">## enetLTS estimator 

## Call:  enetLTS(xx = X, yy = y, crit.plot=TRUE) 

## number of the nonzero coefficients:
## [1] 23

## alpha: 0.8
## lambda: 0.1043
## lambdaw: 0.1824974</code>
  <p>23 out of 100 independent variables are selected by the enetLTS
  model based on optimal combination of <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha=]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  0.8 and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda=]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  0.1043. Here <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda_w=]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  0.1824974 corresponds to updated tuning parameter for reweighted
  model.</p>
  <p>The main idea to obtain an outlier-free subset is to carry out
  concentration steps (C-steps). This means that in each iteration of
  the algorithm, the value of the objective function improves. Thus, one
  has to start with several initial subsets, and the C-steps will lead
  at least to a local optimum.</p>
  <p>For the argument <monospace>hsize</monospace> one needs to provide
  a numeric value with the trimming percentage used in the penalized
  objective function. The default value is 0.75. The argument
  <monospace>nsamp</monospace> is a numeric vector: The first element
  gives the number of initial subsamples to be used. The second element
  gives the number of subsamples to keep after a number of C-steps has
  been performed. For those remaining subsets, additional C-steps are
  performed until convergence. The default is to start the C-steps with
  500 initial subsamples for a first combination of tuning parameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
  and then to keep the 10 subsamples with the lowest value of the
  objective function for additional C-steps until convergence. For the
  next combination of tuning parameters <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
  the algorithm makes use of the <inline-formula><alternatives>
  <tex-math><![CDATA[warm start]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>w</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  idea, which means that the best subset of the neighboring grid value
  is taken, and C-steps are started from this best subset until
  convergence. The <monospace>nsamp</monospace> entries can also be
  supplied by the user. These arguments are the same for the other
  <monospace>family</monospace> options.</p>
  <p>The main function <monospace>enetLTS()</monospace> allows the user
  to specify a sequence of values for <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  for the elastic net penalty. If this is not provided, a default
  sequence of 41 equally spaced values between 0 and 1 is taken. For the
  other tuning parameter <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  that keeps the strength of the elastic net penalty, a user supplied
  sequence is available. If not provided, the default for
  <monospace>family=&quot;gaussian&quot;</monospace> is chosen with
  steps of size -0.025 lambda0 with <inline-formula><alternatives>
  <tex-math><![CDATA[0\le\lambda\le]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi><mml:mo>≤</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  lambda0, where lambda0 is determined as in Alfons
  (<xref alt="2021" rid="ref-Alfons21R" ref-type="bibr">2021</xref>).</p>
  <p>After computing all candidates based on the best subsets for
  certain combinations of <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
  the combination of the optimal tuning parameters is defined by 5-fold
  cross-validation. The evaluation criterion for 5-fold cross-validation
  is summarized by a heat map, see Figure
  <xref alt="1" rid="figU003AhatmapGauss">1</xref>, if the argument
  <monospace>crit.plot</monospace> is assigned to
  <monospace>&quot;TRUE&quot;</monospace>.</p>
  <fig>
    <caption><p>Heatmap for 5-fold cross-validation
    <styled-content id="figU003AhatmapGauss"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/JOSSgausHeatMap.png" xlink:title="" />
  </fig>
  <p>To determine updated parameter <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  (<monospace>lambdaw</monospace>) in a reweighting step, 5-fold
  cross-validation based on the <monospace>cv.glmnet()</monospace>
  function from <monospace>glmnet</monospace>
  (<xref alt="Friedman et al., 2021" rid="ref-Friedman21R" ref-type="bibr">Friedman
  et al., 2021</xref>) is used.</p>
  <p>Several plots are available for the results.
  <monospace>plotCoef.enetLTS()</monospace> visualizes the coefficients
  where the coefficients which are set to zeros are shown,
  <monospace>plotResid.enetLTS()</monospace> plots the values of
  residuals vs fitted values, and
  <monospace>plotDiagnostic.enetLTS()</monospace> allows to produce
  various diagnostic plots for the final model fit. Some examples of
  these plots are shown in Figure
  <xref alt="2" rid="figU003AplotexamplesGuas">2</xref>.</p>
  <fig>
    <caption><p>Examples of plot functions of residuals (left);
    diagnostic (right) for linear
    regression<styled-content id="figU003AplotexamplesGuas"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/JOSSgausNCI60.png" xlink:title="" />
  </fig>
</sec>
<sec id="example-robust-and-sparse-binary-regression-familybinomial">
  <title>Example: Robust and Sparse Binary Regression
  (<monospace>family=&quot;binomial&quot;</monospace>)</title>
  <p>For binary regression, we have considered the same NCI-60 data set
  with some modifications. In order to provide an example for binary
  regression, the response variable is re-organized as follows. If
  <monospace>mean(y)</monospace> is smaller than 0.5, the response will
  be assigned to 0, otherwise, the response will be assigned to 1. The
  predictors are the same as in the previous section.</p>
  <code language="r script">y &lt;- protein[, 92]
# for binary class 
y.binom &lt;- ifelse(y &lt;= mean(y),0,1)</code>
  <p>For the binary regression, the <monospace>family</monospace>
  argument of <monospace>enetLTS()</monospace> function should be set to
  <monospace>&quot;binomial&quot;</monospace>.</p>
  <code language="r script">alphas &lt;- seq(0,1,length=41)
l0 &lt;- lambda00(X, y.binom, normalize = TRUE, intercept = TRUE)
lambdas &lt;- seq(l0,0.001,by=-0.025*l0)

# fit the model for family=&quot;binomial&quot;
set.seed(12)
fit.binomial &lt;- enetLTS(X, y.binom, family=&quot;binomial&quot;, alphas=alphas, 
                                    lambdas=lambdas)
fit.binomial</code>
  <code language="r script">## enetLTS estimator 

## Call:  enetLTS(xx = X, yy = y.binom, family = &quot;binomial&quot;, alphas = alphas, 
##               lambdas = lambdas) 

## number of the nonzero coefficients:
## [1] 48

## alpha: 0.325
## lambda: 0.0011
## lambdaw: 0.01456879</code>
  <p>48 out of 100 independent variables are selected by the enetLTS
  model based on optimal combination of <inline-formula><alternatives>
  <tex-math><![CDATA[\alpha=]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  0.325 and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda=]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  0.0011.</p>
  <p>The main function <monospace>enetLTS()</monospace> provides similar
  options for the values of the elastic net penalty. For the tuning
  parameter <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>,
  a user supplied sequence option is available. If this is not provided,
  the default is chosen with steps of size -0.025 lambda00 with
  <inline-formula><alternatives>
  <tex-math><![CDATA[0\le\lambda\le]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:mi>λ</mml:mi><mml:mo>≤</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  lambda00, where lambda00 is determined based on the robustified
  point-biserial correlation, see Kurnaz et al.
  (<xref alt="2018" rid="ref-Kurnaz18" ref-type="bibr">2018</xref>).</p>
  <p>The evaluation criterion results for to the candidates of tuning
  parameters is available in a heat map if the argument
  <monospace>crit.plot</monospace> is assigned to
  <monospace>&quot;TRUE&quot;</monospace> (which is omitted here). To
  determine the updated parameter <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
  (<monospace>lambdaw</monospace>) for the reweighting step, 5-fold
  cross-validation based on the <monospace>cv.glmnet()</monospace>
  function is used from the <monospace>glmnet</monospace> package for
  the current <monospace>family</monospace> option.</p>
  <p>Similarly, <monospace>plotCoef.enetLTS()</monospace> visualizes the
  coefficients. The other plot functions are re-organized for binary
  regression. In <monospace>plotResid.enetLTS()</monospace>, residuals
  are turned into the deviances, and this plot function produces two
  plots which are deviances vs index, and deviances vs fitted values
  (link function). <monospace>plotDiagnostic.enetLTS()</monospace> shows
  the response variable vs fitted values (link function). Some of these
  plots are presented in Figure
  <xref alt="3" rid="figU003AResidDiagbinom">3</xref>.</p>
  <fig>
    <caption><p>Examples of plot functions of deviances (left);
    diagnostic (right) for binary
    regression<styled-content id="figU003AResidDiagbinom"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/JOSSbinomResidDiagNCI60.png" xlink:title="" />
  </fig>
  <sec id="example-robust-and-sparse-multinomial-regression-familymultinomial">
    <title>Example: Robust and Sparse Multinomial Regression
    (<monospace>family=&quot;multinomial&quot;</monospace>)</title>
    <p>The fruit data set has been well-known in the context of robust
    discrimination studies. Therefore, we have considered the fruit data
    set in order to illustrate multinomial regression. It contains
    spectral information with 256 wavelengths for observations from 3
    different cultivars of the same fruit, named D, M, and HA, with
    group sizes 490, 106, and 500. This data set is available in the R
    package <monospace>rrcov</monospace>.</p>
    <code language="r script"># load data
library(rrcov)
data(fruit)

d &lt;- fruit[,-1]  # first column includes the fruit names 
X &lt;- as.matrix(d)
# define response variable
grp &lt;- c(rep(1,490),rep(2,106),rep(3,500)) 
y &lt;- factor(grp-1)</code>
    <p>With <monospace>family=&quot;multinomial&quot;</monospace>, the
    model <monospace>enetLTS()</monospace> produces the results of
    multinomial regression. Here user supplied values of
    <monospace>lambdas</monospace> are considered.</p>
    <code language="r script">lambdas=seq(from=0.01,to=0.1,by=0.01)
set.seed(4)
fit.multinom &lt;- enetLTS(X, y, family=&quot;multinomial&quot;, lambdas=lambdas, 
                        crit.plot=FALSE)
## [1] &quot;optimal model: lambda = 0.01 alpha = 0.02&quot;

fit.mutinom </code>
    <code language="r script">## enetLTS estimator 

## Call:  enetLTS(xx = X, yy = y, family = &quot;multinomial&quot;, lambdas=lambdas, 
##               crit.plot = FALSE) 

## number of the nonzero coefficients:
## [1] 704

## alpha: 0.02
## lambda: 0.01
## lambdaw: 0.003971358</code>
    <p>704 out of 1096 independent variables are selected by the enetLTS
    model based on optimal combination of <inline-formula><alternatives>
    <tex-math><![CDATA[\alpha=]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    0.2 and <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda=]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    0.01. Here <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda_w =]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    0.003971358 corresponds to updated tuning parameter for reweighted
    model. The effect of tuning parameter <inline-formula><alternatives>
    <tex-math><![CDATA[\alpha]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
    on the model is very clear from less sparsity.</p>
    <p>The main function <monospace>enetLTS()</monospace> provides
    similar options for the <inline-formula><alternatives>
    <tex-math><![CDATA[\alpha]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>α</mml:mi></mml:math></alternatives></inline-formula>
    sequence of the elastic net penalty. The default for the tuning
    parameters <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
    are values from 0.95 to 0.05 with steps of size -0.05, see Kurnaz
    &amp; Filzmoser
    (<xref alt="2022" rid="ref-Kurnaz22Arx" ref-type="bibr">2022</xref>).</p>
    <p>The combination of the optimal tuning parameters is evaluated by
    5-fold cross-validation. A heat map is available if the argument
    <monospace>crit.plot</monospace> is assigned to
    <monospace>&quot;TRUE&quot;</monospace>. As for the other models, an
    updated tuning parameter <inline-formula><alternatives>
    <tex-math><![CDATA[\lambda]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>λ</mml:mi></mml:math></alternatives></inline-formula>
    (<monospace>lambdaw</monospace>) for the reweighting step is
    obtained by the <monospace>cv.glmnet()</monospace> function from the
    package <monospace>glmnet</monospace>
    (<xref alt="Friedman et al., 2021" rid="ref-Friedman21R" ref-type="bibr">Friedman
    et al., 2021</xref>).</p>
    <p>The plot functions are adjusted for multinomial regression.
    <monospace>plotCoef.enetLTS()</monospace> gives the coefficients
    plots which includes group information. In
    <monospace>plotResid.enetLTS()</monospace>, residuals are turned
    into deviances, as in the binary regression case, with group
    information. <monospace>plotDiagnostic.enetLTS()</monospace> shows
    the scores of all groups in the space of the first two principal
    components.</p>
    <p>Especially for ‘family=“multinomial”’, run time is long because
    the algorithm is based on repeated C-steps.</p>
  </sec>
</sec>
<sec id="related-software">
  <title>Related Software</title>
  <p>The package <monospace>robustHD</monospace> provides the sparseLTS
  estimator for linear regression based on trimming of the lasso
  penalized for high dimensional linear regression
  (<xref alt="Alfons, 2021" rid="ref-Alfons21R" ref-type="bibr">Alfons,
  2021</xref>). The package <monospace>pense</monospace> provides
  implementations of robust S- and MM-type estimators using elastic net
  regularization for linear regression
  (<xref alt="Kepplinger et al., 2021" rid="ref-Keplinger21R" ref-type="bibr">Kepplinger
  et al., 2021</xref>). These packages are designed for linear
  regression, but not extended for binary or multinomial regression. On
  the other hand, the package glmnet implements the elastic net
  estimator for linear, binary, multinomial regression models and more
  (<xref alt="Friedman et al., 2021" rid="ref-Friedman21R" ref-type="bibr">Friedman
  et al., 2021</xref>). The procedure of the R package enetLTS
  (<xref alt="Kurnaz et al., 2022" rid="ref-Kurnaz22Rcran" ref-type="bibr">Kurnaz
  et al., 2022</xref>) is implemented using the R package glmnet
  (<xref alt="Friedman et al., 2021" rid="ref-Friedman21R" ref-type="bibr">Friedman
  et al., 2021</xref>). Therefore, taking the advantages of this
  internal usage, the package enetLTS provides a robust and sparse
  estimator based on trimming of the elastic net penalized for high
  dimensional linear, binary and multinomial regression.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Fatma Sevinc KURNAZ is supported by grant TUBITAK 2219 from
  Scientific and Technological Research Council of Turkey (TUBITAK).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Alfons13">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alfons</surname><given-names>A.</given-names></name>
        <name><surname>Croux</surname><given-names>C.</given-names></name>
        <name><surname>Gelper</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>Sparse least trimmed squares regression for analyzing high-dimensional large data sets</article-title>
      <source>Annals of Applied Statistics</source>
      <year iso-8601-date="2013">2013</year>
      <volume>7</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1214/12-AOAS575</pub-id>
      <fpage>226</fpage>
      <lpage>248</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Alfons21R">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alfons</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>robustHD: An R package for robust regression with high-dimensional data</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>67</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03786</pub-id>
      <fpage>3786</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Friedman10">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friedman</surname><given-names>J.</given-names></name>
        <name><surname>Hastie</surname><given-names>T.</given-names></name>
        <name><surname>Tibshirani</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>Regularization paths for generalized linear models via coordinate descent</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2010">2010</year>
      <volume>33</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v033.i01</pub-id>
      <fpage>1</fpage>
      <lpage>22</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Friedman21R">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friedman</surname><given-names>J.</given-names></name>
        <name><surname>Hastie</surname><given-names>T.</given-names></name>
        <name><surname>Tibshirani</surname><given-names>R.</given-names></name>
        <name><surname>Narasimhan</surname><given-names>B.</given-names></name>
        <name><surname>Tay</surname><given-names>K.</given-names></name>
        <name><surname>Simon</surname><given-names>N.</given-names></name>
        <name><surname>Qian</surname><given-names>J.</given-names></name>
        <name><surname>Yang</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models</article-title>
      <source>R Foundation for Statistical Computing, Vienna, Austria. R package version 4.1–3 https://CRAN.R-project.org/package=glmnet</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-Insolia21a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Insolia</surname><given-names>L.</given-names></name>
        <name><surname>Kenney</surname><given-names>A.</given-names></name>
        <name><surname>Chianomante</surname><given-names>F.</given-names></name>
        <name><surname>Felici</surname><given-names>G.</given-names></name>
      </person-group>
      <article-title>Simultaneous feature selection and outlier detection with optimality guarantees</article-title>
      <source>Biometrics</source>
      <year iso-8601-date="2022">2022</year>
      <volume></volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.1111/biom.13553</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Insolia21b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Insolia</surname><given-names>L.</given-names></name>
        <name><surname>Kenney</surname><given-names>A.</given-names></name>
        <name><surname>Calovi</surname><given-names>M.</given-names></name>
        <name><surname>F.Chiaromonte</surname></name>
      </person-group>
      <article-title>Robust variable selection with optimality guarantees for high-dimensional logistic regression</article-title>
      <source>Stats</source>
      <year iso-8601-date="2021">2021</year>
      <volume>4</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.3390/stats4030040</pub-id>
      <fpage>665</fpage>
      <lpage>681</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Keplinger21R">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kepplinger</surname><given-names>D.</given-names></name>
        <name><surname>Salibian-Barrera</surname><given-names>M.</given-names></name>
        <name><surname>Freue</surname><given-names>G. Cohen</given-names></name>
      </person-group>
      <article-title>Pense: Penalized elastic net s/MM-estimator of regression</article-title>
      <source>R Foundation for Statistical Computing, Vienna, Austria. https://CRAN.R-project.org/package=pense</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-Kurnaz18">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kurnaz</surname><given-names>F. S.</given-names></name>
        <name><surname>Hoffmann</surname><given-names>I.</given-names></name>
        <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Robust and sparse estimation methods for high-dimensional linear and logistic regression</article-title>
      <source>Chemometrics and Intelligent Laboratory Systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>172</volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.1016/j.chemolab.2017.11.017</pub-id>
      <fpage>211</fpage>
      <lpage>222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Kurnaz22Arx">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kurnaz</surname><given-names>F. S.</given-names></name>
        <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Robust and sparse multinomial regression in high dimensions</article-title>
      <source>Arxiv</source>
      <year iso-8601-date="2022">2022</year>
      <volume></volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.48550/arXiv.2205.11835</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Kurnaz22Rcran">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kurnaz</surname><given-names>F. S.</given-names></name>
        <name><surname>Hoffmann</surname><given-names>I.</given-names></name>
        <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>enetLTS: Robust and sparse estimation methods for high-dimensional linear and binary and multinomial regression</article-title>
      <source>R Foundation for Statistical Computing, Vienna, Austria. R package version 1.1.0 https://CRAN.R-project.org/package=enetLTS</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-Monti21">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Monti</surname><given-names>G. S.</given-names></name>
        <name><surname>Filzmoser</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Robust logistic zero-sum regression for microbiome compositional data</article-title>
      <source>Advances in data analysis and classification</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/s11634-021-00465-4</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Jensch22">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jensch</surname><given-names>A.</given-names></name>
        <name><surname>Lopes</surname><given-names>M. B.</given-names></name>
        <name><surname>Vinga</surname><given-names>S.</given-names></name>
        <name><surname>Radde</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>ROSIE: RObust sparse ensemble for outlier detection and gene selection in cancer omics data</article-title>
      <source>Statistical Methods in Medical Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>31</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1177/09622802211072456</pub-id>
      <fpage>947</fpage>
      <lpage>958</lpage>
    </element-citation>
  </ref>
  <ref id="ref-R21">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>R-Development-Core-Team</surname></name>
      </person-group>
      <article-title>R Foundation for Statistical Computing Vienna Austria. Https://www.r-project.org/</article-title>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-Reinhold12">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Reinhold</surname><given-names>W. C.</given-names></name>
        <name><surname>Sunshine</surname><given-names>M.</given-names></name>
        <name><surname>Liu</surname><given-names>H.</given-names></name>
        <name><surname>Varma</surname><given-names>S.</given-names></name>
        <name><surname>Kohn</surname><given-names>K. W.</given-names></name>
        <name><surname>Morris</surname><given-names>J. J.</given-names></name>
        <name><surname>Doroshow</surname><given-names>J.</given-names></name>
        <name><surname>Pommier</surname><given-names>Y.</given-names></name>
      </person-group>
      <article-title>CellMiner: A web-based suite of genomic and pharmacologic tools to explore transcript and drug patterns in the NCI-60 cell line set.</article-title>
      <source>Cancer Research</source>
      <year iso-8601-date="2012">2012</year>
      <volume>72</volume>
      <issue>14</issue>
      <pub-id pub-id-type="doi">10.1158/0008-5472.can-12-1370</pub-id>
      <fpage>3499</fpage>
      <lpage>3511</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Segaert18">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Segaert</surname><given-names>P.</given-names></name>
        <name><surname>Lopes</surname><given-names>M. B.</given-names></name>
        <name><surname>Casimiro</surname><given-names>S.</given-names></name>
        <name><surname>Vinga</surname><given-names>S.</given-names></name>
        <name><surname>Rousseeuw</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Robust identification of target genes and outliers in triple-negative breast cancer data</article-title>
      <source>Statistical Methods in Medical Research</source>
      <year iso-8601-date="2018">2018</year>
      <volume>28</volume>
      <pub-id pub-id-type="doi">10.1177/0962280218794722</pub-id>
      <fpage>3042</fpage>
      <lpage>3056</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
