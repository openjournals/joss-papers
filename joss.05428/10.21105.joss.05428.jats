<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5428</article-id>
<article-id pub-id-type="doi">10.21105/joss.05428</article-id>
<title-group>
<article-title>PyVBMC: Efficient Bayesian inference in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-3475-5964</contrib-id>
<name>
<surname>Huggins</surname>
<given-names>Bobby</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5848-910X</contrib-id>
<name>
<surname>Li</surname>
<given-names>Chengkun</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9778-0853</contrib-id>
<name>
<surname>Tobaben</surname>
<given-names>Marlon</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Aarnos</surname>
<given-names>Mikko J.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7471-7336</contrib-id>
<name>
<surname>Acerbi</surname>
<given-names>Luigi</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-2"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Helsinki</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
<corresp id="cor-2">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-02-27">
<day>27</day>
<month>2</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>5428</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bayesian statistics</kwd>
<kwd>Bayesian inference</kwd>
<kwd>Probabilistic programming</kwd>
<kwd>Model evidence</kwd>
<kwd>Machine learning</kwd>
<kwd>Simulator-based inference</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>PyVBMC is a Python implementation of the Variational Bayesian Monte
  Carlo (VBMC) algorithm for posterior and model inference for
  <italic>black-box</italic> computational models
  (<xref alt="Acerbi, 2018" rid="ref-acerbi_variational_2018" ref-type="bibr">Acerbi,
  2018</xref>,
  <xref alt="2020" rid="ref-acerbi_variational_2020" ref-type="bibr">2020</xref>).
  VBMC is an approximate inference method designed for efficient
  parameter estimation and model assessment when model evaluations are
  mildly-to-very expensive (e.g., a second or more) and/or noisy.
  Specifically, VBMC computes:</p>
  <list list-type="bullet">
    <list-item>
      <p>a flexible (non-Gaussian) approximate posterior distribution of
      the model parameters, from which statistics and posterior samples
      can be easily extracted;</p>
    </list-item>
    <list-item>
      <p>an approximation of the model evidence or marginal likelihood,
      a metric used for Bayesian model selection.</p>
    </list-item>
  </list>
  <p>PyVBMC can be applied to any computational or statistical model
  with up to roughly 10-15 continuous parameters, with the only
  requirement that the user can provide a Python function that computes
  the target log likelihood of the model, or an approximation thereof
  (e.g., an estimate of the likelihood obtained via simulation or Monte
  Carlo methods). PyVBMC is particularly effective when the model takes
  more than about a second per evaluation, with dramatic speed-ups of
  1-2 orders of magnitude when compared to traditional approximate
  inference methods.</p>
  <p>Extensive benchmarks on both artificial test problems and a large
  number of real models from the computational sciences, particularly
  computational and cognitive neuroscience, show that VBMC generally —
  and often vastly — outperforms alternative methods for
  sample-efficient Bayesian inference, and is applicable to both exact
  and simulator-based models
  (<xref alt="Acerbi, 2018" rid="ref-acerbi_variational_2018" ref-type="bibr">Acerbi,
  2018</xref>,
  <xref alt="2019" rid="ref-acerbi_exploration_2019" ref-type="bibr">2019</xref>,
  <xref alt="2020" rid="ref-acerbi_variational_2020" ref-type="bibr">2020</xref>).
  PyVBMC brings this state-of-the-art inference algorithm to Python,
  along with an easy-to-use Pythonic interface for running the algorithm
  and manipulating and visualizing its results.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Standard methods for Bayesian inference over arbitrary statistical
  and computational models, such as Markov Chain Monte Carlo (MCMC)
  methods, traditional variational inference, and nested sampling,
  require a large number of evaluations of the target density, and/or a
  model which is differentiable with respect to its parameters
  (<xref alt="Martin et al., 2020" rid="ref-martin_computing_2020" ref-type="bibr">Martin
  et al., 2020</xref>;
  <xref alt="Murphy, 2023" rid="ref-murphy_probabilistic_2023" ref-type="bibr">Murphy,
  2023</xref>). PyVBMC targets problems that are not amenable to these
  existing approaches: It uses no parameter gradients, so it is
  applicable to black-box models, and it is
  <italic>sample-efficient</italic>, requiring very few likelihood
  evaluations, typically on the order of a few hundred, as opposed to
  the many (tens of) thousands required by most other approximate
  inference methods.</p>
  <sec id="method">
    <title>Method</title>
    <p>PyVBMC achieves practical sample efficiency in a unique manner by
    simultaneously building two approximations of the true, expensive
    target posterior distribution:</p>
    <list list-type="bullet">
      <list-item>
        <p>A Gaussian process (GP) surrogate of the target
        (unnormalized) log posterior density.</p>
      </list-item>
      <list-item>
        <p>A variational approximation — an expressive mixture of
        Gaussian distributions with an arbitrary number of components —
        fit to the GP surrogate.</p>
      </list-item>
    </list>
    <p>The uncertainty-aware GP surrogate is built iteratively via
    active sampling, selecting which points to evaluate next so as to
    explore the posterior landscape and reduce uncertainty in the
    approximation
    (<xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>).
    In this respect, PyVBMC works similarly to Bayesian optimization
    methods
    (<xref alt="Garnett, 2023" rid="ref-garnett_bayesian_2023" ref-type="bibr">Garnett,
    2023</xref>), although with the different objective of learning the
    full shape of the target posterior as opposed to only searching for
    the optimum of the target.</p>
    <fig>
      <caption><p>Contour plots and marginal histograms show PyVBMC
      exploring a two-dimensional posterior (a
      <ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Rosenbrock_function">Rosenbrock</ext-link>
      likelihood with Gaussian prior). The solid orange circles indicate
      new points chosen by active sampling, the empty blue circles
      indicate previously sampled points, and the red crosses indicate
      centers of the variational posterior mixture components. 10 points
      are sampled initially, and each iteration consists of 5 actively
      sampled points, meaning that the final result is obtained with
      only 80 evaluations of the target
      density.<styled-content id="figU003Aexample"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/combined_figures_with_gt.png" />
    </fig>
    <p>At the same time, in each iteration of PyVBMC a variational
    approximation is fit to the current GP surrogate by optimizing a
    lower bound to the log model evidence (ELBO). This second
    approximation thus provides an estimate of the normalization
    constant of the posterior, useful for Bayesian model selection, and
    yields a tractable distribution (a very flexible mixture of
    Gaussians) we can easily compute with and draw samples from.
    Crucially, obtaining this variational approximation is inexpensive
    because the ELBO and its gradients can be efficiently estimated via
    Bayesian quadrature
    (<xref alt="Ghahramani &amp; Rasmussen, 2002" rid="ref-ghahramani_bayesian_2002" ref-type="bibr">Ghahramani
    &amp; Rasmussen, 2002</xref>;
    <xref alt="O’Hagan, 1991" rid="ref-ohagan_bayesian_1991" ref-type="bibr">O’Hagan,
    1991</xref>), without relying on additional evaluations of the true
    target posterior.</p>
    <p>The variational approximation is a unique feature of the VBMC
    algorithm that makes it particularly efficient and robust, as both
    the current ELBO and the variational posterior are used throughout
    all steps of the algorithm, for example:</p>
    <list list-type="bullet">
      <list-item>
        <p>as diagnostics for convergence and stability of the
        solution;</p>
      </list-item>
      <list-item>
        <p>to obtain a better representation of the target by rotating
        the axes via <italic>variational whitening</italic>;</p>
      </list-item>
      <list-item>
        <p>to estimate complex integrated acquisition functions for
        active sampling with noisy targets.</p>
      </list-item>
    </list>
    <p>All of these algorithmic steps would be cumbersome if not
    infeasible without an easy, tractable representation of the
    posterior at each iteration. Notably, the GP itself is not tractable
    as its normalization constant is unknown and we cannot directly draw
    posterior samples from the surrogate log density: this is where the
    variational posterior approximation steps in.</p>
    <p>In practice, PyVBMC returns the final variational posterior as an
    object that can be easily manipulated by the user, and an estimate
    of the ELBO and of its uncertainty. Importantly, several diagnostics
    are automatically applied to detect lack of convergence of the
    method.</p>
  </sec>
  <sec id="related-work">
    <title>Related work</title>
    <p>Algorithms predating VBMC, such as WSABI
    (<xref alt="Gunter et al., 2014" rid="ref-gunter_sampling_2014" ref-type="bibr">Gunter
    et al., 2014</xref>), BBQ
    (<xref alt="Osborne et al., 2012" rid="ref-osborne_active_2012" ref-type="bibr">Osborne
    et al., 2012</xref>), BAPE
    (<xref alt="Kandasamy et al., 2015" rid="ref-kandasamy_bayesian_2015" ref-type="bibr">Kandasamy
    et al., 2015</xref>), and AGP
    (<xref alt="Wang &amp; Li, 2018" rid="ref-wang_adaptive_2018" ref-type="bibr">Wang
    &amp; Li, 2018</xref>), employ similar strategies but underperform
    VBMC in previous experiments, and to our knowledge are not readily
    available as packaged and user-friendly software for Bayesian
    inference<xref ref-type="fn" rid="fn1">1</xref>. The Python package
    GPry
    (<xref alt="Gammal et al., 2022" rid="ref-gammal_fast_2022" ref-type="bibr">Gammal
    et al., 2022</xref>), released after VBMC but before PyVBMC,
    likewise relies on surrogate modeling and active sampling, but
    learns only a GP surrogate — not a variational posterior — and so
    generating samples or estimating the model evidence requires further
    post-processing.</p>
    <p>Furthermore, neither GPry nor the aforementioned algorithms are
    designed to handle noisy log density evaluations. Standard
    approaches to active sampling based on a surrogate GP can be easily
    fooled if the model’s likelihood is not a deterministic function of
    the parameters, but is instead measured only up to random error
    (<xref alt="Järvenpää et al., 2021" rid="ref-jarvenpaa_parallel_2021" ref-type="bibr">Järvenpää
    et al., 2021</xref>). Noisy evaluations of the log density occur
    frequently in simulator-based inference, where the likelihood is
    estimated from simulated data and statistics thereof. PyVBMC
    includes separate defaults and robust integrated acquisition
    functions for working with noisy likelihoods and therefore is
    particularly well suited to noisy and simulator-based contexts.</p>
  </sec>
  <sec id="applications-and-usage">
    <title>Applications and usage</title>
    <p>The VBMC algorithm, in its MATLAB implementation, has already
    been applied to fields as diverse as neuroscience
    (<xref alt="Stine et al., 2020" rid="ref-stine_differentiating_2020" ref-type="bibr">Stine
    et al., 2020</xref>), nuclear engineering
    (<xref alt="Che et al., 2021" rid="ref-che_application_2021" ref-type="bibr">Che
    et al., 2021</xref>), geophysical inverse problems
    (<xref alt="Hao et al., 2022" rid="ref-hao_application_2022" ref-type="bibr">Hao
    et al., 2022</xref>), and cancer research
    (<xref alt="Demetriades et al., 2022" rid="ref-demetriades_interrogating_2022" ref-type="bibr">Demetriades
    et al., 2022</xref>). With PyVBMC, we bring the same
    sample-efficient and robust inference to the wider open-source
    Python community, while improving the interface, test coverage, and
    documentation.</p>
    <p>The package is available on both PyPI
    (<monospace>pip install pyvbmc</monospace>) and
    <monospace>conda-forge</monospace>, and provides an idiomatic and
    accessible interface, only depending on standard, widely available
    scientific Python packages
    (<xref alt="Foreman-Mackey, 2016" rid="ref-foreman-mackey_corner_2016" ref-type="bibr">Foreman-Mackey,
    2016</xref>;
    <xref alt="Harris et al., 2020" rid="ref-harris_array_2020" ref-type="bibr">Harris
    et al., 2020</xref>). The user only needs to give a few basic
    details about their model and its parameter space, and PyVBMC
    handles the rest of the inference process. PyVBMC runs out of the
    box without tunable parameters and includes automatic handling of
    bounded variables, robust termination conditions, and sensible
    default settings. At the same time, experienced users can easily
    supply their own options. We have extensively tested the algorithm
    and implementation details for correctness and performance. We
    provide detailed
    <ext-link ext-link-type="uri" xlink:href="https://github.com/acerbilab/pyvbmc/tree/main/examples">tutorials</ext-link>,
    so that PyVBMC may be accessible to those not already familiar with
    approximate Bayesian inference, and our comprehensive
    <ext-link ext-link-type="uri" xlink:href="https://acerbilab.github.io/pyvbmc">documentation</ext-link>
    will aid not only new users but future contributors as well.</p>
  </sec>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>Work on the PyVBMC package is supported by the Academy of Finland
  Flagship programme: Finnish Center for Artificial Intelligence
  FCAI.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-acerbi_variational_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>Variational Bayesian Monte Carlo</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <fpage>8222</fpage>
      <lpage>8232</lpage>
    </element-citation>
  </ref>
  <ref id="ref-acerbi_variational_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>Variational Bayesian Monte Carlo with noisy likelihoods</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <fpage>8211</fpage>
      <lpage>8222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-acerbi_exploration_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>An exploration of acquisition and mean functions in Variational Bayesian Monte Carlo</article-title>
      <source>PMLR</source>
      <year iso-8601-date="2019">2019</year>
      <volume>96</volume>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-murphy_probabilistic_2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Murphy</surname><given-names>Kevin P.</given-names></name>
      </person-group>
      <source>Probabilistic Machine Learning: Advanced Topics</source>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>http://probml.github.io/book2</uri>
    </element-citation>
  </ref>
  <ref id="ref-garnett_bayesian_2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Garnett</surname><given-names>Roman</given-names></name>
      </person-group>
      <source>Bayesian Optimization</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-gammal_fast_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gammal</surname><given-names>Jonas El</given-names></name>
        <name><surname>Schöneberg</surname><given-names>Nils</given-names></name>
        <name><surname>Torrado</surname><given-names>Jesús</given-names></name>
        <name><surname>Fidler</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>Fast and robust Bayesian inference using Gaussian processes with GPry</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-02-22">2023</year><month>02</month><day>22</day></date-in-citation>
      <uri>http://arxiv.org/abs/2211.02045</uri>
    </element-citation>
  </ref>
  <ref id="ref-stine_differentiating_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stine</surname><given-names>Gabriel M</given-names></name>
        <name><surname>Zylberberg</surname><given-names>Ariel</given-names></name>
        <name><surname>Ditterich</surname><given-names>Jochen</given-names></name>
        <name><surname>Shadlen</surname><given-names>Michael N</given-names></name>
      </person-group>
      <article-title>Differentiating between integration and non-integration strategies in perceptual decision making</article-title>
      <source>eLife</source>
      <person-group person-group-type="editor">
        <name><surname>Wyart</surname><given-names>Valentin</given-names></name>
        <name><surname>Frank</surname><given-names>Michael J</given-names></name>
        <name><surname>Wyart</surname><given-names>Valentin</given-names></name>
        <name><surname>Usher</surname><given-names>Marius</given-names></name>
      </person-group>
      <publisher-name>eLife Sciences Publications, Ltd</publisher-name>
      <year iso-8601-date="2020-04">2020</year><month>04</month>
      <volume>9</volume>
      <issn>2050-084X</issn>
      <uri>https://doi.org/10.7554/eLife.55365</uri>
      <pub-id pub-id-type="doi">10.7554/eLife.55365</pub-id>
      <fpage>e55365</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-che_application_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Che</surname><given-names>Yifeng</given-names></name>
        <name><surname>Wu</surname><given-names>Xu</given-names></name>
        <name><surname>Pastore</surname><given-names>Giovanni</given-names></name>
        <name><surname>Li</surname><given-names>Wei</given-names></name>
        <name><surname>Shirvan</surname><given-names>Koroush</given-names></name>
      </person-group>
      <article-title>Application of kriging and Variational Bayesian Monte Carlo method for improved prediction of doped UO2 fission gas release</article-title>
      <source>Annals of Nuclear Energy</source>
      <year iso-8601-date="2021">2021</year>
      <volume>153</volume>
      <issn>0306-4549</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0306454920307428</uri>
      <pub-id pub-id-type="doi">10.1016/j.anucene.2020.108046</pub-id>
      <fpage>108046</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hao_application_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hao</surname><given-names>Wang</given-names></name>
        <name><surname>Duan</surname><given-names>Rui</given-names></name>
        <name><surname>Yang</surname><given-names>Kunde</given-names></name>
      </person-group>
      <article-title>Application of dual-source modal dispersion and Variational Bayesian Monte Carlo method for local geoacoustic inversion in weakly range-dependent shallow water</article-title>
      <source>Acoustics Australia</source>
      <year iso-8601-date="2022-08-25">2022</year><month>08</month><day>25</day>
      <issn>1839-2571</issn>
      <uri>https://doi.org/10.1007/s40857-022-00277-2</uri>
      <pub-id pub-id-type="doi">10.1007/s40857-022-00277-2</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-demetriades_interrogating_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Demetriades</surname><given-names>Marios</given-names></name>
        <name><surname>Zivanovic</surname><given-names>Marko</given-names></name>
        <name><surname>Hadjicharalambous</surname><given-names>Myrianthi</given-names></name>
        <name><surname>Ioannou</surname><given-names>Eleftherios</given-names></name>
        <name><surname>Ljujic</surname><given-names>Biljana</given-names></name>
        <name><surname>Vucicevic</surname><given-names>Ksenija</given-names></name>
        <name><surname>Ivosevic</surname><given-names>Zeljko</given-names></name>
        <name><surname>Dagovic</surname><given-names>Aleksandar</given-names></name>
        <name><surname>Milivojevic</surname><given-names>Nevena</given-names></name>
        <name><surname>Kokkinos</surname><given-names>Odysseas</given-names></name>
        <name><surname>Bauer</surname><given-names>Roman</given-names></name>
        <name><surname>Vavourakis</surname><given-names>Vasileios</given-names></name>
      </person-group>
      <article-title>Interrogating and quantifying in vitro cancer drug pharmacodynamics via agent-based and Bayesian Monte Carlo modelling</article-title>
      <source>Pharmaceutics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>4</issue>
      <issn>1999-4923</issn>
      <uri>https://www.mdpi.com/1999-4923/14/4/749</uri>
      <pub-id pub-id-type="doi">10.3390/pharmaceutics14040749</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gunter_sampling_2014">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gunter</surname><given-names>Tom</given-names></name>
        <name><surname>Osborne</surname><given-names>Michael A</given-names></name>
        <name><surname>Garnett</surname><given-names>Roman</given-names></name>
        <name><surname>Hennig</surname><given-names>Philipp</given-names></name>
        <name><surname>Roberts</surname><given-names>Stephen J</given-names></name>
      </person-group>
      <article-title>Sampling for inference in probabilistic models with fast Bayesian quadrature</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Ghahramani</surname><given-names>Z.</given-names></name>
        <name><surname>Welling</surname><given-names>M.</given-names></name>
        <name><surname>Cortes</surname><given-names>C.</given-names></name>
        <name><surname>Lawrence</surname><given-names>N.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>27</volume>
      <uri>https://proceedings.neurips.cc/paper/2014/file/e94f63f579e05cb49c05c2d050ead9c0-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-osborne_active_2012">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Osborne</surname><given-names>Michael</given-names></name>
        <name><surname>Garnett</surname><given-names>Roman</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David K</given-names></name>
        <name><surname>Roberts</surname><given-names>Stephen J</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Carl</given-names></name>
      </person-group>
      <article-title>Active learning of model evidence using Bayesian quadrature</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Pereira</surname><given-names>F.</given-names></name>
        <name><surname>Burges</surname><given-names>C. J.</given-names></name>
        <name><surname>Bottou</surname><given-names>L.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>25</volume>
      <uri>https://proceedings.neurips.cc/paper/2012/file/6364d3f0f495b6ab9dcf8d3b5c6e0b01-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kandasamy_bayesian_2015">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kandasamy</surname><given-names>Kirthevasan</given-names></name>
        <name><surname>Schneider</surname><given-names>Jeff</given-names></name>
        <name><surname>Póczos</surname><given-names>Barnabás</given-names></name>
      </person-group>
      <article-title>Bayesian active learning for posterior estimation</article-title>
      <source>Proceedings of the 24th international conference on artificial intelligence</source>
      <publisher-name>AAAI Press</publisher-name>
      <publisher-loc>Buenos Aires, Argentina</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>9781577357384</isbn>
      <fpage>3605</fpage>
      <lpage>3611</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang_adaptive_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Hongqiao</given-names></name>
        <name><surname>Li</surname><given-names>Jinglai</given-names></name>
      </person-group>
      <article-title>Adaptive Gaussian process approximation for Bayesian inference with expensive likelihood functions</article-title>
      <source>Neural Computation</source>
      <year iso-8601-date="2018-11">2018</year><month>11</month>
      <volume>30</volume>
      <issue>11</issue>
      <issn>0899-7667</issn>
      <uri>https://doi.org/10.1162/neco\_a\_01127</uri>
      <pub-id pub-id-type="doi">10.1162/neco_a_01127</pub-id>
      <fpage>3072</fpage>
      <lpage>3094</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paleyes_emulation_2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paleyes</surname><given-names>Andrei</given-names></name>
        <name><surname>Pullin</surname><given-names>Mark</given-names></name>
        <name><surname>Mahsereci</surname><given-names>Maren</given-names></name>
        <name><surname>Lawrence</surname><given-names>Neil</given-names></name>
        <name><surname>González</surname><given-names>Javier</given-names></name>
      </person-group>
      <article-title>Emulation of physical processes with Emukit</article-title>
      <source>Second workshop on machine learning and the physical sciences, NeurIPS</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-jarvenpaa_parallel_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Järvenpää</surname><given-names>Marko</given-names></name>
        <name><surname>Gutmann</surname><given-names>Michael U.</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Marttinen</surname><given-names>Pekka</given-names></name>
      </person-group>
      <article-title>Parallel Gaussian process surrogate Bayesian inference with noisy likelihood evaluations</article-title>
      <source>Bayesian Analysis</source>
      <publisher-name>International Society for Bayesian Analysis</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>16</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1214/20-BA1200</uri>
      <pub-id pub-id-type="doi">10.1214/20-BA1200</pub-id>
      <fpage>147 </fpage>
      <lpage> 178</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ohagan_bayesian_1991">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>O’Hagan</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Bayes–Hermite quadrature</article-title>
      <source>Journal of Statistical Planning and Inference</source>
      <year iso-8601-date="1991">1991</year>
      <volume>29</volume>
      <issue>3</issue>
      <issn>0378-3758</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/037837589190002V</uri>
      <pub-id pub-id-type="doi">10.1016/0378-3758(91)90002-V</pub-id>
      <fpage>245</fpage>
      <lpage>260</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ghahramani_bayesian_2002">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Carl</given-names></name>
      </person-group>
      <article-title>Bayesian Monte Carlo</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Becker</surname><given-names>S.</given-names></name>
        <name><surname>Thrun</surname><given-names>S.</given-names></name>
        <name><surname>Obermayer</surname><given-names>K.</given-names></name>
      </person-group>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2002">2002</year>
      <volume>15</volume>
      <uri>https://proceedings.neurips.cc/paper/2002/file/24917db15c4e37e421866448c9ab23d8-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-martin_computing_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Martin</surname><given-names>Gael M.</given-names></name>
        <name><surname>Frazier</surname><given-names>David T.</given-names></name>
        <name><surname>Robert</surname><given-names>Christian P.</given-names></name>
      </person-group>
      <article-title>Computing Bayes: Bayesian computation from 1763 to the 21st century</article-title>
      <source>arXiv: Computation</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-harris_array_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>van der Walt</surname><given-names>Stéfan J.</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>van Kerkwijk</surname><given-names>Marten H.</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>del Río</surname><given-names>Jaime Fernández</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Gérard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <year iso-8601-date="2020-09-01">2020</year><month>09</month><day>01</day>
      <volume>585</volume>
      <issue>7825</issue>
      <issn>1476-4687</issn>
      <uri>https://doi.org/10.1038/s41586-020-2649-2</uri>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-foreman-mackey_corner_2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Foreman-Mackey</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>Corner.py: Scatterplot matrices in Python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>1</volume>
      <issue>2</issue>
      <uri>https://doi.org/10.21105/joss.00024</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00024</pub-id>
      <fpage>24</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Emukit
    (<xref alt="Paleyes et al., 2019" rid="ref-paleyes_emulation_2019" ref-type="bibr">Paleyes
    et al., 2019</xref>) implements WSABI, but targets Bayesian
    optimization and general-purpose quadrature over inference.</p>
  </fn>
</fn-group>
</back>
</article>
