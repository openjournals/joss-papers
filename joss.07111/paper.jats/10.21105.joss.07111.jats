<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7111</article-id>
<article-id pub-id-type="doi">10.21105/joss.07111</article-id>
<title-group>
<article-title>CoastalLens: A MATLAB UAV Video Stabilization &amp;
Rectification Framework</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7650-9976</contrib-id>
<name>
<surname>Lange</surname>
<given-names>Athina M. Z.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lange</surname>
<given-names>Holger</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4797-8933</contrib-id>
<name>
<surname>Fiedler</surname>
<given-names>Julia W.</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bruder</surname>
<given-names>Brittany L.</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Scripps Institution of Oceanography, University of
California, San Diego, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>AI Werkstatt, United States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University of Hawai’i Sea Level Center, United States of
America</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Coastal and Hydraulics Laboratory, US Army Engineer
Research and Development Center, United States of America</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>9</volume>
<issue>104</issue>
<fpage>7111</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>MATLAB</kwd>
<kwd>Coastal Oceanography</kwd>
<kwd>UAV</kwd>
<kwd>Video stabilization</kwd>
<kwd>Image Rectification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Uncrewed aerial vehicles (UAVs) are an important tool for coastal
  monitoring with their relatively low-cost and rapid deployment
  capabilities. To generate scientific-grade image products, to use for
  wave runup observations, for bathymetry inversions, or tracking
  surfzone currents, the hovering UAV images/videos must be stabilized
  and rectified into world coordinates. Video stabilization is an active
  area of research and is use in many fields including videography, law
  enforcement, and agriculture (see
  (<xref alt="Wilko Guilluy et al., 2021" rid="ref-guilluyU003A2021" ref-type="bibr">Wilko
  Guilluy et al., 2021</xref>),
  (<xref alt="Yiming Wang et al., 2023" rid="ref-wangU003A2023" ref-type="bibr">Yiming
  Wang et al., 2023</xref>) for reviews on video stabilization). Due to
  the limited stationary region of coastal images suitable for control
  points, the processing of coastal UAV-obtained videos can be
  time-consuming and resource-intensive. The
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Coastal-Imaging-Research-Network/CIRN-Quantitative-Coastal-Imaging-Toolbox">CIRN
  Qualitative Coastal Imagining Toolbox</ext-link>
  (<xref alt="Bruder &amp; Brodie, 2020" rid="ref-bruderU003A2020" ref-type="bibr">Bruder
  &amp; Brodie, 2020</xref>) provided a first-of-its-kind open-sourced
  code for rectifying these coastal UAV videos. Limitations of the
  toolbox, however, prompted the development of CoastalLens with an
  efficient data input procedure, providing capabilities to obtain drone
  position (extrinsics) from LiDAR surveys, and using a feature
  detection and matching algorithm to stabilize the video prior to
  rectification. This framework reduces the amount of human oversight,
  now only required during the data input processes. Removing the
  dependency on threshold stability control points provides more stable
  results and can also result in less time in the field. We hope this
  framework will allow for more efficient processing of the
  ever-increasing coastal UAV datasets.</p>
</sec>
<sec id="summary">
  <title>Summary</title>
  <p>CoastalLens is set up as 4 scripts (with an optional 5th script)
  run sequentially from a main entry point script
  (<monospace>UAV_rectification.m</monospace>). This allows users to
  execute parts or all of the full framework depending on their
  workflow. The first script,
  <monospace>input_day_flight_data.m</monospace>, prompts user input and
  returns all the user-specified required input data organized in
  stuctures to be used by the subsequent scripts. The user is required
  to input data for each day and flight to process. Required user inputs
  are the video timezone, camera intrinsics, the Products (types of
  images) to be generated (e.g. Grid/Rectified Image, xTransect or
  yTransect), and the ground control points to determine the camera
  world position (via GPS points or pointcloud)
  ((<xref alt="Hartley &amp; Zisserman, 2004" rid="ref-hartleyU003A2004" ref-type="bibr">Hartley
  &amp; Zisserman, 2004</xref>),
  (<xref alt="Xiao-Shan Gao et al., 2003" rid="ref-gaoU003A2003" ref-type="bibr">Xiao-Shan
  Gao et al., 2003</xref>),
  (<xref alt="Conlin et al., 2020" rid="ref-conlinU003A2020" ref-type="bibr">Conlin
  et al., 2020</xref>)). Users can load in pre-set values for the
  relevant day-specific information from a configuration file. This is
  useful if similar UAV missions are flown repeatedly at the same
  location. The second script,
  <monospace>extract_images_from_UAV.m</monospace>, extracts images from
  the video files at the specified frame rates. This is done via a
  system command to the ffmpeg command line tool. The third script,
  <monospace>stabilize_video.m</monospace>, accounts for the UAV
  movement and returns the 2D projective transformation of the image to
  improve image stabilization through flight. We take an approach
  similar to constructing a panorama image. Static features
  (e.g. corners, windows, lines on the ground) are found in every frame.
  In subsequent frames, these features are matched and the
  movement/change in location of these features between the frames is
  used to estimate the change in position of frame 2 versus frame 1.
  This is used to warp the image into fitting into the full ‘panorama’
  image. This approach allows for good estimates to be obtained even in
  cases where the UAV drift substantially
  ((<xref alt="Brown &amp; Lowe, 2007" rid="ref-brownU003A2007" ref-type="bibr">Brown
  &amp; Lowe, 2007</xref>),
  (<xref alt="Torr &amp; Zisserman, 2000" rid="ref-torrU003A2000" ref-type="bibr">Torr
  &amp; Zisserman, 2000</xref>)). From these stabilized images we can
  produce standard ARGUS products, like time-averaged images, brightest
  and darkest image
  (<xref alt="Holman &amp; Stanley, 2007" rid="ref-holmanU003A2007" ref-type="bibr">Holman
  &amp; Stanley, 2007</xref>). In the final main script,
  <monospace>get_products.m</monospace>, the image coordinates
  corresponding to the world coordinates of the previously defined
  products are determined. These image coordinates are used to extract
  the pixels from each frame. <monospace>save_products.m</monospace> is
  an optional code to save the resulting rectified images as png’s.</p>
  <fig>
    <caption><p>Figure 1: Example of matched features and the 2D
    projective transformation of the image. (left) Image 1 (red) and
    Image 2 (blue) are taken 1 minute apart (extreme case) and features
    have been detected and matched between the two frames. Note the
    shift in the lifeguard tower on the right, or the pedestrian
    crosswalk in the middle of the image. (right) Image 1 (green) and
    Image 2 (purple) shown after they have been warped into the
    ‘panorama’ image. Note large grey region at the bottom and the
    horizon at the top of the image where the two frames match and the
    stabilization has
    succeeded.<styled-content id="figU003A1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="get_extrinsics_fd_example.png" />
  </fig>
  <fig>
    <caption><p>Figure 2: Example of 17 minutes of video stitched
    together. Extreme drift in the UAV can be seen, but horizon at the
    top and road at the bottom of the image remain
    stable.<styled-content id="figU003A2"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="20211026_Torrey_01_Panorama.png" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This study was funded by the U.S. Army Corps of Engineers
  (W912HZ1920020) and the California Department of Parks and Recreation
  (C19E0026). Thank you to Rob Grenzeback, Julia Fiedler, Alex Simpson
  and Holden Leslie-Bole for help collecting this data and testing the
  framework.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-wangU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>Yiming Wang</string-name>
        <string-name>Qian Huang</string-name>
        <string-name>Chuanxu Jiang</string-name>
        <string-name>Jiwen Liu</string-name>
        <string-name>Mingzhou Shang</string-name>
        <string-name>Zhuang Miao</string-name>
      </person-group>
      <article-title>Video stabilization: A comprehensive survey</article-title>
      <source>Neurocomputing</source>
      <year iso-8601-date="2023-01">2023</year><month>01</month>
      <volume>516</volume>
      <uri>https://www.sciencedirect.com/science/article/abs/pii/S092523122201270X</uri>
      <pub-id pub-id-type="doi">10.1016/j.neucom.2022.10.008</pub-id>
      <fpage>205</fpage>
      <lpage>230</lpage>
    </element-citation>
  </ref>
  <ref id="ref-guilluyU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>Wilko Guilluy</string-name>
        <string-name>Laurent Oudre</string-name>
        <string-name>Azeddine Beghdadi</string-name>
      </person-group>
      <article-title>Video stabilization: Overview, challenges and perspectives</article-title>
      <source>Signal Processing: Image Communication</source>
      <year iso-8601-date="2021-01">2021</year><month>01</month>
      <volume>90</volume>
      <uri>https://www.sciencedirect.com/science/article/abs/pii/S0923596520301697</uri>
      <pub-id pub-id-type="doi">10.1016/j.image.2020.116015</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-brownU003A2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Matthew</given-names></name>
        <name><surname>Lowe</surname><given-names>David G.</given-names></name>
      </person-group>
      <article-title>Automatic panoramic image stitching using invariant features</article-title>
      <source>International Journal of Computer Vision</source>
      <year iso-8601-date="2007-08">2007</year><month>08</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>74</volume>
      <issue>1</issue>
      <issn>0920-5691</issn>
      <uri>https://link.springer.com/10.1007/s11263-006-0002-3</uri>
      <pub-id pub-id-type="doi">10.1007/s11263-006-0002-3</pub-id>
      <fpage>59</fpage>
      <lpage>73</lpage>
    </element-citation>
  </ref>
  <ref id="ref-conlinU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Conlin</surname><given-names>Matthew P.</given-names></name>
        <name><surname>Adams</surname><given-names>Peter N.</given-names></name>
        <name><surname>Benjamin</surname><given-names>Wilkinson</given-names></name>
        <name><surname>Dusek</surname><suffix>Gregory</suffix></name>
        <name><surname>Palmsten</surname><given-names>Margaret L.</given-names></name>
        <name><surname>Brown</surname><given-names>Jenna A.</given-names></name>
      </person-group>
      <article-title>SurfRCaT: A tool for remote calibration of pre-existing coastal cameras to enable their use as quantitative coastal monitoring tools</article-title>
      <source>SoftwareX</source>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>12</volume>
      <pub-id pub-id-type="doi">10.1016/j.softx.2020.100584</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gaoU003A2003">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>Xiao-Shan Gao</string-name>
        <string-name>Xiao-Rong Hou</string-name>
        <string-name>Jianliang Tang</string-name>
        <string-name>Hang-Fei Cheng</string-name>
      </person-group>
      <article-title>Complete solution classification for the perspective-three-point problem</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2003-08">2003</year><month>08</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>25</volume>
      <issue>8</issue>
      <issn>0162-8828</issn>
      <uri>http://ieeexplore.ieee.org/document/1217599/</uri>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2003.1217599</pub-id>
      <fpage>930</fpage>
      <lpage>943</lpage>
    </element-citation>
  </ref>
  <ref id="ref-torrU003A2000">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Torr</surname><given-names>P. H. S.</given-names></name>
        <name><surname>Zisserman</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>MLESAC: A new robust estimator with application to estimating image geometry</article-title>
      <source>Computer Vision and Image Understanding</source>
      <year iso-8601-date="2000-04">2000</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>78</volume>
      <issue>1</issue>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S1077314299908329</uri>
      <pub-id pub-id-type="doi">10.1006/cviu.1999.0832</pub-id>
      <fpage>138</fpage>
      <lpage>156</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hartleyU003A2004">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Hartley</surname><given-names>Richard</given-names></name>
        <name><surname>Zisserman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <source>Multiple view geometry in computer vision</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <publisher-loc>Cambridge, UK</publisher-loc>
      <year iso-8601-date="2004">2004</year>
      <edition>Second edition</edition>
      <isbn>978-0-511-18711-7</isbn>
    </element-citation>
  </ref>
  <ref id="ref-holmanU003A2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Holman</surname><given-names>R. A.</given-names></name>
        <name><surname>Stanley</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>The history and technical capabilities of argus</article-title>
      <source>Coastal Engineering</source>
      <year iso-8601-date="2007-06">2007</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>54</volume>
      <issue>6</issue>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S037838390700018X</uri>
      <pub-id pub-id-type="doi">10.1016/j.coastaleng.2007.01.003</pub-id>
      <fpage>477</fpage>
      <lpage>491</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bruderU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bruder</surname><given-names>Brittany L.</given-names></name>
        <name><surname>Brodie</surname><given-names>Katherine L.</given-names></name>
      </person-group>
      <article-title>CIRN quantitative coastal imaging toolbox</article-title>
      <source>SoftwareX</source>
      <year iso-8601-date="2020-07">2020</year><month>07</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-02-12">2024</year><month>02</month><day>12</day></date-in-citation>
      <volume>12</volume>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S2352711020302958</uri>
      <pub-id pub-id-type="doi">10.1016/j.softx.2020.100582</pub-id>
      <fpage>100582</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
