<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6524</article-id>
<article-id pub-id-type="doi">10.21105/joss.06524</article-id>
<title-group>
<article-title>libsoni: A Python Toolbox for Sonifying Music Annotations
and Feature Representations</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2235-8655</contrib-id>
<name>
<surname>Özer</surname>
<given-names>Yigitcan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Brütting</surname>
<given-names>Leo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5780-557X</contrib-id>
<name>
<surname>Schwär</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6062-7524</contrib-id>
<name>
<surname>Müller</surname>
<given-names>Meinard</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>International Audio Laboratories Erlangen</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-03-12">
<day>12</day>
<month>3</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>98</issue>
<fpage>6524</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Music information retrieval</kwd>
<kwd>Music sonification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Music Information Retrieval (MIR) stands as a dedicated research
  field focused on advancing methodologies and tools for organizing,
  analyzing, retrieving, and generating data related to music. Key tasks
  within MIR include beat tracking, structural analysis, chord
  recognition, melody extraction, and source separation, just to name a
  few. These tasks involve extracting musically relevant information
  from audio recordings, typically accomplished by transforming music
  signals into feature representations such as spectrograms,
  chromagrams, or tempograms
  (<xref alt="Müller, 2015" rid="ref-Mueller15_FMP_SPRINGER" ref-type="bibr">Müller,
  2015</xref>). Furthermore, musically relevant annotations such as
  beats, chords, keys, or structure boundaries become indispensable for
  training and evaluating MIR approaches.</p>
  <p>When evaluating and enhancing MIR systems, it is crucial to
  thoroughly examine the properties of feature representations and
  annotations to gain a deeper understanding of algorithmic behavior and
  the underlying data. In the musical context, alongside conventional
  data visualization techniques, data sonification techniques are
  emerging as a promising avenue for providing auditory feedback on
  extracted features or annotated information. This is particularly
  advantageous given the finely tuned human perception to subtle
  variations in frequency and timing within the musical domain.</p>
  <p>This paper introduces <italic>libsoni</italic>, an open-source
  Python toolbox tailored for the sonification of music annotations and
  feature representations. By employing explicit and easy-to-understand
  sound synthesis techniques, libsoni offers functionalities for
  generating and triggering sound events, enabling the sonification of
  spectral, harmonic, tonal, melodic, and rhythmic aspects. Unlike
  existing software libraries focused on creative applications of sound
  generation, libsoni is designed to meet the specific needs of MIR
  researchers and educators. It aims to simplify the process of music
  exploration, promoting a more intuitive and efficient approach to data
  analysis by enabling users to interact with their data in acoustically
  meaningful ways. As a result, libsoni not only improves the analytical
  capabilities of music scientists but also opens up new avenues for
  innovative music analysis and discovery. Furthermore, libsoni provides
  well-documented and stand-alone functions covering all essential
  building blocks crucial for both sound generation and sonification,
  enabling users to efficiently apply and easily extend the methods.
  Additionally, the toolbox includes educational Jupyter notebooks with
  illustrative code examples demonstrating the application of
  sonification and visualization methods to deepen understanding within
  specific MIR scenarios.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Music data, characterized by attributes such as pitch, melody,
  harmony, rhythm, structure, and timbre, is inherently intricate.
  Visualizations are crucial in deciphering this complexity by
  presenting music representations graphically, enabling researchers to
  identify patterns, trends, and relationships not immediately evident
  in raw data. For instance, visualizing time-dependent two-dimensional
  feature representations such as spectrograms (time–frequency),
  chromagrams (time–chroma), or tempograms (time–tempo) enhances
  comprehension of signal processing concepts and reveals insights into
  the musical and acoustic properties of audio signals. Moreover, the
  combined visualization of extracted features and reference annotations
  facilitates detailed examination of algorithmic approaches at a
  granular level. These qualitative assessments, alongside quantitative
  metrics, are essential for comprehending the strengths, weaknesses,
  and assumptions underlying music processing algorithms. The MIR
  community has developed numerous toolboxes, such as essentia
  (<xref alt="Bogdanov et al., 2013" rid="ref-BogdanovWGGHMRSZS13_essentia_ISMIR" ref-type="bibr">Bogdanov
  et al., 2013</xref>), madmom
  (<xref alt="Böck et al., 2016" rid="ref-BoeckKSKW16_madmom_ACM-MM" ref-type="bibr">Böck
  et al., 2016</xref>), Chroma Toolbox
  (<xref alt="Müller &amp; Ewert, 2011" rid="ref-MuellerEwert11_ChromaToolbox_ISMIR" ref-type="bibr">Müller
  &amp; Ewert, 2011</xref>), Tempogram Toolbox
  (<xref alt="Grosche &amp; Müller, 2011" rid="ref-GroscheM11_TempogramToolbox_ISMIR-lateBreaking" ref-type="bibr">Grosche
  &amp; Müller, 2011</xref>), Sync Toolbox
  (<xref alt="Müller et al., 2021" rid="ref-MuellerOKPD21_SyncToolbox_JOSS" ref-type="bibr">Müller
  et al., 2021</xref>), Marsyas
  (<xref alt="Tzanetakis, 2009" rid="ref-Tzanetakis09_MARSYAS_ACM-MM" ref-type="bibr">Tzanetakis,
  2009</xref>), or the MIRtoolbox
  (<xref alt="Lartillot &amp; Toiviainen, 2007" rid="ref-LartillotT07_MirToolbox_ISMIR" ref-type="bibr">Lartillot
  &amp; Toiviainen, 2007</xref>), offering modular code for music signal
  processing and analysis, many of which also include data visualization
  methods. Notably, the two Python packages librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>) and libfmp
  (<xref alt="Müller &amp; Zalkow, 2021" rid="ref-MuellerZ21_libfmp_JOSS" ref-type="bibr">Müller
  &amp; Zalkow, 2021</xref>) aim to lower the barrier to entry for MIR
  research by providing accessible code alongside visualization
  functions, bridging the gap between education and research.</p>
  <p>As an alternative or addition to visualizing data, one can employ
  data sonification techniques to produce acoustic feedback on extracted
  or annotated information
  (<xref alt="Kramer et al., 1999" rid="ref-KramerEtAl99SonificAR" ref-type="bibr">Kramer
  et al., 1999</xref>). This is especially important in music, where
  humans excel at detecting even minor variations in the frequency and
  timing of sound events. For instance, people can readily perceive
  irregularities and subtle changes in rhythm when they listen to a
  pulse track converted into a sequence of click sounds. This ability is
  particularly valuable for MIR tasks such as beat tracking and rhythm
  analysis. Moreover, transforming frequency trajectories into sound
  using sinusoidal models can offer insights for tasks like estimating
  melody or separating singing voices. Furthermore, an auditory
  representation of a chromagram provides listeners with an
  understanding of the harmony-related tonal information contained in an
  audio signal. Therefore, by converting data into sound, sonification
  can reveal subtle audible details in music that may not be immediately
  apparent within visual representations.</p>
  <p>In the MIR context, sonification methods have been employed to
  provide deeper insights into various music annotations and feature
  representations. For instance, the Python package librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>) offers a function
  <monospace>librosa.clicks</monospace> that generates an audio signal
  with click sounds positioned at specified times, with options to
  adjust the frequency and duration of the clicks. Additionally, the
  Python toolbox libf0
  (<xref alt="Rosenzweig et al., 2022" rid="ref-RosenzweigSM22_libf0_ISMIR-LBD" ref-type="bibr">Rosenzweig
  et al., 2022</xref>) provides a function
  (<monospace>libf0.utils.sonify_trajectory_with_sinusoid</monospace>)
  for sonifying F0 trajectories using sinusoids. Moreover, the Python
  package libfmp
  (<xref alt="Müller &amp; Zalkow, 2021" rid="ref-MuellerZ21_libfmp_JOSS" ref-type="bibr">Müller
  &amp; Zalkow, 2021</xref>) includes a function
  (<monospace>libfmp.b.sonify_chromagram_with_signal</monospace>) for
  sonifying time–chroma representations. Testing these methods, our
  experiments have revealed that current implementations sometimes rely
  on inefficient event-based looping, which may result in excessively
  long runtimes.</p>
  <p>In our Python toolbox, libsoni, we offer implementations of various
  sonification methods, including those mentioned above. These
  implementations feature a coherent API and are based on
  straightforward methods that are transparent and easy to understand
  (ensuring a trade-off between efficiency and code readability).
  Additionally, libsoni includes all essential components for sound
  synthesis, operating as a standalone tool that can be easily extended
  and customized. The methods in libsoni enable interactivity, allowing
  for data manipulation and sonification, as well as the ability to
  alter feature extraction or sonification techniques. While real-time
  capabilities are not currently included in libsoni, this could be a
  potential future extension. Hence, libsoni may not only be beneficial
  for MIR researchers but also for educators, students, composers, sound
  designers, and individuals exploring new musical concepts.</p>
</sec>
<sec id="core-functionalities">
  <title>Core Functionalities</title>
  <p>In the following, we briefly describe some of the main modules
  included in the Python toolbox libsoni. For an illustration of some
  core functionalities, we also refer to
  <xref alt="[fig:libsoni]" rid="figU003Alibsoni">[fig:libsoni]</xref>.
  A comprehensive API documentation of libsoni is publicly accessible
  through GitHub<xref ref-type="fn" rid="fn1">1</xref>. Furthermore, the
  applications of core functionalities are illustrated by educational
  Jupyter notebooks as an integral part of libsoni, providing
  illustrative code examples within concrete MIR scenarios.</p>
  <fig>
    <caption><p>Illustration of some core functionalities provided by
    the Python toolbox libsoni. <bold>(a)</bold> Logo of libsoni.
    <bold>(b)</bold> Sonification of F0 trajectories. <bold>(c)</bold>
    Sonification of piano roll representations. <bold>(d)</bold>
    Sonification of time–frequency
    representations.<styled-content id="figU003Alibsoni"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="teaser.pdf" />
  </fig>
  <sec id="triggered-sound-events">
    <title>Triggered Sound Events
    (<monospace>libsoni.tse</monospace>)</title>
    <p>The Triggered Sound Events (TSE) module of libsoni contains
    various functions for the sonification of temporal events. In this
    scenario, one typically has a music recording and a list of time
    positions that indicate the presence of certain musical events.
    These events could include onset positions of specific notes, beat
    positions, or structural boundaries between musical parts. The TSE
    module allows for generating succinct acoustic stimuli at each of
    the time positions, providing the listener with precise temporal
    feedback. Ideally, these stimuli should be perceivable even when
    overlaid with the original music recording. Often, the time
    positions are further classified into different categories (e.g.,
    downbeat and upbeat positions). To facilitate this classification,
    similar to librosa
    (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
    et al., 2015</xref>), the TSE module allows for generating
    distinguishable stimuli with different ``colorations’’ that can be
    easily associated with the different categories. Additionally, the
    TSE module enables the playback of pre-recorded stimuli at different
    relative time positions and, if specified by suitable parameter
    settings, with time–scale modifications and pitch shifting.</p>
  </sec>
  <sec id="fundamental-frequency">
    <title>Fundamental Frequency
    (<monospace>libsoni.f0</monospace>)</title>
    <p>When describing a specific song, we often have the ability to
    sing or hum the main melody, which can be loosely defined as a
    linear succession of musical tones expressing a particular musical
    idea. In the context of a music recording (rather than a musical
    score), the melody corresponds to a sequence of fundamental
    frequency values (also called F0 values) representing the pitches of
    the tones. In real performances, these sequences often form complex
    time–frequency patterns known as frequency trajectories, which may
    include continuous frequency glides (glissando) or frequency
    modulations (vibrato). In libsoni, the F0 module allows for
    sonifying a sequence of frame-wise frequency values that correspond
    to manually annotated or estimated F0 values (see also
    <xref alt="[fig:libsoni]" rid="figU003Alibsoni">[fig:libsoni]</xref>b).
    This module offers a variety of adjustable parameters, allowing for
    the inclusion of additional partials to tonally enrich the
    sonification, thereby generating sounds of different timbre.
    Moreover, users have the option to adjust the amplitude of each
    predicted F0 value based on its confidence level, as provided by an
    F0 estimator. This allows for insights into the reliability of the
    predictions.</p>
  </sec>
  <sec id="piano-roll-representations">
    <title>Piano-Roll Representations
    (<monospace>libsoni.pianoroll</monospace>)</title>
    <p>A symbolic score-based representation describes each note by
    parameters such as start time, duration, pitch, and other
    attributes. This representation is closely related to MIDI encodings
    and is often visualized in the form of two-dimensional piano-roll
    representations (see also
    <xref alt="[fig:libsoni]" rid="figU003Alibsoni">[fig:libsoni]</xref>c).
    In these representations, time is encoded on the horizontal axis,
    pitch on the vertical axis, and each note is represented by an
    axis-parallel rectangle indicating onset, pitch, and duration. This
    representation is widely used in several MIR tasks, including
    automatic music transcription
    (<xref alt="Benetos et al., 2019" rid="ref-BenetosDDE19_MusicTranscription_SPM" ref-type="bibr">Benetos
    et al., 2019</xref>) and music score–audio music synchronization
    (<xref alt="Müller, 2015" rid="ref-Mueller15_FMP_SPRINGER" ref-type="bibr">Müller,
    2015</xref>). The simplest method in libsoni to sonify piano-roll
    representations is based on straightforward sinusoidal models
    (potentially enriched by harmonics). When the score information is
    synchronized with a music recording (e.g., using alignment methods
    provided by the Sync Toolbox,
    <xref alt="Müller et al., 2021" rid="ref-MuellerOKPD21_SyncToolbox_JOSS" ref-type="bibr">Müller
    et al., 2021</xref>), libsoni enables the creation of a stereo
    signal with the sonification in one channel and the original
    recording in the other channel. This setup provides an intuitive way
    to understand the accuracy for a range of musical analysis and
    transcription tasks. Furthermore, these sonifications may be
    superimposed with further onset-based stimuli provided by the TSE
    module.</p>
  </sec>
  <sec id="chromagram-representations">
    <title>Chromagram Representations
    (<monospace>libsoni.chroma</monospace>)</title>
    <p>Humans perceive pitch in a periodic manner, meaning that pitches
    separated by an octave are perceived as having a similar quality or
    acoustic color, known as chroma. This concept motivates the use of
    time–chroma representations or chromagrams, where pitch bands that
    differ spectrally by one or several octaves are combined to form a
    single chroma band
    (<xref alt="Müller &amp; Ewert, 2011" rid="ref-MuellerEwert11_ChromaToolbox_ISMIR" ref-type="bibr">Müller
    &amp; Ewert, 2011</xref>). These representations capture tonal
    information related to harmony and melody while exhibiting a high
    degree of invariance with respect to timbre and instrumentation.
    Chromagrams are widely used in MIR research for various tasks,
    including chord recognition and structure analysis. The Chroma
    module of libsoni provides sonification methods for chromagrams
    based on Shepard tones. These tones are weighted combinations of
    sinusoids separated by octaves and serve as acoustic counterparts to
    chroma values. The functions offered by libsoni enable the
    generation of various Shepard tone variants and can be applied to
    symbolic representations (such as piano roll representations or
    chord annotations) or to chroma features extracted from music
    recordings. This facilitates deeper insights for listeners into
    chord recognition results or the harmony-related tonal information
    contained within an audio signal.</p>
  </sec>
  <sec id="spectrogram-representations">
    <title>Spectrogram Representations
    (<monospace>libsoni.spectrogram</monospace>)</title>
    <p>Similar to chromagrams, pitch-based feature representations can
    be derived directly from music recordings using transforms such as
    the constant-Q-transform (CQT), see
    (<xref alt="Schörkhuber &amp; Klapuri, 2010" rid="ref-SchoerkhuberK10_ConstantQTransform_SMC" ref-type="bibr">Schörkhuber
    &amp; Klapuri, 2010</xref>). These representations are a special
    type of log-frequency spectrograms, where the frequency axis is
    logarithmically spaced to form a pitch-based axis. More generally,
    in audio signal processing, there exists a multitude of different
    time–frequency representations. For example, classic spectrograms
    have a linear frequency axis, usually computed via the short-time
    Fourier transform (STFT). Additionally, mel-frequency spectrograms
    utilize the mel scale, which approximates the human auditory
    system’s response to different frequencies. The Spectrogram module
    of libsoni is designed to sonify various types of spectrograms with
    frequency axes spaced according to linear, logarithmic, or mel
    scales. Essentially, each point on the scale corresponds to a
    specific center frequency, meaning that each row of the spectrogram
    represents the energy profile of a specific frequency over time. Our
    sonification approach generates sinusoids for each center frequency
    value with time-varying amplitude values, in accordance with the
    provided energy profiles, and then superimposes all these sinusoids.
    Transforming spectrogram-like representations into an auditory
    experience, our sonification approach allows for a more intuitive
    understanding of the frequency and energy characteristics within a
    given music recording. Finally, we would like to emphasize that
    sonifying each time–frequency bin is computationally expensive. To
    enhance efficiency, we integrated additional functions that employs
    multiprocessing, alongside a simpler function that uses a for
    loop.</p>
  </sec>
</sec>
<sec id="design-choices">
  <title>Design Choices</title>
  <p>When designing the Python toolbox libsoni, we had several
  objectives in mind. Firstly, we aimed to maintain close connections
  with existing sonification methods provided in McFee et al.
  (<xref alt="2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">2015</xref>)
  and libfmp
  (<xref alt="Müller &amp; Zalkow, 2021" rid="ref-MuellerZ21_libfmp_JOSS" ref-type="bibr">Müller
  &amp; Zalkow, 2021</xref>). Secondly, we re-implemented and included
  all necessary components (e.g., sound generators based on sinusoidal
  models and click sounds), even though similar basic functionality is
  available in librosa and libfmp. By doing so, libsoni offers a
  coherent API along with convenient but easily modifiable parameter
  presets. Thirdly, we adopted many design principles suggested by
  librosa
  (<xref alt="McFee et al., 2015" rid="ref-McFeeRLEMBN15_librosa_Python" ref-type="bibr">McFee
  et al., 2015</xref>) and detailed in
  (<xref alt="McFee et al., 2019" rid="ref-McFeeKCSBB19_OpenSourcePractices_IEEE-SPM" ref-type="bibr">McFee
  et al., 2019</xref>) to lower the entry barrier for students and
  researchers who may not be coding experts. This includes maintaining
  an explicit and straightforward programming style with a flat,
  functional hierarchy to facilitate ease of use and comprehension. The
  source code for libsoni, along with comprehensive API
  documentation<xref ref-type="fn" rid="fn2">2</xref>, is publicly
  accessible through a dedicated GitHub
  repository<xref ref-type="fn" rid="fn3">3</xref>. We showcase all
  components, including introductions to MIR scenarios, illustrations,
  and sound examples via Jupyter notebooks. Finally, we have included
  the toolbox in the Python Package Index (PyPI), enabling installation
  with the standard Python package manager,
  pip<xref ref-type="fn" rid="fn4">4</xref>.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The libsoni package originated from collaboration with various
  individuals over the past years. We express our gratitude to former
  and current students, collaborators, and colleagues, including
  Jonathan Driedger, Angel Villar-Corrales, and Tim Zunner, for their
  support and influence in creating this Python package. This work was
  funded by the Deutsche Forschungsgemeinschaft (DFG, German Research
  Foundation) under Grant No. 500643750 (DFG-MU 2686/15-1) and Grant No.
  328416299 (MU 2686/10-2). The International Audio Laboratories
  Erlangen are a joint institution of the
  Friedrich-Alexander-Universität Erlangen-Nürnberg (FAU) and Fraunhofer
  Institute for Integrated Circuits IIS.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-RosenzweigSM22_libf0_ISMIR-LBD">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rosenzweig</surname><given-names>Sebastian</given-names></name>
        <name><surname>Schwär</surname><given-names>Simon</given-names></name>
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
      </person-group>
      <article-title>libf0: A Python library for fundamental frequency estimation</article-title>
      <source>Demos and late breaking news of the international society for music information retrieval conference (ISMIR)</source>
      <publisher-loc>Bengaluru, India</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.7512227</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-MuellerOKPD21_SyncToolbox_JOSS">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        <name><surname>Özer</surname><given-names>Yigitcan</given-names></name>
        <name><surname>Krause</surname><given-names>Michael</given-names></name>
        <name><surname>Prätzlich</surname><given-names>Thomas</given-names></name>
        <name><surname>Driedger</surname><given-names>Jonathan</given-names></name>
      </person-group>
      <article-title>Sync Toolbox: A Python package for efficient, robust, and accurate music synchronization</article-title>
      <source>Journal of Open Source Software (JOSS)</source>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>64</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03434</pub-id>
      <fpage>3434:1</fpage>
      <lpage>4</lpage>
    </element-citation>
  </ref>
  <ref id="ref-MuellerZ21_libfmp_JOSS">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        <name><surname>Zalkow</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>libfmp: A Python package for fundamentals of music processing</article-title>
      <source>Journal of Open Source Software (JOSS)</source>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>63</issue>
      <pub-id pub-id-type="doi">10.21105/joss.03326</pub-id>
      <fpage>3326:1</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BenetosDDE19_MusicTranscription_SPM">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Benetos</surname><given-names>Emmanouil</given-names></name>
        <name><surname>Dixon</surname><given-names>Simon</given-names></name>
        <name><surname>Duan</surname><given-names>Zhiyao</given-names></name>
        <name><surname>Ewert</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Automatic music transcription: An overview</article-title>
      <source>IEEE Signal Processing Magazine</source>
      <year iso-8601-date="2019">2019</year>
      <volume>36</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/MSP.2018.2869928</pub-id>
      <fpage>20</fpage>
      <lpage>30</lpage>
    </element-citation>
  </ref>
  <ref id="ref-McFeeKCSBB19_OpenSourcePractices_IEEE-SPM">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Kim</surname><given-names>Jong Wook</given-names></name>
        <name><surname>Cartwright</surname><given-names>Mark</given-names></name>
        <name><surname>Salamon</surname><given-names>Justin</given-names></name>
        <name><surname>Bittner</surname><given-names>Rachel M.</given-names></name>
        <name><surname>Bello</surname><given-names>Juan Pablo</given-names></name>
      </person-group>
      <article-title>Open-source practices for music signal processing research: Recommendations for transparent, sustainable, and reproducible audio research</article-title>
      <source>IEEE Signal Processing Magazine</source>
      <year iso-8601-date="2019">2019</year>
      <volume>36</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/MSP.2018.2875349</pub-id>
      <fpage>128</fpage>
      <lpage>137</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BoeckKSKW16_madmom_ACM-MM">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Böck</surname><given-names>Sebastian</given-names></name>
        <name><surname>Korzeniowski</surname><given-names>Filip</given-names></name>
        <name><surname>Schlüter</surname><given-names>Jan</given-names></name>
        <name><surname>Krebs</surname><given-names>Florian</given-names></name>
        <name><surname>Widmer</surname><given-names>Gerhard</given-names></name>
      </person-group>
      <article-title>Madmom: A new Python audio and music signal processing library</article-title>
      <source>Proceedings of the ACM international conference on multimedia (ACM-MM)</source>
      <publisher-loc>Amsterdam, The Netherlands</publisher-loc>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1145/2964284.2973795</pub-id>
      <fpage>1174</fpage>
      <lpage>1178</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Mueller15_FMP_SPRINGER">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
      </person-group>
      <source>Fundamentals of music processing – audio, analysis, algorithms, applications</source>
      <publisher-name>Springer Verlag</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <isbn>978-3-319-21944-8</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-319-21945-5</pub-id>
      <fpage>1</fpage>
      <lpage>480</lpage>
    </element-citation>
  </ref>
  <ref id="ref-McFeeRLEMBN15_librosa_Python">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Raffel</surname><given-names>Colin</given-names></name>
        <name><surname>Liang</surname><given-names>Dawen</given-names></name>
        <name><surname>Ellis</surname><given-names>Daniel P. W.</given-names></name>
        <name><surname>McVicar</surname><given-names>Matt</given-names></name>
        <name><surname>Battenberg</surname><given-names>Eric</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
      </person-group>
      <article-title>Librosa: Audio and music signal analysis in Python</article-title>
      <source>Proceedings the python science conference</source>
      <publisher-loc>Austin, Texas, USA</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-003</pub-id>
      <fpage>18</fpage>
      <lpage>25</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BogdanovWGGHMRSZS13_essentia_ISMIR">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bogdanov</surname><given-names>Dmitry</given-names></name>
        <name><surname>Wack</surname><given-names>Nicolas</given-names></name>
        <name><surname>Gómez</surname><given-names>Emilia</given-names></name>
        <name><surname>Gulati</surname><given-names>Sankalp</given-names></name>
        <name><surname>Herrera</surname><given-names>Perfecto</given-names></name>
        <name><surname>Mayor</surname><given-names>Oscar</given-names></name>
        <name><surname>Roma</surname><given-names>Gerard</given-names></name>
        <name><surname>Salamon</surname><given-names>Justin</given-names></name>
        <name><surname>Zapata</surname><given-names>José R.</given-names></name>
        <name><surname>Serra</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>Essentia: An audio analysis library for music information retrieval</article-title>
      <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
      <publisher-loc>Curitiba, Brazil</publisher-loc>
      <year iso-8601-date="2013">2013</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.1415016</pub-id>
      <fpage>493</fpage>
      <lpage>498</lpage>
    </element-citation>
  </ref>
  <ref id="ref-MuellerEwert11_ChromaToolbox_ISMIR">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
        <name><surname>Ewert</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Chroma Toolbox: MATLAB implementations for extracting variants of chroma-based audio features</article-title>
      <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
      <publisher-loc>Miami, Florida, USA</publisher-loc>
      <year iso-8601-date="2011">2011</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.1416032</pub-id>
      <fpage>215</fpage>
      <lpage>220</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GroscheM11_TempogramToolbox_ISMIR-lateBreaking">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Grosche</surname><given-names>Peter</given-names></name>
        <name><surname>Müller</surname><given-names>Meinard</given-names></name>
      </person-group>
      <article-title>Tempogram Toolbox: MATLAB tempo and pulse analysis of music recordings</article-title>
      <source>Demos and late breaking news of the international society for music information retrieval conference (ISMIR)</source>
      <publisher-loc>Miami, Florida, USA</publisher-loc>
      <year iso-8601-date="2011">2011</year>
    </element-citation>
  </ref>
  <ref id="ref-SchoerkhuberK10_ConstantQTransform_SMC">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schörkhuber</surname><given-names>Christian</given-names></name>
        <name><surname>Klapuri</surname><given-names>Anssi P.</given-names></name>
      </person-group>
      <article-title>Constant-Q transform toolbox for music processing</article-title>
      <source>Proceedings of the sound and music computing conference (SMC)</source>
      <publisher-loc>Barcelona, Spain</publisher-loc>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.849741</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Tzanetakis09_MARSYAS_ACM-MM">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tzanetakis</surname><given-names>George</given-names></name>
      </person-group>
      <article-title>Music analysis, retrieval and synthesis of audio signals MARSYAS</article-title>
      <source>Proceedings of the ACM international conference on multimedia (ACM-MM)</source>
      <publisher-loc>Vancouver, British Columbia, Canada</publisher-loc>
      <year iso-8601-date="2009">2009</year>
      <pub-id pub-id-type="doi">10.1145/1631272.1631459</pub-id>
      <fpage>931</fpage>
      <lpage>932</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LartillotT07_MirToolbox_ISMIR">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lartillot</surname><given-names>Olivier</given-names></name>
        <name><surname>Toiviainen</surname><given-names>Petri</given-names></name>
      </person-group>
      <article-title>MIR in MATLAB (II): A toolbox for musical feature extraction from audio</article-title>
      <source>Proceedings of the international society for music information retrieval conference (ISMIR)</source>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2007">2007</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.1417145</pub-id>
      <fpage>127</fpage>
      <lpage>130</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KramerEtAl99SonificAR">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Kramer</surname><given-names>Gregory</given-names></name>
        <name><surname>Walker</surname><given-names>Bruce</given-names></name>
        <name><surname>Bonebright</surname><given-names>Terri</given-names></name>
        <name><surname>Cook</surname><given-names>Perry</given-names></name>
        <name><surname>Flowers</surname><given-names>John H.</given-names></name>
        <name><surname>Miner</surname><given-names>Nadine</given-names></name>
        <name><surname>Neuhoff</surname><given-names>John</given-names></name>
      </person-group>
      <source>Sonification report: Status of the field and research agenda</source>
      <publisher-name>International Community for Auditory Display</publisher-name>
      <year iso-8601-date="1999">1999</year>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://groupmm.github.io/libsoni">https://groupmm.github.io/libsoni</ext-link></p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://groupmm.github.io/libsoni">https://groupmm.github.io/libsoni</ext-link></p>
  </fn>
  <fn id="fn3">
    <label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/groupmm/libsoni">https://github.com/groupmm/libsoni</ext-link></p>
  </fn>
  <fn id="fn4">
    <label>4</label><p><ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/libsoni">https://pypi.org/project/libsoni</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
