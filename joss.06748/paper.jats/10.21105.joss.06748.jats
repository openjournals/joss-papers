<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6748</article-id>
<article-id pub-id-type="doi">10.21105/joss.06748</article-id>
<title-group>
<article-title>GATree: Evolutionary decision tree classifier in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-9689-2991</contrib-id>
<name>
<surname>Lahovnik</surname>
<given-names>Tadej</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4441-9690</contrib-id>
<name>
<surname>Karakatič</surname>
<given-names>Sašo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Maribor, Maribor, Slovenia</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-03-06">
<day>6</day>
<month>3</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>100</issue>
<fpage>6748</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>genetic algorithm</kwd>
<kwd>evolutionary algorithm</kwd>
<kwd>classifier</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><italic>GATree</italic> is a Python library that simplifies the way
  decision trees are constructed and optimised for classification
  machine learning tasks<xref ref-type="fn" rid="fn1">1</xref>.
  Leveraging the principles of standard genetic algorithms,
  <italic>GATree</italic> allows for the dynamic evolution of decision
  tree structures, providing a flexible and powerful tool for machine
  learning practitioners. Unlike traditional decision tree algorithms
  that follow a deterministic path based on statistical models or
  information theory, <italic>GATree</italic> introduces an evolutionary
  process where selection, mutation, and crossover operations guide the
  development of optimised trees. This method enhances the adaptability
  and performance of decision trees and opens new possibilities for
  addressing complex classification problems. <italic>GATree</italic>
  stands out as a user-friendly, highly customisable solution, enabling
  users to tailor fitness functions and algorithm parameters to meet
  specific project needs, whether in academic research or practical
  applications.</p>
</sec>
<sec id="overview">
  <title>Overview</title>
  <p>At the heart of <italic>GATree</italic>’s methodology lies the
  integration of genetic algorithms with decision tree construction, a
  process inspired by natural evolution
  (<xref alt="Koza, 1990" rid="ref-koza1990concept" ref-type="bibr">Koza,
  1990</xref>). This evolutionary approach begins with the random
  generation of an initial population of decision trees, each evaluated
  for their fitness<xref ref-type="fn" rid="fn2">2</xref> in solving a
  given supervised task on the training data. Fitness evaluation
  typically considers factors such as classification accuracy and tree
  complexity, striving for a balance that rewards both the quality of
  decisions and the generalisability of the decisions
  (<xref alt="Barros et al., 2012" rid="ref-barros2012survey" ref-type="bibr">Barros
  et al., 2012</xref>;
  <xref alt="Bot &amp; Langdon, 2000" rid="ref-bot2000application" ref-type="bibr">Bot
  &amp; Langdon, 2000</xref>).</p>
  <fig>
    <caption><p>Overview of the evolution
    process.<styled-content id="figU003Aga"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./images/ga.png" />
  </fig>
  <p>Following the principles of natural selection, trees that perform
  better are more likely to contribute to the next generation, either
  through direct selection or by producing offspring via crossover and
  mutation operations. Crossover involves the exchange of genetic
  material (i.e., tree nodes or branches) between two parent trees,
  while mutation introduces random changes to a tree’s structure,
  promoting genetic diversity within the population. This iterative
  process of selection, crossover, and mutation (presented in
  <xref alt="[fig:ga]" rid="figU003Aga">[fig:ga]</xref>) continues
  across generations, with the algorithm converging towards more
  effective decision tree solutions over time.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The development of decision tree classifiers has long been a focal
  point in machine learning due to their interpretability and efficacy
  in various machine learning tasks. Traditional algorithms, however,
  often fall short when dealing with complex data structures or require
  extensive fine-tuning to avoid overfitting or underfitting.
  <italic>GATree</italic> addresses these challenges by introducing an
  evolutionary approach to decision tree optimisation, allowing for a
  more nuanced exploration of the solution space than is possible with
  conventional methods
  (<xref alt="Karakatič &amp; Podgorelec, 2018" rid="ref-karakativc2018building" ref-type="bibr">Karakatič
  &amp; Podgorelec, 2018</xref>;
  <xref alt="Rivera-Lopez et al., 2022" rid="ref-RIVERALOPEZ2022101006" ref-type="bibr">Rivera-Lopez
  et al., 2022</xref>).</p>
  <p>This evolutionary strategy ensures that <italic>GATree</italic> can
  adaptively fine-tune decision trees, exploring a broader range of
  potential solutions and dynamically adjusting to achieve optimal
  performance. Such flexibility is precious in fields where
  classification tasks are complex, and data can exhibit varied and
  unpredictable patterns. Furthermore, <italic>GATree</italic>’s ability
  to customise fitness functions allows for incorporating
  domain-specific knowledge into the evolutionary process, enhancing the
  relevance and quality of the resulting decision trees.</p>
  <p>Even though there are existing Python libraries that use various
  meta-heuristic approaches to form machine learning tree models (i.e.,
  <italic>gplearn</italic><xref ref-type="fn" rid="fn3">3</xref>,
  <italic>tinyGP</italic><xref ref-type="fn" rid="fn4">4</xref> and
  <italic>TensorGP</italic>
  (<xref alt="Baeta et al., 2021" rid="ref-baeta2021tensorgp" ref-type="bibr">Baeta
  et al., 2021</xref>)), they use symbolic regression and not decision
  trees. In the broader context of machine learning and data mining,
  <italic>GATree</italic> represents a significant advancement, offering
  a novel solution to the limitations of existing libraries. By
  integrating the principles of standard genetic algorithms with
  decision tree construction, <italic>GATree</italic> not only enhances
  the adaptability and performance of these classifiers but also
  provides a rich platform for further research and development in
  evolutionary computing and its applications in machine learning.</p>
  <sec id="architecture">
    <title>Architecture</title>
    <p><italic>GATree</italic> is a Python library with a modular and
    extensible architecture. The package is implemented using two
    classes: <italic>GATree</italic> and <italic>Node</italic>. The
    <italic>GATree</italic> class is responsible for the genetic
    algorithm by utilising operator classes, such as
    <italic>Selection</italic> (with optional elitism),
    <italic>Crossover</italic>, and <italic>Mutation</italic>. The
    <italic>Node</italic> class handles the decision tree structure and
    its operations, such as tree generation, tree evaluation, and class
    prediction.</p>
    <p>The library is user-friendly and highly customisable - users can
    easily define custom fitness
    functions<xref ref-type="fn" rid="fn5">5</xref> and other parameters
    to meet their needs. It is implemented to be compatible with the
    de-facto standard <italic>scikit-learn</italic> machine learning
    library; thus, the main methods of use (i.e., <italic>fit()</italic>
    and <italic>predict()</italic>) are present in
    <italic>GATree</italic>.</p>
  </sec>
  <sec id="usage-and-customisation">
    <title>Usage and customisation</title>
    <p>The following example shows how to perform classification of the
    <italic>iris</italic> dataset using the <italic>GATree</italic>
    library.</p>
    <code language="python">import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from gatree import GATree

# Load the iris dataset
iris = load_iris()
X = pd.DataFrame(iris.data, columns=iris.feature_names)
y = pd.Series(iris.target, name='target')

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=10)

# Create and fit the GATree classifier
gatree = GATree(n_jobs=16, random_state=32)
gatree.fit(X=X_train, y=y_train, population_size=100, max_iter=100)

# Make predictions on the testing set
y_pred = gatree.predict(X_test)

# Evaluate the accuracy of the classifier
acc = accuracy_score(y_test, y_pred)</code>
    <p>In this example, we load the iris dataset and split it into
    training and testing sets. Next, we create an instance of the
    <italic>GATree</italic> classifier and define its parameters, such
    as the number of jobs to run in parallel and the random state for
    reproducibility. We then fit the classifier to the training data
    using a population size of 100 and a maximum of 100 iterations.
    Finally, we make predictions on the testing set and evaluate the
    accuracy of the classifier. The <italic>GATree</italic> classifier
    uses a genetic algorithm to evolve and optimise the decision tree
    structure for the classification task. This configuration achieves
    an accuracy of 100% on the testing set, demonstrating the
    effectiveness of GATree for classification tasks.</p>
    <fig>
      <caption><p>Average fitness value and best fitness value at each
      iteration of the genetic algorithm for the iris
      dataset.<styled-content id="figU003Afitness_plot"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./images/fitness_value.png" />
    </fig>
    <p><xref alt="[fig:fitness_plot]" rid="figU003Afitness_plot">[fig:fitness_plot]</xref>
    provides a comprehensive visualisation of the genetic algorithm’s
    progress on the <italic>iris</italic> dataset. The line graph on the
    left showcases the average fitness
    value<xref ref-type="fn" rid="fn6">6</xref> of each decision tree in
    the population across iterations, offering insight into the
    algorithm’s overall performance over time. We can observe the most
    significant improvement in the average fitness value in the first 50
    iterations. We can see a slight decline in average fitness values
    after the 50th iteration, indicating getting stuck in the local
    optimum while building the decision trees. The slight variations in
    the final iterations indicate that the population is still changing
    due to crossover and mutation. However, the average quality of the
    decision trees in the population stays roughly the same. On the
    right half, a similar line graph displays the best fitness
    value<xref ref-type="fn" rid="fn7">7</xref> at each iteration,
    providing a more detailed view of the algorithm’s progress. The
    graph shows that the best fitness value improves rapidly in the
    first 30 iterations. The best decision tree is unaffected by
    evolving local optimums around the 70th iteration as the average
    decision tree does but remains near the global optimum, mainly due
    to the elitism operator.</p>
    <p><xref alt="[fig:decision_tree]" rid="figU003Adecision_tree">[fig:decision_tree]</xref>
    shows the final decision tree built with the <italic>GATree</italic>
    classifier after fitting it to the <italic>iris</italic>
    dataset.</p>
    <fig>
      <caption><p>Final decision tree built with the GATree
      classifier.<styled-content id="figU003Adecision_tree"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./images/decision_tree.png" />
    </fig>
    <p>The fitness function can be customised to suit the specific
    requirements of the classification task. For example, we can define
    a custom fitness function that considers the decision tree’s size,
    penalising larger trees to encourage simplicity and
    interpretability. The following example demonstrates defining and
    using a custom fitness function with the <italic>GATree</italic>
    classifier.</p>
    <code language="python"># Custom fitness function
def fitness_function(root):
    return 1 - accuracy_score(root.y_true, root.y_pred) + (0.05 * root.size())

# Create and fit the GATree classifier
gatree = GATree(fitness_function=fitness_function, n_jobs=16, random_state=10)
gatree.fit(X=X_train, y=y_train, population_size=100, max_iter=100)

# Make predictions on the testing set
y_pred = gatree.predict(X_test)</code>
  </sec>
  <sec id="experiment">
    <title>Experiment</title>
    <p>To test the performance of the <italic>GATree</italic>
    classifier, we conducted a series of experiments on the
    <italic>adult</italic> dataset. The <italic>adult</italic> dataset
    contains 48.842 instances with 14 attributes (e.g., sex, age, native
    country, marital status, education, work class, occupation, etc.).
    The outcome variable is the income level, which is divided into two
    classes: &lt;=50K and &gt;50K (binary outcome, suitable for
    classification tasks). The dataset is imbalanced, with 76% of
    instances belonging to the &lt;=50K class and 24% to the &gt;50K
    class.</p>
    <p>We evaluated the classifier’s accuracy and F1-score across 100
    runs with different parameter settings (see
    <xref alt="Table 1" rid="tabU003Aexperiment_results">Table 1</xref>)
    and compared the results with other classifiers, such as
    <italic>DecisionTreeClassifier</italic><xref ref-type="fn" rid="fn8">8</xref>
    (scikit-learn) and
    <italic>SymbolicClassifier</italic><xref ref-type="fn" rid="fn9">9</xref>
    (gplearn). The <italic>DecisionTreeClassifier</italic> is a standard
    decision tree classifier, while the
    <italic>SymbolicClassifier</italic> is a symbolic regression
    classifier that uses genetic programming to evolve symbolic
    expressions. The code used to conduct the experiment is available in
    the <italic>GATree</italic>
    repository<xref ref-type="fn" rid="fn10">10</xref>.</p>
    <p>The results demonstrate that <italic>GATree</italic> achieves
    competitive performance in terms of accuracy and F1-score.
    <italic>GATree</italic>’s performance improves with more generations
    and higher population sizes, indicating the importance of these
    parameters in the evolutionary process.</p>
    <table-wrap>
      <caption>
        <p>Results of the conducted
        experiment.<styled-content id="tabU003Aexperiment_results"></styled-content></p>
      </caption>
      <table>
        <colgroup>
          <col width="27%" />
          <col width="33%" />
          <col width="19%" />
          <col width="19%" />
        </colgroup>
        <thead>
          <tr>
            <th>Classifier</th>
            <th>Parameters</th>
            <th>Avg. max<break/>accuracy<break/>(95% CI)</th>
            <th align="left">Avg. max<break/>F1-score<break/>(95%
            CI)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>GATree</td>
            <td>mutation_probability=0.10<break/>population_size=25<break/>elite_size=1<break/>max_depth=5<break/>max_iter=50<break/></td>
            <td>0.800<break/>(0.799, 0.801)</td>
            <td align="left">0.351<break/>(0.341, 0.362)</td>
          </tr>
          <tr>
            <td>GATree</td>
            <td>mutation_probability=0.15<break/>population_size=50<break/>elite_size=2<break/>max_depth=5<break/>max_iter=100<break/></td>
            <td>0.807<break/>(0.806, 0.809)</td>
            <td align="left">0.379<break/>(0.368, 0.390)</td>
          </tr>
          <tr>
            <td>GATree</td>
            <td>mutation_probability=0.20<break/>population_size=50<break/>elite_size=5<break/>max_depth=5<break/>max_iter=200<break/></td>
            <td>0.810<break/>(0.808, 0.811)</td>
            <td align="left">0.392<break/>(0.382, 0.403)</td>
          </tr>
          <tr>
            <td>DecisionTreeClassifier</td>
            <td>criterion=‘gini’
            splitter=‘random’<break/>max_depth=5<break/></td>
            <td>0.806<break/>(0.804, 0.806)</td>
            <td align="left">0.451<break/>(0.430, 0.473)</td>
          </tr>
          <tr>
            <td>SymbolicClassifier</td>
            <td>parsimony_coefficient=0.01<break/>population_size=50<break/>generations=100<break/>init_depth=(5,
            5)</td>
            <td>0.739<break/>(0.722, 0.756)</td>
            <td align="left">0.034<break/>(0.013, 0.055)</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-barros2012survey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barros</surname><given-names>Rodrigo Coelho</given-names></name>
        <name><surname>Basgalupp</surname><given-names>Marcio Porto</given-names></name>
        <name><surname>De Carvalho</surname><given-names>ACPLF</given-names></name>
        <name><surname>Freitas</surname><given-names>Alex Alves</given-names></name>
      </person-group>
      <article-title>A survey of evolutionary algorithms for decision-tree induction</article-title>
      <source>Systems, Man, and Cybernetics, Part C: Applications and Reviews, IEEE Transactions on</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>42</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/TSMCC.2011.2157494</pub-id>
      <fpage>291</fpage>
      <lpage>312</lpage>
    </element-citation>
  </ref>
  <ref id="ref-RIVERALOPEZ2022101006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rivera-Lopez</surname><given-names>Rafael</given-names></name>
        <name><surname>Canul-Reich</surname><given-names>Juana</given-names></name>
        <name><surname>Mezura-Montes</surname><given-names>Efrén</given-names></name>
        <name><surname>Cruz-Chávez</surname><given-names>Marco Antonio</given-names></name>
      </person-group>
      <article-title>Induction of decision trees as classification models through metaheuristics</article-title>
      <source>Swarm and Evolutionary Computation</source>
      <year iso-8601-date="2022">2022</year>
      <volume>69</volume>
      <issn>2210-6502</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S2210650221001681</uri>
      <pub-id pub-id-type="doi">10.1016/j.swevo.2021.101006</pub-id>
      <fpage>101006</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-bot2000application">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bot</surname><given-names>Martijn CJ</given-names></name>
        <name><surname>Langdon</surname><given-names>William B</given-names></name>
      </person-group>
      <article-title>Application of genetic programming to induction of linear classification trees</article-title>
      <source>Genetic programming: European conference, EuroGP 2000, edinburgh, scotland, UK, april 15-16, 2000. Proceedings 3</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2000">2000</year>
      <pub-id pub-id-type="doi">10.1007/978-3-540-46239-2_18</pub-id>
      <fpage>247</fpage>
      <lpage>258</lpage>
    </element-citation>
  </ref>
  <ref id="ref-koza1990concept">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Koza</surname><given-names>John R</given-names></name>
      </person-group>
      <article-title>Concept formation and decision tree induction using the genetic programming paradigm</article-title>
      <source>International conference on parallel problem solving from nature</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="1990">1990</year>
      <pub-id pub-id-type="doi">10.1007/BFb0029742</pub-id>
      <fpage>124</fpage>
      <lpage>128</lpage>
    </element-citation>
  </ref>
  <ref id="ref-karakativc2018building">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Karakatič</surname><given-names>Sašo</given-names></name>
        <name><surname>Podgorelec</surname><given-names>Vili</given-names></name>
      </person-group>
      <article-title>Building boosted classification tree ensemble with genetic programming</article-title>
      <source>Proceedings of the genetic and evolutionary computation conference companion</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1145/3205651.3205774</pub-id>
      <fpage>165</fpage>
      <lpage>166</lpage>
    </element-citation>
  </ref>
  <ref id="ref-baeta2021tensorgp">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Baeta</surname><given-names>Francisco</given-names></name>
        <name><surname>Correia</surname><given-names>João</given-names></name>
        <name><surname>Martins</surname><given-names>Tiago</given-names></name>
        <name><surname>Machado</surname><given-names>Penousal</given-names></name>
      </person-group>
      <article-title>TensorGP – genetic programming engine in TensorFlow</article-title>
      <source>Applications of evolutionary computation - 24th international conference, EvoApplications 2021</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-72699-7_48</pub-id>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><italic>GATree</italic> is limited to
    classification tasks, with support for regression tasks planned for
    future releases.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Fitness is the estimation of the quality of the
    individual decision trees, which determines whether a decision tree
    survives into the next generation or not.</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/trevorstephens/gplearn">https://github.com/trevorstephens/gplearn</ext-link></p>
  </fn>
  <fn id="fn4">
    <label>4</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/moshesipper/tiny_gp">https://github.com/moshesipper/tiny_gp</ext-link></p>
  </fn>
  <fn id="fn5">
    <label>5</label><p>The default fitness function is calculated as the
    combination of accuracy on the test set (preferring better/higher
    accuracy) and the tree size (preferring smaller, more generalisable
    trees).</p>
  </fn>
  <fn id="fn6">
    <label>6</label><p>The average fitness is the actual average value
    of all the fitness values of the entire population.</p>
  </fn>
  <fn id="fn7">
    <label>7</label><p>The best fitness is only the one fitness value -
    the one from the best individual in the population.</p>
  </fn>
  <fn id="fn8">
    <label>8</label><p><ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html">https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html</ext-link></p>
  </fn>
  <fn id="fn9">
    <label>9</label><p><ext-link ext-link-type="uri" xlink:href="https://gplearn.readthedocs.io/en/stable/reference.html#symbolic-classifier">https://gplearn.readthedocs.io/en/stable/reference.html#symbolic-classifier</ext-link></p>
  </fn>
  <fn id="fn10">
    <label>10</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/lahovniktadej/gatree/blob/main/examples/joss_experiment.py">https://github.com/lahovniktadej/gatree/blob/main/examples/joss_experiment.py</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
