<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8621</article-id>
<article-id pub-id-type="doi">10.21105/joss.08621</article-id>
<title-group>
<article-title>HiGP: A high-performance Python package for Gaussian
Processes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1060-5639</contrib-id>
<name>
<surname>Huang</surname>
<given-names>Hua</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3119-1957</contrib-id>
<name>
<surname>Xu</surname>
<given-names>Tianshi</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4720-9915</contrib-id>
<name>
<surname>Xi</surname>
<given-names>Yuanzhe</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0474-3752</contrib-id>
<name>
<surname>Chow</surname>
<given-names>Edmond</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>School of Computational Science and Engineering, Georgia
Institute of Technology, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Mathematics, Emory University,
USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-07-03">
<day>3</day>
<month>7</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>117</issue>
<fpage>8621</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Gaussian Process</kwd>
<kwd>iterative method</kwd>
<kwd>preconditioner</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Gaussian Processes (GPs)
  (<xref alt="Rasmussen &amp; Williams, 2005" rid="ref-RasmussenU003A2005" ref-type="bibr">Rasmussen
  &amp; Williams, 2005</xref>) are flexible, nonparametric Bayesian
  models widely used for regression and classification because of their
  ability to capture complex data patterns and quantify predictive
  uncertainty. However, the <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(n^3)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  computational cost of kernel matrix operations poses a major obstacle
  to applying GPs at scale. HiGP is a high-performance Python package
  designed to overcome these scalability limitations through advanced
  numerical linear algebra and hierarchical kernel representations. It
  integrates <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  matrices to achieve near-linear complexity in both storage and
  computation for spatial datasets, supports on-the-fly kernel
  evaluation to avoid explicit storage in large-scale problems, and
  incorporates a robust Adaptive Factorized Nystr√∂m (AFN) preconditioner
  (<xref alt="Zhao et al., 2024" rid="ref-ZhaoU003A2024" ref-type="bibr">Zhao
  et al., 2024</xref>) that accelerates convergence of iterative solvers
  across a broad range of kernel spectra. These computational kernels
  are implemented in C++ for maximum performance and exposed through
  Python interfaces, enabling seamless integration with modern machine
  learning workflows. HiGP also includes analytically derived gradient
  computations for efficient hyperparameter optimization, avoiding the
  inefficiencies of automatic differentiation in iterative solvers. By
  serving as a reusable numerical engine, HiGP complements existing GP
  frameworks such as GPJax
  (<xref alt="Pinder &amp; Dodd, 2022" rid="ref-PinderU003A2022" ref-type="bibr">Pinder
  &amp; Dodd, 2022</xref>), KeOps
  (<xref alt="Charlier et al., 2021" rid="ref-CharlierU003A2021" ref-type="bibr">Charlier
  et al., 2021</xref>), and GaussianProcesses.jl
  (<xref alt="Fairbrother et al., 2022" rid="ref-FairbrotherU003A2022" ref-type="bibr">Fairbrother
  et al., 2022</xref>), providing a reliable and scalable computational
  backbone for large-scale Gaussian Process regression and
  classification.</p>
</sec>
<sec id="gaussian-processes">
  <title>Gaussian Processes</title>
  <p>For training points <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X} \in \mathbb{R}^{n \times d}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùêó</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>√ó</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  a noisy training observation set <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{y} \in \mathbb{R}^{n}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùê≤</mml:mi><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>n</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  and testing points <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}_\ast \in \mathbb{R}^{m \times d}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>ùêó</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>,
  a standard GP model assumes that the noise-free testing observations
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{y}_\ast \in \mathbb{R}^{m}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>ùê≤</mml:mi><mml:mo>*</mml:mo></mml:msub><mml:mo>‚àà</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>m</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  follow a joint Gaussian distribution that depends on a set of
  parameters, including scale <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>,
  noise level <inline-formula><alternatives>
  <tex-math><![CDATA[s]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>s</mml:mi></mml:math></alternatives></inline-formula>,
  and kernel parameters <inline-formula><alternatives>
  <tex-math><![CDATA[l]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>l</mml:mi></mml:math></alternatives></inline-formula>.
  The GP model finds the optimal parameters
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Theta:=(s,f,l)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œò</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  by minimizing the negative log marginal likelihood:
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  L(\Theta) = \frac{1}{2} \left( \mathbf{y}^{\top} \widehat{\mathbf{K}}^{-1} \mathbf{y} 
  + \log|\widehat{\mathbf{K}}| + n\log 2\pi \right),
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œò</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùê≤</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ùê≤</mml:mi><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>log</mml:mo><mml:mn>2</mml:mn><mml:mi>œÄ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\widehat{\mathbf{K}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
  denotes the regularized kernel matrix. An optimization process usually
  requires the gradient of <inline-formula><alternatives>
  <tex-math><![CDATA[L(\Theta)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œò</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:
  <disp-formula><alternatives>
  <tex-math><![CDATA[
  \frac{\partial L}{\partial \theta} =
  \frac{1}{2} \left(-\mathbf{y}^{\top} \widehat{\mathbf{K}}^{-1} 
  \frac{\partial \widehat{\mathbf{K}}}{\partial \theta} \widehat{\mathbf{K}}^{-1}\mathbf{y} +
  \text{tr}{\left( \widehat{\mathbf{K}}^{-1} \frac{\partial \widehat{\mathbf{K}}}{\partial \theta} \right)}\right),
  \quad \theta \in \Theta.
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>‚àí</mml:mi><mml:msup><mml:mi>ùê≤</mml:mi><mml:mi>‚ä§</mml:mi></mml:msup><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ùê≤</mml:mi><mml:mo>+</mml:mo><mml:mtext mathvariant="normal">tr</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mi>Œ∏</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>Œò</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  Using preconditioned iterative methods with preconditioner
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{M} \approx \widehat{\mathbf{K}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùêå</mml:mi><mml:mo>‚âà</mml:mo><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>
  is a common option
  (<xref alt="Aune et al., 2014" rid="ref-AuneU003A2014" ref-type="bibr">Aune
  et al., 2014</xref>;
  <xref alt="Chen et al., 2023" rid="ref-ChenU003A2023" ref-type="bibr">Chen
  et al., 2023</xref>;
  <xref alt="Hensman et al., 2013" rid="ref-HensmanU003A2013" ref-type="bibr">Hensman
  et al., 2013</xref>;
  <xref alt="Pleiss et al., 2018" rid="ref-PleissU003A2018" ref-type="bibr">Pleiss
  et al., 2018</xref>;
  <xref alt="Wenger et al., 2022" rid="ref-WengerU003A2022" ref-type="bibr">Wenger
  et al., 2022</xref>;
  <xref alt="Wilson et al., 2015" rid="ref-WilsonU003A2015" ref-type="bibr">Wilson
  et al., 2015</xref>;
  <xref alt="Zhang et al., 2024" rid="ref-ZhangU003A2024" ref-type="bibr">Zhang
  et al., 2024</xref>). In this approach, <inline-formula><alternatives>
  <tex-math><![CDATA[\widehat{\mathbf{K}}^{-1}\mathbf{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>ùê≤</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is approximated via the preconditioned conjugate gradient (PCG) method
  (<xref alt="Saad, 2003" rid="ref-SaadU003A2003" ref-type="bibr">Saad,
  2003</xref>). To handle the logarithmic determinant and trace terms,
  they are first rewritten as <disp-formula><alternatives>
  <tex-math><![CDATA[
  \log|\widehat{\mathbf{K}}|=\log|\mathbf{M}| + \log|\mathbf{M}^{-1/2}\widehat{\mathbf{K}}\mathbf{M}^{-1/2}|, \tag{1}
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>ùêå</mml:mi><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:msup><mml:mi>ùêå</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:msup><mml:mi>ùêå</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p><named-content id="eqU003Atrace" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[
  \text{tr}{(\widehat{\mathbf{K}}^{-1} \frac{\partial \widehat{\mathbf{K}}}{\partial \theta})}={\text{tr}}\left({{\mathbf{M}}^{-1}}\frac{\partial {\mathbf{M}}}{\partial \theta}\right)+
   {\text{tr}}\left({\widehat{\mathbf{K}}^{-1}}\frac{\partial \widehat{\mathbf{K}}}{\partial \theta}-{{\mathbf{M}}^{-1}}\frac{\partial {\mathbf{M}}}{\partial \theta}\right). \tag{2}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">tr</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mtext mathvariant="normal">tr</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>ùêå</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>ùêå</mml:mi></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mtext mathvariant="normal">tr</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mover><mml:mi>ùêä</mml:mi><mml:mo accent="true">ÃÇ</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo>‚àí</mml:mo><mml:msup><mml:mi>ùêå</mml:mi><mml:mrow><mml:mi>‚àí</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mfrac><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>ùêå</mml:mi></mml:mrow><mml:mrow><mml:mi>‚àÇ</mml:mi><mml:mi>Œ∏</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></named-content>
  The second component of each new expression is then estimated using
  the stochastic Lanczos quadrature
  (<xref alt="Ubaru et al., 2017" rid="ref-UbaruU003A2017" ref-type="bibr">Ubaru
  et al., 2017</xref>) and the Hutchinson estimator
  (<xref alt="Hutchinson, 1989" rid="ref-HutchinsonU003A1989" ref-type="bibr">Hutchinson,
  1989</xref>;
  <xref alt="Meyer et al., 2021" rid="ref-MeyerU003A2021" ref-type="bibr">Meyer
  et al., 2021</xref>), respectively.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>The Gaussian Process (GP) community has advanced rapidly in recent
  years, developing scalable inference frameworks and more efficient
  kernel representations. Modern libraries such as GPyTorch
  (<xref alt="Gardner et al., 2018" rid="ref-GardnerU003A2018" ref-type="bibr">Gardner
  et al., 2018</xref>), GPflow
  (<xref alt="Matthews et al., 2017" rid="ref-GPflowU003A2017" ref-type="bibr">Matthews
  et al., 2017</xref>;
  <xref alt="van der Wilk et al., 2020" rid="ref-GPflowU003A2020" ref-type="bibr">van
  der Wilk et al., 2020</xref>), GPJax
  (<xref alt="Pinder &amp; Dodd, 2022" rid="ref-PinderU003A2022" ref-type="bibr">Pinder
  &amp; Dodd, 2022</xref>), KeOps
  (<xref alt="Charlier et al., 2021" rid="ref-CharlierU003A2021" ref-type="bibr">Charlier
  et al., 2021</xref>), and GaussianProcesses.jl
  (<xref alt="Fairbrother et al., 2022" rid="ref-FairbrotherU003A2022" ref-type="bibr">Fairbrother
  et al., 2022</xref>) leverage GPUs and automatic differentiation to
  perform GP inference efficiently on moderately large datasets.
  Concurrently, new algorithms, including preconditioned optimization
  methods
  (<xref alt="Wenger et al., 2022" rid="ref-WengerU003A2022" ref-type="bibr">Wenger
  et al., 2022</xref>), alternating-projection solvers
  (<xref alt="Wu et al., 2024" rid="ref-WuU003A2024" ref-type="bibr">Wu
  et al., 2024</xref>), GPU-accelerated Vecchia approximations for
  spatial data
  (<xref alt="Pan et al., 2024" rid="ref-PanU003A2024" ref-type="bibr">Pan
  et al., 2024</xref>), robust relevance-pursuit inference
  (<xref alt="Ament et al., 2024" rid="ref-AmentU003A2024" ref-type="bibr">Ament
  et al., 2024</xref>), and latent Kronecker formulations for structured
  covariance matrices
  (<xref alt="Lin et al., 2025" rid="ref-LinU003A2025" ref-type="bibr">Lin
  et al., 2025</xref>), have further improved the scalability and
  robustness of GP models. Yet, most existing frameworks emphasize
  modeling flexibility and seamless integration with autodiff
  ecosystems, rather than optimizing the low-level numerical routines
  that dominate runtime for very large or ill-conditioned kernel
  systems. HiGP is designed to address this computational gap by
  focusing on the numerical core of GP inference. It provides robust,
  scalable, and hardware-efficient implementations of kernel algebra,
  preconditioned iterative solvers, and gradient computations, offering
  three primary contributions.</p>
  <p>Firstly, HiGP addresses the efficiency of MatVec, the most
  performance-critical operation in iterative methods. For large 2D or
  3D datasets, the dense kernel matrix is compressed into a
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  matrix
  (<xref alt="Hackbusch et al., 2000" rid="ref-HackbuschU003A2000" ref-type="bibr">Hackbusch
  et al., 2000</xref>;
  <xref alt="Hackbusch &amp; B√∂rm, 2002" rid="ref-HackbuschU003A2002" ref-type="bibr">Hackbusch
  &amp; B√∂rm, 2002</xref>) in HiGP, resulting in
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(n)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  storage and computation costs. For large high-dimensional datasets,
  HiGP computes small kernel matrix blocks on-the-fly and immediately
  uses them in MatVec and discards them, which allows HiGP to handle
  extremely large datasets with a moderate memory size.</p>
  <p>Secondly, HiGP uses iterative solvers with the newly proposed AFN
  preconditioner
  (<xref alt="Zhao et al., 2024" rid="ref-ZhaoU003A2024" ref-type="bibr">Zhao
  et al., 2024</xref>), which is designed for robust preconditioning of
  kernel matrices. Experiments demonstrate that AFN can significantly
  improve the accuracy and robustness of iterative solvers for kernel
  matrix systems. Furthermore, AFN and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  matrix computation rely on evaluating many small kernel matrices in
  parallel, which is easily handled in C++ but would incur large
  overhead in Python, making implementation in other libraries such as
  GPyTorch or GPFlow more challenging.</p>
  <p>Lastly, HiGP uses accurate and efficient hand-coded gradient
  calculations. GPyTorch relies on the automatic differentiation
  (autodiff) provided in PyTorch to calculate gradients
  (<xref alt="Equation¬†1" rid="eqU003Atrace">Equation¬†1</xref>).
  However, autodiff can be inefficient and inaccurate for computing the
  gradient of the preconditioner, so we use hand-coded gradient
  calculations for better performance and accuracy.</p>
</sec>
<sec id="design-and-implementation">
  <title>Design and Implementation</title>
  <p>We implemented HiGP in Python 3 and C++ with the goal of providing
  both a set of ready-to-use out-of-the-box Python interfaces for
  regular users and a set of reusable high-performance shared-memory
  multithreading computational primitives for advanced users. The HiGP
  C++ code implements all performance-critical operations. The HiGP
  Python code wraps the C++ units into four basic Python modules:
  <monospace>krnlmatmodule</monospace> for computing kernel matrices and
  its derivatives, <monospace>precondmodule</monospace> for PCG solver
  with AFN preconditioner, <monospace>gprproblemmodule</monospace> and
  <monospace>gpcproblemmodule</monospace> for computing the the loss and
  gradient for GP regression and classification. The two modules
  <monospace>gprproblemmodule</monospace> and
  <monospace>gpcproblemmodule</monospace> allow a user to train a GP
  model with any gradient-based optimizer.</p>
  <p>We further implemented two high-level modules
  <monospace>GPRModel</monospace> and <monospace>GPCModel</monospace>
  using PyTorch parameter registration and optimizer to simplify the
  training and use of GP models. Listing 1 shows an example of defining
  and training a GP regression and using the trained model for
  prediction.</p>
  <code language="python"># Listing 1: HiGP example code of training and using a GPR model
gprproblem = higp.gprproblem.setup(data=train_x, label=train_y, 
                                   kernel_type=higp.GaussianKernel)
model = higp.GPRModel(gprproblem)
optimizer = torch.optim.Adam(model.parameters(), lr=0.1)
for i in ranges(max_steps):
    loss = model.calc_loss_grad()
    optimizer.step()
params = model.get_params()
pred = higp.gpr_prediction(train_x, train_y, test_x, 
                           higp.GaussianKernel, params)</code>
  <p>We note that the HiGP Python interfaces (except for
  <monospace>GPRModel</monospace> and <monospace>GPCModel</monospace>
  models) are <italic>stateless</italic>. This design aims to simplify
  the interface and decouple different operations. A user can train and
  use different GP models with the same or different data and
  configurations in the same file.</p>
</sec>
<sec id="numerical-experiments">
  <title>Numerical Experiments</title>
  <p>We conducted numerical experiments on an Ubuntu 20.04 LTS machine
  with dual Intel Xeon Gold 6248R CPU (2x12 cores in total). We used
  PyTorch 2.8.0, GPyTorch 1.14, and HiGP version 2025.11.3 for the
  tests.</p>
  <p>We tested two data sets from the UCI Machine Learning Datasets: the
  ‚ÄúBike Sharing‚Äù and the ‚Äú3D Road Network‚Äù data sets. We also tested
  three synthetic target functions from the Virtual Library of
  Simulation Experiments with randomly sampled data points: Rosenbrock,
  Rastrigin, and Branin. All datasets were normalized with Z-score
  normalization (<inline-formula><alternatives>
  <tex-math><![CDATA[\mu=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œº</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma=1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>œÉ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>)
  applied to both features and targets using statistics from the
  training set. The results represent averages over three independent
  runs for statistical reliability. Both HiGP and GPyTorch were
  configured with identical computational budgets to ensure a fair
  comparison. E2E tests use the following settings:</p>
  <list list-type="bullet">
    <list-item>
      <p>Optimizer steps: 50</p>
    </list-item>
    <list-item>
      <p>CG iterations: 20 (training), 50 (prediction)</p>
    </list-item>
    <list-item>
      <p>Preconditioner/AFN rank: 10 (training), 100 (prediction)</p>
    </list-item>
    <list-item>
      <p>Optimizer: Adam with a learning rate of 0.01</p>
    </list-item>
    <list-item>
      <p>Precision: 32-bit floating point (FP32)</p>
    </list-item>
  </list>
  <p>We first compared the end-to-end (E2E) accuracy and performance
  between HiGP and GPyTorch.
  <xref alt="Table¬†1" rid="table_accuracy_test">Table¬†1</xref> shows
  HiGP achieves equivalent GP accuracy compared to GPyTorch, and
  <xref alt="Table¬†2" rid="table_gpytorch_perf_comp">Table¬†2</xref>
  shows HiGP has better performance than GPyTorch under fixed
  computational budget constraints and can handle some large datasets
  that GPyTorch cannot handle.</p>
  <table-wrap>
    <caption>
      <p>HiGP accuracy tests on small data sets
      <styled-content id="table_accuracy_test"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="12%" />
        <col width="12%" />
        <col width="11%" />
        <col width="15%" />
        <col width="23%" />
        <col width="28%" />
      </colgroup>
      <thead>
        <tr>
          <th>Dataset</th>
          <th>n_train</th>
          <th>Kernel</th>
          <th>HiGP Mode</th>
          <th>HiGP Final RMSE</th>
          <th>GPyTorch Final RMSE</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Bike</td>
          <td>3,000</td>
          <td>RBF</td>
          <td>dense</td>
          <td>0.0285</td>
          <td>0.0284</td>
        </tr>
        <tr>
          <td>Rosenbrock (5D)</td>
          <td>3,000</td>
          <td>Matern32</td>
          <td>dense</td>
          <td>0.0603</td>
          <td>0.0658</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap>
    <caption>
      <p>Performance comparison between HiGP and GPyTorch on large
      datasets
      <styled-content id="table_gpytorch_perf_comp"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="13%" />
        <col width="13%" />
        <col width="11%" />
        <col width="16%" />
        <col width="21%" />
        <col width="26%" />
      </colgroup>
      <thead>
        <tr>
          <th>Dataset</th>
          <th>n_train</th>
          <th>Kernel</th>
          <th>HiGP Mode</th>
          <th>HiGP Time (s)</th>
          <th>GPyTorch Time (s)</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Road3D</td>
          <td>50,000</td>
          <td>RBF</td>
          <td>H2</td>
          <td>191.2</td>
          <td>14,739.3</td>
        </tr>
        <tr>
          <td>Road3D</td>
          <td>150,000</td>
          <td>RBF</td>
          <td>H2</td>
          <td>383.5</td>
          <td>‚Äî</td>
        </tr>
        <tr>
          <td>Branin</td>
          <td>50,000</td>
          <td>RBF</td>
          <td>H2</td>
          <td>132.2</td>
          <td>‚Äî</td>
        </tr>
        <tr>
          <td>Branin</td>
          <td>150,000</td>
          <td>RBF</td>
          <td>H2</td>
          <td>262.9</td>
          <td>‚Äî</td>
        </tr>
        <tr>
          <td>Rastrigin (2D)</td>
          <td>30,000</td>
          <td>Matern32</td>
          <td>H2</td>
          <td>100.6</td>
          <td>278.2</td>
        </tr>
        <tr>
          <td>Rastrigin (20D)</td>
          <td>30,000</td>
          <td>Matern32</td>
          <td>on-the-fly</td>
          <td>198.5</td>
          <td>275.6</td>
        </tr>
        <tr>
          <td>Rosenbrock (2D)</td>
          <td>30,000</td>
          <td>RBF</td>
          <td>H2</td>
          <td>82.2</td>
          <td>231.3</td>
        </tr>
        <tr>
          <td>Rosenbrock (20D)</td>
          <td>30,000</td>
          <td>RBF</td>
          <td>on-the-fly</td>
          <td>190.8</td>
          <td>230.6</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>We also tested the parallel strong scaling ability of HiGP.
  <xref alt="Table¬†3" rid="table_scaling_h2">Table¬†3</xref> and
  <xref alt="Table¬†4" rid="table_scaling_dense">Table¬†4</xref> show HiGP
  has a good parallel scalability, and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  matrix results match with the strong scaling results in H2Pack
  (<xref alt="Huang et al., 2020" rid="ref-HuangU003A2020" ref-type="bibr">Huang
  et al., 2020</xref>) since HiGP uses the same
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  matrix parallel algorithms as H2Pack.</p>
  <table-wrap>
    <caption>
      <p>HiGP strong scaling performance test results with the 2D
      Rastrigin data set, Matern32 kernel, and using
      <inline-formula><alternatives>
      <tex-math><![CDATA[\mathcal{H}^2]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>‚Ñã</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
      matrix method
      <styled-content id="table_scaling_h2"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="10%" />
        <col width="14%" />
        <col width="13%" />
        <col width="17%" />
        <col width="14%" />
        <col width="13%" />
        <col width="17%" />
      </colgroup>
      <thead>
        <tr>
          <th>Cores</th>
          <th>Training</th>
          <th></th>
          <th></th>
          <th>Inference</th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td></td>
          <td>Time (s)</td>
          <td>Speedup</td>
          <td>Efficiency</td>
          <td>Time (s)</td>
          <td>Speedup</td>
          <td>Efficiency</td>
        </tr>
        <tr>
          <td>1</td>
          <td>1480.51</td>
          <td>1.00x</td>
          <td>100%</td>
          <td>30.62</td>
          <td>1.00x</td>
          <td>100%</td>
        </tr>
        <tr>
          <td>2</td>
          <td>759.45</td>
          <td>1.95x</td>
          <td>97%</td>
          <td>15.73</td>
          <td>1.95x</td>
          <td>97%</td>
        </tr>
        <tr>
          <td>4</td>
          <td>399.82</td>
          <td>3.70x</td>
          <td>93%</td>
          <td>8.33</td>
          <td>3.67x</td>
          <td>92%</td>
        </tr>
        <tr>
          <td>8</td>
          <td>215.49</td>
          <td>6.87x</td>
          <td>86%</td>
          <td>4.47</td>
          <td>6.85x</td>
          <td>86%</td>
        </tr>
        <tr>
          <td>16</td>
          <td>133.78</td>
          <td>11.07x</td>
          <td>69%</td>
          <td>2.86</td>
          <td>10.71x</td>
          <td>67%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <table-wrap>
    <caption>
      <p>HiGP strong scaling performance test results with the 20-D
      Rastrigin data set, Matern32 kernel, and using dense/on-the-fly
      method
      <styled-content id="table_scaling_dense"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="10%" />
        <col width="14%" />
        <col width="13%" />
        <col width="17%" />
        <col width="14%" />
        <col width="13%" />
        <col width="17%" />
      </colgroup>
      <thead>
        <tr>
          <th>Cores</th>
          <th>Training</th>
          <th></th>
          <th></th>
          <th>Inference</th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td></td>
          <td>Time (s)</td>
          <td>Speedup</td>
          <td>Efficiency</td>
          <td>Time (s)</td>
          <td>Speedup</td>
          <td>Efficiency</td>
        </tr>
        <tr>
          <td>1</td>
          <td>2599.17</td>
          <td>1.00x</td>
          <td>100%</td>
          <td>16.57</td>
          <td>1.00x</td>
          <td>100%</td>
        </tr>
        <tr>
          <td>2</td>
          <td>1411.49</td>
          <td>1.84x</td>
          <td>92%</td>
          <td>9.02</td>
          <td>1.84x</td>
          <td>92%</td>
        </tr>
        <tr>
          <td>4</td>
          <td>744.72</td>
          <td>3.49x</td>
          <td>87%</td>
          <td>5.03</td>
          <td>3.29x</td>
          <td>82%</td>
        </tr>
        <tr>
          <td>8</td>
          <td>398.91</td>
          <td>6.52x</td>
          <td>81%</td>
          <td>2.80</td>
          <td>5.92x</td>
          <td>74%</td>
        </tr>
        <tr>
          <td>16</td>
          <td>229.92</td>
          <td>11.30x</td>
          <td>71%</td>
          <td>1.76</td>
          <td>9.41x</td>
          <td>59%</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The research of Y. Xi is supported by NSF awards DMS-2338904. The
  research of H. Huang and E. Chow is supported by NSF award
  OAC-2003683.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-AmentU003A2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ament</surname><given-names>Sebastian</given-names></name>
        <name><surname>Santorella</surname><given-names>Elizabeth</given-names></name>
        <name><surname>Eriksson</surname><given-names>David</given-names></name>
        <name><surname>Letham</surname><given-names>Benjamin</given-names></name>
        <name><surname>Balandat</surname><given-names>Maximilian</given-names></name>
        <name><surname>Bakshy</surname><given-names>Eytan</given-names></name>
      </person-group>
      <article-title>Robust Gaussian processes via relevance pursuit</article-title>
      <source>The thirty-eighth annual conference on neural information processing systems</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://dl.acm.org/doi/10.5555/3737916.3739888</uri>
    </element-citation>
  </ref>
  <ref id="ref-AuneU003A2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Aune</surname><given-names>Erlend</given-names></name>
        <name><surname>Simpson</surname><given-names>Daniel P.</given-names></name>
        <name><surname>Eidsvik</surname><given-names>Jo</given-names></name>
      </person-group>
      <article-title>Parameter estimation in high dimensional Gaussian distributions</article-title>
      <source>Statistics and Computing</source>
      <year iso-8601-date="2014">2014</year>
      <volume>24</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1007/s11222-012-9368-y</pub-id>
      <fpage>247</fpage>
      <lpage>263</lpage>
    </element-citation>
  </ref>
  <ref id="ref-CharlierU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Charlier</surname><given-names>Benjamin</given-names></name>
        <name><surname>Feydy</surname><given-names>Jean</given-names></name>
        <name><surname>Glaun√®s</surname><given-names>Joan Alexis</given-names></name>
        <name><surname>Collin</surname><given-names>Fran√ßois-David</given-names></name>
        <name><surname>Durif</surname><given-names>Ghislain</given-names></name>
      </person-group>
      <article-title>Kernel operations on the GPU, with autodiff, without memory overflows</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>74</issue>
      <uri>https://dl.acm.org/doi/10.5555/3546258.3546332</uri>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ChenU003A2023">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Lujing</given-names></name>
        <name><surname>Chen</surname><given-names>Tianshi</given-names></name>
        <name><surname>Detha</surname><given-names>Utkarsh</given-names></name>
        <name><surname>Andersen</surname><given-names>Martin S.</given-names></name>
      </person-group>
      <article-title>Towards scalable kernel-based regularized system identification</article-title>
      <source>2023 62nd IEEE conference on decision and control (CDC)</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/CDC49753.2023.10384051</pub-id>
      <fpage>1498</fpage>
      <lpage>1504</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FairbrotherU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fairbrother</surname><given-names>Jamie</given-names></name>
        <name><surname>Nemeth</surname><given-names>Christopher</given-names></name>
        <name><surname>Rischard</surname><given-names>Maxime</given-names></name>
        <name><surname>Brea</surname><given-names>Johanni</given-names></name>
        <name><surname>Pinder</surname><given-names>Thomas</given-names></name>
      </person-group>
      <article-title>GaussianProcesses.jl: A nonparametric Bayes package for the Julia language</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2022">2022</year>
      <volume>102</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v102.i01</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-GardnerU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gardner</surname><given-names>Jacob</given-names></name>
        <name><surname>Pleiss</surname><given-names>Geoff</given-names></name>
        <name><surname>Weinberger</surname><given-names>Kilian Q</given-names></name>
        <name><surname>Bindel</surname><given-names>David</given-names></name>
        <name><surname>Wilson</surname><given-names>Andrew G</given-names></name>
      </person-group>
      <article-title>GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <uri>https://dl.acm.org/doi/10.5555/3327757.3327857</uri>
    </element-citation>
  </ref>
  <ref id="ref-GPflowU003A2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Matthews</surname><given-names>Alexander G. de G.</given-names></name>
        <name><surname>van der Wilk</surname><given-names>Mark</given-names></name>
        <name><surname>Nickson</surname><given-names>Tom</given-names></name>
        <name><surname>Fujii</surname><given-names>Keisuke.</given-names></name>
        <name><surname>Boukouvalas</surname><given-names>Alexis</given-names></name>
        <name><surname>Le√≥n-Villagr√°</surname><given-names>Pablo</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Hensman</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>GPflow: A Gaussian process library using TensorFlow</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2017-04">2017</year><month>04</month>
      <volume>18</volume>
      <issue>40</issue>
      <uri>https://dl.acm.org/doi/10.5555/3122009.3122049</uri>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GPflowU003A2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>van der Wilk</surname><given-names>Mark</given-names></name>
        <name><surname>Dutordoir</surname><given-names>Vincent</given-names></name>
        <name><surname>John</surname><given-names>ST</given-names></name>
        <name><surname>Artemev</surname><given-names>Artem</given-names></name>
        <name><surname>Adam</surname><given-names>Vincent</given-names></name>
        <name><surname>Hensman</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>A framework for interdomain and multioutput Gaussian processes</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2003.01115</uri>
    </element-citation>
  </ref>
  <ref id="ref-HackbuschU003A2000">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Hackbusch</surname><given-names>Wolfgang</given-names></name>
        <name><surname>Khoromskij</surname><given-names>Boris N.</given-names></name>
        <name><surname>Sauter</surname><given-names>Stefan A.</given-names></name>
      </person-group>
      <article-title>On \mathcal{H}^2-matrices</article-title>
      <source>Lectures on applied mathematics: Proceedings of the symposium organized by the Sonderforschungsbereich 438 on the occasion of Karl-Heinz Hoffmann‚Äôs 60th birthday, Munich, June 30 - July 1, 1999</source>
      <publisher-name>Max Planck Institute for Mathematics in the Sciences; Springer</publisher-name>
      <publisher-loc>Berlin</publisher-loc>
      <year iso-8601-date="2000">2000</year>
      <pub-id pub-id-type="doi">10.1007/978-3-642-59709-1_2</pub-id>
      <fpage>9</fpage>
      <lpage>29</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HackbuschU003A2002">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hackbusch</surname><given-names>Wolfgang</given-names></name>
        <name><surname>B√∂rm</surname><given-names>Steffen</given-names></name>
      </person-group>
      <article-title>Data-sparse approximation by adaptive \mathcal{H}^2-matrices</article-title>
      <source>Computing</source>
      <year iso-8601-date="2002">2002</year>
      <volume>69</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1007/s00607-002-1450-4</pub-id>
      <fpage>1</fpage>
      <lpage>35</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HensmanU003A2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hensman</surname><given-names>James</given-names></name>
        <name><surname>Fusi</surname><given-names>Nicol√≤</given-names></name>
        <name><surname>Lawrence</surname><given-names>Neil D.</given-names></name>
      </person-group>
      <article-title>Gaussian processes for big data</article-title>
      <publisher-name>AUAI Press</publisher-name>
      <publisher-loc>Bellevue, WA</publisher-loc>
      <year iso-8601-date="2013-09">2013</year><month>09</month>
      <uri>https://dl.acm.org/doi/10.5555/3023638.3023667</uri>
      <fpage>282</fpage>
      <lpage>290</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HuangU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Hua</given-names></name>
        <name><surname>Xing</surname><given-names>Xin</given-names></name>
        <name><surname>Chow</surname><given-names>Edmond</given-names></name>
      </person-group>
      <article-title>H2Pack: High-performance \mathcal{H}^2 matrix package for kernel matrices using the proxy point method</article-title>
      <source>ACM Transactions on Mathematical Software</source>
      <year iso-8601-date="2020-12">2020</year><month>12</month>
      <volume>47</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1145/3412850</pub-id>
      <fpage>1</fpage>
      <lpage>29</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HutchinsonU003A1989">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hutchinson</surname><given-names>M. F.</given-names></name>
      </person-group>
      <article-title>A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</article-title>
      <source>Communications in Statistics - Simulation and Computation</source>
      <year iso-8601-date="1989-01">1989</year><month>01</month>
      <volume>18</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1080/03610918908812806</pub-id>
      <fpage>1059</fpage>
      <lpage>1076</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LinU003A2025">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lin</surname><given-names>Jihao Andreas</given-names></name>
        <name><surname>Ament</surname><given-names>Sebastian</given-names></name>
        <name><surname>Balandat</surname><given-names>Maximilian</given-names></name>
        <name><surname>Eriksson</surname><given-names>David</given-names></name>
        <name><surname>Hern√°ndez-Lobato</surname><given-names>Jos√© Miguel</given-names></name>
        <name><surname>Bakshy</surname><given-names>Eytan</given-names></name>
      </person-group>
      <article-title>Scalable Gaussian processes with latent Kronecker structure</article-title>
      <source>Forty-second international conference on machine learning</source>
      <year iso-8601-date="2025">2025</year>
      <uri>https://openreview.net/forum?id=Nv70EgUAA7</uri>
    </element-citation>
  </ref>
  <ref id="ref-MeyerU003A2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Meyer</surname><given-names>Raphael A</given-names></name>
        <name><surname>Musco</surname><given-names>Cameron</given-names></name>
        <name><surname>Musco</surname><given-names>Christopher</given-names></name>
        <name><surname>Woodruff</surname><given-names>David P</given-names></name>
      </person-group>
      <article-title>Hutch++: Optimal stochastic trace estimation</article-title>
      <source>Symposium on simplicity in algorithms (SOSA)</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1137/1.9781611976496.16</pub-id>
      <fpage>142</fpage>
      <lpage>155</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PanU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pan</surname><given-names>Qilong</given-names></name>
        <name><surname>Abdulah</surname><given-names>Sameh</given-names></name>
        <name><surname>Genton</surname><given-names>Marc G.</given-names></name>
        <name><surname>Keyes</surname><given-names>David E.</given-names></name>
        <name><surname>Ltaief</surname><given-names>Hatem</given-names></name>
        <name><surname>Sun</surname><given-names>Ying</given-names></name>
      </person-group>
      <article-title>GPU-accelerated Vecchia approximations of Gaussian processes for geospatial data using batched matrix computations</article-title>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.23919/ISC.2024.10528930</pub-id>
      <fpage>1</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PinderU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pinder</surname><given-names>Thomas</given-names></name>
        <name><surname>Dodd</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>GPJax: A Gaussian process framework in JAX</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>75</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04455</pub-id>
      <fpage>4455</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-PleissU003A2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pleiss</surname><given-names>Geoff</given-names></name>
        <name><surname>Gardner</surname><given-names>Jacob</given-names></name>
        <name><surname>Weinberger</surname><given-names>Kilian</given-names></name>
        <name><surname>Wilson</surname><given-names>Andrew Gordon</given-names></name>
      </person-group>
      <article-title>Constant-time predictive distributions for Gaussian processes</article-title>
      <source>Proceedings of the 35th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <publisher-loc>Proceedings of Machine Learning Research</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <volume>80</volume>
      <fpage>4114</fpage>
      <lpage>4123</lpage>
    </element-citation>
  </ref>
  <ref id="ref-RasmussenU003A2005">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Rasmussen</surname><given-names>Carl Edward</given-names></name>
        <name><surname>Williams</surname><given-names>Christopher K. I.</given-names></name>
      </person-group>
      <source>Gaussian processes for machine learning</source>
      <publisher-name>The MIT Press</publisher-name>
      <year iso-8601-date="2005">2005</year>
      <isbn>978-0-262-25683-4</isbn>
      <pub-id pub-id-type="doi">10.7551/mitpress/3206.001.0001</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-SaadU003A2003">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Saad</surname><given-names>Yousef</given-names></name>
      </person-group>
      <source>Iterative methods for sparse linear systems</source>
      <publisher-name>Society for Industrial; Applied Mathematics</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <pub-id pub-id-type="doi">10.1137/1.9780898718003</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-UbaruU003A2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ubaru</surname><given-names>Shashanka</given-names></name>
        <name><surname>Chen</surname><given-names>Jie</given-names></name>
        <name><surname>Saad</surname><given-names>Yousef</given-names></name>
      </person-group>
      <article-title>Fast estimation of tr(f(A)) via stochastic Lanczos quadrature</article-title>
      <source>SIAM Journal on Matrix Analysis and Applications</source>
      <publisher-name>Society for Industrial; Applied Mathematics</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>38</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1137/16M1104974</pub-id>
      <fpage>1075</fpage>
      <lpage>1099</lpage>
    </element-citation>
  </ref>
  <ref id="ref-WengerU003A2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wenger</surname><given-names>Jonathan</given-names></name>
        <name><surname>Pleiss</surname><given-names>Geoff</given-names></name>
        <name><surname>Hennig</surname><given-names>Philipp</given-names></name>
        <name><surname>Cunningham</surname><given-names>John</given-names></name>
        <name><surname>Gardner</surname><given-names>Jacob</given-names></name>
      </person-group>
      <article-title>Preconditioning for scalable Gaussian process hyperparameter optimization</article-title>
      <source>Proceedings of the 39th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2022-06">2022</year><month>06</month>
      <fpage>23751</fpage>
      <lpage>23780</lpage>
    </element-citation>
  </ref>
  <ref id="ref-WilsonU003A2015">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wilson</surname><given-names>Andrew Gordon</given-names></name>
        <name><surname>Dann</surname><given-names>Christoph</given-names></name>
        <name><surname>Nickisch</surname><given-names>Hannes</given-names></name>
      </person-group>
      <article-title>Thoughts on massively scalable Gaussian processes</article-title>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-WuU003A2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Kenan</given-names></name>
        <name><surname>Meinshausen</surname><given-names>Nicolai</given-names></name>
        <name><surname>Cunningham</surname><given-names>John P.</given-names></name>
        <name><surname>Pozzi</surname><given-names>Matteo</given-names></name>
      </person-group>
      <article-title>Large-scale Gaussian processes via alternating projection</article-title>
      <source>Proceedings of the 27th international conference on artificial intelligence and statistics</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <fpage>2810</fpage>
      <lpage>2818</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZhangU003A2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Meng</given-names></name>
        <name><surname>Chen</surname><given-names>Tianshi</given-names></name>
        <name><surname>Mu</surname><given-names>Biqiang</given-names></name>
      </person-group>
      <article-title>Asymptotic properties of generalized maximum likelihood hyper-parameter estimator for regularized system identification</article-title>
      <source>2024 IEEE 63rd conference on decision and control (CDC)</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/CDC56724.2024.10885816</pub-id>
      <fpage>4961</fpage>
      <lpage>4966</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ZhaoU003A2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhao</surname><given-names>Shifan</given-names></name>
        <name><surname>Xu</surname><given-names>Tianshi</given-names></name>
        <name><surname>Huang</surname><given-names>Hua</given-names></name>
        <name><surname>Chow</surname><given-names>Edmond</given-names></name>
        <name><surname>Xi</surname><given-names>Yuanzhe</given-names></name>
      </person-group>
      <article-title>An adaptive factorized Nystr√∂m preconditioner for regularized kernel matrices</article-title>
      <source>SIAM Journal on Scientific Computing</source>
      <year iso-8601-date="2024">2024</year>
      <volume>46</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1137/23M1565139</pub-id>
      <fpage>A2351</fpage>
      <lpage>A2376</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
