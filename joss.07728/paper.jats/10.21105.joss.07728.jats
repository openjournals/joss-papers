<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7728</article-id>
<article-id pub-id-type="doi">10.21105/joss.07728</article-id>
<title-group>
<article-title>EvoVis: Dashboard for Visualizing Evolutionary Neural
Architecture Search Algorithms</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-6056-0725</contrib-id>
<name>
<surname>Dang</surname>
<given-names>Lea</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3405-1311</contrib-id>
<name>
<surname>Groh</surname>
<given-names>René</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3643-7776</contrib-id>
<name>
<surname>Kist</surname>
<given-names>Andreas M.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Friedrich-Alexander-Universität Erlangen-Nürnberg,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-11-12">
<day>12</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>111</issue>
<fpage>7728</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Evolutionary neural architecture search algorithm</kwd>
<kwd>Neural architecture search algorithm</kwd>
<kwd>Optimization algorithm</kwd>
<kwd>Deep learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>EvoVis is a dashboard designed to visualize the key components of
  Evolutionary Neural Architecture Search (ENAS) algorithms. ENAS is an
  optimization method that mimics biological evolution to optimize one
  or multiple objectives, ultimately discovering novel neural network
  architectures tailored to specific tasks. EvoVis offers a holistic
  view of the ENAS process: It provides insights into hyperparameters,
  potential neural architecture topologies, the family tree of
  architectures across generations, and performance trends. Key features
  include interactive gene pool and family tree graphs, as well as
  performance plots for monitoring the ENAS runs. The dashboard’s
  generic data structure interface facilitates integration with various
  ENAS algorithms. The code is available at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/ankilab/EvoVis">https://github.com/ankilab/EvoVis</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In the past few years, the field of data science has gained a lot
  of attention in the public media, industry, and academia. One
  essential part is the rise of deep learning with neural networks as
  the state-of-the-art method for solving a variety of highly cognitive
  and, until now, hard-to-automate tasks
  (<xref alt="LeCun et al., 2015" rid="ref-lecun2015deep" ref-type="bibr">LeCun
  et al., 2015</xref>;
  <xref alt="White et al., 2023" rid="ref-NAS-1000-paper-review" ref-type="bibr">White
  et al., 2023</xref>). However, one drawback of these models is that
  the architectural design is a bottleneck in the overall performance,
  as manual design approaches involve time-consuming trial-and-error
  processes
  (<xref alt="White et al., 2023" rid="ref-NAS-1000-paper-review" ref-type="bibr">White
  et al., 2023</xref>). They lack the expertise and depth of exploration
  required to tackle complex applications, especially those with
  multiple conflicting optimization objectives. This limitation has
  spurred the rise of automated approaches for discovering neural
  architectures, known as Neural Architecture Search (NAS) algorithms
  (<xref alt="S. Liu et al., 2022" rid="ref-NAS-review-efficient-performance-prediction" ref-type="bibr">S.
  Liu et al., 2022</xref>). The use of NAS not only reduces the manual
  effort required, but also enables a more efficient exploration of the
  architectural search space, which in turn leads to the generation of
  superior models for specialized tasks.</p>
  <p>Among others (reinforcement learning
  (<xref alt="Zoph &amp; Le, 2016" rid="ref-NAS-reinforcement-learning" ref-type="bibr">Zoph
  &amp; Le, 2016</xref>), Bayesian optimization
  (<xref alt="White et al., 2021" rid="ref-NAS-BANANAS" ref-type="bibr">White
  et al., 2021</xref>)), Evolutionary Neural Architecture Search (ENAS)
  is one NAS optimization method which mimics biological evolution,
  relying on the principle of survival of the fittest. Initially, the
  algorithm selects a starting population from the given gene pool
  (search space). In each generation, including the initial one, every
  individual in the population is evaluated to determine its fitness.
  The processes of selection, crossover (combining the structure of
  high-performing architectures), and mutation (introducing random
  structural changes) are then applied to progressively enhance the
  objective value (fitness) over time until a stopping criterion is
  fulfilled
  (<xref alt="Y. Liu et al., 2023" rid="ref-EvoNAS-survey" ref-type="bibr">Y.
  Liu et al., 2023</xref>).</p>
  <fig>
    <caption><p>Evolutionary neural architecture search
    overview</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/evonas-overview-white.png" />
  </fig>
  <p>ENAS algorithms have demonstrated remarkable success in generating
  state-of-the-art neural architectures
  (<xref alt="Lu et al., 2019" rid="ref-EvoNAS-NSGA-Net-multi-obj" ref-type="bibr">Lu
  et al., 2019</xref>;
  <xref alt="Real et al., 2019" rid="ref-EvoNAS-regularized-evolution-img-classifier" ref-type="bibr">Real
  et al., 2019</xref>,
  <xref alt="2017" rid="ref-EvoNAS-large-scale-evolution-img-classifier" ref-type="bibr">2017</xref>;
  <xref alt="Sun et al., 2020" rid="ref-EvoNAS-CNN-design" ref-type="bibr">Sun
  et al., 2020</xref>). However, understanding their behavior and
  interpreting the results pose significant challenges. There is a need
  for comprehensive visualization of both the overall process and the
  individual neural architectures generated. First, the visualization of
  the process settings, from the hyperparameters to the search space of
  possible neural architectures, helps to understand the origins of the
  ENAS’ run performances and the diversity of architectures explored,
  which can be used for hyperparameter tuning. Secondly, visualizing a
  family tree of neural architectures enables researchers to investigate
  the network structure of individuals that have successfully inherited
  traits while converging toward an optimal solution. Finally, by
  visualizing aggregated performance trends of individuals across
  generations, researchers can gain insight into the evolutionary
  trajectory and intervene if necessary.</p>
  <p>Popular machine learning tools like TensorBoard, Neptune, and
  MLFlow excel at visualizing certain aspects of the ENAS pipeline, such
  as real-time monitoring of training metrics and visualizing model
  architectures
  (<xref alt="Budras et al., 2022" rid="ref-ML-tracking-frameworks" ref-type="bibr">Budras
  et al., 2022</xref>;
  <xref alt="Goldsborough, 2016" rid="ref-tensorboard" ref-type="bibr">Goldsborough,
  2016</xref>). However, they lack the capability to track the
  relationships and evolutionary progress of architectures over time.
  This gap underscores the need for a comprehensive tool that integrates
  all these features, which EvoVis is specifically designed to
  provide.</p>
</sec>
<sec id="evovis-key-features">
  <title>EvoVis Key Features</title>
  <p>This paper presents a new dashboard tailored for ENAS algorithms
  named EvoVis. EvoVis provides continuous updates on performance
  metrics for concurrent trend analysis and quick identification of
  issues. EvoVis is built with <monospace>Plotly Dash</monospace>,
  enabling rapid development of interactive web applications with
  minimal code, seamless integration with CSS and frameworks like
  <monospace>Bootstrap</monospace> and <monospace>Mantine</monospace>
  for a polished user interface, and support for interactive graph
  components like <monospace>Cytoscape.js</monospace> to provide dynamic
  visualizations of the gene pool and family tree graphs for enhanced
  user interaction.</p>
  <sec id="generic-interface">
    <title>Generic Interface</title>
    <p>EvoVis features a generic interface by accepting a standardized
    data structure as input to ensure compatibility with different ENAS
    algorithms. EvoVis requires the ENAS run to output a directory named
    according to a selected run identifier. The run directory contains
    subdirectories representing all generations that have been
    processed. The generation subdirectories store each individual of
    the generation along with its fitness values and the chromosome
    specifying the individual’s neural architecture. In addition to the
    collection of generations, a configuration file containing the
    hyperparameters, a crossover log, and the search space configuration
    specifying the gene pool directed acyclic graph (DAG) is stored in
    the run directory. A more detailed description of the folder and
    file structure as well as a sample dataset is provided in the
    documentation of EvoVis.</p>
    <fig>
      <caption><p>Run Results File Structure</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/run-results-file-structure.png" />
    </fig>
  </sec>
  <sec id="hyperparameter-overview">
    <title>Hyperparameter Overview</title>
    <p>The default view of EvoVis is the
    <monospace>Hyperparameters</monospace> page, which enables users to
    understand the configurations of an ENAS run and provides clarity on
    the inputs that influence the evolutionary trajectory from the
    beginning. Users can access additional descriptions by hovering over
    the hyperparameter component icons, revealing tooltips that clarify
    the purpose of each hyperparameter. This comprehensive experiment
    logging enhances reproducibility, thereby increasing the reliability
    of research results
    (<xref alt="Quaranta et al., 2022" rid="ref-ml-reproducability" ref-type="bibr">Quaranta
    et al., 2022</xref>).</p>
    <fig>
      <caption><p>Hyperparameter Page</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/hyperparameter-page.png" />
    </fig>
  </sec>
  <sec id="gene-pool-graph">
    <title>Gene Pool Graph</title>
    <p>The <monospace>Gene Pool</monospace> page generates a graph
    representing potential neural architectures that form each
    individual and thus each population of an ENAS run. Each chromosome
    is derived from this gene pool, resulting in a wide variety of
    neural architectures. This feature serves both as an analytical
    support and as an experiment log of ENAS experiments
    (<xref alt="Quaranta et al., 2022" rid="ref-ml-reproducability" ref-type="bibr">Quaranta
    et al., 2022</xref>). It not only captures the logic of the search
    space but also presents it in a comprehensive graph format. This
    automated visualization of the DAG reduces the time spent manually
    identifying the structure, thereby enhancing reproducibility by
    transparently documenting the exploration of the architectural
    space.</p>
    <fig>
      <caption><p>Gene Pool Page</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/genepool-page.png" />
    </fig>
  </sec>
  <sec id="family-tree-graph">
    <title>Family Tree Graph</title>
    <p>Another component of EvoVis is a family tree, which illustrates
    the relationships between different architectures, allowing users to
    quickly identify the ancestors of each architecture. By navigating
    through the family tree, users can access performance metrics and
    chromosomes for each architecture. This feature facilitates tracing
    the lineage of successful architectures and helps diagnose training
    problems by enabling fine-grained analysis of the evolutionary
    process.</p>
    <fig>
      <caption><p>Family Tree Page</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/family-tree-page.png" />
    </fig>
  </sec>
  <sec id="run-results-plots">
    <title>Run Results Plots</title>
    <p>Arguably the most important page in EvoVis is the
    <monospace>Run Results</monospace> page, which provides an
    aggregated analysis of an ENAS run. This page features a variety of
    plots, including performance metrics across generations, fitness
    values and accuracies as well as metrics like power consumption and
    memory usage. For multi-objective problems, a graph illustrating the
    trade-off between competing objectives in the fitness function is
    presented so that users can identify optimal solutions.
    Additionally, the run results include the best individuals of each
    generation, along with their corresponding architectures and
    performance evaluations.</p>
    <fig>
      <caption><p>Run Results Page</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="./src/assets/media/run-results-page.png" />
    </fig>
  </sec>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>We present EvoVis, a novel dashboard for visualizing
  hyperparameters, gene pools, family trees, and optimization results of
  ENAS algorithms. By providing an intuitive Python-based interface,
  EvoVis aims to improve the workflow of researchers developing
  innovative ENAS algorithms. Future work will extend its capabilities
  to include visualizations of results from prominent ENAS algorithms,
  fostering greater integration with the existing literature and
  increasing the utility of EvoVis for the ENAS community.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We would like to thank Mateo Avila Pava and Stefan Dendorfer for
  their valuable feedback during the development phase of EvoVis. We
  received funding from the Bavarian State Ministry of Science and the
  Arts (StMWK) and Fonds de recherche du Québec (FRQ) under the
  Collaborative Bilateral Research Program Bavaria – Québec managed by
  WKS at Bavarian Research Alliance (BayFOR) and Fonds de recherche du
  Québec – Santé (FRQS). The presented content is solely the
  responsibility of the authors and does not necessarily represent the
  official views of the above funding agencies.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-NAS-1000-paper-review">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>White</surname><given-names>Colin</given-names></name>
        <name><surname>Safari</surname><given-names>Mahmoud</given-names></name>
        <name><surname>Sukthanker</surname><given-names>Rhea</given-names></name>
        <name><surname>Ru</surname><given-names>Binxin</given-names></name>
        <name><surname>Elsken</surname><given-names>Thomas</given-names></name>
        <name><surname>Zela</surname><given-names>Arber</given-names></name>
        <name><surname>Dey</surname><given-names>Debadeepta</given-names></name>
        <name><surname>Hutter</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>Neural architecture search: Insights from 1000 papers</article-title>
      <source>arXiv e-prints</source>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2301.08727</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2301.08727</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-NAS-review-efficient-performance-prediction">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Shiqing</given-names></name>
        <name><surname>Zhang</surname><given-names>Haoyu</given-names></name>
        <name><surname>Jin</surname><given-names>Yaochu</given-names></name>
      </person-group>
      <article-title>A survey on computationally efficient neural architecture search</article-title>
      <source>Journal of Automation and Intelligence</source>
      <year iso-8601-date="2022">2022</year>
      <volume>1</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1016/j.jai.2022.100002</pub-id>
      <fpage>100002</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvoNAS-survey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Yuqiao</given-names></name>
        <name><surname>Sun</surname><given-names>Yanan</given-names></name>
        <name><surname>Xue</surname><given-names>Bing</given-names></name>
        <name><surname>Zhang</surname><given-names>Mengjie</given-names></name>
        <name><surname>Yen</surname><given-names>Gary G.</given-names></name>
        <name><surname>Tan</surname><given-names>Kay Chen</given-names></name>
      </person-group>
      <article-title>A survey on evolutionary neural architecture search</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <year iso-8601-date="2023">2023</year>
      <volume>34</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/TNNLS.2021.3100554</pub-id>
      <fpage>550</fpage>
      <lpage>570</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvoNAS-NSGA-Net-multi-obj">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Zhichao</given-names></name>
        <name><surname>Whalen</surname><given-names>Ian</given-names></name>
        <name><surname>Boddeti</surname><given-names>Vishnu</given-names></name>
        <name><surname>Dhebar</surname><given-names>Yashesh</given-names></name>
        <name><surname>Deb</surname><given-names>Kalyanmoy</given-names></name>
        <name><surname>Goodman</surname><given-names>Erik</given-names></name>
        <name><surname>Banzhaf</surname><given-names>Wolfgang</given-names></name>
      </person-group>
      <article-title>NSGA-net: Neural architecture search using multi-objective genetic algorithm</article-title>
      <source>Proceedings of the genetic and evolutionary computation conference</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1145/3321707.3321729</pub-id>
      <fpage>419</fpage>
      <lpage>427</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvoNAS-regularized-evolution-img-classifier">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Real</surname><given-names>Esteban</given-names></name>
        <name><surname>Aggarwal</surname><given-names>Alok</given-names></name>
        <name><surname>Huang</surname><given-names>Yanping</given-names></name>
        <name><surname>Le</surname><given-names>Quoc V</given-names></name>
      </person-group>
      <article-title>Regularized evolution for image classifier architecture search</article-title>
      <source>Proceedings of the AAAI conference on artificial intelligence</source>
      <year iso-8601-date="2019">2019</year>
      <volume>33</volume>
      <pub-id pub-id-type="doi">10.1609/aaai.v33i01.33014780</pub-id>
      <fpage>4780</fpage>
      <lpage>4789</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvoNAS-CNN-design">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sun</surname><given-names>Yanan</given-names></name>
        <name><surname>Xue</surname><given-names>Bing</given-names></name>
        <name><surname>Zhang</surname><given-names>Mengjie</given-names></name>
        <name><surname>Yen</surname><given-names>Gary G.</given-names></name>
        <name><surname>Lv</surname><given-names>Jiancheng</given-names></name>
      </person-group>
      <article-title>Automatically designing CNN architectures using the genetic algorithm for image classification</article-title>
      <source>IEEE Transactions on Cybernetics</source>
      <year iso-8601-date="2020">2020</year>
      <volume>50</volume>
      <issue>9</issue>
      <pub-id pub-id-type="doi">10.1109/TCYB.2020.2983860</pub-id>
      <fpage>3840</fpage>
      <lpage>3854</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvoNAS-large-scale-evolution-img-classifier">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Real</surname><given-names>Esteban</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Selle</surname><given-names>Andrew</given-names></name>
        <name><surname>Saxena</surname><given-names>Saurabh</given-names></name>
        <name><surname>Suematsu</surname><given-names>Yutaka Leon</given-names></name>
        <name><surname>Tan</surname><given-names>Jie</given-names></name>
        <name><surname>Le</surname><given-names>Quoc V.</given-names></name>
        <name><surname>Kurakin</surname><given-names>Alexey</given-names></name>
      </person-group>
      <article-title>Large-scale evolution of image classifiers</article-title>
      <source>Proceedings of the 34th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>70</volume>
      <fpage>2902</fpage>
      <lpage>2911</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tensorboard">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goldsborough</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>A tour of TensorFlow</article-title>
      <source>arXiv e-prints</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/abs/1610.01178</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1610.01178</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ML-tracking-frameworks">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Budras</surname><given-names>Tim</given-names></name>
        <name><surname>Blanck</surname><given-names>Maximilian</given-names></name>
        <name><surname>Berger</surname><given-names>Tilman</given-names></name>
        <name><surname>Schmidt</surname><given-names>Andreas</given-names></name>
      </person-group>
      <article-title>Comparison of experiment tracking frameworks in machine learning environments</article-title>
      <source>Proceedings of the fourteenth international conference on advances in databases, knowledge, and data applications</source>
      <year iso-8601-date="2022">2022</year>
      <fpage>21</fpage>
      <lpage>28</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ml-reproducability">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Quaranta</surname><given-names>Luigi</given-names></name>
        <name><surname>Calefato</surname><given-names>Fabio</given-names></name>
        <name><surname>Lanubile</surname><given-names>Filippo</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>A taxonomy of tools for reproducible machine learning experiments</article-title>
      <source>CEUR workshop proceedings</source>
      <year iso-8601-date="2022">2022</year>
      <volume>3078</volume>
      <pub-id pub-id-type="doi">11586/380827</pub-id>
      <fpage>65</fpage>
      <lpage>76</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NAS-reinforcement-learning">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zoph</surname><given-names>Barret</given-names></name>
        <name><surname>Le</surname><given-names>Quoc</given-names></name>
      </person-group>
      <article-title>Neural architecture search with reinforcement learning</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-NAS-BANANAS">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>White</surname><given-names>Colin</given-names></name>
        <name><surname>Neiswanger</surname><given-names>Willie</given-names></name>
        <name><surname>Savani</surname><given-names>Yash</given-names></name>
      </person-group>
      <article-title>Bananas: Bayesian optimization with neural architectures for neural architecture search</article-title>
      <source>Proceedings of the AAAI conference on artificial intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>35</volume>
      <pub-id pub-id-type="doi">10.1609/aaai.v35i12.17233</pub-id>
      <fpage>10293</fpage>
      <lpage>10301</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lecun2015deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
      </person-group>
      <article-title>Deep learning</article-title>
      <source>nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>521</volume>
      <issue>7553</issue>
      <pub-id pub-id-type="doi">10.1038/nature14539</pub-id>
      <fpage>436</fpage>
      <lpage>444</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
