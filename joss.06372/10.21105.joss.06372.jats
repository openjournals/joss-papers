<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6372</article-id>
<article-id pub-id-type="doi">10.21105/joss.06372</article-id>
<title-group>
<article-title>CalibrateEmulateSample.jl: Accelerated Parametric
Uncertainty Quantification</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7374-0382</contrib-id>
<name>
<surname>Dunbar</surname>
<given-names>Oliver R. A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Bieli</surname>
<given-names>Melanie</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3279-619X</contrib-id>
<name>
<surname>Garbuno-I√±igo</surname>
<given-names>Alfredo</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2878-3874</contrib-id>
<name>
<surname>Howland</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9906-7824</contrib-id>
<name>
<surname>de Souza</surname>
<given-names>Andre Nogueira</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6285-6045</contrib-id>
<name>
<surname>Mansfield</surname>
<given-names>Laura Anne</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5317-2445</contrib-id>
<name>
<surname>Wagner</surname>
<given-names>Gregory L.</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Efrat-Henrici</surname>
<given-names>N.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Geological and Planetary Sciences, California Institute of
Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Swiss Re Ltd.</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Statistics, Mexico Autonomous Institute of
Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Civil and Environmental Engineering, Massachusetts
Institute of Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Earth, Atmospheric, and Planetary Sciences, Massachusetts
Institute of Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Earth System Science, Doerr School of Sustainability,
Stanford University</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-01-02">
<day>2</day>
<month>1</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>97</issue>
<fpage>6372</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>machine learning</kwd>
<kwd>optimization</kwd>
<kwd>bayesian</kwd>
<kwd>data assimilation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>A Julia language
  (<xref alt="Bezanson et al., 2017" rid="ref-julia" ref-type="bibr">Bezanson
  et al., 2017</xref>) package providing practical and modular
  implementation of ``Calibrate, Emulate, Sample‚Äù
  (<xref alt="Cleary et al., 2021" rid="ref-ClearyU003A2021" ref-type="bibr">Cleary
  et al., 2021</xref>), hereafter CES, an accelerated workflow for
  obtaining model parametric uncertainty is presented. This is also
  known as Bayesian inversion or uncertainty quantification. To apply
  CES one requires a computer model (written in any programming
  language) dependent on free parameters, a prior distribution encoding
  some prior knowledge about the distribution over the free parameters,
  and some data with which to constrain this prior distribution. The
  pipeline has three stages, most easily explained in reverse:</p>
  <list list-type="order">
    <list-item>
      <p>The goal of the workflow is to draw samples (Sample) from the
      Bayesian posterior distribution, that is, the prior distribution
      conditioned on the observed data,</p>
    </list-item>
    <list-item>
      <p>To accelerate and regularize sampling we train statistical
      emulators to represent the user-provided parameter-to-data map
      (Emulate),</p>
    </list-item>
    <list-item>
      <p>The training points for these emulators are generated by the
      computer model, and selected adaptively around regions of high
      posterior mass (Calibrate).</p>
    </list-item>
  </list>
  <p>We describe CES as an accelerated workflow, as it is often able to
  use dramatically fewer evaluations of the computer model when compared
  with applying sampling algorithms, such as Markov chain Monte Carlo
  (MCMC), directly.</p>
  <list list-type="bullet">
    <list-item>
      <p>Calibration tools: We recommend choosing adaptive training
      points with Ensemble Kalman methods such as EKI
      (<xref alt="Iglesias et al., 2013" rid="ref-IglesiasU003A2013" ref-type="bibr">Iglesias
      et al., 2013</xref>) and its variants
      (<xref alt="Huang et al., 2022" rid="ref-HuangU003A2022" ref-type="bibr">Huang
      et al., 2022</xref>); and CES provides explicit utilities from the
      codebase EnsembleKalmanProcesses.jl
      (<xref alt="Dunbar, Lopez-Gomez, et al., 2022" rid="ref-DunbarU003A2022a" ref-type="bibr">Dunbar,
      Lopez-Gomez, et al., 2022</xref>).</p>
    </list-item>
    <list-item>
      <p>Emulation tools: CES integrates any statistical emulator,
      currently implemented are Gaussian Processes (GP)
      (<xref alt="Williams &amp; Rasmussen, 2006" rid="ref-RasmussenU003A2006" ref-type="bibr">Williams
      &amp; Rasmussen, 2006</xref>), explicitly provided through
      packages SciKitLearn.jl
      (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
      et al., 2011</xref>) and GaussianProcesses.jl
      (<xref alt="Fairbrother et al., 2022" rid="ref-FairbrotherU003A2022" ref-type="bibr">Fairbrother
      et al., 2022</xref>), and Random Features
      (<xref alt="Liu et al., 2022" rid="ref-LiuU003A2022" ref-type="bibr">Liu
      et al., 2022</xref>;
      <xref alt="Rahimi et al., 2007" rid="ref-RahimiU003A2007" ref-type="bibr">Rahimi
      et al., 2007</xref>;
      <xref alt="Rahimi &amp; Recht, 2008" rid="ref-RahimiU003A2008" ref-type="bibr">Rahimi
      &amp; Recht, 2008</xref>), explicitly provided through
      <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.7141158">RandomFeatures.jl</ext-link>
      that can provide additional flexibility and scalability,
      particularly in higher dimensions.</p>
    </list-item>
    <list-item>
      <p>Sampling tools: The regularized and accelerated sampling
      problem is solved with MCMC, and CES provides the variants of
      Random Walk Metropolis
      (<xref alt="Metropolis et al., 1953" rid="ref-MetropolisU003A1953" ref-type="bibr">Metropolis
      et al., 1953</xref>;
      <xref alt="Sherlock et al., 2010" rid="ref-SherlockU003A2010" ref-type="bibr">Sherlock
      et al., 2010</xref>), and preconditioned Crank-Nicholson
      (<xref alt="Cotter et al., 2013" rid="ref-CotterU003A2013" ref-type="bibr">Cotter
      et al., 2013</xref>), using APIs from
      <ext-link ext-link-type="uri" xlink:href="https://turinglang.org/">Turing.jl</ext-link>.
      Some regular emulator mean functions are differentiable, and
      including accelerations of derivative-based MCMC into CES, (e.g.,
      NUTS,
      <xref alt="Hoffman et al., 2014" rid="ref-hoffmanU003A2014" ref-type="bibr">Hoffman
      et al., 2014</xref>; Barker,
      <xref alt="Livingstone &amp; Zanella, 2022" rid="ref-livingstoneU003A2022" ref-type="bibr">Livingstone
      &amp; Zanella, 2022</xref>); is an active direction of work.</p>
    </list-item>
  </list>
  <p>To highlight code accessibility, we also provide a suite of
  detailed scientifically-inspired examples, with documentation that
  walks users through some use cases. Such use cases not only
  demonstrate the capability of the CES pipeline, but also teach users
  about typical interface and workflow experience.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Computationally expensive computer codes for predictive modelling
  are ubiquitous across science and engineering disciplines. Free
  parameter values that exist within these modelling frameworks are
  typically constrained by observations to produce accurate and robust
  predictions about the system they are approximating numerically. In a
  Bayesian setting, this is viewed as evolving an initial parameter
  distribution (based on prior information) with the input of observed
  data, to a more informative data-consistent distribution (posterior).
  Unfortunately, this task is intensely computationally expensive,
  commonly requiring over <inline-formula><alternatives>
  <tex-math><![CDATA[10^5]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  evaluations of the expensive computer code (e.g., Random Walk
  Metropolis), with accelerations relying on intrusive model
  information, such as a derivative of the parameter-to-data map. CES is
  able to approximate and accelerate this process in a non-intrusive
  fashion and requiring only on the order of
  <inline-formula><alternatives>
  <tex-math><![CDATA[10^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
  evaluations of the original computer model. This opens the doors for
  quantifying parametric uncertainty for a class of numerically
  intensive computer codes that has previously been unavailable.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>In Julia there are a few tools for performing non-accelerated
  uncertainty quantification, from classical sensitivity analysis
  approaches, for example,
  <ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/10149017">UncertaintyQuantification.jl</ext-link>,
  GlobalSensitivity.jl
  (<xref alt="Dixit &amp; Rackauckas, 2022" rid="ref-DixitU003A2022" ref-type="bibr">Dixit
  &amp; Rackauckas, 2022</xref>), and MCMC, for example,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/brian-j-smith/Mamba.jl">Mamba.jl</ext-link>
  or
  <ext-link ext-link-type="uri" xlink:href="https://turinglang.org/">Turing.jl</ext-link>.
  For computational efficiency, ensemble methods also provide
  approximate sampling,
  (<xref alt="Dunbar, Lopez-Gomez, et al., 2022" rid="ref-DunbarU003A2022a" ref-type="bibr">Dunbar,
  Lopez-Gomez, et al., 2022</xref>; e.g., the Ensemble Kalman Sampler
  <xref alt="Garbuno-Inigo et al., 2020" rid="ref-Garbuno-InigoU003A2020b" ref-type="bibr">Garbuno-Inigo
  et al., 2020</xref>), though these only provide Gaussian
  approximations of the posterior.</p>
  <p>Accelerated uncertainty quantification tools also exist for the
  related approach of Approximate Bayesian Computation (ABC), for
  example, GpABC
  (<xref alt="Tankhilevich et al., 2020" rid="ref-TankhilevichU003A2020" ref-type="bibr">Tankhilevich
  et al., 2020</xref>) or
  <ext-link ext-link-type="uri" xlink:href="https://github.com/marcjwilliams1/ApproxBayes.jl?tab=readme-ov-file">ApproxBayes.jl</ext-link>;
  these tools both approximately sample from the posterior distribution.
  In ABC, this approximation comes from bypassing the likelihood that is
  usually required in sampling methods, such as MCMC. Instead, the goal
  of ABC is to replace the likelihood with a scalar-valued sampling
  objective that compares model and data. In CES, the approximation
  comes from learning the parameter-to-data map, then following this it
  calculates an explicit likelihood and uses exact sampling via MCMC.
  Some ABC algorithms also make use of statistical emulators to further
  accelerate sampling (GpABC). Although flexible, ABC encounters
  challenges due to the subjectivity of summary statistics and distance
  metrics, that may lead to approximation errors particularly in
  high-dimensional settings
  (<xref alt="Nott et al., 2018" rid="ref-NottU003A2018" ref-type="bibr">Nott
  et al., 2018</xref>). CES is more restrictive due to use of an
  explicit Gaussian likelihood, but also leverages this structure to
  deal with high dimensional data.</p>
  <p>Several other tools are available in other languages for a purpose
  of accelerated learning of the posterior distribution or posterior
  sampling. Two such examples, written in Python, approximate the
  log-posterior distribution directly with a Gaussian process:
  <ext-link ext-link-type="uri" xlink:href="https://github.com/acerbilab/pyvbmc">PyVBMC</ext-link>
  (<xref alt="Huggins et al., 2023" rid="ref-HugginsU003A2023" ref-type="bibr">Huggins
  et al., 2023</xref>) additionaly uses variational approximations to
  calculate the normalization constant, and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/jonaselgammal/GPry">GPry</ext-link>
  (<xref alt="Gammal et al., 2023" rid="ref-GammalU003A2023" ref-type="bibr">Gammal
  et al., 2023</xref>), which iteratively trains the GP with an active
  training point selection algorithm. Such algorithms are distinct from
  CES, which approximates the parameter-to-data map with the Gaussian
  process, and advocates ensemble Kalman methods to select training
  points.</p>
</sec>
<sec id="a-simple-example-from-the-code-documentation">
  <title>A simple example from the code documentation</title>
  <p>We sketch an end-to-end example of the pipeline, with
  fully-detailed walkthrough given in the online documentation.</p>
  <p>We have a model of a sinusoidal signal that is a function of
  parameters <inline-formula><alternatives>
  <tex-math><![CDATA[\theta=(A,v)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ∏</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>
  is the amplitude of the signal and <inline-formula><alternatives>
  <tex-math><![CDATA[v]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>
  is vertical shift of the signal <disp-formula><alternatives>
  <tex-math><![CDATA[f(A, v) = A \sin(\phi + t) + v, \forall t \in [0,2\pi].]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>œï</mml:mi><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mo>‚àÄ</mml:mo><mml:mi>t</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>œÄ</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  Here, <inline-formula><alternatives>
  <tex-math><![CDATA[\phi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>œï</mml:mi></mml:math></alternatives></inline-formula>
  is the random phase of each signal. The goal is to estimate not just
  point estimates of the parameters <inline-formula><alternatives>
  <tex-math><![CDATA[\theta=(A,v)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ∏</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  but entire probability distributions of them, given some noisy
  observations. We will use the range and mean of a signal as our
  observable: <disp-formula><alternatives>
  <tex-math><![CDATA[ G(\theta) = \big[ \text{range}\big(f(\theta)\big), \text{mean}\big(f(\theta)\big) \big] ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">[</mml:mo><mml:mtext mathvariant="normal">range</mml:mtext><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mml:mo><mml:mo>,</mml:mo><mml:mtext mathvariant="normal">mean</mml:mtext><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="prefix">(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">)</mml:mo><mml:mo minsize="1.2" maxsize="1.2" stretchy="false" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  Then, our noisy observations, <inline-formula><alternatives>
  <tex-math><![CDATA[y_{obs}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>,
  can be written as: <disp-formula><alternatives>
  <tex-math><![CDATA[ y_{obs} = G(\theta^\dagger) + \mathcal{N}(0, \Gamma)]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>Œ∏</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ùí©</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>Œì</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\Gamma]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œì</mml:mi></mml:math></alternatives></inline-formula>
  is the observational covariance matrix. We will assume the noise to be
  independent for each observable, giving us a diagonal covariance
  matrix.</p>
  <fig>
    <caption><p>The true and observed range and mean.
    <styled-content id="figU003Asignal"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sinusoid_true_vs_observed_signal.png" />
  </fig>
  <p>For this experiment <inline-formula><alternatives>
  <tex-math><![CDATA[\theta^\dagger = (A^\dagger,v^\dagger) = (3.0, 7.0)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>Œ∏</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>3.0</mml:mn><mml:mo>,</mml:mo><mml:mn>7.0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and the noisy observations are displayed in blue in
  <xref alt="[fig:signal]" rid="figU003Asignal">[fig:signal]</xref>.</p>
  <p>We define prior distributions on the two parameters. For the
  amplitude, we define a prior with mean 2 and standard deviation 1. It
  is additionally constrained to be nonnegative. For the vertical shift
  we define a prior with mean 0 and standard deviation 5.</p>
  <code language="julia">const PD = CalibrateEmulateSample.ParameterDistributions
prior_u1 = PD.constrained_gaussian(&quot;amplitude&quot;, 2, 1, 0, Inf)
prior_u2 = PD.constrained_gaussian(&quot;vert_shift&quot;, 0, 5, -Inf, Inf)
prior = PD.combine_distributions([prior_u1, prior_u2])</code>
  <fig>
    <caption><p>Marginal distributions of the prior
    <styled-content id="figU003Aprior"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sinusoid_prior.png" />
  </fig>
  <p>The prior is displayed in
  <xref alt="[fig:prior]" rid="figU003Aprior">[fig:prior]</xref>.</p>
  <p>We now adaptively find input-output pairs from our map
  <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  in a region of interest using an inversion method (an ensemble Kalman
  process). This is the Calibrate stage, and iteratively generates
  parameter combinations, that refine around a region of high posterior
  mass.</p>
  <code language="julia">const EKP = CalibrateEmulateSample.EnsembleKalmanProcesses
N_ensemble = 10
N_iterations = 5
initial_ensemble = EKP.construct_initial_ensemble(prior, N_ensemble)
ensemble_kalman_process = EKP.EnsembleKalmanProcess(
    initial_ensemble, y_obs, Œì, EKP.Inversion(); 
)
for i in 1:N_iterations
    params_i = EKP.get_phi_final(prior, ensemble_kalman_process)
    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)
    EKP.update_ensemble!(ensemble_kalman_process, G_ens)
end</code>
  <fig>
    <caption><p>The resulting ensemble from a calibration.
    <styled-content id="figU003Aeki"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sinusoid_eki_pairs.png" />
  </fig>
  <p>The adaptively refined training points from EKP are displayed in
  <xref alt="[fig:eki]" rid="figU003Aeki">[fig:eki]</xref>. We now build
  an basic Gaussian process emulator from the GaussianProcesses.jl
  package to emulate the map <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  using these points.</p>
  <code language="julia">const UT = CalibrateEmulateSample.Utilities
const EM = CalibrateEmulateSample.Emulators

input_output_pairs = UT.get_training_points(
    ensemble_kalman_process, N_iterations,
)
gppackage = EM.GPJL() 
gauss_proc = EM.GaussianProcess(gppackage, noise_learn = false)
emulator = EM.Emulator(
    gauss_proc, input_output_pairs, normalize_inputs = true,  obs_noise_cov = Œì,
) 
EM.optimize_hyperparameters!(emulator) # train the emulator</code>
  <fig>
    <caption><p>The Gaussian process emulator of the range and mean
    maps, trained on the re-used calibration pairs
    <styled-content id="figU003AGP_emulator"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sinusoid_GP_emulator_contours.png" />
  </fig>
  <p>We evaluate the mean of this emulator on a grid, and also show the
  value of the true <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
  at training point locations in
  <xref alt="[fig:GP_emulator]" rid="figU003AGP_emulator">[fig:GP_emulator]</xref>.</p>
  <p>We can then sample with this emulator using an MCMC scheme. We
  first choose a good step size (an algorithm parameter) by running some
  short sampling runs (of length 2,000 steps). Then we run the 100,000
  step sampling run to generate samples of the joint posterior
  distribution.</p>
  <code language="julia">const MC = CalibrateEmulateSample.MarkovChainMonteCarlo
mcmc = MC.MCMCWrapper(
    MC.RWMHSampling(), y_obs, prior, emulator,
)
# choose a step size
new_step = MC.optimize_stepsize(
    mcmc; init_stepsize = 0.1, N = 2000,
)
# Now begin the actual MCMC
chain = MC.sample(
    mcmc, 100_000; stepsize = new_step, discard_initial = 2_000,
)</code>
  <fig>
    <caption><p>The joint posterior distribution histogram
    <styled-content id="figU003AGP_2d_posterior"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sinusoid_MCMC_hist_GP.png" />
  </fig>
  <p>A histogram of the samples from the CES algorithm is displayed in
  <xref alt="[fig:GP_2d_posterior]" rid="figU003AGP_2d_posterior">[fig:GP_2d_posterior]</xref>.
  We see that the posterior distribution contains the true value
  <inline-formula><alternatives>
  <tex-math><![CDATA[(3.0, 7.0)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>3.0</mml:mn><mml:mo>,</mml:mo><mml:mn>7.0</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  with high probability.</p>
</sec>
<sec id="research-projects-using-the-package">
  <title>Research projects using the package</title>
  <p>Some research projects that use this codebase, or modifications of
  it, are</p>
  <list list-type="bullet">
    <list-item>
      <p>(<xref alt="Dunbar et al., 2021" rid="ref-DunbarU003A2021" ref-type="bibr">Dunbar
      et al., 2021</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Bieli et al., 2022" rid="ref-BieliU003A2022" ref-type="bibr">Bieli
      et al., 2022</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Hillier, 2022" rid="ref-HillierU003A2022" ref-type="bibr">Hillier,
      2022</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Howland et al., 2022" rid="ref-HowlandU003A2022" ref-type="bibr">Howland
      et al., 2022</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Dunbar, Howland, et al., 2022" rid="ref-DunbarU003A2022b" ref-type="bibr">Dunbar,
      Howland, et al., 2022</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="Mansfield &amp; Sheshadri, 2022" rid="ref-MansfieldU003A2022" ref-type="bibr">Mansfield
      &amp; Sheshadri, 2022</xref>)</p>
    </list-item>
    <list-item>
      <p>(<xref alt="King et al., 2023" rid="ref-KingU003A2023" ref-type="bibr">King
      et al., 2023</xref>)</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from several others who played a role
  in the evolution of this package. These include Adeline Hillier,
  Ignacio Lopez Gomez and Thomas Jackson. The development of this
  package was supported by the generosity of Eric and Wendy Schmidt by
  recommendation of the Schmidt Futures program, National Science
  Foundation Grant AGS-1835860, the Defense Advanced Research Projects
  Agency (Agreement No.¬†HR00112290030), the Heising-Simons Foundation,
  Audi Environmental Foundation, and the Cisco Foundation.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM Review</source>
      <publisher-name>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>59</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NottU003A2018">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Nott</surname><given-names>David J.</given-names></name>
        <name><surname>Ong</surname><given-names>Victor M.-H.</given-names></name>
        <name><surname>Fan</surname><given-names>Y.</given-names></name>
        <name><surname>Sisson</surname><given-names>S. A.</given-names></name>
      </person-group>
      <article-title>High-Dimensional ABC</article-title>
      <source>Handbook of Approximate Bayesian Computation</source>
      <publisher-name>CRC Press</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <isbn>978-1-315-11719-5</isbn>
      <pub-id pub-id-type="doi">10.1201/9781315117195-8</pub-id>
      <fpage>211</fpage>
      <lpage>241</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ClearyU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cleary</surname><given-names>Emmet</given-names></name>
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Lan</surname><given-names>Shiwei</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Calibrate, emulate, sample</article-title>
      <source>Journal of Computational Physics</source>
      <year iso-8601-date="2021">2021</year>
      <volume>424</volume>
      <issn>0021-9991</issn>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2020.109716</pub-id>
      <fpage>109716</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-DunbarU003A2022a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Lopez-Gomez</surname><given-names>Ignacio</given-names></name>
        <name><surname>Garbuno-I√±igo</surname><given-names>Alfredo Garbuno-I√±igo</given-names></name>
        <name><surname>Huang</surname><given-names>Daniel Zhengyu</given-names></name>
        <name><surname>Bach</surname><given-names>Eviatar</given-names></name>
        <name><surname>Wu</surname><given-names>Jin-long</given-names></name>
      </person-group>
      <article-title>EnsembleKalmanProcesses.jl: Derivative-free ensemble-based model calibration</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>80</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04869</pub-id>
      <fpage>4869</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-BieliU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bieli</surname><given-names>Melanie</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Jong</surname><given-names>Emily K. de</given-names></name>
        <name><surname>Jaruga</surname><given-names>Anna</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Bischoff</surname><given-names>Tobias</given-names></name>
      </person-group>
      <article-title>An efficient Bayesian approach to learning droplet collision kernels: Proof of concept using ‚ÄúCloudy,‚Äù a new n-moment bulk microphysics scheme</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1029/2022MS002994</pub-id>
      <fpage>e2022MS002994</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HillierU003A2022">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Hillier</surname><given-names>Adeline</given-names></name>
      </person-group>
      <article-title>Supervised calibration and uncertainty quantification of subgrid closure parameters using ensemble Kalman inversion</article-title>
      <publisher-name>Massachusetts Institute of Technology. Department of Electrical Engineering; Computer Science</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-RasmussenU003A2006">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Williams</surname><given-names>Christopher KI</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Carl Edward</given-names></name>
      </person-group>
      <source>Gaussian processes for machine learning</source>
      <publisher-name>MIT press Cambridge, MA</publisher-name>
      <year iso-8601-date="2006">2006</year>
      <volume>2</volume>
      <pub-id pub-id-type="doi">10.1142/S0129065704001899</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-IglesiasU003A2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Iglesias</surname><given-names>Marco A</given-names></name>
        <name><surname>Law</surname><given-names>Kody JH</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Ensemble kalman methods for inverse problems</article-title>
      <source>Inverse Problems</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>29</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1088/0266-5611/29/4/045001</pub-id>
      <fpage>045001</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-RahimiU003A2007">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rahimi</surname><given-names>Ali</given-names></name>
        <name><surname>Recht</surname><given-names>Benjamin</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Random features for large-scale kernel machines.</article-title>
      <source>NIPS</source>
      <year iso-8601-date="2007">2007</year>
      <volume>3</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf</uri>
      <fpage>5</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-RahimiU003A2008">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rahimi</surname><given-names>Ali</given-names></name>
        <name><surname>Recht</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>Uniform approximation of functions with random bases</article-title>
      <source>2008 46th annual allerton conference on communication, control, and computing</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <pub-id pub-id-type="doi">10.1109/allerton.2008.4797607</pub-id>
      <fpage>555</fpage>
      <lpage>561</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LiuU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Fanghui</given-names></name>
        <name><surname>Huang</surname><given-names>Xiaolin</given-names></name>
        <name><surname>Chen</surname><given-names>Yudong</given-names></name>
        <name><surname>Suykens</surname><given-names>Johan A. K.</given-names></name>
      </person-group>
      <article-title>Random features for kernel approximation: A survey on algorithms, theory, and beyond</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <year iso-8601-date="2022">2022</year>
      <volume>44</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2021.3097011</pub-id>
      <fpage>7128</fpage>
      <lpage>7148</lpage>
    </element-citation>
  </ref>
  <ref id="ref-CotterU003A2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cotter</surname><given-names>S. L.</given-names></name>
        <name><surname>Roberts</surname><given-names>G. O.</given-names></name>
        <name><surname>Stuart</surname><given-names>A. M.</given-names></name>
        <name><surname>White</surname><given-names>D.</given-names></name>
      </person-group>
      <article-title>MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster</article-title>
      <source>Statistical Science</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>28</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1214/13-STS421</pub-id>
      <fpage>424 </fpage>
      <lpage> 446</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SherlockU003A2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sherlock</surname><given-names>Chris</given-names></name>
        <name><surname>Fearnhead</surname><given-names>Paul</given-names></name>
        <name><surname>Roberts</surname><given-names>Gareth O.</given-names></name>
      </person-group>
      <article-title>The random walk metropolis: Linking theory and practice through a case study</article-title>
      <source>Statistical Science</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>25</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1214/10-sts327</pub-id>
      <fpage>172</fpage>
      <lpage>190</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DunbarU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Calibration and uncertainty quantification of convective parameters in an idealized GCM</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>13</volume>
      <issue>9</issue>
      <pub-id pub-id-type="doi">10.1029/2020MS002454</pub-id>
      <fpage>e2020MS002454</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HowlandU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howland</surname><given-names>Michael F.</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
      </person-group>
      <article-title>Parameter uncertainty quantification in an idealized GCM with a seasonal cycle</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1029/2021MS002735</pub-id>
      <fpage>e2021MS002735</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-DunbarU003A2022b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Howland</surname><given-names>Michael F.</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Ensemble-based experimental design for targeting data acquisition to inform climate models</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>9</issue>
      <pub-id pub-id-type="doi">10.1029/2022MS002997</pub-id>
      <fpage>e2022MS002997</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-scikit-learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FairbrotherU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fairbrother</surname><given-names>Jamie</given-names></name>
        <name><surname>Nemeth</surname><given-names>Christopher</given-names></name>
        <name><surname>Rischard</surname><given-names>Maxime</given-names></name>
        <name><surname>Brea</surname><given-names>Johanni</given-names></name>
        <name><surname>Pinder</surname><given-names>Thomas</given-names></name>
      </person-group>
      <article-title>GaussianProcesses. Jl: A nonparametric bayes package for the julia language</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2022">2022</year>
      <volume>102</volume>
      <pub-id pub-id-type="doi">10.18637/jss.v102.i01</pub-id>
      <fpage>1</fpage>
      <lpage>36</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DixitU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dixit</surname><given-names>Vaibhav Kumar</given-names></name>
        <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>GlobalSensitivity.jl: Performant and parallel global sensitivity analysis with julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>76</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04561</pub-id>
      <fpage>4561</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Garbuno-InigoU003A2020b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>NuÃàsken</surname><given-names>Nikolas</given-names></name>
        <name><surname>Reich</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Affine invariant interacting Langevin dynamics for Bayesian inference</article-title>
      <source>SIAM Journal on Applied Dynamical Systems</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>19</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1137/19M1304891</pub-id>
      <fpage>1633</fpage>
      <lpage>1658</lpage>
    </element-citation>
  </ref>
  <ref id="ref-TankhilevichU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tankhilevich</surname><given-names>Evgeny</given-names></name>
        <name><surname>Ish-Horowicz</surname><given-names>Jonathan</given-names></name>
        <name><surname>Hameed</surname><given-names>Tara</given-names></name>
        <name><surname>Roesch</surname><given-names>Elisabeth</given-names></name>
        <name><surname>Kleijn</surname><given-names>Istvan</given-names></name>
        <name><surname>Stumpf</surname><given-names>Michael P H</given-names></name>
        <name><surname>He</surname><given-names>Fei</given-names></name>
      </person-group>
      <article-title>GpABC: a Julia package for approximate Bayesian computation with Gaussian process emulation</article-title>
      <source>Bioinformatics</source>
      <year iso-8601-date="2020-02">2020</year><month>02</month>
      <issn>1367-4803</issn>
      <pub-id pub-id-type="doi">10.1093/bioinformatics/btaa078</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-HuangU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Daniel Zhengyu</given-names></name>
        <name><surname>Huang</surname><given-names>Jiaoyang</given-names></name>
        <name><surname>Reich</surname><given-names>Sebastian</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Efficient derivative-free bayesian inference for large-scale inverse problems</article-title>
      <source>Inverse Problems</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2022-10">2022</year><month>10</month>
      <volume>38</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1088/1361-6420/ac99fa</pub-id>
      <fpage>125006</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-MansfieldU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mansfield</surname><given-names>L. A.</given-names></name>
        <name><surname>Sheshadri</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Calibration and uncertainty quantification of a gravity wave parameterization: A case study of the Quasi-Biennial Oscillation in an intermediate complexity climate model</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>11</issue>
      <issn>1942-2466</issn>
      <pub-id pub-id-type="doi">10.1029/2022MS003245</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-KingU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>King</surname><given-names>Robert C</given-names></name>
        <name><surname>Mansfield</surname><given-names>Laura A</given-names></name>
        <name><surname>Sheshadri</surname><given-names>Aditi</given-names></name>
      </person-group>
      <article-title>Bayesian history matching applied to the calibration of a gravity wave parameterization</article-title>
      <publisher-name>Preprints</publisher-name>
      <year iso-8601-date="2023-12">2023</year><month>12</month>
      <pub-id pub-id-type="doi">10.22541/essoar.170365299.96491153/v1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-MetropolisU003A1953">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Metropolis</surname><given-names>Nicholas</given-names></name>
        <name><surname>Rosenbluth</surname><given-names>Arianna W</given-names></name>
        <name><surname>Rosenbluth</surname><given-names>Marshall N</given-names></name>
        <name><surname>Teller</surname><given-names>Augusta H</given-names></name>
        <name><surname>Teller</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>Equation of state calculations by fast computing machines</article-title>
      <source>The journal of chemical physics</source>
      <publisher-name>American Institute of Physics</publisher-name>
      <year iso-8601-date="1953">1953</year>
      <volume>21</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1063/1.1699114</pub-id>
      <fpage>1087</fpage>
      <lpage>1092</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HugginsU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huggins</surname><given-names>Bobby</given-names></name>
        <name><surname>Li</surname><given-names>Chengkun</given-names></name>
        <name><surname>Tobaben</surname><given-names>Marlon</given-names></name>
        <name><surname>Aarnos</surname><given-names>Mikko J.</given-names></name>
        <name><surname>Acerbi</surname><given-names>Luigi</given-names></name>
      </person-group>
      <article-title>PyVBMC: Efficient bayesian inference in python</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>86</issue>
      <pub-id pub-id-type="doi">10.21105/joss.05428</pub-id>
      <fpage>5428</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-GammalU003A2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gammal</surname><given-names>Jonas El</given-names></name>
        <name><surname>Sch√∂neberg</surname><given-names>Nils</given-names></name>
        <name><surname>Torrado</surname><given-names>Jes√∫s</given-names></name>
        <name><surname>Fidler</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>Fast and robust bayesian inference using gaussian processes with GPry</article-title>
      <source>Journal of Cosmology and Astroparticle Physics</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2023-10">2023</year><month>10</month>
      <volume>2023</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1088/1475-7516/2023/10/021</pub-id>
      <fpage>021</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-livingstoneU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Livingstone</surname><given-names>Samuel</given-names></name>
        <name><surname>Zanella</surname><given-names>Giacomo</given-names></name>
      </person-group>
      <article-title>The barker proposal: Combining robustness and efficiency in gradient-based MCMC</article-title>
      <source>Journal of the Royal Statistical Society Series B: Statistical Methodology</source>
      <publisher-name>Oxford University Press</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>84</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1111/rssb.12482</pub-id>
      <fpage>496</fpage>
      <lpage>523</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hoffmanU003A2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matthew D</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>The no-u-turn sampler: Adaptively setting path lengths in hamiltonian monte carlo.</article-title>
      <source>J. Mach. Learn. Res.</source>
      <year iso-8601-date="2014">2014</year>
      <volume>15</volume>
      <issue>1</issue>
      <fpage>1593</fpage>
      <lpage>1623</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
