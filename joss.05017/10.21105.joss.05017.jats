<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5017</article-id>
<article-id pub-id-type="doi">10.21105/joss.05017</article-id>
<title-group>
<article-title>DeBEIR: A Python Package for Dense Bi-Encoder Information
Retrieval</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1787-8090</contrib-id>
<name>
<surname>Nguyen</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4927-3937</contrib-id>
<name>
<surname>Karimi</surname>
<given-names>Sarvnaz</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7663-1421</contrib-id>
<name>
<surname>Xing</surname>
<given-names>Zhenchang</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Australian National University, School of
Computing</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Commonwealth Scientific and Industrial Research
Organisation, Data61</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-10-04">
<day>4</day>
<month>10</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>87</issue>
<fpage>5017</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>information retrieval</kwd>
<kwd>dense retrieval</kwd>
<kwd>bi-encoder</kwd>
<kwd>transformers</kwd>
<kwd>pytorch</kwd>
<kwd>python</kwd>
<kwd>deep learning</kwd>
<kwd>neural networks</kwd>
<kwd>machine learning</kwd>
<kwd>natural language processing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Information Retrieval (IR) is the task of retrieving documents
  given a query or information need. These documents are retrieved and
  ranked based on a relevance function or relevance model such as
  Best-Matching 25 (BM25)
  (<xref alt="Robertson et al., 1995" rid="ref-bm25" ref-type="bibr">Robertson
  et al., 1995</xref>). Although deep learning has been successful in
  other computer science fields, such as computer vision with AlexNet
  (<xref alt="Krizhevsky et al., 2012" rid="ref-alexnet" ref-type="bibr">Krizhevsky
  et al., 2012</xref>) and Inception
  (<xref alt="Szegedy et al., 2014" rid="ref-googlenet" ref-type="bibr">Szegedy
  et al., 2014</xref>) and natural language processing with transformers
  (<xref alt="Devlin et al., 2019" rid="ref-orig-bert-2018" ref-type="bibr">Devlin
  et al., 2019</xref>;
  <xref alt="Lee et al., 2019" rid="ref-biobert" ref-type="bibr">Lee et
  al., 2019</xref>;
  <xref alt="Yang Liu &amp; Lapata, 2019" rid="ref-liuU003A2019" ref-type="bibr">Yang
  Liu &amp; Lapata, 2019</xref>); success in information retrieval was
  limited due to comparisons against weak baselines
  (<xref alt="Yang et al., 2019" rid="ref-yang2019critically" ref-type="bibr">Yang
  et al., 2019</xref>). However, in 2019
  (<xref alt="Lin, 2019" rid="ref-lin-neural-recantation" ref-type="bibr">Lin,
  2019</xref>), deep learning in information retrieval could surpass
  less computationally intensive keyword-based statistical models in
  terms of retrieval effectiveness, sparking a resurgence in the field
  of dense retrieval. Dense retrieval is the task of retrieving
  documents given a query or information need using a dense vector
  representation of the query and documents
  (<xref alt="Lin et al., 2021" rid="ref-lin2021pretrained" ref-type="bibr">Lin
  et al., 2021</xref>). The dense vector representation is obtained by
  passing the query and documents through a neural network. The neural
  network is usually a pre-trained language model such as BERT
  (<xref alt="Devlin et al., 2019" rid="ref-orig-bert-2018" ref-type="bibr">Devlin
  et al., 2019</xref>) or RoBERTa
  (<xref alt="Yinhan Liu et al., 2019" rid="ref-roberta" ref-type="bibr">Yinhan
  Liu et al., 2019</xref>). The dense query vector representation is
  then used to retrieve documents using a similarity function such as
  cosine similarity.</p>
  <p>Unlike statistical learning, tuning deep learning retrieval methods
  is often costly and time-consuming. This cost makes it essential to
  efficently automate much of the training, tuning and evaluation
  processes.</p>
  <p>We present DeBEIR a library for: (1) facilitating dense retrieval
  research, primarily focusing on bi-encoder dense retrieval where query
  and documents dense vectors are generated separately
  (<xref alt="Reimers &amp; Gurevych, 2019" rid="ref-reimers2019" ref-type="bibr">Reimers
  &amp; Gurevych, 2019</xref>), (2) expedited experimentation in dense
  retrieval research by reducing boilerplate code through an
  interchangeable pipeline API and code extendability through the
  inheritance of general classes; (3) abstractions for standard training
  loops and hyperparameter tuning from easy-to-define configuration
  files.</p>
  <p>DeBEIR is aimed at helping practitioners, researchers and data
  scientists experimenting with bi-encoders by providing them with dense
  retrieval methods that are easy to use out of the box but also have
  additional extendability for more nuanced research. Furthermore, our
  pipeline runs asynchronously to reduce I/O performance bottlenecks,
  facilitating faster experiments and research.</p>
  <p>A brief summary of the pipeline is
  (<xref alt="[fig:training]" rid="figU003Atraining">[fig:training]</xref>):</p>
  <list list-type="order">
    <list-item>
      <p>Configuration based on Tomâ€™s Obvious Minimal Language (TOML)
      files; these are loaded in a class factory to create pipeline
      objects.</p>
    </list-item>
    <list-item>
      <p>An executor object takes in a query builder object. The purpose
      of the query builder object is to define the mapping of the
      documents and which parts of the query to use for query
      execution.</p>
    </list-item>
    <list-item>
      <p>The executor object asynchronously runs the queries.</p>
    </list-item>
    <list-item>
      <p>Finally, an evaluator object uses the results to list metrics
      defined by a configuration file against an oracle test set.</p>
    </list-item>
  </list>
  <p>This pipeline is condensed into a single class that can be built
  from a configuration file.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Dense retrieval has been popular in Information Retrieval since
  2015 (<xref alt="Guo et al., 2017" rid="ref-drmm" ref-type="bibr">Guo
  et al., 2017</xref>;
  <xref alt="Hui et al., 2017" rid="ref-paccr" ref-type="bibr">Hui et
  al., 2017</xref>;
  <xref alt="Yin et al., 2015" rid="ref-abcnn" ref-type="bibr">Yin et
  al., 2015</xref>). Retrieval effectiveness of these dense retrieval
  methods was often compared against weaker baselines and was not shown
  significantly stronger than statistical models
  (<xref alt="Yang et al., 2019" rid="ref-yang2019critically" ref-type="bibr">Yang
  et al., 2019</xref>), such as a well-tuned BM25 model while being
  considerably slower. This situation is similar to what happened in the
  early 2000s, where there was a slow down in retrieval effectiveness
  from the use of less robust baselines
  (<xref alt="Armstrong et al., 2009" rid="ref-ArmstrongU003A2009" ref-type="bibr">Armstrong
  et al., 2009</xref>) when proposing new methods.</p>
  <p>However, attitudes on dense retrieval changed when transformer
  models were found to be effective once fine-tuned on Natural Language
  Inference tasks or Ms-Marco
  (<xref alt="T. Nguyen et al., 2016" rid="ref-msmarco" ref-type="bibr">T.
  Nguyen et al., 2016</xref>) as a cross-encoder
  (<xref alt="Lin, 2019" rid="ref-lin-neural-recantation" ref-type="bibr">Lin,
  2019</xref>), significantly overtaking even the best BM25 models.</p>
  <p>There are generally two classes of dense retrieval models for IR:
  (1) the cross-encoder, which encodes queries and documents at query
  time and (2) the bi-encoder, which can encode documents at index time
  and queries at query time. The cross-encoder is generally more
  effective than the bi-encoder model for retrieval
  (<xref alt="Lin et al., 2021" rid="ref-lin2021pretrained" ref-type="bibr">Lin
  et al., 2021</xref>). However, this increased effectiveness requires a
  more substantial computation and can be a bottleneck in production
  systems. Therefore, a less expensive model such as BM25 is typically
  used to retrieve smaller candidate lists (first-stage retrieval) to be
  fed to second-stage retrieval re-ranking by a cross-encoder.</p>
  <p>Although cross-encoders are more accurate than bi-encoders,
  bi-encoder are more effective than BM25
  (<xref alt="V. Nguyen et al., 2022" rid="ref-search-like-an-expert-2022" ref-type="bibr">V.
  Nguyen et al., 2022</xref>) and are faster than cross-encoders.
  Therefore, a gap in the literature in IR is to replace BM25
  first-stage retrieval with a bi-encoder or otherwise used as the sole
  ranking system, without a second-stage re-ranker. However, current
  libraries do not address this use case because it requires integration
  with the indexing and querying pipeline of the search engine.</p>
  <p>DeBEIR is a library that addresses this gap by facilitating
  bi-encoder research and provides base classes with flexible
  functionality through inheritance. While we provide cross-encoder
  re-rankers for feature completeness, the libraryâ€™s priority is
  facilitating bi-encoder research. The strength of bi-encoders lies in
  the offline indexing of dense vectors. These vectors can then be used
  for first-stage retrieval and potentially passed to a second-stage
  retrieval system such as a cross-encoder. Bi-encoders can be used as
  the sole retrieval system when there is a lack of training data
  (<xref alt="V. Nguyen et al., 2022" rid="ref-search-like-an-expert-2022" ref-type="bibr">V.
  Nguyen et al., 2022</xref>) and, therefore, can be more useful in
  areas such as biomedical IR, where training data is expensive to
  annotate and therefore scarce. Cross-encoders, however, require large
  amounts of training data for effectiveness.</p>
  <p>The DeBEIR library offers an API for commonly used functions for
  training, hyper-parameter tuning
  (<xref alt="[fig:training]" rid="figU003Atraining">[fig:training]</xref>)
  and evaluation of transformer-based models. The pipeline can be broken
  up into multiple stages: parsing, query building, query execution,
  serialization and evaluation
  (<xref alt="[fig:pipeline]" rid="figU003Apipeline">[fig:pipeline]</xref>).
  Furthermore, we package our caching mechanism for the expensive
  encoding operations to speed up the pipeline during repeated
  experimentation.</p>
  <p>Although similar libraries exist, such as sentence-transformers
  (<xref alt="Reimers &amp; Gurevych, 2019" rid="ref-reimers2019" ref-type="bibr">Reimers
  &amp; Gurevych, 2019</xref>), and openNIR
  (<xref alt="MacAvaney, 2020" rid="ref-opennir" ref-type="bibr">MacAvaney,
  2020</xref>), they have less of a focus on the early stages of the
  dense retrieval pipeline. This stage involves indexing the textual
  data from the corpora and indexing dense vector representations, which
  is only helpful for bi-encoder type models over the traditional
  cross-encoder and is thus not typically explored by other libraries.
  Other limitations include a lack of extendability which restricts the
  usersâ€™ options for training customization (we provide base classes
  that can be inherited) or the library is tailored to general-purpose
  machine learning rather than informational retrieval. Finally, these
  libraries have a limited caching mechanism, as cross-encoders
  typically does not require this capability as it is decoupled from the
  index. Bi-encoders can have queries cached at query time to make
  repeated query calls to the index significantly faster.</p>
  <p>DeBEIR will help facilitate early-stage dense retrieval and rapid
  experimentation research with bi-encoders. It is also flexible enough
  for second-stage retrieval using cross-encoders from this library or
  other libraries. We will continue to improve this tool over time.</p>
  <fig>
    <caption><p>Standard flow of the DeEBIR query/evaluation
    loop.<styled-content id="figU003Apipeline"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/pipeline.pdf" />
  </fig>
  <fig>
    <caption><p>Standard flow of the DeEBIR training
    loop.<styled-content id="figU003Atraining"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/training.pdf" />
  </fig>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>The DeBEIR library uses Sentence-Transformers, Hugging Faceâ€™s
  transformers and Datasets, allRank, Optuna, Elasticsearch and
  Trectools python packages.</p>
  <p>This search is supported by CSIRO Data61, an Australian Government
  agency through the Precision Medicine FSP program and the Australian
  Research Training Program. We extend thanks to Brian Jin (Data61) for
  providing a code review.</p>
</sec>
<sec id="examples">
  <title>Examples</title>
  <sec id="pipeline">
    <title>Pipeline</title>
    <p>The pipeline is a single class that can be built from a
    configuration file. The configuration file is a TOML file that
    defines the pipeline stages and their parameters. The pipeline is
    built using a class factory that takes in the configuration file and
    creates the pipeline stages. The pipeline stages are then executed
    in order.</p>
    <code language="python">from debeir.interfaces.pipeline import NIRPipeline
from debeir.interfaces.callbacks import (SerializationCallback,
                                        EvaluationCallback)
from debeir.evaluation import Evaluator

p = NIRPipeline.build_from_config(config_fp=&quot;./tests/config.toml&quot;,
                                  engine=&quot;elasticsearch&quot;,
                                  nir_config_fp=&quot;./tests/nir_config.toml&quot;)

# Optional callbacks to serialize to disk
serial_cb = SerializationCallback(p.config, p.nir_settings)

# Or evaluation
evaluator = Evaluator.build_from_config(p.config, metrics_config=p.metrics_config)
evaluate_cb = EvaluationCallback(evaluator,
                                 config=p.config)

p.add_callback(serial_cb)
p.add_callback(evaluate_cb)

# Asynchronously execute queries
results = await p.run_pipeline()

# Post processing of results can go here</code>
  </sec>
  <sec id="training-a-model">
    <title>Training a model</title>
    <code language="python">import wandb

from debeir.training.hparm_tuning.trainer import SentenceTransformerTrainer
from debeir.training.hparm_tuning.config import HparamConfig
from sentence_transformers import evaluation

# Load a hyper-parameter configuration file
hparam_config = HparamConfig.from_json(
        &quot;./configs/training/submission.json&quot;
)

# Integration with wandb
wandb.wandb.init(project=&quot;My Project&quot;)

# Create a trainer object
trainer = SentenceTransformerTrainer(
    dataset=get_dataset(), # Specify some dataloading function here
    evaluator_fn=evaluation.BinaryClassificationEvaluator,
    hparams_config=hparam_config,
    use_wandb=True
)

# Foward parameters to underlying SentenceTransformer model
trainer.fit(
    save_best_model=True,
    checkpoint_save_steps=179
)</code>
  </sec>
  <sec id="hyperparameter-tuning">
    <title>Hyperparameter tuning</title>
    <code language="python">from sentence_transformers import evaluation
from debeir.training.hparm_tuning.optuna_rank import (run_optuna_with_wandb,
                                                      print_optuna_stats)
from debeir.training.hparm_tuning.trainer import SentenceTransformerHparamTrainer
from debeir.training.hparm_tuning.config import HparamConfig

# Load a hyper-parameter configuration file with optuna parameters
hparam_config = HparamConfig.from_json(
   &quot;./configs/hparam/trec2021_tuning.json&quot;
)

trainer = SentenceTransformerHparamTrainer(
   dataset_loading_fn=data_loading_fn,
   evaluator_fn=evaluation.BinaryClassificationEvaluator,
   hparams_config=hparam_config,
)

# Run optuna with wandb integration
study = run_optuna_with_wandb(trainer, wandb_kwargs={
    &quot;project&quot;: &quot;my-hparam-tuning-project&quot;
})

# Print optuna stats and best run
print_optuna_stats(study)</code>
    <p>More information on the library is found on the GitHub page,
    <ext-link ext-link-type="uri" xlink:href="https://www.github.com/ayuei/debeir">DeBEIR</ext-link>.
    Any feedback and suggestions are welcome by opening a thread in
    <ext-link ext-link-type="uri" xlink:href="https://www.github.com/ayuei/debeir/issues">DeBEIR
    issues</ext-link>.</p>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-bm25">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Robertson</surname><given-names>Stephen</given-names></name>
        <name><surname>Walker</surname><given-names>Steve</given-names></name>
        <name><surname>Jones</surname><given-names>Susan</given-names></name>
        <name><surname>Hancock-Beaulieu</surname><given-names>Micheline</given-names></name>
        <name><surname>Gatford</surname><given-names>Mike</given-names></name>
      </person-group>
      <article-title>Okapi at TREC-3.</article-title>
      <source>TREC</source>
      <publisher-loc>Gaithersburg, MD, US</publisher-loc>
      <year iso-8601-date="1995-01">1995</year><month>01</month>
      <uri>https://trec.nist.gov/pubs/trec3/t3\_proceedings.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-alexnet">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Krizhevsky</surname><given-names>Alex</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey E</given-names></name>
      </person-group>
      <article-title>ImageNet classification with deep convolutional neural networks</article-title>
      <source>Advances in neural information processing systems 25</source>
      <person-group person-group-type="editor">
        <name><surname>Pereira</surname><given-names>F.</given-names></name>
        <name><surname>Burges</surname><given-names>C. J. C.</given-names></name>
        <name><surname>Bottou</surname><given-names>L.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <pub-id pub-id-type="doi">10.1145/3065386</pub-id>
      <fpage>1097</fpage>
      <lpage>1105</lpage>
    </element-citation>
  </ref>
  <ref id="ref-googlenet">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Szegedy</surname><given-names>Christian</given-names></name>
        <name><surname>Liu</surname><given-names>Wei</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Sermanet</surname><given-names>Pierre</given-names></name>
        <name><surname>Reed</surname><given-names>Scott</given-names></name>
        <name><surname>Anguelov</surname><given-names>Dragomir</given-names></name>
        <name><surname>Erhan</surname><given-names>Dumitru</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Rabinovich</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Going deeper with convolutions</article-title>
      <source>IEEE conference on computer vision and pattern recognition</source>
      <publisher-loc>Boston, MA</publisher-loc>
      <year iso-8601-date="2014">2014</year>
      <issn>1063-6919</issn>
      <pub-id pub-id-type="doi">10.1109/CVPR.2015.7298594</pub-id>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-orig-bert-2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Devlin</surname><given-names>Jacob</given-names></name>
        <name><surname>Chang</surname><given-names>Ming-Wei</given-names></name>
        <name><surname>Lee</surname><given-names>Kenton</given-names></name>
        <name><surname>Toutanova</surname><given-names>Kristina</given-names></name>
      </person-group>
      <article-title>BERT: Pre-training of deep bidirectional transformers for language understanding</article-title>
      <source>Proceedings of the conference of the north American chapter of the association for computational linguistics: Human language technologies</source>
      <publisher-loc>Minneapolis, MN</publisher-loc>
      <year iso-8601-date="2019-06">2019</year><month>06</month>
      <uri>https://www.aclweb.org/anthology/N19-1423</uri>
      <pub-id pub-id-type="doi">10.18653/v1/N19-1423</pub-id>
      <fpage>4171</fpage>
      <lpage>4186</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liuU003A2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Yang</given-names></name>
        <name><surname>Lapata</surname><given-names>Mirella</given-names></name>
      </person-group>
      <article-title>Text summarization with pretrained encoders</article-title>
      <source>Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)</source>
      <publisher-loc>Hong Kong, China</publisher-loc>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <uri>https://www.aclweb.org/anthology/D19-1387</uri>
      <pub-id pub-id-type="doi">10.18653/v1/D19-1387</pub-id>
      <fpage>3730</fpage>
      <lpage>3740</lpage>
    </element-citation>
  </ref>
  <ref id="ref-biobert">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Jinhyuk</given-names></name>
        <name><surname>Yoon</surname><given-names>Wonjin</given-names></name>
        <name><surname>Kim</surname><given-names>Sungdong</given-names></name>
        <name><surname>Kim</surname><given-names>Donghyeon</given-names></name>
        <name><surname>Kim</surname><given-names>Sunkyu</given-names></name>
        <name><surname>So</surname><given-names>Chan Ho</given-names></name>
        <name><surname>Kang</surname><given-names>Jaewoo</given-names></name>
      </person-group>
      <article-title>BioBERT: a pre-trained biomedical language representation model for biomedical text mining</article-title>
      <source>Bioinformatics</source>
      <year iso-8601-date="2019-09">2019</year><month>09</month>
      <volume>36</volume>
      <issue>4</issue>
      <issn>1367-4803</issn>
      <uri>https://doi.org/10.1093/bioinformatics/btz682</uri>
      <pub-id pub-id-type="doi">10.1093/bioinformatics/btz682</pub-id>
      <fpage>1234</fpage>
      <lpage>1240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lin-neural-recantation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lin</surname><given-names>Jimmy</given-names></name>
      </person-group>
      <article-title>Neural hype, justified! A recantation</article-title>
      <source>ACM SIGIR Forum</source>
      <year iso-8601-date="2019">2019</year>
      <volume>53</volume>
      <uri>http://sigir.org/wp-content/uploads/2019/december/p088.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-yang2019critically">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Wei</given-names></name>
        <name><surname>Lu</surname><given-names>Kuang</given-names></name>
        <name><surname>Yang</surname><given-names>Peilin</given-names></name>
        <name><surname>Lin</surname><given-names>Jimmy</given-names></name>
      </person-group>
      <article-title>Critically examining the&quot; neural hype&quot; weak baselines and the additivity of effectiveness gains from neural ranking models</article-title>
      <source>Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</source>
      <publisher-loc>Paris, France</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <uri>https://dl.acm.org/doi/10.1145/3331184.3331340</uri>
      <pub-id pub-id-type="doi">10.1145/3331184.3331340</pub-id>
      <fpage>1129</fpage>
      <lpage>1132</lpage>
    </element-citation>
  </ref>
  <ref id="ref-roberta">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Yinhan</given-names></name>
        <name><surname>Ott</surname><given-names>Myle</given-names></name>
        <name><surname>Goyal</surname><given-names>Naman</given-names></name>
        <name><surname>Du</surname><given-names>Jingfei</given-names></name>
        <name><surname>Joshi</surname><given-names>Mandar</given-names></name>
        <name><surname>Chen</surname><given-names>Danqi</given-names></name>
        <name><surname>Levy</surname><given-names>Omer</given-names></name>
        <name><surname>Lewis</surname><given-names>Mike</given-names></name>
        <name><surname>Zettlemoyer</surname><given-names>Luke</given-names></name>
        <name><surname>Stoyanov</surname><given-names>Veselin</given-names></name>
      </person-group>
      <article-title>RoBERTa: A robustly optimized BERT pretraining approach</article-title>
      <source>Computing Research Repository</source>
      <year iso-8601-date="2019">2019</year>
      <volume>abs/1907.11692</volume>
      <uri>http://arxiv.org/abs/1907.11692</uri>
    </element-citation>
  </ref>
  <ref id="ref-drmm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Guo</surname><given-names>Jiafeng</given-names></name>
        <name><surname>Fan</surname><given-names>Yixing</given-names></name>
        <name><surname>Ai</surname><given-names>Qingyao</given-names></name>
        <name><surname>Croft</surname><given-names>Bruce</given-names></name>
      </person-group>
      <article-title>A deep relevance matching model for ad-hoc retrieval</article-title>
      <source>Computing Research Repository</source>
      <publisher-loc>Indianapolis, IN</publisher-loc>
      <year iso-8601-date="2017">2017</year>
      <volume>abs/1711.08611</volume>
      <uri>http://arxiv.org/abs/1711.08611</uri>
      <fpage>55</fpage>
      <lpage>64</lpage>
    </element-citation>
  </ref>
  <ref id="ref-abcnn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yin</surname><given-names>Wenpeng</given-names></name>
        <name><surname>SchÃ¼tze</surname><given-names>Hinrich</given-names></name>
        <name><surname>Xiang</surname><given-names>Bing</given-names></name>
        <name><surname>Zhou</surname><given-names>Bowen</given-names></name>
      </person-group>
      <article-title>ABCNN: Attention-based convolutional neural network for modeling sentence pairs</article-title>
      <source>Computing Research Repository</source>
      <year iso-8601-date="2015">2015</year>
      <volume>abs/1512.05193</volume>
      <uri>http://arxiv.org/abs/1512.05193</uri>
    </element-citation>
  </ref>
  <ref id="ref-paccr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hui</surname><given-names>Kai</given-names></name>
        <name><surname>Yates</surname><given-names>Andrew</given-names></name>
        <name><surname>Berberich</surname><given-names>Klaus</given-names></name>
        <name><surname>Melo</surname><given-names>Gerard de</given-names></name>
      </person-group>
      <article-title>A position-aware deep model for relevance matching in information retrieval</article-title>
      <source>Computing Research Repository</source>
      <year iso-8601-date="2017">2017</year>
      <volume>abs/1704.03940</volume>
      <uri>http://arxiv.org/abs/1704.03940</uri>
    </element-citation>
  </ref>
  <ref id="ref-reimers2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Reimers</surname><given-names>Nils</given-names></name>
        <name><surname>Gurevych</surname><given-names>Iryna</given-names></name>
      </person-group>
      <article-title>Sentence-BERT: Sentence embeddings using Siamese BERT-networks</article-title>
      <source>EMNLP</source>
      <publisher-loc>Hong Kong, China</publisher-loc>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <uri>https://www.aclweb.org/anthology/D19-1410.pdf</uri>
      <pub-id pub-id-type="doi">10.18653/v1/D19-1410</pub-id>
      <fpage>3982</fpage>
      <lpage>3992</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ArmstrongU003A2009">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Armstrong</surname><given-names>T.</given-names></name>
        <name><surname>Moffat</surname><given-names>A.</given-names></name>
        <name><surname>Webber</surname><given-names>W.</given-names></name>
        <name><surname>Zobel</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Improvements that donâ€™t add up: Ad-hoc retrieval results since 1998</article-title>
      <source>CIKM</source>
      <publisher-loc>Hong Kong, China</publisher-loc>
      <year iso-8601-date="2009">2009</year>
      <fpage>601</fpage>
      <lpage>610</lpage>
    </element-citation>
  </ref>
  <ref id="ref-search-like-an-expert-2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Vincent</given-names></name>
        <name><surname>Rybinski</surname><given-names>Maciej</given-names></name>
        <name><surname>Karimi</surname><given-names>Sarvnaz</given-names></name>
        <name><surname>Xing</surname><given-names>Zhenchang</given-names></name>
      </person-group>
      <article-title>Search like an expert: Reducing expertise disparity using a hybrid neural index for COVID-19 queries</article-title>
      <source>Journal of Biomedical Informatics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>127</volume>
      <issn>1532-0464</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S1532046422000211</uri>
      <pub-id pub-id-type="doi">10.1016/j.jbi.2022.104005</pub-id>
      <fpage>104005</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-opennir">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>MacAvaney</surname><given-names>Sean</given-names></name>
      </person-group>
      <article-title>OpenNIR: A complete neural ad-hoc ranking pipeline</article-title>
      <source>Proceedings of the 13th international conference on web search and data mining</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2020">2020</year>
      <isbn>9781450368223</isbn>
      <uri>https://doi.org/10.1145/3336191.3371864</uri>
      <pub-id pub-id-type="doi">10.1145/3336191.3371864</pub-id>
      <fpage>845</fpage>
      <lpage>848</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lin2021pretrained">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lin</surname><given-names>Jimmy</given-names></name>
        <name><surname>Nogueira</surname><given-names>Rodrigo</given-names></name>
        <name><surname>Yates</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Pretrained transformers for text ranking: Bert and beyond</article-title>
      <source>Synthesis Lectures on Human Language Technologies</source>
      <publisher-name>Morgan &amp; Claypool Publishers</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>14</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1162/coli_r_00468</pub-id>
      <fpage>1</fpage>
      <lpage>325</lpage>
    </element-citation>
  </ref>
  <ref id="ref-msmarco">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Tri</given-names></name>
        <name><surname>Rosenberg</surname><given-names>Mir</given-names></name>
        <name><surname>Song</surname><given-names>Xia</given-names></name>
        <name><surname>Gao</surname><given-names>Jianfeng</given-names></name>
        <name><surname>Tiwary</surname><given-names>Saurabh</given-names></name>
        <name><surname>Majumder</surname><given-names>Rangan</given-names></name>
        <name><surname>Deng</surname><given-names>Li</given-names></name>
      </person-group>
      <article-title>MS MARCO: A human generated MAchine reading COmprehension dataset</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2016">2016</year>
      <volume>abs/1611.09268</volume>
      <uri>http://arxiv.org/abs/1611.09268</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
