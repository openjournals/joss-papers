<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230705T125536-ebfb483e6de5921658498c38199cd6e754130072</doi_batch_id>
    <timestamp>20230705125536</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>87</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>DeBEIR: A Python Package for Dense Bi-Encoder
Information Retrieval</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Vincent</given_name>
            <surname>Nguyen</surname>
            <ORCID>https://orcid.org/0000-0003-1787-8090</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Sarvnaz</given_name>
            <surname>Karimi</surname>
            <ORCID>https://orcid.org/0000-0002-4927-3937</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Zhenchang</given_name>
            <surname>Xing</surname>
            <ORCID>https://orcid.org/0000-0001-7663-1421</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>05</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5017</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05017</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.8103783</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5017</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05017</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05017</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05017.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="bm25">
            <article_title>Okapi at TREC-3.</article_title>
            <author>Robertson</author>
            <journal_title>TREC</journal_title>
            <cYear>1995</cYear>
            <unstructured_citation>Robertson, S., Walker, S., Jones, S.,
Hancock-Beaulieu, M., &amp; Gatford, M. (1995, January). Okapi at
TREC-3. TREC.
https://trec.nist.gov/pubs/trec3/t3\_proceedings.html</unstructured_citation>
          </citation>
          <citation key="alexnet">
            <article_title>ImageNet classification with deep
convolutional neural networks</article_title>
            <author>Krizhevsky</author>
            <journal_title>Advances in neural information processing
systems 25</journal_title>
            <doi>10.1145/3065386</doi>
            <cYear>2012</cYear>
            <unstructured_citation>Krizhevsky, A., Sutskever, I., &amp;
Hinton, G. E. (2012). ImageNet classification with deep convolutional
neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q.
Weinberger (Eds.), Advances in neural information processing systems 25
(pp. 1097–1105). Curran Associates, Inc.
https://doi.org/10.1145/3065386</unstructured_citation>
          </citation>
          <citation key="googlenet">
            <article_title>Going deeper with
convolutions</article_title>
            <author>Szegedy</author>
            <journal_title>IEEE conference on computer vision and
pattern recognition</journal_title>
            <doi>10.1109/CVPR.2015.7298594</doi>
            <issn>1063-6919</issn>
            <cYear>2014</cYear>
            <unstructured_citation>Szegedy, C., Liu, W., Jia, Y.,
Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., &amp;
Rabinovich, A. (2014). Going deeper with convolutions. IEEE Conference
on Computer Vision and Pattern Recognition, 1–9.
https://doi.org/10.1109/CVPR.2015.7298594</unstructured_citation>
          </citation>
          <citation key="orig-bert-2018">
            <article_title>BERT: Pre-training of deep bidirectional
transformers for language understanding</article_title>
            <author>Devlin</author>
            <journal_title>Proceedings of the conference of the north
American chapter of the association for computational linguistics: Human
language technologies</journal_title>
            <doi>10.18653/v1/N19-1423</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Devlin, J., Chang, M.-W., Lee, K.,
&amp; Toutanova, K. (2019). BERT: Pre-training of deep bidirectional
transformers for language understanding. Proceedings of the Conference
of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, 4171–4186.
https://doi.org/10.18653/v1/N19-1423</unstructured_citation>
          </citation>
          <citation key="liu:2019">
            <article_title>Text summarization with pretrained
encoders</article_title>
            <author>Liu</author>
            <journal_title>Proceedings of the 2019 conference on
empirical methods in natural language processing and the 9th
international joint conference on natural language processing
(EMNLP-IJCNLP)</journal_title>
            <doi>10.18653/v1/D19-1387</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Liu, Y., &amp; Lapata, M. (2019).
Text summarization with pretrained encoders. Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 3730–3740.
https://doi.org/10.18653/v1/D19-1387</unstructured_citation>
          </citation>
          <citation key="biobert">
            <article_title>BioBERT: a pre-trained biomedical language
representation model for biomedical text mining</article_title>
            <author>Lee</author>
            <journal_title>Bioinformatics</journal_title>
            <issue>4</issue>
            <volume>36</volume>
            <doi>10.1093/bioinformatics/btz682</doi>
            <issn>1367-4803</issn>
            <cYear>2019</cYear>
            <unstructured_citation>Lee, J., Yoon, W., Kim, S., Kim, D.,
Kim, S., So, C. H., &amp; Kang, J. (2019). BioBERT: a pre-trained
biomedical language representation model for biomedical text mining.
Bioinformatics, 36(4), 1234–1240.
https://doi.org/10.1093/bioinformatics/btz682</unstructured_citation>
          </citation>
          <citation key="lin-neural-recantation">
            <article_title>Neural hype, justified! A
recantation</article_title>
            <author>Lin</author>
            <journal_title>ACM SIGIR Forum</journal_title>
            <volume>53</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Lin, J. (2019). Neural hype,
justified! A recantation. ACM SIGIR Forum, 53.
http://sigir.org/wp-content/uploads/2019/december/p088.pdf</unstructured_citation>
          </citation>
          <citation key="yang2019critically">
            <article_title>Critically examining the" neural hype" weak
baselines and the additivity of effectiveness gains from neural ranking
models</article_title>
            <author>Yang</author>
            <journal_title>Proceedings of the 42nd international ACM
SIGIR conference on research and development in information
retrieval</journal_title>
            <doi>10.1145/3331184.3331340</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Yang, W., Lu, K., Yang, P., &amp;
Lin, J. (2019). Critically examining the" neural hype" weak baselines
and the additivity of effectiveness gains from neural ranking models.
Proceedings of the 42nd International ACM SIGIR Conference on Research
and Development in Information Retrieval, 1129–1132.
https://doi.org/10.1145/3331184.3331340</unstructured_citation>
          </citation>
          <citation key="roberta">
            <article_title>RoBERTa: A robustly optimized BERT
pretraining approach</article_title>
            <author>Liu</author>
            <journal_title>Computing Research Repository</journal_title>
            <volume>abs/1907.11692</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Liu, Y., Ott, M., Goyal, N., Du, J.,
Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;
Stoyanov, V. (2019). RoBERTa: A robustly optimized BERT pretraining
approach. Computing Research Repository, abs/1907.11692.
http://arxiv.org/abs/1907.11692</unstructured_citation>
          </citation>
          <citation key="drmm">
            <article_title>A deep relevance matching model for ad-hoc
retrieval</article_title>
            <author>Guo</author>
            <journal_title>Computing Research Repository</journal_title>
            <volume>abs/1711.08611</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Guo, J., Fan, Y., Ai, Q., &amp;
Croft, B. (2017). A deep relevance matching model for ad-hoc retrieval.
Computing Research Repository, abs/1711.08611, 55–64.
http://arxiv.org/abs/1711.08611</unstructured_citation>
          </citation>
          <citation key="abcnn">
            <article_title>ABCNN: Attention-based convolutional neural
network for modeling sentence pairs</article_title>
            <author>Yin</author>
            <journal_title>Computing Research Repository</journal_title>
            <volume>abs/1512.05193</volume>
            <cYear>2015</cYear>
            <unstructured_citation>Yin, W., Schütze, H., Xiang, B.,
&amp; Zhou, B. (2015). ABCNN: Attention-based convolutional neural
network for modeling sentence pairs. Computing Research Repository,
abs/1512.05193. http://arxiv.org/abs/1512.05193</unstructured_citation>
          </citation>
          <citation key="paccr">
            <article_title>A position-aware deep model for relevance
matching in information retrieval</article_title>
            <author>Hui</author>
            <journal_title>Computing Research Repository</journal_title>
            <volume>abs/1704.03940</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Hui, K., Yates, A., Berberich, K.,
&amp; Melo, G. de. (2017). A position-aware deep model for relevance
matching in information retrieval. Computing Research Repository,
abs/1704.03940. http://arxiv.org/abs/1704.03940</unstructured_citation>
          </citation>
          <citation key="reimers2019">
            <article_title>Sentence-BERT: Sentence embeddings using
Siamese BERT-networks</article_title>
            <author>Reimers</author>
            <journal_title>EMNLP</journal_title>
            <doi>10.18653/v1/D19-1410</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Reimers, N., &amp; Gurevych, I.
(2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks.
EMNLP, 3982–3992.
https://doi.org/10.18653/v1/D19-1410</unstructured_citation>
          </citation>
          <citation key="Armstrong:2009">
            <article_title>Improvements that don’t add up: Ad-hoc
retrieval results since 1998</article_title>
            <author>Armstrong</author>
            <journal_title>CIKM</journal_title>
            <cYear>2009</cYear>
            <unstructured_citation>Armstrong, T., Moffat, A., Webber,
W., &amp; Zobel, J. (2009). Improvements that don’t add up: Ad-hoc
retrieval results since 1998. CIKM, 601–610.</unstructured_citation>
          </citation>
          <citation key="search-like-an-expert-2022">
            <article_title>Search like an expert: Reducing expertise
disparity using a hybrid neural index for COVID-19
queries</article_title>
            <author>Nguyen</author>
            <journal_title>Journal of Biomedical
Informatics</journal_title>
            <volume>127</volume>
            <doi>10.1016/j.jbi.2022.104005</doi>
            <issn>1532-0464</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Nguyen, V., Rybinski, M., Karimi, S.,
&amp; Xing, Z. (2022). Search like an expert: Reducing expertise
disparity using a hybrid neural index for COVID-19 queries. Journal of
Biomedical Informatics, 127, 104005.
https://doi.org/10.1016/j.jbi.2022.104005</unstructured_citation>
          </citation>
          <citation key="opennir">
            <article_title>OpenNIR: A complete neural ad-hoc ranking
pipeline</article_title>
            <author>MacAvaney</author>
            <journal_title>Proceedings of the 13th international
conference on web search and data mining</journal_title>
            <doi>10.1145/3336191.3371864</doi>
            <isbn>9781450368223</isbn>
            <cYear>2020</cYear>
            <unstructured_citation>MacAvaney, S. (2020). OpenNIR: A
complete neural ad-hoc ranking pipeline. Proceedings of the 13th
International Conference on Web Search and Data Mining, 845–848.
https://doi.org/10.1145/3336191.3371864</unstructured_citation>
          </citation>
          <citation key="lin2021pretrained">
            <article_title>Pretrained transformers for text ranking:
Bert and beyond</article_title>
            <author>Lin</author>
            <journal_title>Synthesis Lectures on Human Language
Technologies</journal_title>
            <issue>4</issue>
            <volume>14</volume>
            <doi>10.1162/coli_r_00468</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Lin, J., Nogueira, R., &amp; Yates,
A. (2021). Pretrained transformers for text ranking: Bert and beyond.
Synthesis Lectures on Human Language Technologies, 14(4), 1–325.
https://doi.org/10.1162/coli_r_00468</unstructured_citation>
          </citation>
          <citation key="msmarco">
            <article_title>MS MARCO: A human generated MAchine reading
COmprehension dataset</article_title>
            <author>Nguyen</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1611.09268</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Nguyen, T., Rosenberg, M., Song, X.,
Gao, J., Tiwary, S., Majumder, R., &amp; Deng, L. (2016). MS MARCO: A
human generated MAchine reading COmprehension dataset. CoRR,
abs/1611.09268. http://arxiv.org/abs/1611.09268</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
