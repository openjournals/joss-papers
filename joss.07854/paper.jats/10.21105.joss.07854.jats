<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7854</article-id>
<article-id pub-id-type="doi">10.21105/joss.07854</article-id>
<title-group>
<article-title>Syclops: A Modular Pipeline for Procedural Generation of
Synthetic Data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0008-0389-8545</contrib-id>
<name>
<surname>Elmiger</surname>
<given-names>Anton</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1814-8463</contrib-id>
<name>
<surname>von Szadkowski</surname>
<given-names>Kai</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4297-7197</contrib-id>
<name>
<surname>Korthals</surname>
<given-names>Timo</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>German Research Center for Artificial Intelligence (DFKI),
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>CLAAS E-Systems, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-01-22">
<day>22</day>
<month>1</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>110</issue>
<fpage>7854</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>synthetic data</kwd>
<kwd>procedural generation</kwd>
<kwd>computer vision</kwd>
<kwd>Blender</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Syclops is an open-source, modular pipeline for generating
  large-scale, photorealistic synthetic datasets with pixel-perfect
  ground truth annotations. Built on Blender’s Cycles engine
  (<xref alt="Community, 2019" rid="ref-blender" ref-type="bibr">Community,
  2019</xref>), it offers a flexible framework for researchers in
  computer vision, robotics, and related fields. Key features
  include:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Plugin-based architecture</bold> for easy
      extensibility</p>
    </list-item>
    <list-item>
      <p><bold>Procedural generation</bold> of diverse, large-scale
      environments</p>
    </list-item>
    <list-item>
      <p><bold>Photorealistic rendering</bold></p>
    </list-item>
    <list-item>
      <p><bold>Multi-modal sensor simulation</bold> (RGB cameras, depth
      sensors, stereo cameras)</p>
    </list-item>
    <list-item>
      <p><bold>Comprehensive ground truth annotations</bold></p>
    </list-item>
    <list-item>
      <p><bold>Dynamic scene configuration</bold> using YAML</p>
    </list-item>
    <list-item>
      <p><bold>Scalability</bold> for millions of objects</p>
    </list-item>
  </list>
  <p>Syclops is especially useful when collecting real-world data is
  impractical due to cost or difficulty, making it a valuable tool for
  generating high-quality synthetic data.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Machine learning models, particularly in computer vision and
  robotics, depend on the diversity and quality of training data.
  Real-world data collection is often expensive and challenging,
  especially for rare events
  (<xref alt="Tabkhi, 2022" rid="ref-tabkhi2022real" ref-type="bibr">Tabkhi,
  2022</xref>). Synthetic data generation offers an efficient
  alternative, producing large, annotated datasets
  (<xref alt="Mumuni et al., 2024" rid="ref-mumuni2024survey" ref-type="bibr">Mumuni
  et al., 2024</xref>).</p>
  <p>Syclops addresses this need with its focus on large-scale,
  procedural scene creation—particularly for outdoor and agricultural
  scenarios. Compared to tools like Kubric
  (<xref alt="Greff et al., 2022" rid="ref-greff2022kubric" ref-type="bibr">Greff
  et al., 2022</xref>), Blenderproc2
  (<xref alt="Denninger et al., 2023" rid="ref-denninger2023blenderproc2" ref-type="bibr">Denninger
  et al., 2023</xref>), NViSII, NDDS, and iGibson, Syclops offers a
  YAML-based scene description that simplifies customization and
  reproducibility. The following table
  (<xref alt="Table 1" rid="tabU003Acomparison">Table 1</xref>)
  highlights key differences.</p>
  <table-wrap>
    <caption>
      <p>Comparison of synthetic data tools with abbreviations:
      SS=Semantic Segmentation, IS=Instance Segmentation, D=Depth,
      OF=Optical Flow, SN=Surface Normals, OC=Object Coordinates,
      BB=Bounding Box, OP=Object Pose, V=Volume, KP=Keypoints, PS=Python
      Script, C=Camera, SC=Stereo Camera, L=Lidar
      <styled-content id="tabU003Acomparison"></styled-content></p>
    </caption>
    <table>
      <colgroup>
        <col width="14%" />
        <col width="19%" />
        <col width="16%" />
        <col width="42%" />
        <col width="8%" />
      </colgroup>
      <thead>
        <tr>
          <th>Tool</th>
          <th>Rendering Engine</th>
          <th>Scene Creation</th>
          <th>Output Annotations</th>
          <th>Sensors</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Syclops</td>
          <td>Blender Cycles</td>
          <td>YAML</td>
          <td>SS, IS, D, OF, SN, OC, BB, OP, KP, V</td>
          <td>C, SC</td>
        </tr>
        <tr>
          <td>Kubric</td>
          <td>Blender Cycles</td>
          <td>PS</td>
          <td>SS, IS, D, OF, SN, OC, BB, OP</td>
          <td>C</td>
        </tr>
        <tr>
          <td>Blenderproc2</td>
          <td>Blender Cycles</td>
          <td>PS</td>
          <td>SS, IS, D, OF, SN, OC, BB, OP</td>
          <td>C, SC</td>
        </tr>
        <tr>
          <td>NViSII</td>
          <td>Nvidia Optix</td>
          <td>PS</td>
          <td>SS, D, OF, SN, OC, BB, OP</td>
          <td>C</td>
        </tr>
        <tr>
          <td>NDDS</td>
          <td>Unreal Engine 4</td>
          <td>UE4 GUI</td>
          <td>SS, D, BB, OP, KP</td>
          <td>C</td>
        </tr>
        <tr>
          <td>iGibson</td>
          <td>PBR Rastering</td>
          <td>PS</td>
          <td>SS, IS, D, OF, BB</td>
          <td>C, L</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
</sec>
<sec id="key-features">
  <title>Key Features</title>
  <list list-type="order">
    <list-item>
      <p><bold>Large-scale Procedural Generation:</bold>
      Efficiently create vast environments with millions of objects,
      ideal for outdoor settings such as agricultural fields.</p>
    </list-item>
    <list-item>
      <p><bold>YAML-based Configuration:</bold>
      Define and customize scenes easily with YAML syntax, enhancing
      reproducibility.</p>
    </list-item>
    <list-item>
      <p><bold>Modular Architecture:</bold>
      Extend functionality with plugins for custom scene elements,
      sensors, and outputs.</p>
    </list-item>
    <list-item>
      <p><bold>Multi-modal Sensor Simulation:</bold>
      Simulate various sensors (e.g., RGB and stereo cameras with
      projected light) for versatile data generation.</p>
    </list-item>
    <list-item>
      <p><bold>Comprehensive Annotations:</bold>
      Generate detailed ground truth data including segmentation, depth
      maps, object coordinates, bounding boxes, poses, keypoints, and
      volumes.</p>
    </list-item>
    <list-item>
      <p><bold>Off-Highway Focus:</bold>
      Special emphasis on agricultural and off-highway scenarios fills a
      niche in current synthetic data tools.</p>
    </list-item>
  </list>
</sec>
<sec id="architecture-and-implementation">
  <title>Architecture and Implementation</title>
  <p>Syclops is implemented in Python and leverages Blender’s Python API
  for scene creation and rendering
  (<xref alt="[fig:architecture]" rid="figU003Aarchitecture">[fig:architecture]</xref>
  for an overview). Its architecture comprises:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Job Configuration:</bold>
      YAML-based files define scene composition, sensor properties, and
      outputs.</p>
    </list-item>
    <list-item>
      <p><bold>Asset Management:</bold>
      A module for organizing and accessing 3D models, textures, and
      materials.</p>
    </list-item>
    <list-item>
      <p><bold>Scene Generation:</bold>
      Plugins efficiently place and manipulate large numbers of objects
      using Blender’s Geometry nodes and object instancing.</p>
    </list-item>
    <list-item>
      <p><bold>Sensor Simulation:</bold>
      Modules replicate various sensor modalities.</p>
    </list-item>
    <list-item>
      <p><bold>Output Generation:</bold>
      Plugins produce sensor outputs and ground truth annotations.</p>
    </list-item>
    <list-item>
      <p><bold>Postprocessing:</bold>
      Tools refine and process the generated data, enabling additional
      annotations and data augmentation.</p>
    </list-item>
  </list>
  <p>For instance, Syclops can use convex decomposition for efficient
  rigid body simulation, allowing for dynamic scene interactions.</p>
  <fig id="figU003Aarchitecture">
    <caption><p>Architecture overview showing Syclops’ components and
    their relationships.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="syclops_overview.png" />
  </fig>
</sec>
<sec id="example-usage">
  <title>Example Usage</title>
  <p>A simple YAML configuration below demonstrates how to generate a
  synthetic dataset of RGB and depth images of trees scattered on a flat
  ground:</p>
  <code language="yaml"># job_config.yaml
general:
  steps: 100
  seeds:
    numpy: 42
    cycles: 42

scene:
  syclops_plugin_ground:
    - name: &quot;Ground&quot;
      size: 50
      texture: &quot;Example Assets/Muddy Dry Ground&quot;
      class_id: 1

  syclops_plugin_scatter:
    - name: &quot;Trees&quot;
      models: &quot;Example Assets/Trees&quot;
      floor_object: &quot;Ground&quot;
      density_max: 0.1
      class_id: 2

sensor:
  syclops_sensor_camera:
    - name: &quot;main_camera&quot;
      frame_id: &quot;camera_link&quot;
      resolution: [1280, 720]
      focal_length: 35
      outputs:
        syclops_output_rgb:
          - id: &quot;main_rgb&quot;
            samples: 256
        syclops_output_pixel_annotation:
          - semantic_segmentation:
              id: &quot;main_semantic&quot;
          - depth:
              id: &quot;main_depth&quot;</code>
  <p>Run the dataset generation with:</p>
  <code language="bash">syclops -j job_config.yaml</code>
  <p>The graphical assets included in the repository demonstrate the
  tool’s capabilities.</p>
</sec>
<sec id="use-cases">
  <title>Use Cases</title>
  <p>Syclops has been applied in various real-world scenarios (see
  <xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>
  for an example). It has generated datasets for:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Semantic segmentation</bold> of crop and weed plants in
      agricultural fields, achieving a mIoU of 80.7 on the Phenobench
      Benchmark
      (<xref alt="Weyler et al., 2024" rid="ref-weyler2024phenobench" ref-type="bibr">Weyler
      et al., 2024</xref>) compared to 85.97 with real images.</p>
    </list-item>
    <list-item>
      <p><bold>Volume estimation</bold> of vegetables on a conveyor belt
      with physics simulation, showcasing its industrial automation
      potential.</p>
    </list-item>
  </list>
  <p>These applications underline Syclops’ versatility across outdoor
  and indoor settings, simulating complex object interactions.</p>
  <fig id="figU003Aexample">
    <caption><p>Data synthesized by Syclops for selective weeding in
    sugarbeets. Left to right: RGB image, instance segmentation,
    semantic segmentation, depth.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="syclops_output.png" />
  </fig>
</sec>
<sec id="limitations-and-future-work">
  <title>Limitations and Future Work</title>
  <p>Syclops currently does not support the procedural generation of
  individual graphical assets. High-quality assets are essential for
  realistic data synthesis, and future work will address this
  limitation. Planned enhancements include:</p>
  <list list-type="bullet">
    <list-item>
      <p>Developing tools for procedural asset generation.</p>
    </list-item>
    <list-item>
      <p>Expanding sensor simulation capabilities.</p>
    </list-item>
    <list-item>
      <p>Improving rendering realism and scene generation
      efficiency.</p>
    </list-item>
  </list>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>Syclops is a powerful tool for generating high-quality synthetic
  datasets in computer vision and robotics. Its modular, YAML-based
  architecture and focus on large-scale procedural generation make it
  especially suitable for off-highway applications such as agriculture.
  By providing extensive annotations and flexible configuration, Syclops
  supports accelerated research and development in challenging data
  collection scenarios.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Henning Wübben, Florian Rahe, Thilo Steckel, and Stefan
  Stiene for their valuable feedback. Syclops was developed as part of
  the Agri-Gaia project, supported by the German Federal Ministry for
  Economic Affairs and Climate Action (grant number: 01MK21004A) and
  sponsored by the Ministry of Science and Culture of Lower Saxony and
  the VolkswagenStiftung.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-weyler2024phenobench">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Weyler</surname><given-names>Jan</given-names></name>
        <name><surname>Magistri</surname><given-names>Federico</given-names></name>
        <name><surname>Marks</surname><given-names>Elias</given-names></name>
        <name><surname>Chong</surname><given-names>Yue Linn</given-names></name>
        <name><surname>Sodano</surname><given-names>Matteo</given-names></name>
        <name><surname>Roggiolani</surname><given-names>Gianmarco</given-names></name>
        <name><surname>Chebrolu</surname><given-names>Nived</given-names></name>
        <name><surname>Stachniss</surname><given-names>Cyrill</given-names></name>
        <name><surname>Behley</surname><given-names>Jens</given-names></name>
      </person-group>
      <article-title>PhenoBench: A large dataset and benchmarks for semantic image interpretation in the agricultural domain</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2024.3419548</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-greff2022kubric">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Greff</surname><given-names>Klaus</given-names></name>
        <name><surname>Belletti</surname><given-names>Francois</given-names></name>
        <name><surname>Beyer</surname><given-names>Lucas</given-names></name>
        <name><surname>Doersch</surname><given-names>Carl</given-names></name>
        <name><surname>Du</surname><given-names>Yilun</given-names></name>
        <name><surname>Duckworth</surname><given-names>Daniel</given-names></name>
        <name><surname>Fleet</surname><given-names>David J</given-names></name>
        <name><surname>Gnanapragasam</surname><given-names>Dan</given-names></name>
        <name><surname>Golemo</surname><given-names>Florian</given-names></name>
        <name><surname>Herrmann</surname><given-names>Charles</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Kubric: A scalable dataset generator</article-title>
      <source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/CVPR52688.2022.00373</pub-id>
      <fpage>3749</fpage>
      <lpage>3761</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mumuni2024survey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mumuni</surname><given-names>Alhassan</given-names></name>
        <name><surname>Mumuni</surname><given-names>Fuseini</given-names></name>
        <name><surname>Gerrar</surname><given-names>Nana Kobina</given-names></name>
      </person-group>
      <article-title>A survey of synthetic data augmentation methods in computer vision</article-title>
      <source>arXiv preprint arXiv:2403.10075</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1007/s11633-022-1411-7</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-denninger2023blenderproc2">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Denninger</surname><given-names>Maximilian</given-names></name>
        <name><surname>Winkelbauer</surname><given-names>Dominik</given-names></name>
        <name><surname>Sundermeyer</surname><given-names>Martin</given-names></name>
        <name><surname>Boerdijk</surname><given-names>Wout</given-names></name>
        <name><surname>Knauer</surname><given-names>Markus Wendelin</given-names></name>
        <name><surname>Strobl</surname><given-names>Klaus H</given-names></name>
        <name><surname>Humt</surname><given-names>Matthias</given-names></name>
        <name><surname>Triebel</surname><given-names>Rudolph</given-names></name>
      </person-group>
      <article-title>Blenderproc2: A procedural pipeline for photorealistic rendering</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>Journal of Open Source Software</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>82</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04901</pub-id>
      <fpage>4901</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tabkhi2022real">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tabkhi</surname><given-names>Hamed</given-names></name>
      </person-group>
      <article-title>Real-world computer vision for real-world applications: Challenges and directions</article-title>
      <source>Proceedings of SAI intelligent systems conference</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1007/978-3-031-16072-1_53</pub-id>
      <fpage>727</fpage>
      <lpage>750</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blender">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Community</surname><given-names>Blender Online</given-names></name>
      </person-group>
      <source>Blender - a 3D modelling and rendering package</source>
      <publisher-name>Blender Foundation</publisher-name>
      <publisher-loc>Stichting Blender Foundation, Amsterdam</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <uri>http://www.blender.org</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
