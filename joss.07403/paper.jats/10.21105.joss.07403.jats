<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7403</article-id>
<article-id pub-id-type="doi">10.21105/joss.07403</article-id>
<title-group>
<article-title>ExpFamilyPCA.jl: A Julia Package for Exponential Family
Principal Component Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-3978-9462</contrib-id>
<name>
<surname>Bhamidipaty</surname>
<given-names>Logan Mondal</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7238-9663</contrib-id>
<name>
<surname>Kochenderfer</surname>
<given-names>Mykel J.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0164-3142</contrib-id>
<name>
<surname>Hastie</surname>
<given-names>Trevor</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Stanford University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-09-09">
<day>9</day>
<month>9</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>105</issue>
<fpage>7403</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Julia</kwd>
<kwd>compression</kwd>
<kwd>dimensionality reduction</kwd>
<kwd>PCA</kwd>
<kwd>exponential family</kwd>
<kwd>EPCA</kwd>
<kwd>open-source</kwd>
<kwd>POMDP</kwd>
<kwd>MDP</kwd>
<kwd>sequential decision making</kwd>
<kwd>RL</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Principal component analysis (PCA)
  (<xref alt="Hotelling, 1933" rid="ref-PCA2" ref-type="bibr">Hotelling,
  1933</xref>;
  <xref alt="Jolliffe, 2002" rid="ref-PCA3" ref-type="bibr">Jolliffe,
  2002</xref>;
  <xref alt="Pearson, 1901" rid="ref-PCA1" ref-type="bibr">Pearson,
  1901</xref>) is popular for compressing, denoising, and interpreting
  high-dimensional data, but it underperforms on binary, count, and
  compositional data because the objective assumes data is normally
  distributed. Exponential family PCA (EPCA)
  (<xref alt="Collins et al., 2001" rid="ref-EPCA" ref-type="bibr">Collins
  et al., 2001</xref>) generalizes PCA to accommodate data from any
  exponential family distribution, making it more suitable for fields
  where these data types are common, such as geochemistry, marketing,
  genomics, political science, and machine learning
  (<xref alt="Greenacre, 2021" rid="ref-composition" ref-type="bibr">Greenacre,
  2021</xref>;
  <xref alt="Hastie et al., 2009" rid="ref-elements" ref-type="bibr">Hastie
  et al., 2009</xref>).</p>
  <p><monospace>ExpFamilyPCA.jl</monospace> is a library for EPCA
  written in Julia, a dynamic language for scientific computing
  (<xref alt="Bezanson et al., 2017" rid="ref-Julia" ref-type="bibr">Bezanson
  et al., 2017</xref>). It is the first EPCA package in Julia and the
  first in any language to support EPCA for multiple distributions.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>EPCA is used in reinforcement learning
  (<xref alt="Roy et al., 2005" rid="ref-Roy" ref-type="bibr">Roy et
  al., 2005</xref>), sample debiasing
  (<xref alt="R. Huang &amp; Lee, 2023" rid="ref-debiasing" ref-type="bibr">R.
  Huang &amp; Lee, 2023</xref>), and compositional analysis
  (<xref alt="Gan &amp; Valdez, 2024" rid="ref-gans" ref-type="bibr">Gan
  &amp; Valdez, 2024</xref>). Wider adoption, however, remains limited
  due to the lack of implementations. The only other EPCA package is
  written in MATLAB and supports just one distribution
  (<xref alt="Chambrier, 2016" rid="ref-epca-MATLAB" ref-type="bibr">Chambrier,
  2016</xref>). This is surprising, as other Bregman-based optimization
  techniques have been successful in areas like mass spectrometry
  (<xref alt="Nozaki &amp; Nakamoto, 2017" rid="ref-spectrum" ref-type="bibr">Nozaki
  &amp; Nakamoto, 2017</xref>), ultrasound denoising
  (<xref alt="J. Huang &amp; Yang, 2013" rid="ref-ultrasound" ref-type="bibr">J.
  Huang &amp; Yang, 2013</xref>), topological data analysis
  (<xref alt="Edelsbrunner &amp; Wagner, 2019" rid="ref-topological" ref-type="bibr">Edelsbrunner
  &amp; Wagner, 2019</xref>), and robust clustering
  (<xref alt="Banerjee et al., 2005" rid="ref-clustering" ref-type="bibr">Banerjee
  et al., 2005</xref>). These successes suggest that EPCA holds untapped
  potential in signal processing and machine learning.</p>
  <p>The absence of a general EPCA library likely stems from the limited
  interoperability between fast symbolic differentiation and
  optimization libraries in popular languages like Python and C. Julia,
  by contrast, uses multiple dispatch which promotes high levels of
  generic code reuse
  (<xref alt="Karpinski, 2019" rid="ref-dispatch" ref-type="bibr">Karpinski,
  2019</xref>). Multiple dispatch allows
  <monospace>ExpFamilyPCA.jl</monospace> to integrate fast symbolic
  differentiation
  (<xref alt="Gowda et al., 2022" rid="ref-symbolics" ref-type="bibr">Gowda
  et al., 2022</xref>), optimization
  (<xref alt="Mogensen &amp; Riseth, 2018" rid="ref-optim" ref-type="bibr">Mogensen
  &amp; Riseth, 2018</xref>), and numerically stable computation
  (<xref alt="Mächler, 2015" rid="ref-stable_exp" ref-type="bibr">Mächler,
  2015</xref>) without requiring costly API
  conversions.<xref ref-type="fn" rid="fn1">1</xref> As a result,
  <monospace>ExpFamilyPCA.jl</monospace> delivers speed, stability, and
  flexibility, with built-in support for most common distributions (§
  <xref alt="Supported Distributions" rid="supported-distributions">Supported
  Distributions</xref>) and flexible constructors for custom
  distributions (§
  <xref alt="Custom Distributions" rid="supported-distributions">Custom
  Distributions</xref>).</p>
  <sec id="principal-component-analysis">
    <title>Principal Component Analysis</title>
    <sec id="geometric-interpretation">
      <title>Geometric Interpretation</title>
      <p>Given a data matrix <inline-formula><alternatives>
      <tex-math><![CDATA[X \in \mathbb{R}^{n \times d}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      with <inline-formula><alternatives>
      <tex-math><![CDATA[n]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
      observations and <inline-formula><alternatives>
      <tex-math><![CDATA[d]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>
      features, PCA seeks the closest low-rank approximation
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Theta \in \mathbb{R}^{n \times d}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Θ</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      by minimizing the reconstruction error</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\begin{aligned}
      & \underset{\Theta}{\text{minimize}}
      & & \frac{1}{2}\|X - \Theta\|_F^2 \\
      & \text{subject to}
      & & \mathrm{rank}\left(\Theta\right) = k
      \end{aligned}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:munder><mml:mtext mathvariant="normal">minimize</mml:mtext><mml:mi>Θ</mml:mi></mml:munder></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>X</mml:mi><mml:mo>−</mml:mo><mml:mi>Θ</mml:mi><mml:msubsup><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>F</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">subject to</mml:mtext></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[\| \cdot \|_F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>F</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
      denotes the Frobenius norm. The optimal
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Theta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Θ</mml:mi></mml:math></alternatives></inline-formula>
      is a <inline-formula><alternatives>
      <tex-math><![CDATA[k]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>-dimensional
      linear subspace that can be written as the product of the
      projected observations <inline-formula><alternatives>
      <tex-math><![CDATA[A \in \mathbb{R}^{n \times k}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      and the basis <inline-formula><alternatives>
      <tex-math><![CDATA[V \in \mathbb{R}^{k \times d}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>V</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      X \approx \Theta = AV.
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mo>≈</mml:mo><mml:mi>Θ</mml:mi><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>V</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This suggests that each observation
      <inline-formula><alternatives>
      <tex-math><![CDATA[x_i \in \mathrm{rows}(X)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      can be well-approximated by a linear combination of
      <inline-formula><alternatives>
      <tex-math><![CDATA[k]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
      basis vectors (the rows of <inline-formula><alternatives>
      <tex-math><![CDATA[V]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>V</mml:mi></mml:math></alternatives></inline-formula>):</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[x_i \approx \theta_i = a_i V]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>V</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>for <inline-formula><alternatives>
      <tex-math><![CDATA[i = 1, \dots, n]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    </sec>
    <sec id="probabilistic-interpretation">
      <title>Probabilistic Interpretation</title>
      <p>The PCA objective is equivalent to maximum likelihood
      estimation for a Gaussian model. Under this lens, each observation
      <inline-formula><alternatives>
      <tex-math><![CDATA[x_i]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is a noisy realization of a <inline-formula><alternatives>
      <tex-math><![CDATA[d]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>-dimensional
      Gaussian at <inline-formula><alternatives>
      <tex-math><![CDATA[\theta_i \in \mathrm{rows}(\Theta)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      x_i \sim \mathcal{N}(\theta_i, I).
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mi>𝒩</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>To recover the latent structure <inline-formula><alternatives>
      <tex-math><![CDATA[\Theta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Θ</mml:mi></mml:math></alternatives></inline-formula>,
      PCA solves</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\begin{aligned}
      & \underset{\Theta}{\text{maximize}}
      & & \sum_{i=1}^{n}\log \mathcal{L}(x_i; \theta_i) \\
      & \text{subject to}
      & & \mathrm{rank}\left(\Theta\right) = k
      \end{aligned}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:munder><mml:mtext mathvariant="normal">maximize</mml:mtext><mml:mi>Θ</mml:mi></mml:munder></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mo>log</mml:mo><mml:mi>ℒ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">subject to</mml:mtext></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[\mathcal{L}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ℒ</mml:mi></mml:math></alternatives></inline-formula>
      is the likelihood function.</p>
    </sec>
  </sec>
  <sec id="exponential-family-pca">
    <title>Exponential Family PCA</title>
    <sec id="exponential-family">
      <title>Exponential Family</title>
      <p>Following Forster &amp; Warmuth
      (<xref alt="2002" rid="ref-forster" ref-type="bibr">2002</xref>),
      we define the exponential family as the set of distributions with
      densities of the form</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      p_\theta(x) = \exp(\theta \cdot x - G(\theta))
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>⋅</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[\theta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>θ</mml:mi></mml:math></alternatives></inline-formula>
      is the natural parameter and <inline-formula><alternatives>
      <tex-math><![CDATA[G]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
      is the log-partition function.</p>
    </sec>
    <sec id="link-function">
      <title>Link Function</title>
      <p>The link function <inline-formula><alternatives>
      <tex-math><![CDATA[g(\theta)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      connects the natural parameter <inline-formula><alternatives>
      <tex-math><![CDATA[\theta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>θ</mml:mi></mml:math></alternatives></inline-formula>
      to the mean parameter <inline-formula><alternatives>
      <tex-math><![CDATA[\mu]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>μ</mml:mi></mml:math></alternatives></inline-formula>
      of an exponential family distribution. It is defined as the
      gradient of the log-partition function
      <inline-formula><alternatives>
      <tex-math><![CDATA[G(\theta)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      \mu = g(\theta) = \nabla G(\theta).
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>∇</mml:mi><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>The link function serves a role analogous to that in
      generalized linear models (GLMs)
      (<xref alt="McCullagh &amp; Nelder, 1989" rid="ref-GLM" ref-type="bibr">McCullagh
      &amp; Nelder, 1989</xref>). In GLMs, the link function connects
      the linear predictor to the mean of the distribution, enabling
      flexibility in modeling various data types. Similarly, in EPCA,
      the link function maps the low-dimensional latent variables to the
      expectation parameters of the exponential family, thereby
      generalizing the linear assumptions of traditional PCA to
      accommodate diverse distributions (see
      <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/appendix/gaussian/">appendix</ext-link>).</p>
    </sec>
    <sec id="bregman-divergences">
      <title>Bregman Divergences</title>
      <p>EPCA extends the probabilistic interpretation of PCA using a
      measure of statistical difference called the Bregman divergence
      (<xref alt="Bregman, 1967" rid="ref-Bregman" ref-type="bibr">Bregman,
      1967</xref>;
      <xref alt="Efron, 2004" rid="ref-Brad" ref-type="bibr">Efron,
      2004</xref>). The Bregman divergence
      <inline-formula><alternatives>
      <tex-math><![CDATA[B_F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      for a strictly convex, continuously differentiable function
      <inline-formula><alternatives>
      <tex-math><![CDATA[F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>F</mml:mi></mml:math></alternatives></inline-formula>
      is</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      B_F(p \| q) = F(p) - F(q) - \langle \nabla F(q), p - q \rangle.
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false" form="prefix">⟨</mml:mo><mml:mi>∇</mml:mi><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false" form="postfix">⟩</mml:mo><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This can be interpreted as the difference between
      <inline-formula><alternatives>
      <tex-math><![CDATA[F(p)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      and its linear approximation about <inline-formula><alternatives>
      <tex-math><![CDATA[q]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>q</mml:mi></mml:math></alternatives></inline-formula>.
      When <inline-formula><alternatives>
      <tex-math><![CDATA[F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>F</mml:mi></mml:math></alternatives></inline-formula>
      is the convex conjugate of the log-partition function of an
      exponential family distribution, minimizing the Bregman divergence
      corresponds to maximizing the associated log-likelihood
      (<xref alt="Azoury &amp; Warmuth, 2001" rid="ref-azoury" ref-type="bibr">Azoury
      &amp; Warmuth, 2001</xref>;
      <xref alt="Forster &amp; Warmuth, 2002" rid="ref-forster" ref-type="bibr">Forster
      &amp; Warmuth, 2002</xref>) (see
      <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/bregman/">documentation</ext-link>).</p>
    </sec>
    <sec id="loss-function">
      <title>Loss Function</title>
      <p>EPCA generalizes the PCA objective as a Bregman divergence
      between the data <inline-formula><alternatives>
      <tex-math><![CDATA[X]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
      and the expectation parameters <inline-formula><alternatives>
      <tex-math><![CDATA[g(\Theta)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\begin{aligned}
      & \underset{\Theta}{\text{minimize}}
      & & B_F(X \| g(\Theta)) \\
      & \text{subject to}
      & & \mathrm{rank}\left(\Theta\right) = k
      \end{aligned}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:munder><mml:mtext mathvariant="normal">minimize</mml:mtext><mml:mi>Θ</mml:mi></mml:munder></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">subject to</mml:mtext></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
      <p>where</p>
      <list list-type="bullet">
        <list-item>
          <p><inline-formula><alternatives>
          <tex-math><![CDATA[g(\theta)]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
          is the <bold>link function</bold> and the gradient of
          <inline-formula><alternatives>
          <tex-math><![CDATA[G]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>,</p>
        </list-item>
        <list-item>
          <p><inline-formula><alternatives>
          <tex-math><![CDATA[G(\theta)]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
          is a strictly convex, continuously differentiable function
          (usually the <bold>log-partition</bold> of an exponential
          family distribution),</p>
        </list-item>
        <list-item>
          <p>and <inline-formula><alternatives>
          <tex-math><![CDATA[F(\mu)]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
          is the <bold>convex conjugate</bold> of
          <inline-formula><alternatives>
          <tex-math><![CDATA[G]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>
          defined by</p>
        </list-item>
      </list>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[
      F(\mu) = \max_{\theta} \langle \mu, \theta \rangle - G(\theta).
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>max</mml:mo><mml:mi>θ</mml:mi></mml:munder><mml:mo stretchy="false" form="prefix">⟨</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false" form="postfix">⟩</mml:mo><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
      <p>This suggests that data from the exponential family is
      well-approximated by expectation parameters</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[ 
      x_i \approx  g(\theta_i) = g(a_i V).
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>V</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
    </sec>
    <sec id="regularization">
      <title>Regularization</title>
      <p>Following Collins et al.
      (<xref alt="2001" rid="ref-EPCA" ref-type="bibr">2001</xref>), we
      introduce a regularization term to ensure the optimum
      converges</p>
      <p><disp-formula><alternatives>
      <tex-math><![CDATA[\begin{aligned}
      & \underset{\Theta}{\text{minimize}}
      & & B_F(X \| g(\Theta)) + \epsilon B_F(\mu_0 \| g(\Theta)) \\
      & \text{subject to}
      & & \mathrm{rank}\left(\Theta\right) = k
      \end{aligned}]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:munder><mml:mtext mathvariant="normal">minimize</mml:mtext><mml:mi>Θ</mml:mi></mml:munder></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:msub><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi><mml:msub><mml:mi>B</mml:mi><mml:mi>F</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false" form="postfix">∥</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">subject to</mml:mtext></mml:mtd><mml:mtd columnalign="right" style="text-align: right"></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
      <p>where <inline-formula><alternatives>
      <tex-math><![CDATA[\epsilon > 0]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[\mu_0 \in \mathrm{range}(g)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.<xref ref-type="fn" rid="fn2">2</xref></p>
    </sec>
    <sec id="example-poisson-epca">
      <title>Example: Poisson EPCA</title>
      <p>The Poisson EPCA objective is the generalized Kullback-Leibler
      (KL) divergence (see
      <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/appendix/poisson/">appendix</ext-link>),
      making Poisson EPCA ideal for compressing discrete distribution
      data.</p>
      <p>This is useful in applications like belief compression in
      reinforcement learning
      (<xref alt="Roy et al., 2005" rid="ref-Roy" ref-type="bibr">Roy et
      al., 2005</xref>), where high-dimensional belief states can be
      effectively reduced with minimal information loss. Below we
      recreate similar figures<xref ref-type="fn" rid="fn3">3</xref> to
      Roy &amp; Gordon
      (<xref alt="2002" rid="ref-shortRoy" ref-type="bibr">2002</xref>)
      and Roy et al.
      (<xref alt="2005" rid="ref-Roy" ref-type="bibr">2005</xref>) and
      observe that Poisson EPCA almost perfectly reconstructs a
      <inline-formula><alternatives>
      <tex-math><![CDATA[41]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>41</mml:mn></mml:math></alternatives></inline-formula>-dimensional
      belief distribution using just <inline-formula><alternatives>
      <tex-math><![CDATA[5]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>5</mml:mn></mml:math></alternatives></inline-formula>
      basis components. For a larger environment with
      <inline-formula><alternatives>
      <tex-math><![CDATA[200]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>200</mml:mn></mml:math></alternatives></inline-formula>
      states, PCA struggles even with <inline-formula><alternatives>
      <tex-math><![CDATA[10]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>10</mml:mn></mml:math></alternatives></inline-formula>
      basis components.</p>
      <fig>
        <caption><p>Left - KL Divergence for Poisson EPCA versus PCA.
        Right - Reconstructions from the models.</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="./scripts/combo.png" />
      </fig>
    </sec>
  </sec>
</sec>
<sec id="api">
  <title>API</title>
  <sec id="supported-distributions">
    <title>Supported Distributions</title>
    <p><monospace>ExpFamilyPCA.jl</monospace> includes efficient EPCA
    implementations for several exponential family distributions.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="33%" />
          <col width="67%" />
        </colgroup>
        <thead>
          <tr>
            <th>Julia</th>
            <th>Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><monospace>BernoulliEPCA</monospace></td>
            <td>For binary data</td>
          </tr>
          <tr>
            <td><monospace>BinomialEPCA</monospace></td>
            <td>For count data with a fixed number of trials</td>
          </tr>
          <tr>
            <td><monospace>ContinuousBernoulliEPCA</monospace></td>
            <td>For modeling probabilities between
            <inline-formula><alternatives>
            <tex-math><![CDATA[0]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>0</mml:mn></mml:math></alternatives></inline-formula>
            and <inline-formula><alternatives>
            <tex-math><![CDATA[1]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td><monospace>GammaEPCA</monospace></td>
            <td>For positive continuous data</td>
          </tr>
          <tr>
            <td><monospace>GaussianEPCA</monospace></td>
            <td>Standard PCA for real-valued data</td>
          </tr>
          <tr>
            <td><monospace>NegativeBinomialEPCA</monospace></td>
            <td>For over-dispersed count data</td>
          </tr>
          <tr>
            <td><monospace>ParetoEPCA</monospace></td>
            <td>For modeling heavy-tailed distributions</td>
          </tr>
          <tr>
            <td><monospace>PoissonEPCA</monospace></td>
            <td>For count and discrete distribution data</td>
          </tr>
          <tr>
            <td><monospace>WeibullEPCA</monospace></td>
            <td>For modeling life data and survival analysis</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="custom-distributions">
    <title>Custom Distributions</title>
    <p>When working with custom distributions, certain specifications
    are often more convenient and computationally efficient than others.
    For example, inducing the gamma EPCA objective from the
    log-partition <inline-formula><alternatives>
    <tex-math><![CDATA[G(\theta) = -\log(-\theta)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>−</mml:mi><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    and its derivative <inline-formula><alternatives>
    <tex-math><![CDATA[g(\theta) = -1/\theta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
    is much simpler than implementing the full the Itakura-Saito
    distance
    (<xref alt="Itakura &amp; Saito, 1968" rid="ref-ItakuraSaito" ref-type="bibr">Itakura
    &amp; Saito, 1968</xref>) (see
    <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/appendix/gamma/">appendix</ext-link>):</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[
    D(P(\omega), \hat{P}(\omega)) =\frac{1}{2\pi} \int_{-\pi}^{\pi} \Bigg[ \frac{P(\omega)}{\hat{P}(\omega)} - \log \frac{P(\omega)}{\hat{P}{\omega}} - 1\Bigg] \, d\omega.
    ]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>D</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover><mml:mi>P</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mi>−</mml:mi><mml:mi>π</mml:mi></mml:mrow><mml:mi>π</mml:mi></mml:msubsup><mml:mo minsize="3.0" maxsize="3.0" stretchy="false" form="prefix">[</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mo>log</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ω</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mi>ω</mml:mi></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo minsize="3.0" maxsize="3.0" stretchy="false" form="postfix">]</mml:mo><mml:mspace width="0.167em"></mml:mspace><mml:mi>d</mml:mi><mml:mi>ω</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>In <monospace>ExpFamilyPCA.jl</monospace>, we would write:</p>
    <code language="julia">G(θ) = -log(-θ)
g(θ) = -1 / θ
gamma_epca = EPCA(indim, outdim, G, g, Val((:G, :g)); options = NegativeDomain())</code>
    <p>A lengthier discussion of the <monospace>EPCA</monospace>
    constructors and math is provided in the
    <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/objectives/">documentation</ext-link>.</p>
  </sec>
  <sec id="usage">
    <title>Usage</title>
    <p>Each <monospace>EPCA</monospace> object supports a three-method
    interface: <monospace>fit!</monospace>,
    <monospace>compress</monospace>, and
    <monospace>decompress</monospace>. <monospace>fit!</monospace>
    trains the model and returns the compressed training data;
    <monospace>compress</monospace> returns compressed input; and
    <monospace>decompress</monospace> reconstructs the original data
    from the compressed representation.</p>
    <code language="julia">X = sample_from_gamma(n1, indim)  # matrix of gamma-distributed data
Y = sample_from_gamma(n2, indim)

X_compressed = fit!(gamma_epca, X)
Y_compressed = compress(gamma_epca, Y)
Y_reconstructed = decompress(gamma_epca, Y_compressed)</code>
  </sec>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We thank Ryan Tibshirani, Arec Jamgochian, Robert Moss, and Dylan
  Asmar for their help and guidance.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-EPCA">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Collins</surname><given-names>Michael</given-names></name>
        <name><surname>Dasgupta</surname><given-names>Sanjoy</given-names></name>
        <name><surname>Schapire</surname><given-names>Robert E</given-names></name>
      </person-group>
      <article-title>A generalization of principal components analysis to the exponential family</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2001">2001</year>
      <volume>14</volume>
      <pub-id pub-id-type="doi">10.7551/mitpress/1120.003.0084</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-elements">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        <name><surname>Friedman</surname><given-names>Jerome H</given-names></name>
        <name><surname>Friedman</surname><given-names>Jerome H</given-names></name>
      </person-group>
      <source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>2</volume>
      <pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-composition">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Greenacre</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Compositional data analysis</article-title>
      <source>Annual Review of Statistics and its Application</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>8</volume>
      <issue>1</issue>
      <fpage>271</fpage>
      <lpage>299</lpage>
    </element-citation>
  </ref>
  <ref id="ref-debiasing">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Ruochen</given-names></name>
        <name><surname>Lee</surname><given-names>Yoonkyung</given-names></name>
      </person-group>
      <article-title>Debiasing sample loadings and scores in exponential family PCA for sparse count data</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2312.13430</uri>
    </element-citation>
  </ref>
  <ref id="ref-topological">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Edelsbrunner</surname><given-names>Herbert</given-names></name>
        <name><surname>Wagner</surname><given-names>Hubert</given-names></name>
      </person-group>
      <article-title>Topological data analysis with Bregman divergences</article-title>
      <source>Journal of Computational Geometry</source>
      <publisher-name>Journal of Computational Geometry</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://jocg.org/index.php/jocg/article/view/3066</uri>
      <pub-id pub-id-type="doi">10.20382/JOCG.V9I2A6</pub-id>
      <fpage>Vol. 9 No. 2 (2018): Special Issue of Selected Papers from SoCG 2017</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-clustering">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Banerjee</surname><given-names>Arindam</given-names></name>
        <name><surname>Merugu</surname><given-names>Srujana</given-names></name>
        <name><surname>Dhillon</surname><given-names>Inderjit S.</given-names></name>
        <name><surname>Ghosh</surname><given-names>Joydeep</given-names></name>
      </person-group>
      <article-title>Clustering with Bregman divergences</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2005">2005</year>
      <volume>6</volume>
      <issue>58</issue>
      <uri>http://jmlr.org/papers/v6/banerjee05b.html</uri>
      <fpage>1705</fpage>
      <lpage>1749</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ultrasound">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Jie</given-names></name>
        <name><surname>Yang</surname><given-names>Xiaoping</given-names></name>
      </person-group>
      <article-title>Fast reduction of speckle noise in real ultrasound images</article-title>
      <source>Signal Processing</source>
      <year iso-8601-date="2013">2013</year>
      <volume>93</volume>
      <issue>4</issue>
      <issn>0165-1684</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0165168412003258</uri>
      <pub-id pub-id-type="doi">10.1016/j.sigpro.2012.09.005</pub-id>
      <fpage>684</fpage>
      <lpage>694</lpage>
    </element-citation>
  </ref>
  <ref id="ref-spectrum">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nozaki</surname><given-names>Yuji</given-names></name>
        <name><surname>Nakamoto</surname><given-names>Takamichi</given-names></name>
      </person-group>
      <article-title>Itakura-Saito distance based autoencoder for dimensionality reduction of mass spectra</article-title>
      <source>Chemometrics and Intelligent Laboratory Systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>167</volume>
      <issn>0169-7439</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0169743917300436</uri>
      <pub-id pub-id-type="doi">10.1016/j.chemolab.2017.05.002</pub-id>
      <fpage>63</fpage>
      <lpage>68</lpage>
    </element-citation>
  </ref>
  <ref id="ref-azoury">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Azoury</surname><given-names>Katy S</given-names></name>
        <name><surname>Warmuth</surname><given-names>Manfred K</given-names></name>
      </person-group>
      <article-title>Relative loss bounds for on-line density estimation with the exponential family of distributions</article-title>
      <source>Machine learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2001">2001</year>
      <volume>43</volume>
      <pub-id pub-id-type="doi">10.1023/A:1010896012157</pub-id>
      <fpage>211</fpage>
      <lpage>246</lpage>
    </element-citation>
  </ref>
  <ref id="ref-forster">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Forster</surname><given-names>Jürgen</given-names></name>
        <name><surname>Warmuth</surname><given-names>Manfred K.</given-names></name>
      </person-group>
      <article-title>Relative expected instantaneous loss bounds</article-title>
      <source>Journal of Computer and System Sciences</source>
      <year iso-8601-date="2002">2002</year>
      <volume>64</volume>
      <issue>1</issue>
      <issn>0022-0000</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0022000001917982</uri>
      <pub-id pub-id-type="doi">10.1006/jcss.2001.1798</pub-id>
      <fpage>76</fpage>
      <lpage>102</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gans">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gan</surname><given-names>Guojun</given-names></name>
        <name><surname>Valdez</surname><given-names>Emiliano A.</given-names></name>
      </person-group>
      <article-title>Compositional Data Regression in Insurance with Exponential Family PCA</article-title>
      <source>Variance</source>
      <year iso-8601-date="2024">2024</year>
      <volume>17</volume>
      <issue>1</issue>
    </element-citation>
  </ref>
  <ref id="ref-Julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM Review</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>59</volume>
      <issue>1</issue>
      <uri>https://epubs.siam.org/doi/10.1137/141000671</uri>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GLM">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>McCullagh</surname><given-names>P</given-names></name>
        <name><surname>Nelder</surname><given-names>John A</given-names></name>
      </person-group>
      <source>Generalized Linear Models</source>
      <publisher-name>Chapman &amp; Hall/CRC</publisher-name>
      <publisher-loc>Philadelphia, PA</publisher-loc>
      <year iso-8601-date="1989-08">1989</year><month>08</month>
      <edition>2</edition>
      <pub-id pub-id-type="doi">10.1201/9780203753736</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-optim">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mogensen</surname><given-names>Patrick K.</given-names></name>
        <name><surname>Riseth</surname><given-names>Asbjørn N.</given-names></name>
      </person-group>
      <article-title>Optim: A mathematical optimization package for Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>24</issue>
      <uri>https://doi.org/10.21105/joss.00615</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00615</pub-id>
      <fpage>615</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bregman">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bregman</surname><given-names>L. M.</given-names></name>
      </person-group>
      <article-title>The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming</article-title>
      <source>USSR Computational Mathematics and Mathematical Physics</source>
      <year iso-8601-date="1967">1967</year>
      <volume>7</volume>
      <issue>3</issue>
      <issn>0041-5553</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/0041555367900407</uri>
      <pub-id pub-id-type="doi">10.1016/0041-5553(67)90040-7</pub-id>
      <fpage>200</fpage>
      <lpage>217</lpage>
    </element-citation>
  </ref>
  <ref id="ref-logexp">
    <element-citation>
      <article-title>LogExpFunctions.jl</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/JuliaStats/LogExpFunctions.jl</uri>
    </element-citation>
  </ref>
  <ref id="ref-stable_exp">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Mächler</surname><given-names>Martin</given-names></name>
      </person-group>
      <article-title>Accurately computing \log(1 - \exp(-|a|) assessed by the ‘Rmpfr‘ package</article-title>
      <year iso-8601-date="2015-09">2015</year><month>09</month>
      <pub-id pub-id-type="doi">10.13140/RG.2.2.11834.70084</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-symbolics">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gowda</surname><given-names>Shashi</given-names></name>
        <name><surname>Ma</surname><given-names>Yingbo</given-names></name>
        <name><surname>Cheli</surname><given-names>Alessandro</given-names></name>
        <name><surname>Gwóźzdź</surname><given-names>Maja</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>High-performance symbolic-numerics via multiple dispatch</article-title>
      <source>Association for Computing Machinery Communications in Computer Algebra</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2022-01">2022</year><month>01</month>
      <volume>55</volume>
      <issue>3</issue>
      <issn>1932-2240</issn>
      <uri>https://doi.org/10.1145/3511528.3511535</uri>
      <pub-id pub-id-type="doi">10.1145/3511528.3511535</pub-id>
      <fpage>92</fpage>
      <lpage>96</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dispatch">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
      </person-group>
      <article-title>The unreasonable effectiveness of multiple dispatch</article-title>
      <publisher-name>Conference Talk at JuliaCon 2019, available at https://www.youtube.com/watch?v=kc9HwsxE1OY</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-ItakuraSaito">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Itakura</surname><given-names>Fumitada</given-names></name>
        <name><surname>Saito</surname><given-names>Shuzo</given-names></name>
      </person-group>
      <article-title>Analysis synthesis telephony based on the maximum likelihood method</article-title>
      <source>Proceedings of the 6th international congress on acoustics</source>
      <publisher-name>IEEE</publisher-name>
      <publisher-loc>Los Alamitos, CA</publisher-loc>
      <year iso-8601-date="1968">1968</year>
      <fpage>C</fpage>
      <lpage>17-C-20</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Roy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roy</surname><given-names>Nicholas</given-names></name>
        <name><surname>Gordon</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Thrun</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Finding approximate POMDP solutions through belief compression</article-title>
      <source>Journal of Artificial Intelligence Research</source>
      <publisher-name>AI Access Foundation</publisher-name>
      <year iso-8601-date="2005-01">2005</year><month>01</month>
      <volume>23</volume>
      <issn>1076-9757</issn>
      <uri>http://dx.doi.org/10.1613/jair.1496</uri>
      <pub-id pub-id-type="doi">10.1613/jair.1496</pub-id>
      <fpage>1</fpage>
      <lpage>40</lpage>
    </element-citation>
  </ref>
  <ref id="ref-shortRoy">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Roy</surname><given-names>Nicholas</given-names></name>
        <name><surname>Gordon</surname><given-names>Geoffrey</given-names></name>
      </person-group>
      <article-title>Exponential family PCA for belief compression in POMDPs</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <person-group person-group-type="editor">
        <name><surname>Becker</surname><given-names>S.</given-names></name>
        <name><surname>Thrun</surname><given-names>S.</given-names></name>
        <name><surname>Obermayer</surname><given-names>K.</given-names></name>
      </person-group>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2002">2002</year>
      <volume>15</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2002/file/a11f9e533f28593768ebf87075ab34f2-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Brad">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Efron</surname><given-names>Bradley</given-names></name>
      </person-group>
      <article-title>The estimation of prediction error</article-title>
      <source>Journal of the American Statistical Association</source>
      <publisher-name>ASA Website</publisher-name>
      <year iso-8601-date="2004">2004</year>
      <volume>99</volume>
      <issue>467</issue>
      <uri>https://doi.org/10.1198/016214504000000692</uri>
      <pub-id pub-id-type="doi">10.1198/016214504000000692</pub-id>
      <fpage>619</fpage>
      <lpage>632</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PCA1">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pearson</surname><given-names>Karl</given-names></name>
      </person-group>
      <article-title>On lines and planes of closest fit to systems of points in space</article-title>
      <source>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1901">1901</year>
      <volume>2</volume>
      <issue>11</issue>
      <uri>https://doi.org/10.1080/14786440109462720</uri>
      <pub-id pub-id-type="doi">10.1080/14786440109462720</pub-id>
      <fpage>559</fpage>
      <lpage>572</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PCA3">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Jolliffe</surname><given-names>Ian T</given-names></name>
      </person-group>
      <source>Principal component analysis for special types of data</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2002">2002</year>
      <pub-id pub-id-type="doi">10.1007/0-387-22440-8_13</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-PCA2">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hotelling</surname><given-names>Harold</given-names></name>
      </person-group>
      <article-title>Analysis of a complex of statistical variables into principal components</article-title>
      <source>Journal of Educational Psychology</source>
      <year iso-8601-date="1933">1933</year>
      <volume>24</volume>
      <uri>https://api.semanticscholar.org/CorpusID:144828484</uri>
      <pub-id pub-id-type="doi">10.1037/h0071325</pub-id>
      <fpage>498</fpage>
      <lpage>520</lpage>
    </element-citation>
  </ref>
  <ref id="ref-epca-MATLAB">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chambrier</surname><given-names>Guillaume de</given-names></name>
      </person-group>
      <article-title>E-PCA</article-title>
      <year iso-8601-date="2016">2016</year>
      <uri>https://github.com/gpldecha/e-pca</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Symbolic differentiation is essential for
    flexibly specifying the EPCA objective (see
    <ext-link ext-link-type="uri" xlink:href="https://sisl.github.io/ExpFamilyPCA.jl/v2.0/math/objectives/#2.-Using-F-and-f">documentation</ext-link>).
    While algorithmic differentiation is faster in general, symbolic
    differentiation is performed only once to generate a closed form for
    the optimizer (e.g., <monospace>Optim.jl</monospace>
    (<xref alt="Mogensen &amp; Riseth, 2018" rid="ref-optim" ref-type="bibr">Mogensen
    &amp; Riseth, 2018</xref>)), making it more efficient here.
    <italic>LogExpFunctions.jl</italic>
    (<xref alt="2024" rid="ref-logexp" ref-type="bibr">2024</xref>)
    (which implements ideas from Mächler
    (<xref alt="2015" rid="ref-stable_exp" ref-type="bibr">2015</xref>))
    mitigates overflow and underflow in exponential and logarithmic
    operations.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>In practice, we allow
    <inline-formula><alternatives>
    <tex-math><![CDATA[\epsilon \geq 0]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    because special cases of EPCA like traditional PCA are well-known to
    converge without regularization. Similarly, we pick
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mu_0]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
    to simplify terms in the objective.</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>See Figure 3(a) in Roy &amp; Gordon
    (<xref alt="2002" rid="ref-shortRoy" ref-type="bibr">2002</xref>)
    and Figure 12(c) in Roy et al.
    (<xref alt="2005" rid="ref-Roy" ref-type="bibr">2005</xref>).</p>
  </fn>
</fn-group>
</back>
</article>
