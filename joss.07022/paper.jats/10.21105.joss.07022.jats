<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7022</article-id>
<article-id pub-id-type="doi">10.21105/joss.07022</article-id>
<title-group>
<article-title>HoverFast: an accurate, high-throughput, clinically
deployable nuclear segmentation tool for brightfield digital pathology
images</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-2015-6795</contrib-id>
<name>
<surname>Liakopoulos</surname>
<given-names>Petros</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0004-9515-6100</contrib-id>
<name>
<surname>Massonnet</surname>
<given-names>Julien</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-8165-6897</contrib-id>
<name>
<surname>Bonjour</surname>
<given-names>Jonatan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Mizrakli</surname>
<given-names>Medya Tekes</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Graham</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cuendet</surname>
<given-names>Michel A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Seipel</surname>
<given-names>Amanda H.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Michielin</surname>
<given-names>Olivier</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Merkler</surname>
<given-names>Doron</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Janowczyk</surname>
<given-names>Andrew</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Service of Precision Oncology, Department of Oncology,
University of Geneva and Geneva University Hospitals, Geneva,
Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Division of Clinical Pathology, , Departments of Pathology
and Immunology &amp; Diagnostics, University of Geneva and Geneva
University Hospitals, Geneva, Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Section of Communication Systems, School of Computer and
Communication Sciences, École Polytechnique Fédérale de Lausanne,
Lausanne, Switzerland.</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Histofy Ltd, Birmingham, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Emory University, Atlanta, Georgia, United States of
America.</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-04-28">
<day>28</day>
<month>4</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>101</issue>
<fpage>7022</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>digital pathology</kwd>
<kwd>deep learning</kwd>
<kwd>biomedical imaging</kwd>
<kwd>nuclear segmentation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In computational digital pathology, accurate nuclear segmentation
  of Hematoxylin and Eosin (H&amp;E) stained whole slide images (WSIs)
  is a critical step for many analyses and tissue characterizations. One
  popular deep learning-based nuclear segmentation approach, HoverNet
  (<xref alt="Graham et al., 2019" rid="ref-graham2019hover" ref-type="bibr">Graham
  et al., 2019</xref>), offers remarkably accurate results but lacks the
  high-throughput performance needed for clinical deployment in
  resource-constrained settings. Our approach, HoverFast, aims to
  provide fast and accurate nuclear segmentation in H&amp;E images using
  knowledge distillation from HoverNet. By redesigning the tool with
  software engineering best practices, HoverFast introduces advanced
  parallel processing capabilities, efficient data handling, and
  optimized postprocessing. These improvements facilitate scalable
  high-throughput performance, making HoverFast more suitable for
  real-time analysis and application in resource-limited environments.
  Using a consumer grade Nvidia A5000 GPU, HoverFast showed a 21x speed
  improvement as compared to HoverNet; reducing mean analysis time for
  40x WSIs from ~2 hours to 6 minutes while retaining a concordant mean
  Dice score of 0.91 against the original HoverNet output. Peak memory
  usage was also reduced 71% from 44.4GB, to 12.8GB, without requiring
  SSD-based caching. To ease adoption in research and clinical contexts,
  HoverFast aligns with best-practices in terms of (a) installation, and
  (b) containerization, while (c) providing outputs compatible with
  existing popular open-source image viewing tools such as QuPath
  (<xref alt="Bankhead et al., 2017" rid="ref-bankhead2017qupath" ref-type="bibr">Bankhead
  et al., 2017</xref>). HoverFast has been made open-source and is
  available at
  <ext-link ext-link-type="uri" xlink:href="andrewjanowczyk.com/open-source-tools/hoverfast">andrewjanowczyk.com/open-source-tools/hoverfast</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The increasing popularity of digitized pathology images in both
  research and clinical practice has spurred the widespread adoption of
  deep learning (DL) approaches for automating various tasks, with
  nuclear segmentation standing out as a crucial step in many analyses.
  This segmentation process involves delineating the contours of cell
  nuclei within a 2D whole slide image (WSI). Nuclei, rather than
  complete cells, are targeted due to strong contrast afforded by
  routinely employed hematoxylin staining. Hematoxylin’s selective
  affinity for nucleic acids results in the distinct visualization of
  nuclei in purple, facilitating their clear identification amidst less
  prominently stained cytoplasm and other cellular constituents. Given
  the small size of nuclei, their segmentation typically takes place at
  40x magnification (~0.25 microns per pixel (mpp)); the highest
  magnification supported by most current digital slide scanners.
  Working at this scale can be time-intensive for algorithms, especially
  on consumer grade GPUs, as WSIs are especially large, reaching up to
  120,000x120,000 pixels. While several existing tools like StarDist
  (<xref alt="Schmidt et al., 2018" rid="ref-schmidt2018" ref-type="bibr">Schmidt
  et al., 2018</xref>)
  (<xref alt="Weigert et al., 2020" rid="ref-weigert2020" ref-type="bibr">Weigert
  et al., 2020</xref>) and CellPose
  (<xref alt="Stringer et al., 2021" rid="ref-stringer2021cellpose" ref-type="bibr">Stringer
  et al., 2021</xref>) have been developed to tackle the challenge of
  nuclear segmentation,
  HoverNet(<xref alt="Graham et al., 2019" rid="ref-graham2019hover" ref-type="bibr">Graham
  et al., 2019</xref>) has emerged as one of the leading solutions in
  terms of segmentation accuracy, particularly for its application to
  H&amp;E-stained tissue.</p>
  <p>Despite its accurate results, HoverNet remains resource-intensive
  and time-consuming due to its high model parameter count and lengthy
  post-processing steps. HoverNet additionally requires significant SSD
  storage for caching during runtime, often reaching over 120GB per WSI.
  These properties make it challenging to deploy in more resource
  limited settings such as consumer grade workstations or in clinical
  environments requiring high-throughput processing. Therefore, there is
  an emerging need for a fast, accurate, and computationally efficient
  tool that can make large-scale nuclear segmentation more accessible
  for both research and clinical applications.</p>
  <p>Motivated by the need for accurate yet efficient nuclear
  segmentation, we introduce HoverFast. This tool replicates the output
  of the established HoverNet model while achieving superior
  computational efficiency. HoverFast achieves this through knowledge
  distillation, a technique where a smaller “student” model (HoverFast)
  learns to capture the knowledge from a larger “teacher” model
  (HoverNet). The goal is to enable the student model to achieve
  comparable performance to that of the teacher model, while requiring
  significantly less computational resources for inference
  (<xref alt="Hinton et al., 2015" rid="ref-hinton2015distilling" ref-type="bibr">Hinton
  et al., 2015</xref>),
  (<xref alt="Hu et al., 2022" rid="ref-hu2022teacher" ref-type="bibr">Hu
  et al., 2022</xref>).</p>
  <p>To facilitate the knowledge distillation process, HoverFast
  presents a training pipeline, using HoverNet output as ground truth
  (see <bold>Figure 1</bold>), that enables the resulting model to have
  30 times fewer parameters. As implemented, HoverFast provides:</p>
  <list list-type="bullet">
    <list-item>
      <p>A containerized docker script to generate HoverNet ground truth
      on user-provided data</p>
    </list-item>
    <list-item>
      <p>A training pipeline for a custom HoverFast model</p>
    </list-item>
    <list-item>
      <p>Alternatively, a pre-trained cross-organ model for
      inference</p>
    </list-item>
    <list-item>
      <p>An inference pipeline for tiles and WSIs with tissue masks to
      delineate area of computation</p>
    </list-item>
    <list-item>
      <p>Compressed JSON output file directly compatible with QuPath</p>
    </list-item>
    <list-item>
      <p>A speedup of 21x over HoverNet, on consumer-grade compute
      infrastructure</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Overview of training pipeline for HoverFast. H&amp;E
    tiles are segmented using HoverNet. The nuclei masks, as well as the
    original H&amp;E tiles, are passed to the smaller HoverFast
    architecture for training. The resulting HoverFast model provides a
    highly optimized inference and post-processing framework that can
    then be used for nuclear segmentation on WSIs with a 21x speed
    improvement over
    HoverNet.<styled-content id="figU003AFigureU00201"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="fig_1_hoverfast.png" />
  </fig>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <sec id="inference">
    <title>Inference</title>
    <p>HoverFast has a command-line interface (CLI) written in Python
    3.11 and utilizes the PyTorch framework
    (<xref alt="Paszke et al., 2017" rid="ref-paszke2017automatic" ref-type="bibr">Paszke
    et al., 2017</xref>). We replicated the structure of the HoverNet
    model as described by Graham et al
    (<xref alt="Graham et al., 2019" rid="ref-graham2019hover" ref-type="bibr">Graham
    et al., 2019</xref>) without the nuclear classification branch. For
    the backbone, we used a modified 940k parameter Multi-scale UNet
    (<xref alt="Su et al., 2021" rid="ref-su2021msu" ref-type="bibr">Su
    et al., 2021</xref>) in place of HoverNet’s 33.6 million parameter
    ResNet50, yielding a reduction in model parameter count by a factor
    of 30 (see <bold>Appendix 1</bold>).</p>
    <p>HoverFast’s post-processing pipeline was heavily optimized using
    scikit-image’s
    (<xref alt="Walt et al., 2014" rid="ref-van_der_Walt_2014" ref-type="bibr">Walt
    et al., 2014</xref>) regionprops and watershed functions to
    effectively identify and split merged cells. To improve throughput
    after batch model inference, regions are processed in parallel using
    a “multi-worker, single writer” approach. This involves each worker
    independently (a) post-processing its assigned region, and then (b)
    generating nuclei polygon coordinates using OpenCV
    (<xref alt="Bradski, 2000" rid="ref-opencv_library" ref-type="bibr">Bradski,
    2000</xref>), before (c) sending to the single writing process for
    saving as a QuPath
    (<xref alt="Bankhead et al., 2017" rid="ref-bankhead2017qupath" ref-type="bibr">Bankhead
    et al., 2017</xref>) compatible gzip-compressed JSON file. A Docker
    and Singularity container of HoverFast are provided.</p>
  </sec>
  <sec id="training">
    <title>Training</title>
    <p>To help users train their own models, we provide a Docker
    container with HoverNet installed, and a script that (a) accepts a
    directory of WSIs (or tiles), (b) randomly extracts a user-specified
    number of tiles, (c) employs HoverNet on these tiles to generate
    labeled masks of nuclei, and finally (d) saves the original images
    and associated masks into two PyTables files, one for training and
    one for validation. HoverFast can then accept these PyTables files
    as arguments in its training script to yield a use-case specific
    model. We employ knowledge distillation during HoverFast training
    using the same loss function as HoverNet. The teacher model,
    HoverNet, guides the student model, HoverFast, by providing the
    ground truth binary mask as target. Additionally, HoverFast learns
    to reproduce the horizontal and vertical distance maps allowing
    HoverFast to inherit HoverNet’s post-processing abilities for
    separating touching nuclei.</p>
  </sec>
</sec>
<sec id="experiments">
  <title>Experiments</title>
  <sec id="experiment-1-comparison-of-hovernet-and-hoverfast-on-a-cross-organ-dataset">
    <title>Experiment 1: Comparison of HoverNet and HoverFast on a cross
    organ dataset</title>
    <p>Employing n=97 WSIs of diverse tissue types, from The Cancer
    Genome Atlas (TCGA)
    (<xref alt="Weinstein et al., 2013" rid="ref-weinstein2013cancer" ref-type="bibr">Weinstein
    et al., 2013</xref>), 15 randomly selected tiles of 1,024x1,024
    pixels from each WSI were extracted. A HoverFast model was then
    trained as described in
    <xref alt="Training" rid="training">Training</xref>, for 100 epochs
    with a batch size of 16. For validation, 74 tiles of 1,024x1,024
    pixels from 14 slides of diverse tissue types were used. Inference
    using both HoverNet and HoverFast was performed on the validation
    tiles, and the binary masks of predicted cell nuclei were overlapped
    to obtain a Dice score between the two tools. The resulting Dice
    score of 0.91 appears concordant with the qualitative results (see
    <bold>Figure 2</bold>); these show very similar segmentation
    results, with HoverFast able to segment slightly more faint nuclei
    than HoverNet.</p>
    <fig>
      <caption><p>(A) 1,024x1,024 tile of H&amp;E tissue. (B) Overlay of
      binary masks. Pixels in white are predicted by both models, with
      those in green/pink predicted only by HoverFast/HoverNet,
      respectively. For this patch, the Dice score is 0.91. Here we can
      generally see that while HoverFast detects more nuclei, those
      predictions remain reasonable and clearly visible on H&amp;E,
      suggesting a potentially higher quality
      result.<styled-content id="figU003AFigureU00202"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="fig_2_hoverfast.png" />
    </fig>
  </sec>
  <sec id="experiment-2-comparison-of-hovernet-cross-tissue-hoverfast-and-site-specific-hoverfast">
    <title>Experiment 2: Comparison of HoverNet, cross-tissue HoverFast,
    and site-specific HoverFast</title>
    <p>From n=54 melanoma samples, (a) for training: 20 1,024x1,024
    tiles were randomly selected per slide from within available tumor
    masks, and (b) for validation: 50 tiles of 1,024x1,024 from 8 slides
    were selected. For evaluation, 3 models were compared: (i) HoverNet,
    as a baseline, (ii) the HoverFast model trained in
    <xref alt="Experiment 1" rid="experiment-1-comparison-of-hovernet-and-hoverfast-on-a-cross-organ-dataset">Experiment
    1</xref>, and (iii) a melanoma specific model trained following the
    procedure in <xref alt="Training" rid="training">Training</xref>.
    The HoverFast models had Dice scores of 0.88 and 0.91 respectively
    against HoverNet, with qualitative results indicating a high degree
    of similarity. There were slight changes on nuclei edges and faint
    nuclei (see <bold>Figure 3</bold>), with a systematic superiority
    for the tissue-specific output. Taken together, the increased
    accuracy in the melanoma specific model demonstrates that investing
    in training a dataset-specific model appears to provide added
    value.</p>
    <fig>
      <caption><p>(A) 1,024x1,024 tile of H&amp;E tissue (top) with a
      higher magnification region of interest (bottom). (B) Overlay of
      binary masks for the single tissue model. Pixels in white are
      predicted by both models, with those in green/pink predicted only
      by HoverFast/HoverNet, respectively. (C) Overlay of binary mask
      for the cross-tissue model. The tissue specific model shows a
      closer resemblance to HoverNet in terms of cell
      outlines.<styled-content id="figU003AFigureU00203"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="fig_3_hoverfast.png" />
    </fig>
  </sec>
  <sec id="experiment-3-benchmarks-comparing-processing-time-and-memory-footprint">
    <title>Experiment 3: Benchmarks comparing processing time and memory
    footprint</title>
    <p>To compare computational speed, n=4 slides from TCGA with
    corresponding tissue masks generated with HistoQC
    (<xref alt="Janowczyk et al., 2019" rid="ref-janowczyk2019histoqc" ref-type="bibr">Janowczyk
    et al., 2019</xref>) were analyzed on a machine with a 16 core
    Intel(R) Core(TM) i9-12900K CPU, a Nvidia A5000 GPU with 24GB of
    VRAM, and 128Gb of DDR5 RAM. For both HoverNet and HoverFast, the
    GPU batch size was set to maximize GPU memory usage. For HoverNet, a
    batch size of 90 was used, with 20 CPU threads for pre- and
    post-processing. Similarly, for HoverFast, a batch size of 11 and 20
    CPU threads were used. A mean speed improvement of 20.8x times (see
    <bold>Table 1</bold>) was demonstrated. The maximum RAM consumption
    was reduced by 71% with 44.4 GB for HoverNet versus 12.8 GB for
    HoverFast. Additionally, HoverNet required a peak of 118 GB of SSD
    space for its cache during run-time, while HoverFast did not appear
    to require any.</p>
    <table-wrap>
      <caption>
        <p>Detailed table of computation time per slide for each tool
        with associated speedup </p>
      </caption>
      <table>
        <thead>
          <tr>
            <th><italic>Slide ID</italic></th>
            <th><italic>HoverNet</italic></th>
            <th><italic>HoverFast</italic></th>
            <th><italic>Speedup</italic></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Slide 1</td>
            <td>58mins 5s</td>
            <td>3mins 1s</td>
            <td>19.2x</td>
          </tr>
          <tr>
            <td>Slide 2</td>
            <td>1hr 11mins 38s</td>
            <td>3mins 33s</td>
            <td>20.2x</td>
          </tr>
          <tr>
            <td>Slide 3</td>
            <td>2hrs 55mins 24s</td>
            <td>8mins 4s</td>
            <td>21.7x</td>
          </tr>
          <tr>
            <td>Slide 4</td>
            <td>3hrs 4mins 25s</td>
            <td>8mins 50s</td>
            <td>20.9x</td>
          </tr>
          <tr>
            <td><bold>Total</bold></td>
            <td><bold>8hrs 9mins 32s</bold></td>
            <td><bold>23mins 28s</bold></td>
            <td><bold>20.8x</bold></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
</sec>
<sec id="discussion-and-conclusions">
  <title>Discussion and Conclusions:</title>
  <p>HoverFast represents a practical solution to the challenge of
  nuclear segmentation in WSIs, emphasizing speed, resource efficiency
  and local trainability. It distinguishes itself by providing a
  significant speedup in processing time with a 21x improvement over
  HoverNet on consumer grade hardware in addition to a more than 3x
  reduction in RAM footprint while also eliminating hard-drive based
  caching. This efficiency is crucial for users with limited resources,
  enabling faster analysis while retaining segmentation results highly
  comparable to those of HoverNet. While a pre-trained cross-tissue
  model is provided with the software, if higher accuracy and greater
  similarity to HoverNet is required, a cohort specific model should be
  trained. Additionally, although HoverFast does have a built-in feature
  for tissue detection, we highly recommend the use of quality control
  tools, such as HistoQC to obtain more robust tissue masks, thus
  avoiding computation on artefactual regions and further reducing
  computation time. HoverFast is easy to install and provides simple
  drag and drop output compatibility with QuPath. It is publicly
  available for use and modification at
  <ext-link ext-link-type="uri" xlink:href="andrewjanowczyk.com/open-source-tools/hoverfast">andrewjanowczyk.com/open-source-tools/hoverfast</ext-link>.</p>
  <sec id="appendix-1">
    <title>Appendix 1</title>
    <graphic mimetype="image" mime-subtype="png" xlink:href="appendix_fig.png" />
    <p>The appendix presents a comparison between HoverFast and HoverNet
    architectures. This information was produced using the Python
    Torchinfo Summary package. The first column outlines the model
    architecture, while the second delves into the number of parameters
    for each layer. It is noteworthy that HoverFast substantially
    smaller number of parameters (roughly 30 times fewer than HoverNet)
    which, along with the optimized post-processing and file handling,
    translates to lower memory footprint and faster processing time.
    This enables HoverFast to handle larger batches of data and allowing
    parallel post-processing computation, ultimately leading to a
    well-suited tool for resource limited environments.</p>
  </sec>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-schmidt2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schmidt</surname><given-names>Uwe</given-names></name>
        <name><surname>Weigert</surname><given-names>Martin</given-names></name>
        <name><surname>Broaddus</surname><given-names>Coleman</given-names></name>
        <name><surname>Myers</surname><given-names>Gene</given-names></name>
      </person-group>
      <article-title>Cell detection with star-convex polygons</article-title>
      <source>Medical image computing and computer assisted intervention - MICCAI 2018 - 21st international conference, granada, spain, september 16-20, 2018, proceedings, part II</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-00934-2_30</pub-id>
      <fpage>265</fpage>
      <lpage>273</lpage>
    </element-citation>
  </ref>
  <ref id="ref-weigert2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Weigert</surname><given-names>Martin</given-names></name>
        <name><surname>Schmidt</surname><given-names>Uwe</given-names></name>
        <name><surname>Haase</surname><given-names>Robert</given-names></name>
        <name><surname>Sugawara</surname><given-names>Ko</given-names></name>
        <name><surname>Myers</surname><given-names>Gene</given-names></name>
      </person-group>
      <article-title>Star-convex polyhedra for 3D object detection and segmentation in microscopy</article-title>
      <source>The IEEE winter conference on applications of computer vision (WACV)</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1109/WACV45572.2020.9093435</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-stringer2021cellpose">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stringer</surname><given-names>Carsen</given-names></name>
        <name><surname>Wang</surname><given-names>Tim</given-names></name>
        <name><surname>Michaelos</surname><given-names>Michalis</given-names></name>
        <name><surname>Pachitariu</surname><given-names>Marius</given-names></name>
      </person-group>
      <article-title>Cellpose: A generalist algorithm for cellular segmentation</article-title>
      <source>Nature methods</source>
      <publisher-name>Nature Publishing Group US New York</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>18</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41592-020-01018-x</pub-id>
      <fpage>100</fpage>
      <lpage>106</lpage>
    </element-citation>
  </ref>
  <ref id="ref-graham2019hover">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Graham</surname><given-names>Simon</given-names></name>
        <name><surname>Vu</surname><given-names>Quoc Dang</given-names></name>
        <name><surname>Raza</surname><given-names>Shan E Ahmed</given-names></name>
        <name><surname>Azam</surname><given-names>Ayesha</given-names></name>
        <name><surname>Tsang</surname><given-names>Yee Wah</given-names></name>
        <name><surname>Kwak</surname><given-names>Jin Tae</given-names></name>
        <name><surname>Rajpoot</surname><given-names>Nasir</given-names></name>
      </person-group>
      <article-title>Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</article-title>
      <source>Medical image analysis</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>58</volume>
      <pub-id pub-id-type="doi">10.1016/j.media.2019.101563</pub-id>
      <fpage>101563</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-bankhead2017qupath">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bankhead</surname><given-names>Peter</given-names></name>
        <name><surname>Loughrey</surname><given-names>Maurice B</given-names></name>
        <name><surname>Fernández</surname><given-names>José A</given-names></name>
        <name><surname>Dombrowski</surname><given-names>Yvonne</given-names></name>
        <name><surname>McArt</surname><given-names>Darragh G</given-names></name>
        <name><surname>Dunne</surname><given-names>Philip D</given-names></name>
        <name><surname>McQuaid</surname><given-names>Stephen</given-names></name>
        <name><surname>Gray</surname><given-names>Ronan T</given-names></name>
        <name><surname>Murray</surname><given-names>Liam J</given-names></name>
        <name><surname>Coleman</surname><given-names>Helen G</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>QuPath: Open source software for digital pathology image analysis</article-title>
      <source>Scientific reports</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>7</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-017-17204-5</pub-id>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-su2021msu">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Su</surname><given-names>Run</given-names></name>
        <name><surname>Zhang</surname><given-names>Deyun</given-names></name>
        <name><surname>Liu</surname><given-names>Jinhuai</given-names></name>
        <name><surname>Cheng</surname><given-names>Chuandong</given-names></name>
      </person-group>
      <article-title>MSU-net: Multi-scale u-net for 2D medical image segmentation</article-title>
      <source>Frontiers in Genetics</source>
      <publisher-name>Frontiers Media SA</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>12</volume>
      <pub-id pub-id-type="doi">10.3389/fgene.2021.639930</pub-id>
      <fpage>639930</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2017automatic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
      </person-group>
      <article-title>Automatic differentiation in PyTorch</article-title>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-van_der_Walt_2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Walt</surname><given-names>Stéfan van der</given-names></name>
        <name><surname>Schönberger</surname><given-names>Johannes L.</given-names></name>
        <name><surname>Nunez-Iglesias</surname><given-names>Juan</given-names></name>
        <name><surname>Boulogne</surname><given-names>François</given-names></name>
        <name><surname>Warner</surname><given-names>Joshua D.</given-names></name>
        <name><surname>Yager</surname><given-names>Neil</given-names></name>
        <name><surname>Gouillart</surname><given-names>Emmanuelle</given-names></name>
        <name><surname>Yu</surname><given-names>Tony</given-names></name>
      </person-group>
      <article-title>Scikit-image: Image processing in python</article-title>
      <source>PeerJ</source>
      <publisher-name>PeerJ</publisher-name>
      <year iso-8601-date="2014-06">2014</year><month>06</month>
      <volume>2</volume>
      <issn>2167-8359</issn>
      <uri>http://dx.doi.org/10.7717/peerj.453</uri>
      <pub-id pub-id-type="doi">10.7717/peerj.453</pub-id>
      <fpage>e453</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-opencv_library">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bradski</surname><given-names>G.</given-names></name>
      </person-group>
      <article-title>The OpenCV Library</article-title>
      <source>Dr. Dobb’s Journal of Software Tools</source>
      <year iso-8601-date="2000">2000</year>
    </element-citation>
  </ref>
  <ref id="ref-weinstein2013cancer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Weinstein</surname><given-names>John N</given-names></name>
        <name><surname>Collisson</surname><given-names>Eric A</given-names></name>
        <name><surname>Mills</surname><given-names>Gordon B</given-names></name>
        <name><surname>Shaw</surname><given-names>Kenna R</given-names></name>
        <name><surname>Ozenberger</surname><given-names>Brad A</given-names></name>
        <name><surname>Ellrott</surname><given-names>Kyle</given-names></name>
        <name><surname>Shmulevich</surname><given-names>Ilya</given-names></name>
        <name><surname>Sander</surname><given-names>Chris</given-names></name>
        <name><surname>Stuart</surname><given-names>Joshua M</given-names></name>
      </person-group>
      <article-title>The cancer genome atlas pan-cancer analysis project</article-title>
      <source>Nature genetics</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>45</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1038/ng.2764</pub-id>
      <fpage>1113</fpage>
      <lpage>1120</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hinton2015distilling">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Dean</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>Distilling the knowledge in a neural network</article-title>
      <source>arXiv preprint arXiv:1503.02531</source>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-hu2022teacher">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hu</surname><given-names>Chengming</given-names></name>
        <name><surname>Li</surname><given-names>Xuan</given-names></name>
        <name><surname>Liu</surname><given-names>Dan</given-names></name>
        <name><surname>Chen</surname><given-names>Xi</given-names></name>
        <name><surname>Wang</surname><given-names>Ju</given-names></name>
        <name><surname>Liu</surname><given-names>Xue</given-names></name>
      </person-group>
      <article-title>Teacher-student architecture for knowledge learning: A survey</article-title>
      <source>arXiv preprint arXiv:2210.17332</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-janowczyk2019histoqc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Janowczyk</surname><given-names>Andrew</given-names></name>
        <name><surname>Zuo</surname><given-names>Ren</given-names></name>
        <name><surname>Gilmore</surname><given-names>Hannah</given-names></name>
        <name><surname>Feldman</surname><given-names>Michael</given-names></name>
        <name><surname>Madabhushi</surname><given-names>Anant</given-names></name>
      </person-group>
      <article-title>HistoQC: An open-source quality control tool for digital pathology slides</article-title>
      <source>JCO clinical cancer informatics</source>
      <publisher-name>American Society of Clinical Oncology</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>3</volume>
      <pub-id pub-id-type="doi">10.1200/CCI.18.00157</pub-id>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
