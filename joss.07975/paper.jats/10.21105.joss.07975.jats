<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7975</article-id>
<article-id pub-id-type="doi">10.21105/joss.07975</article-id>
<title-group>
<article-title>DeepHyper: A Python Package for Massively Parallel
Hyperparameter Optimization in Machine Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8992-8192</contrib-id>
<name>
<surname>Egele</surname>
<given-names>Romain</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0292-5715</contrib-id>
<name>
<surname>Balaprakash</surname>
<given-names>Prasanna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wiggins</surname>
<given-names>Gavin M.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Eiffert</surname>
<given-names>Brett</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Oak Ridge National Laboratory, Oak Ridge, TN, United
States</institution>
<institution-id institution-id-type="ROR">01qz5mb56</institution-id>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-02-03">
<day>3</day>
<month>2</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>109</issue>
<fpage>7975</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>machine learning</kwd>
<kwd>hyperparameter optimization</kwd>
<kwd>multi-fidelity</kwd>
<kwd>neural architecture search</kwd>
<kwd>ensemble</kwd>
<kwd>high-performance computing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Machine learning models are increasingly applied across scientific
  disciplines, yet their effectiveness often hinges on heuristic
  decisions—such as data transformations, training strategies, and model
  architectures—that are not learned by the models themselves.
  Automating the selection of these heuristics and analyzing their
  sensitivity is crucial for building robust and efficient learning
  workflows. <monospace>DeepHyper</monospace> addresses this challenge
  by democratizing hyperparameter optimization, providing accessible
  tools to streamline and enhance machine learning workflows from a
  laptop to the largest supercomputer in the world. Building on top of
  hyperparameter optimization, it unlocks new capabilities around
  ensembles of models for improved accuracy and uncertainty
  quantification. All of these organized around efficient parallel
  computing.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>DeepHyper</monospace> is a Python package for parallel
  hyperparameter optimization or neural architecture search. The project
  started in 2018
  (<xref alt="Balaprakash et al., 2018" rid="ref-balaprakash2018deephyper" ref-type="bibr">Balaprakash
  et al., 2018</xref>) with a focus on making Bayesian optimization more
  efficient on high-performance computing clusters. It provides access
  to a variety of asynchronous parallel black-box optimization
  algorithms via <monospace>deephyper.hpo</monospace>. The software
  offers a variety of parallel programming backends such as Asyncio,
  threading, processes, Ray, and MPI via
  <monospace>deehyper.evaluator</monospace>. The hyperparameter
  optimization can be single or multi-objective, composed of mixed
  variables, using explicit or hidden constraints, and benefit from
  early-discarding strategies via
  <monospace>deephyper.stopper</monospace>. Leveraging the results of
  hyperparameter optimization or neural architecture search it provides
  parallel ensemble algorithms via
  <monospace>deephyper.ensemble</monospace> that can help improve
  accuracy or quantify disentangled predictive uncertainty. A diagram of
  our software architecture is shown in Figure 1.</p>
  <fig>
    <caption><p>DeepHyper Software Architecture</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="deephyper-architecture.png" />
  </fig>
  <p><monospace>DeepHyper</monospace> was designed to help research in
  the field of automated machine learning and also to be used out-of-the
  box in scientific projects where learning workflows are being
  developed.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>Numerous software packages now exist for hyperparameter
  optimization (HPO), including:</p>
  <list list-type="bullet">
    <list-item>
      <p>BoTorch (and Ax platform)
      (<xref alt="Balandat et al., 2020" rid="ref-botorch2020" ref-type="bibr">Balandat
      et al., 2020</xref>),
      (<ext-link ext-link-type="uri" xlink:href="https://botorch.org">doc.</ext-link>)</p>
    </list-item>
    <list-item>
      <p>HyperMapper
      (<xref alt="Nardi et al., 2019" rid="ref-hypermapper2019" ref-type="bibr">Nardi
      et al., 2019</xref>),
      (<ext-link ext-link-type="uri" xlink:href="https://github.com/luinardi/hypermapper">doc.</ext-link>)</p>
    </list-item>
    <list-item>
      <p>Hyperopt
      (<xref alt="J. Bergstra et al., 2013" rid="ref-hyperopt2013" ref-type="bibr">J.
      Bergstra et al., 2013</xref>;
      <xref alt="James Bergstra et al., 2015" rid="ref-hyperopt2015" ref-type="bibr">James
      Bergstra et al., 2015</xref>),
      (<ext-link ext-link-type="uri" xlink:href="http://hyperopt.github.io/hyperopt/">doc.</ext-link>)</p>
    </list-item>
    <list-item>
      <p>OpenBox
      (<xref alt="Jiang et al., 2024" rid="ref-openbox2024" ref-type="bibr">Jiang
      et al., 2024</xref>),
      (<ext-link ext-link-type="uri" xlink:href="https://open-box.readthedocs.io">doc.</ext-link>)</p>
    </list-item>
    <list-item>
      <p>Optuna
      (<xref alt="Akiba et al., 2019" rid="ref-optuna2019" ref-type="bibr">Akiba
      et al., 2019</xref>),
      (<ext-link ext-link-type="uri" xlink:href="https://optuna.readthedocs.io">doc.</ext-link>)</p>
    </list-item>
    <list-item>
      <p>SMAC3
      (<xref alt="Lindauer et al., 2022" rid="ref-smac32022" ref-type="bibr">Lindauer
      et al., 2022</xref>),
      (<ext-link ext-link-type="uri" xlink:href="https://automl.github.io/SMAC3">doc.</ext-link>)</p>
    </list-item>
  </list>
  <p>These tools differ in scope and design: some are research-oriented
  (e.g., SMAC), while others prioritize production-readiness and
  usability (e.g., Optuna). In this section, we focus our comparison on
  SMAC and Optuna, which are representative of these two directions.</p>
  <p><bold>DeepHyper</bold> is designed to maximize HPO efficiency
  across a wide range of parallelization scales, from sequential
  (single-core) runs to massively parallel evaluations on
  high-performance computing (HPC) systems with thousands of cores.</p>
  <sec id="feature-comparison">
    <title>Feature Comparison</title>
    <p>While the feature matrix below provides a high-level overview, it
    necessarily simplifies some nuanced implementation differences. We
    use the <inline-formula><alternatives>
    <tex-math><![CDATA[\checkmark]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula>
    symbol for available features, the <inline-formula><alternatives>
    <tex-math><![CDATA[\approx]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula>
    symbol for incomplete features, and no symbol for missing
    features.</p>
    <sec id="hyperparameter-optimization-capabilities">
      <title>Hyperparameter Optimization Capabilities</title>
      <table-wrap>
        <caption>
          <p>Overview of optimization features available in different
          packages.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="right">Feature</th>
              <th align="center">DeepHyper</th>
              <th align="center">Optuna</th>
              <th align="center">SMAC3</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="right">Single-objective</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            </tr>
            <tr>
              <td align="right">Multi-objective</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            </tr>
            <tr>
              <td align="right">Early stopping</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
            </tr>
            <tr>
              <td align="right">Fault tolerance</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
            </tr>
            <tr>
              <td align="right">Transfer learning</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\approx]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula></td>
            </tr>
            <tr>
              <td align="right">Ensemble construction</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
              <td align="center"></td>
            </tr>
            <tr>
              <td align="right">Visualization</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\approx]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p><bold>Single-objective Optimization</bold>
      DeepHyper employs a surrogate model based on random forests to
      estimate <inline-formula><alternatives>
      <tex-math><![CDATA[P(\text{Objective} \mid \text{Hyperparameters})]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mtext mathvariant="normal">Objective</mml:mtext><mml:mo>∣</mml:mo><mml:mtext mathvariant="normal">Hyperparameters</mml:mtext><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
      similar to SMAC. However, DeepHyper’s implementation is typically
      faster per query, especially when the number of evaluations
      exceeds 200. In contrast, Optuna uses the Tree-structured Parzen
      Estimator (TPE), which models <inline-formula><alternatives>
      <tex-math><![CDATA[P(\text{Hyperparameters} \mid \text{Objective})]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mtext mathvariant="normal">Hyperparameters</mml:mtext><mml:mo>∣</mml:mo><mml:mtext mathvariant="normal">Objective</mml:mtext><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
      TPE offers faster query times but can struggle with complex
      optimization landscapes and tends to be less effective in refining
      continuous hyperparameters.</p>
      <p><bold>Multi-objective Optimization</bold>
      DeepHyper uses scalarization-based approaches inspired by ParEGO
      (also used in SMAC), with randomized weights and a variety of
      scalarization functions. Optuna defaults to NSGA-II, a genetic
      algorithm that evolves solutions along estimated Pareto fronts.
      NSGA-II typically converges more slowly but catches up when the
      evaluation budget is sufficient.</p>
      <p><bold>Early Discarding</bold>
      DeepHyper supports several early stopping techniques, including
      constant thresholds, median stopping rules, successive halving,
      and learning curve extrapolation. Optuna also offers several early
      discarding methods among which some are different from
      DeepHyper’s.</p>
      <p><bold>Fault Tolerance</bold>
      DeepHyper handles failed evaluations by assigning them the worst
      observed objective value, thereby preserving optimizer stability
      and avoiding cascading failures.</p>
      <p><bold>Transfer Learning</bold>
      DeepHyper supports warm-starting optimizations using data from
      prior runs. This is effective when either the objective function
      changes while the search space remains fixed (e.g., different
      datasets), or when the search space expands (e.g., broader neural
      architecture configurations).</p>
      <p><bold>Ensemble Construction</bold>
      DeepHyper enables model ensembling from the pool of evaluated
      configurations, helping to reduce variance and capture epistemic
      uncertainty in predictions.</p>
      <p><bold>Visualization</bold>
      Basic visualization tools are provided via
      <monospace>deephyper.analytics</monospace>. For more interactive
      exploration, we recommend
      <ext-link ext-link-type="uri" xlink:href="https://microsoft.github.io/SandDance/">SandDance</ext-link>,
      a Visual Studio Code plugin. Figure 2 illustrates a 3D
      visualization of a Random Forest optimization, with
      <monospace>min_samples_split</monospace>,
      <monospace>min_weight_fraction_leaf</monospace>, and test accuracy
      as the x, y, and z axes (and color), respectively. The two plots
      compare configurations using
      <monospace>splitter=&quot;best&quot;</monospace> (left) and
      <monospace>splitter=&quot;random&quot;</monospace> (right). Such
      visualizations help identify the sensitivity of the objective to
      different hyperparameters.</p>
      <fig>
        <caption><p>Visualization of hyperparameter optimization results
        with SandDance</p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="sanddance_viz.png" />
      </fig>
    </sec>
    <sec id="parallelization-capabilities">
      <title>Parallelization Capabilities</title>
      <table-wrap>
        <caption>
          <p>Overview of parallelization features available in different
          packages.</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="right"></th>
              <th align="center">DeepHyper</th>
              <th align="center">Optuna</th>
              <th align="center">SMAC3</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="right">Asynchronous optimization</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center">?</td>
            </tr>
            <tr>
              <td align="right">Centralized optimization</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
            </tr>
            <tr>
              <td align="right">Decentralized optimization</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
            </tr>
            <tr>
              <td align="right">Parallelization backends</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\approx]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula></td>
            </tr>
            <tr>
              <td align="right">Memory backends</td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\checkmark]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>✓</mml:mi></mml:math></alternatives></inline-formula></td>
              <td align="center"><inline-formula><alternatives>
              <tex-math><![CDATA[\approx]]></tex-math>
              <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mo>≈</mml:mo></mml:math></alternatives></inline-formula></td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
      <p>The main difference between DeepHyper, Optuna and SMAC related
      to parallelization is that DeepHyper provides out-of-the-box
      parallelization software while Optuna leaves it to the user and
      SMAC limits itself to centralized parallelism with Dask.</p>
      <p><bold>Asynchronous optimization</bold>: DeepHyper’s allows to
      submit and gather hyperparameter configuration by batch and
      asynchronously (in a centralized or decentralized setting).</p>
      <p><bold>Centralized optimization</bold>: DeepHyper’s allows to
      run centralized optimization, including
      <inline-formula><alternatives>
      <tex-math><![CDATA[1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mn>1</mml:mn></mml:math></alternatives></inline-formula>
      master running the optimization and <inline-formula><alternatives>
      <tex-math><![CDATA[N]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
      workers evaluating hyperparameter configurations.</p>
      <p><bold>Decentralized optimization</bold>: DeepHyper’s allows to
      run decentralized optimization, including
      <inline-formula><alternatives>
      <tex-math><![CDATA[N]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>
      workers, each running centralized optimization.</p>
      <p><bold>Parallelization backends</bold>: DeepHyper’s provides
      compatibility with several parallelization backends: AsyncIO
      functions, thread-based, process-based, Ray, and MPI. This allows
      to easily adapt to the context of the execution.</p>
      <p><bold>Memory backends</bold>: DeepHyper’s provides
      compatibility with several shared-memory backends: local memory,
      server managed memory, MPI-based remote memory access, Ray.</p>
    </sec>
  </sec>
  <sec id="black-box-optimization-benchmarks">
    <title>Black-box Optimization Benchmarks</title>
    <p>The benchmark is run on 9 different continuous black-box
    benchmark functions:
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/ackley.html">Ackley</ext-link>
    (5 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/branin.html">Branin</ext-link>
    (2 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/griewank.html">Griewank</ext-link>
    (5 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/hart6.html">Hartmann</ext-link>
    (6 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/levy.html">Levy</ext-link>
    (5 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/michal.html">Michalewicz</ext-link>
    (5 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/rosen.html">Rosen</ext-link>
    (5 dim.),
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/schwef.html">Schwefel</ext-link>
    (5 dim.) and
    <ext-link ext-link-type="uri" xlink:href="https://www.sfu.ca/~ssurjano/shekel.html">Shekel</ext-link>
    (5 dim.). Figures 3-7 present the benchmark results with the average
    regret and the standard error over 10 random repetitions for each
    method. The average regret is given by
    <inline-formula><alternatives>
    <tex-math><![CDATA[y^* - \hat{y}^*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>y</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
    where <inline-formula><alternatives>
    <tex-math><![CDATA[y^*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>y</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
    is the true optimal objective value and
    <inline-formula><alternatives>
    <tex-math><![CDATA[\hat{y}^*]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mover><mml:mi>y</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>*</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
    is the current estimated best optimal objective value. All methods
    are run sequentially (no parallelization features enabled for
    consistent evaluation) with default arguments. For DeepHyper, the
    <monospace>CBO</monospace> search is used. For Optuna, the
    <monospace>TPE</monospace> sampler with default arguments is used.
    For SMAC, the default parameters using the OptunaHub SMAC sampler is
    used.</p>
    <fig>
      <caption><p>Ackley 5D (left) and Branin 2D (right)</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="ackley_5d_and_branin_2d.png" />
    </fig>
    <fig>
      <caption><p>Griewank 5D (left) and Hartmann 6D
      (right)</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="griewank_5d_and_hartmann_6d.png" />
    </fig>
    <fig>
      <caption><p>Levy 5D (left) and Michalewicz 5D
      (right)</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="levy_5d_and_michal_5d.png" />
    </fig>
    <fig>
      <caption><p>Rosen 5D (left) and Schwefel 5D (right)</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="rosen_5d_and_schwefel_5d.png" />
    </fig>
    <fig>
      <caption><p>Shekel 5D</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="shekel_5d.png" />
    </fig>
  </sec>
</sec>
<sec id="black-box-optimization-methods">
  <title>Black-box Optimization Methods</title>
  <p>The algorithms used for hyperparameter optimization are black-box
  optimization algorithms for mixed search spaces. A mixed search space,
  can be composed of real, discrete, or categorical (nominal or ordinal)
  values. The search space can also include constraints (e.g., explicit
  such as <inline-formula><alternatives>
  <tex-math><![CDATA[x_0 < x_1]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
  or implicit such as “unexpected out-of-memory error”). The objective
  function <inline-formula><alternatives>
  <tex-math><![CDATA[Y = f(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  can be stochastic where <inline-formula><alternatives>
  <tex-math><![CDATA[Y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
  is a random variable, <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
  is a vector of input hyperparameters, and
  <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  is the objective function. The <monospace>DeepHyper</monospace>, main
  hyperparameter optimization algorithm, is based on Bayesian
  optimization.</p>
  <p>The Bayesian optimization of <monospace>DeepHyper</monospace>
  relies on Extremely Randomized Forest as surrogate model to estimate
  <inline-formula><alternatives>
  <tex-math><![CDATA[E_Y[Y|X=x]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>Y</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  by default. Extremely Randomized Forests
  (<xref alt="Geurts et al., 2006" rid="ref-geurts2006extremely" ref-type="bibr">Geurts
  et al., 2006</xref>) are a type of Randomized Forest algorithms in
  which the split decision involves a random process for each newly
  created node of a tree. It provides smoother epistemic uncertainty
  estimates with an increasing number of trees (left side in Figure 8)
  compared to usual Random Forests that use a deterministic “best” split
  decision (right side in Figure 8).</p>
  <fig>
    <caption><p>Uncertainty of Randomized Forests, on the left-side with
    random split, and on the right-side with best split.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="random_forest.png" />
  </fig>
  <p>Then, a custom acquisition function <inline-formula><alternatives>
  <tex-math><![CDATA[\text{UCBd}(x) = \mu(x) + \kappa \cdot \sigma_\text{ep}(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">UCBd</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:mo>⋅</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mtext mathvariant="normal">ep</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  combines the mean predicition <inline-formula><alternatives>
  <tex-math><![CDATA[\mu(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  with the epistemic uncertainty <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma_\text{ep}(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mtext mathvariant="normal">ep</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  of this surrogate (purple area in Figure 8) for improved efficiency.
  We also apply a periodic exponential decay (Figure 9), impacting the
  exploration-exploitation parameter <inline-formula><alternatives>
  <tex-math><![CDATA[\kappa]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>κ</mml:mi></mml:math></alternatives></inline-formula>
  to escape local solutions
  (<xref alt="Egelé et al., 2023" rid="ref-egele2023asynchronous" ref-type="bibr">Egelé
  et al., 2023</xref>).</p>
  <fig>
    <caption><p>Periodic Exponential Decay for Bayesian
    Optimization</p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="example-exp-decay.jpg" />
  </fig>
  <p>Batch parallel genetic algorithms are provided to resolve
  efficiently the sub-problem of optimizing the acquisition function and
  it is also more accurate than Monte-Carlo approaches to find the
  optimum of the acquisition function. A cheap and efficient multi-point
  acquisition strategy <monospace>qUCBd</monospace> is provided for
  better parallel scalability
  (<xref alt="Egelé et al., 2023" rid="ref-egele2023asynchronous" ref-type="bibr">Egelé
  et al., 2023</xref>).</p>
  <p>The multi-objective optimization is enabled by scalarization
  functions and objective rescaling
  (<xref alt="Égelé et al., 2023" rid="ref-egele2023dmobo" ref-type="bibr">Égelé
  et al., 2023</xref>).</p>
  <p>The early-discarding strategies include asynchronous successive
  halving and a robust learning curve extrapolation
  (<xref alt="Egele et al., 2024" rid="ref-egele2024unreasonable" ref-type="bibr">Egele
  et al., 2024</xref>).</p>
  <p>The ensemble strategies is modular to allow: exploring models
  tested during hyperparameter optimization, classification and
  regression problems to be treated, disentangled uncertainty
  quantification
  (<xref alt="Égelé et al., 2022" rid="ref-egele2022autodeuq" ref-type="bibr">Égelé
  et al., 2022</xref>). It can leverage the same parallelization
  features as the optimization.</p>
  <p>Finally, a transfer-learning strategy for hyperparameter
  optimization
  (<xref alt="Dorier et al., 2022" rid="ref-dorier2022transferlearning" ref-type="bibr">Dorier
  et al., 2022</xref>) is available. This strategy used to be based on
  variational auto-encoders for tabular data. It is now based on the
  Gaussian mixture-model for tabular data.</p>
</sec>
<sec id="software-development">
  <title>Software Development</title>
  <p>The <monospace>DeepHyper</monospace> package adheres to best
  practices in the Python community by following the
  <ext-link ext-link-type="uri" xlink:href="https://pep8.org">PEP 8
  style guide</ext-link> for Python naming and formatting conventions.
  The layout and configuration of the package follows suggestions made
  by the
  <ext-link ext-link-type="uri" xlink:href="https://packaging.python.org">Python
  Packaging User Guide</ext-link> which is maintained by the Python
  Packaging Authority. A <monospace>pyproject.toml</monospace> file
  provides all configuration settings and meta data for developing and
  publishing the <monospace>DeepHyper</monospace> package. The
  <ext-link ext-link-type="uri" xlink:href="https://astral.sh/ruff">ruff
  tool</ext-link> is used to enforce style and format conventions during
  code development and during continuous integration checks via GitHub
  Actions. Unit tests are also conducted in the CI workflow with the
  <ext-link ext-link-type="uri" xlink:href="https://docs.pytest.org">pytest</ext-link>
  framework. Conda environments and standard Python virtual environments
  are supported by the project to ensure consistent development
  environments. Documentation for the package is generated with the
  <ext-link ext-link-type="uri" xlink:href="https://www.sphinx-doc.org/en/master/">Sphinx
  application</ext-link>. See the Developer’s Guide section in the
  DeepHyper
  <ext-link ext-link-type="uri" xlink:href="https://deephyper.readthedocs.io">documentation</ext-link>
  for contributing guidelines and more software development
  information.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from Misha Salim, Romit Maulik, Venkat
  Vishwanath, Stefan Wild, Joceran Gouneau, Dipendra Jha, Kyle Felker,
  Matthieu Dorier, Felix Perez, Bethany Lush, Gavin M. Wiggins, Tyler H.
  Chang, Yixuan Sun, Shengli Jiang, rmjcs2020, Albert Lam, Taylor
  Childers, Z223I, Zachariah Carmichael, Hongyuan Liu, Sam Foreman,
  Akalanka Galappaththi, Brett Eiffert, Sun Haozhe, Sandeep Madireddy,
  Adrian Perez Dieguez, and Nesar Ramachandra. We also would like to
  thank Prof. Isabelle Guyon for her guidance on the machine learning
  methodologies developed in this software.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-balaprakash2018deephyper">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name>
        <name><surname>Salim</surname><given-names>Michael</given-names></name>
        <name><surname>Uram</surname><given-names>Thomas D.</given-names></name>
        <name><surname>Vishwanath</surname><given-names>Venkat</given-names></name>
        <name><surname>Wild</surname><given-names>Stefan M.</given-names></name>
      </person-group>
      <article-title>DeepHyper: Asynchronous hyperparameter search for deep neural networks</article-title>
      <source>IEEE 25th international conference on high performance computing (HiPC)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1109/HiPC.2018.00014</pub-id>
      <fpage>42</fpage>
      <lpage>51</lpage>
    </element-citation>
  </ref>
  <ref id="ref-egele2022autodeuq">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Égelé</surname><given-names>Romain</given-names></name>
        <name><surname>Maulik</surname><given-names>Romit</given-names></name>
        <name><surname>Raghavan</surname><given-names>Krishnan</given-names></name>
        <name><surname>Lusch</surname><given-names>Bethany</given-names></name>
        <name><surname>Guyon</surname><given-names>Isabelle</given-names></name>
        <name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name>
      </person-group>
      <article-title>AutoDEUQ: Automated deep ensemble with uncertainty quantification</article-title>
      <source>26th international conference on pattern recognition (ICPR)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/ICPR56361.2022.9956231</pub-id>
      <fpage>1908</fpage>
      <lpage>1914</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dorier2022transferlearning">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dorier</surname><given-names>M.</given-names></name>
        <name><surname>Égelé</surname><given-names>R.</given-names></name>
        <name><surname>Balaprakash</surname><given-names>P.</given-names></name>
        <name><surname>Koo</surname><given-names>J.</given-names></name>
        <name><surname>Madireddy</surname><given-names>S.</given-names></name>
        <name><surname>Ramesh</surname><given-names>S.</given-names></name>
        <name><surname>Malony</surname><given-names>A. D.</given-names></name>
        <name><surname>Ross</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>HPC storage service autotuning using variational- autoencoder -guided asynchronous Bayesian optimization</article-title>
      <source>2022 IEEE international conference on cluster computing (CLUSTER)</source>
      <publisher-name>IEEE Computer Society</publisher-name>
      <publisher-loc>Los Alamitos, CA, USA</publisher-loc>
      <year iso-8601-date="2022-09">2022</year><month>09</month>
      <volume></volume>
      <uri>https://doi.ieeecomputersociety.org/10.1109/CLUSTER51413.2022.00049</uri>
      <pub-id pub-id-type="doi">10.1109/CLUSTER51413.2022.00049</pub-id>
      <fpage>381</fpage>
      <lpage>393</lpage>
    </element-citation>
  </ref>
  <ref id="ref-egele2023asynchronous">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Egelé</surname><given-names>Romain</given-names></name>
        <name><surname>Guyon</surname><given-names>Isabelle</given-names></name>
        <name><surname>Vishwanath</surname><given-names>Venkatram</given-names></name>
        <name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name>
      </person-group>
      <article-title>Asynchronous decentralized Bayesian optimization for large scale hyperparameter optimization</article-title>
      <source>2023 IEEE 19th international conference on e-science (e-science)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/e-Science58273.2023.10254839</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-egele2023dmobo">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Égelé</surname><given-names>Romain</given-names></name>
        <name><surname>Chang</surname><given-names>Tyler</given-names></name>
        <name><surname>Sun</surname><given-names>Yixuan</given-names></name>
        <name><surname>Vishwanath</surname><given-names>Venkatram</given-names></name>
        <name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name>
      </person-group>
      <article-title>Parallel multi-objective hyperparameter optimization with uniform normalization and bounded objectives</article-title>
      <source>arXiv preprint arXiv:2309.14936</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2309.14936</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-egele2024unreasonable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Egele</surname><given-names>Romain</given-names></name>
        <name><surname>Mohr</surname><given-names>Felix</given-names></name>
        <name><surname>Viering</surname><given-names>Tom</given-names></name>
        <name><surname>Balaprakash</surname><given-names>Prasanna</given-names></name>
      </person-group>
      <article-title>The unreasonable effectiveness of early discarding after one epoch in neural network hyperparameter optimization</article-title>
      <source>Neurocomputing</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1016/j.neucom.2024.127964</pub-id>
      <fpage>127964</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-geurts2006extremely">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Geurts</surname><given-names>Pierre</given-names></name>
        <name><surname>Ernst</surname><given-names>Damien</given-names></name>
        <name><surname>Zehenkel</surname><given-names>Louis</given-names></name>
      </person-group>
      <article-title>Extremely randomized trees</article-title>
      <source>Machine Learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2006">2006</year>
      <pub-id pub-id-type="doi">10.1007/s10994-006-6226-1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hypermapper2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nardi</surname><given-names>Luigi</given-names></name>
        <name><surname>Souza</surname><given-names>Artur</given-names></name>
        <name><surname>Koeplinger</surname><given-names>David</given-names></name>
        <name><surname>Olukotun</surname><given-names>Kunle</given-names></name>
      </person-group>
      <article-title>HyperMapper: A practical design space exploration framework</article-title>
      <source>2019 IEEE 27th international symposium on modeling, analysis, and simulation of computer and telecommunication systems (MASCOTS)</source>
      <year iso-8601-date="2019">2019</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/MASCOTS.2019.00053</pub-id>
      <fpage>425</fpage>
      <lpage>426</lpage>
    </element-citation>
  </ref>
  <ref id="ref-optuna2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Akiba</surname><given-names>Takuya</given-names></name>
        <name><surname>Sano</surname><given-names>Shotaro</given-names></name>
        <name><surname>Yanase</surname><given-names>Toshihiko</given-names></name>
        <name><surname>Ohta</surname><given-names>Takeru</given-names></name>
        <name><surname>Koyama</surname><given-names>Masanori</given-names></name>
      </person-group>
      <article-title>Optuna: A next-generation hyperparameter optimization framework</article-title>
      <source>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <isbn>9781450362016</isbn>
      <uri>https://doi.org/10.1145/3292500.3330701</uri>
      <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id>
      <fpage>2623</fpage>
      <lpage>2631</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hyperopt2013">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bergstra</surname><given-names>J.</given-names></name>
        <name><surname>Yamins</surname><given-names>D.</given-names></name>
        <name><surname>Cox</surname><given-names>D. D.</given-names></name>
      </person-group>
      <article-title>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</article-title>
      <source>Proceedings of the 30th international conference on international conference on machine learning - volume 28</source>
      <publisher-name>JMLR.org</publisher-name>
      <publisher-loc>Atlanta, GA, USA</publisher-loc>
      <year iso-8601-date="2013">2013</year>
      <uri>https://dl.acm.org/doi/10.5555/3042817.3042832</uri>
      <fpage>I</fpage>
      <lpage>115-I-123</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hyperopt2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bergstra</surname><given-names>James</given-names></name>
        <name><surname>Komer</surname><given-names>Brent</given-names></name>
        <name><surname>Eliasmith</surname><given-names>Chris</given-names></name>
        <name><surname>Yamins</surname><given-names>Dan</given-names></name>
        <name><surname>Cox</surname><given-names>David D</given-names></name>
      </person-group>
      <article-title>Hyperopt: A Python library for model selection and hyperparameter optimization</article-title>
      <source>Computational Science &amp; Discovery</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>8</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1088/1749-4699/8/1/014008</pub-id>
      <fpage>014008</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-botorch2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Balandat</surname><given-names>Maximilian</given-names></name>
        <name><surname>Karrer</surname><given-names>Brian</given-names></name>
        <name><surname>Jiang</surname><given-names>Daniel R.</given-names></name>
        <name><surname>Daulton</surname><given-names>Samuel</given-names></name>
        <name><surname>Letham</surname><given-names>Benjamin</given-names></name>
        <name><surname>Wilson</surname><given-names>Andrew Gordon</given-names></name>
        <name><surname>Bakshy</surname><given-names>Eytan</given-names></name>
      </person-group>
      <article-title>BOTORCH: A framework for efficient monte-carlo Bayesian optimization</article-title>
      <source>Proceedings of the 34th international conference on neural information processing systems</source>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <publisher-loc>Red Hook, NY, USA</publisher-loc>
      <year iso-8601-date="2020">2020</year>
      <isbn>9781713829546</isbn>
      <uri>https://dl.acm.org/doi/10.5555/3495724.3497531</uri>
    </element-citation>
  </ref>
  <ref id="ref-openbox2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jiang</surname><given-names>Huaijun</given-names></name>
        <name><surname>Shen</surname><given-names>Yu</given-names></name>
        <name><surname>Li</surname><given-names>Yang</given-names></name>
        <name><surname>Xu</surname><given-names>Beicheng</given-names></name>
        <name><surname>Du</surname><given-names>Sixian</given-names></name>
        <name><surname>Zhang</surname><given-names>Wentao</given-names></name>
        <name><surname>Zhang</surname><given-names>Ce</given-names></name>
        <name><surname>Cui</surname><given-names>Bin</given-names></name>
      </person-group>
      <article-title>OpenBox: A Python toolkit for generalized black-box optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2024">2024</year>
      <volume>25</volume>
      <issue>120</issue>
      <uri>https://dl.acm.org/doi/10.5555/3722577.3722697</uri>
      <fpage>1</fpage>
      <lpage>11</lpage>
    </element-citation>
  </ref>
  <ref id="ref-smac32022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lindauer</surname><given-names>Marius</given-names></name>
        <name><surname>Eggensperger</surname><given-names>Katharina</given-names></name>
        <name><surname>Feurer</surname><given-names>Matthias</given-names></name>
        <name><surname>Biedenkapp</surname><given-names>AndrÃ©</given-names></name>
        <name><surname>Deng</surname><given-names>Difan</given-names></name>
        <name><surname>Benjamins</surname><given-names>Carolin</given-names></name>
        <name><surname>Ruhkopf</surname><given-names>Tim</given-names></name>
        <name><surname>Sass</surname><given-names>RenÃ©</given-names></name>
        <name><surname>Hutter</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>SMAC3: A versatile Bayesian optimization package for hyperparameter optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>54</issue>
      <uri>https://dl.acm.org/doi/10.5555/3586589.3586643</uri>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
