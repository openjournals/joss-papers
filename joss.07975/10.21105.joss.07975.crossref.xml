<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250512131544-5c5a020295bd8db0a68fe5760b80706f15fc24da</doi_batch_id>
    <timestamp>20250512131544</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>05</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>109</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>DeepHyper: A Python Package for Massively Parallel Hyperparameter Optimization in Machine Learning</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Romain</given_name>
            <surname>Egele</surname>
            <affiliations>
              <institution><institution_name>Oak Ridge National Laboratory, Oak Ridge, TN, United States</institution_name><institution_id type="ror">https://ror.org/01qz5mb56</institution_id></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-8992-8192</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Prasanna</given_name>
            <surname>Balaprakash</surname>
            <affiliations>
              <institution><institution_name>Oak Ridge National Laboratory, Oak Ridge, TN, United States</institution_name><institution_id type="ror">https://ror.org/01qz5mb56</institution_id></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-0292-5715</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Gavin M.</given_name>
            <surname>Wiggins</surname>
            <affiliations>
              <institution><institution_name>Oak Ridge National Laboratory, Oak Ridge, TN, United States</institution_name><institution_id type="ror">https://ror.org/01qz5mb56</institution_id></institution>
            </affiliations>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Brett</given_name>
            <surname>Eiffert</surname>
            <affiliations>
              <institution><institution_name>Oak Ridge National Laboratory, Oak Ridge, TN, United States</institution_name><institution_id type="ror">https://ror.org/01qz5mb56</institution_id></institution>
            </affiliations>
          </person_name>
        </contributors>
        <publication_date>
          <month>05</month>
          <day>12</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>7975</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.07975</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.15387374</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/7975</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.07975</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.07975</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.07975.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="balaprakash2018deephyper">
            <article_title>DeepHyper: Asynchronous hyperparameter search for deep neural networks</article_title>
            <author>Balaprakash</author>
            <journal_title>IEEE 25th international conference on high performance computing (HiPC)</journal_title>
            <doi>10.1109/HiPC.2018.00014</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Balaprakash, P., Salim, M., Uram, T. D., Vishwanath, V., &amp; Wild, S. M. (2018). DeepHyper: Asynchronous hyperparameter search for deep neural networks. IEEE 25th International Conference on High Performance Computing (HiPC), 42–51. https://doi.org/10.1109/HiPC.2018.00014</unstructured_citation>
          </citation>
          <citation key="egele2022autodeuq">
            <article_title>AutoDEUQ: Automated deep ensemble with uncertainty quantification</article_title>
            <author>Égelé</author>
            <journal_title>26th international conference on pattern recognition (ICPR)</journal_title>
            <doi>10.1109/ICPR56361.2022.9956231</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Égelé, R., Maulik, R., Raghavan, K., Lusch, B., Guyon, I., &amp; Balaprakash, P. (2022). AutoDEUQ: Automated deep ensemble with uncertainty quantification. 26th International Conference on Pattern Recognition (ICPR), 1908–1914. https://doi.org/10.1109/ICPR56361.2022.9956231</unstructured_citation>
          </citation>
          <citation key="dorier2022transferlearning">
            <article_title>HPC storage service autotuning using variational- autoencoder -guided asynchronous Bayesian optimization</article_title>
            <author>Dorier</author>
            <journal_title>2022 IEEE international conference on cluster computing (CLUSTER)</journal_title>
            <doi>10.1109/CLUSTER51413.2022.00049</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Dorier, M., Égelé, R., Balaprakash, P., Koo, J., Madireddy, S., Ramesh, S., Malony, A. D., &amp; Ross, R. (2022). HPC storage service autotuning using variational- autoencoder -guided asynchronous Bayesian optimization. 2022 IEEE International Conference on Cluster Computing (CLUSTER), 381–393. https://doi.org/10.1109/CLUSTER51413.2022.00049</unstructured_citation>
          </citation>
          <citation key="egele2023asynchronous">
            <article_title>Asynchronous decentralized Bayesian optimization for large scale hyperparameter optimization</article_title>
            <author>Egelé</author>
            <journal_title>2023 IEEE 19th international conference on e-science (e-science)</journal_title>
            <doi>10.1109/e-Science58273.2023.10254839</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Egelé, R., Guyon, I., Vishwanath, V., &amp; Balaprakash, P. (2023). Asynchronous decentralized Bayesian optimization for large scale hyperparameter optimization. 2023 IEEE 19th International Conference on e-Science (e-Science), 1–10. https://doi.org/10.1109/e-Science58273.2023.10254839</unstructured_citation>
          </citation>
          <citation key="egele2023dmobo">
            <article_title>Parallel multi-objective hyperparameter optimization with uniform normalization and bounded objectives</article_title>
            <author>Égelé</author>
            <journal_title>arXiv preprint arXiv:2309.14936</journal_title>
            <doi>10.48550/arXiv.2309.14936</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Égelé, R., Chang, T., Sun, Y., Vishwanath, V., &amp; Balaprakash, P. (2023). Parallel multi-objective hyperparameter optimization with uniform normalization and bounded objectives. arXiv Preprint arXiv:2309.14936. https://doi.org/10.48550/arXiv.2309.14936</unstructured_citation>
          </citation>
          <citation key="egele2024unreasonable">
            <article_title>The unreasonable effectiveness of early discarding after one epoch in neural network hyperparameter optimization</article_title>
            <author>Egele</author>
            <journal_title>Neurocomputing</journal_title>
            <doi>10.1016/j.neucom.2024.127964</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Egele, R., Mohr, F., Viering, T., &amp; Balaprakash, P. (2024). The unreasonable effectiveness of early discarding after one epoch in neural network hyperparameter optimization. Neurocomputing, 127964. https://doi.org/10.1016/j.neucom.2024.127964</unstructured_citation>
          </citation>
          <citation key="geurts2006extremely">
            <article_title>Extremely randomized trees</article_title>
            <author>Geurts</author>
            <journal_title>Machine Learning</journal_title>
            <doi>10.1007/s10994-006-6226-1</doi>
            <cYear>2006</cYear>
            <unstructured_citation>Geurts, P., Ernst, D., &amp; Zehenkel, L. (2006). Extremely randomized trees. Machine Learning. https://doi.org/10.1007/s10994-006-6226-1</unstructured_citation>
          </citation>
          <citation key="hypermapper2019">
            <article_title>HyperMapper: A practical design space exploration framework</article_title>
            <author>Nardi</author>
            <journal_title>2019 IEEE 27th international symposium on modeling, analysis, and simulation of computer and telecommunication systems (MASCOTS)</journal_title>
            <doi>10.1109/MASCOTS.2019.00053</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Nardi, L., Souza, A., Koeplinger, D., &amp; Olukotun, K. (2019). HyperMapper: A practical design space exploration framework. 2019 IEEE 27th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS), 425–426. https://doi.org/10.1109/MASCOTS.2019.00053</unstructured_citation>
          </citation>
          <citation key="optuna2019">
            <article_title>Optuna: A next-generation hyperparameter optimization framework</article_title>
            <author>Akiba</author>
            <journal_title>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</journal_title>
            <doi>10.1145/3292500.3330701</doi>
            <isbn>9781450362016</isbn>
            <cYear>2019</cYear>
            <unstructured_citation>Akiba, T., Sano, S., Yanase, T., Ohta, T., &amp; Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework. Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, 2623–2631. https://doi.org/10.1145/3292500.3330701</unstructured_citation>
          </citation>
          <citation key="hyperopt2013">
            <article_title>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</article_title>
            <author>Bergstra</author>
            <journal_title>Proceedings of the 30th international conference on international conference on machine learning - volume 28</journal_title>
            <cYear>2013</cYear>
            <unstructured_citation>Bergstra, J., Yamins, D., &amp; Cox, D. D. (2013). Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures. Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, I-115-I-123. https://dl.acm.org/doi/10.5555/3042817.3042832</unstructured_citation>
          </citation>
          <citation key="hyperopt2015">
            <article_title>Hyperopt: A Python library for model selection and hyperparameter optimization</article_title>
            <author>Bergstra</author>
            <journal_title>Computational Science &amp; Discovery</journal_title>
            <issue>1</issue>
            <volume>8</volume>
            <doi>10.1088/1749-4699/8/1/014008</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Bergstra, J., Komer, B., Eliasmith, C., Yamins, D., &amp; Cox, D. D. (2015). Hyperopt: A Python library for model selection and hyperparameter optimization. Computational Science &amp; Discovery, 8(1), 014008. https://doi.org/10.1088/1749-4699/8/1/014008</unstructured_citation>
          </citation>
          <citation key="botorch2020">
            <article_title>BOTORCH: A framework for efficient monte-carlo Bayesian optimization</article_title>
            <author>Balandat</author>
            <journal_title>Proceedings of the 34th international conference on neural information processing systems</journal_title>
            <isbn>9781713829546</isbn>
            <cYear>2020</cYear>
            <unstructured_citation>Balandat, M., Karrer, B., Jiang, D. R., Daulton, S., Letham, B., Wilson, A. G., &amp; Bakshy, E. (2020). BOTORCH: A framework for efficient monte-carlo Bayesian optimization. Proceedings of the 34th International Conference on Neural Information Processing Systems. ISBN: 9781713829546</unstructured_citation>
          </citation>
          <citation key="openbox2024">
            <article_title>OpenBox: A Python toolkit for generalized black-box optimization</article_title>
            <author>Jiang</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <issue>120</issue>
            <volume>25</volume>
            <cYear>2024</cYear>
            <unstructured_citation>Jiang, H., Shen, Y., Li, Y., Xu, B., Du, S., Zhang, W., Zhang, C., &amp; Cui, B. (2024). OpenBox: A Python toolkit for generalized black-box optimization. Journal of Machine Learning Research, 25(120), 1–11. https://dl.acm.org/doi/10.5555/3722577.3722697</unstructured_citation>
          </citation>
          <citation key="smac32022">
            <article_title>SMAC3: A versatile Bayesian optimization package for hyperparameter optimization</article_title>
            <author>Lindauer</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <issue>54</issue>
            <volume>23</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Lindauer, M., Eggensperger, K., Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass, R., &amp; Hutter, F. (2022). SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. Journal of Machine Learning Research, 23(54), 1–9. https://dl.acm.org/doi/10.5555/3586589.3586643</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
