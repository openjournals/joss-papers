<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8869</article-id>
<article-id pub-id-type="doi">10.21105/joss.08869</article-id>
<title-group>
<article-title>BONSAI: A framework for processing and analysing
Electronic Health Records (EHR) data using transformer-based
models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-3809-0513</contrib-id>
<name>
<surname>Montgomery</surname>
<given-names>Maria Elkjær</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8779-0209</contrib-id>
<name>
<surname>Klein</surname>
<given-names>Kiril</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9072-0874</contrib-id>
<name>
<surname>Odgaard</surname>
<given-names>Mikkel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6701-3752</contrib-id>
<name>
<surname>Lorenzen</surname>
<given-names>Stephan Sloth</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sobhaninia</surname>
<given-names>Zahra</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Copenhagen, Denmark</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-06-23">
<day>23</day>
<month>6</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>114</issue>
<fpage>8869</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>EHR</kwd>
<kwd>BERT</kwd>
<kwd>healthcare</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>BONSAI is an end-to-end Python framework for processing and
  analysing Electronic Health Records (EHR) data. It extends the model
  described in the CORE-BEHRT paper
  (<xref alt="Odgaard et al., 2024" rid="ref-odgaard2024core" ref-type="bibr">Odgaard
  et al., 2024</xref>) and is designed to streamline data preparation,
  model pre-training, and fine-tuning for patient-level prediction
  tasks. The framework is built for efficient large-scale processing of
  EHR data, making it suitable for clinical applications involving
  substantial volumes of patient records. This enables models for
  patient outcome prediction and supports scalable clinical
  research.</p>
  <p>The framework accepts EHR data in the MEDS format
  (<xref alt="Kolo et al., 2024" rid="ref-kolo2024meds" ref-type="bibr">Kolo
  et al., 2024</xref>), performs comprehensive preprocessing, and
  prepares the data for BERT-based modeling, following the structure of
  the BEHRT model
  (<xref alt="Li et al., 2020" rid="ref-li2020behrt" ref-type="bibr">Li
  et al., 2020</xref>). This includes converting raw EHR data into
  tokenised inputs by mapping vocabulary to numerical tokens and
  aligning patient histories into temporally ordered sequences of
  medical concepts. Each concept is paired with temporal features such
  as visit timestamps (positions), patient age, and visit-level segment
  encodings. Additional static features, including date of birth (DOB),
  gender, and optionally date of death (DOD), are prepended to each
  sequence. Separator and classification tokens can be optionally
  included. Numeric values can be binned and added as categorical
  tokens. Aggregation of similar concepts is supported via regex-based
  grouping, and the removal of specific concepts is also included via
  regex.</p>
  <p>Modeling is performed using a ModernBERT backbone
  (<xref alt="Warner et al., 2024" rid="ref-warner2024smarter" ref-type="bibr">Warner
  et al., 2024</xref>) from the Hugging Face library. Model
  configurations are specified via a YAML config file, with defaults
  provided. Pre-training uses a masked language modeling (MLM) objective
  with Cross-Entropy loss. Fine-tuning is performed as a binary
  classification task using outcome-specific labels derived from the
  input sequences. Cohort definition and censoring can be done in two
  ways: either by including all data with post-hoc censoring based on
  outcome dates and a user-defined window, or via a simulated
  prospective approach with a fixed cutoff date. The fine-tuning head
  consists of a BiGRU layer that encodes patient sequences into a single
  embedding, which is passed to a linear classification layer trained
  using binary cross-entropy loss and the AdamW optimizer. Optional
  features include the use of a learning rate scheduler and the ability
  to freeze pre-trained layers during fine-tuning. Finally, the pipeline
  also includes an evaluation script that outputs model predictions and,
  optionally, intermediate model embeddings.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The growing adoption of foundation models in Natural Language
  Processing (NLP)
  (<xref alt="Brown et al., 2020" rid="ref-brown2020language" ref-type="bibr">Brown
  et al., 2020</xref>;
  <xref alt="Devlin et al., 2019" rid="ref-devlin2019bert" ref-type="bibr">Devlin
  et al., 2019</xref>;
  <xref alt="Touvron et al., 2023" rid="ref-touvron2023llama" ref-type="bibr">Touvron
  et al., 2023</xref>), coupled with the increasing availability of EHR
  data, has led to a surge in adapting such models to the clinical
  domain
  (<xref alt="Gu &amp; Dao, 2023" rid="ref-gu2023mamba" ref-type="bibr">Gu
  &amp; Dao, 2023</xref>;
  <xref alt="Li et al., 2020" rid="ref-li2020behrt" ref-type="bibr">Li
  et al., 2020</xref>;
  <xref alt="Odgaard et al., 2024" rid="ref-odgaard2024core" ref-type="bibr">Odgaard
  et al., 2024</xref>;
  <xref alt="Pang et al., 2021" rid="ref-pang2021cehr" ref-type="bibr">Pang
  et al., 2021</xref>,
  <xref alt="2024" rid="ref-pang2024cehr" ref-type="bibr">2024</xref>;
  <xref alt="Rasmy et al., 2021" rid="ref-rasmy2021med" ref-type="bibr">Rasmy
  et al., 2021</xref>). However, existing frameworks often differ
  significantly in model architecture, data representations, and
  preprocessing steps, making comparison and reproducibility
  challenging. Notably, recent efforts such as the CEHR Benchmark
  (<xref alt="Pang et al., 2024" rid="ref-pang2024cehr" ref-type="bibr">Pang
  et al., 2024</xref>) have begun to standardise evaluation protocols,
  underscoring the diversity in approaches across the field.</p>
  <p>BONSAI was developed to provide an end-to-end pipeline in a modular
  setup, enabling flexible experimentation with EHR modeling. Users can
  easily switch between data representations, sources, normalisation
  strategies, and fine-tuning heads. Although ModernBERT is the default
  backbone, the framework supports alternative architectures with
  minimal configuration changes. It also includes baseline models for
  comparison and supports deployment on Microsoft Azure, a platform
  commonly used for working with protected health data, making it
  practical for working with real-world clinical data.</p>
  <p>The framework is intended for machine learning researchers and
  clinical data scientists who are working with or interested in EHR
  data and wish to explore, benchmark, or develop transformer-based
  models in a scalable and reproducible manner. While BONSAI builds on
  the earlier CORE-BEHRT framework
  (<xref alt="Odgaard et al., 2024" rid="ref-odgaard2024core" ref-type="bibr">Odgaard
  et al., 2024</xref>), it introduces improvements in modularity and
  configurability, as well as support for more data sources (such as
  numerical input). In contrast to frameworks like Med-BERT
  (<xref alt="Rasmy et al., 2021" rid="ref-rasmy2021med" ref-type="bibr">Rasmy
  et al., 2021</xref>) or BEHRT
  (<xref alt="Li et al., 2020" rid="ref-li2020behrt" ref-type="bibr">Li
  et al., 2020</xref>), which have more rigid assumptions about data
  preprocessing and modeling setup, BONSAI offers a general-purpose,
  flexible design adaptable to a wide range of EHR tasks.</p>
</sec>
<sec id="figures">
  <title>Figures</title>
  <p><xref alt="[fig:pipeline]" rid="figU003Apipeline">[fig:pipeline]</xref>
  depicts the overall pipeline of BONSAI.</p>
  <fig>
    <caption><p>The BONSAI
    pipeline.<styled-content id="figU003Apipeline"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="../docs/BONSAI_pipeline.png" />
  </fig>
  <p>The figures below depict the censoring scheme for the data
  preprocessing, where
  <xref alt="[fig:censoring]" rid="figU003Acensoring">[fig:censoring]</xref>
  A shows the censoring scheme for the post-hoc setup, and
  <xref alt="[fig:censoring]" rid="figU003Acensoring">[fig:censoring]</xref>
  B shows the censoring scheme in a prospective setup.</p>
  <fig>
    <caption><p>Censoring in the post-hoc setup (A) and simulated
    prospective setup
    (B).<styled-content id="figU003Acensoring"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="../docs/BONSAI_censoring.png" />
  </fig>
</sec>
<sec id="software-dependencies">
  <title>Software dependencies</title>
  <p>The codebase is primarily built on PyTorch v2.5.1 for deep learning
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch_2019" ref-type="bibr">Paszke
  et al., 2019</xref>), with transformer architectures implemented using
  the Hugging Face Transformers library (v4.48.0 and above)
  (<xref alt="Hugging Face, 2025" rid="ref-huggingface_transformers" ref-type="bibr">Hugging
  Face, 2025</xref>). Additional support for Azure workflows is provided
  via azureml-mlflow
  (<xref alt="MLflow, 2025" rid="ref-mlflow" ref-type="bibr">MLflow,
  2025</xref>).</p>
  <p>Other key dependencies include:</p>
  <list list-type="bullet">
    <list-item>
      <p>numpy (≥2.1.3)</p>
    </list-item>
    <list-item>
      <p>pandas (≥2.2.3)</p>
    </list-item>
    <list-item>
      <p>pyarrow (≥18.0.0)</p>
    </list-item>
    <list-item>
      <p>python_dateutil (==2.9.0.post0)</p>
    </list-item>
    <list-item>
      <p>PyYAML (≥6.0.2)</p>
    </list-item>
    <list-item>
      <p>scikit-learn (≥1.5.2)</p>
    </list-item>
    <list-item>
      <p>setuptools (==75.6.0)</p>
    </list-item>
    <list-item>
      <p>tqdm (==4.67.1)</p>
    </list-item>
    <list-item>
      <p>xgboost (≥3.0.2)</p>
    </list-item>
  </list>
  <p>All dependencies are listed in the requirements.txt file.
  Installation instructions and environment setup are detailed in the
  main README.md.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Thanks to Mads Nielsen and Martin Sillesen for data access and
  supervision.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-odgaard2024core">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Odgaard</surname><given-names>Mikkel</given-names></name>
        <name><surname>Klein</surname><given-names>Kiril Vadimovic</given-names></name>
        <name><surname>Thysen</surname><given-names>Sanne Møller</given-names></name>
        <name><surname>Jimenez-Solem</surname><given-names>Espen</given-names></name>
        <name><surname>Sillesen</surname><given-names>Martin</given-names></name>
        <name><surname>Nielsen</surname><given-names>Mads</given-names></name>
      </person-group>
      <article-title>Core-BEHRT: A carefully optimized and rigorously evaluated BEHRT</article-title>
      <source>arXiv preprint arXiv:2404.15201</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2404.15201</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kolo2024meds">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kolo</surname><given-names>Aleksia</given-names></name>
        <name><surname>Pang</surname><given-names>Chao</given-names></name>
        <name><surname>Choi</surname><given-names>Edward</given-names></name>
        <name><surname>Steinberg</surname><given-names>Ethan</given-names></name>
        <name><surname>Jeong</surname><given-names>Hyewon</given-names></name>
        <name><surname>Gallifant</surname><given-names>Jack</given-names></name>
        <name><surname>Fries</surname><given-names>Jason A</given-names></name>
        <name><surname>Chiang</surname><given-names>Jeffrey N</given-names></name>
        <name><surname>Oh</surname><given-names>Jungwoo</given-names></name>
        <name><surname>Xu</surname><given-names>Justin</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>MEDS decentralized, extensible validation (MEDS-DEV) benchmark: Establishing reproducibility and comparability in ML for health</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://openreview.net/pdf?id=DExp3tRRel</uri>
    </element-citation>
  </ref>
  <ref id="ref-li2020behrt">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Yikuan</given-names></name>
        <name><surname>Rao</surname><given-names>Shishir</given-names></name>
        <name><surname>Solares</surname><given-names>José Roberto Ayala</given-names></name>
        <name><surname>Hassaine</surname><given-names>Abdelaali</given-names></name>
        <name><surname>Ramakrishnan</surname><given-names>Rema</given-names></name>
        <name><surname>Canoy</surname><given-names>Dexter</given-names></name>
        <name><surname>Zhu</surname><given-names>Yajie</given-names></name>
        <name><surname>Rahimi</surname><given-names>Kazem</given-names></name>
        <name><surname>Salimi-Khorshidi</surname><given-names>Gholamreza</given-names></name>
      </person-group>
      <article-title>BEHRT: Transformer for electronic health records</article-title>
      <source>Scientific reports</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>10</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-020-62922-y</pub-id>
      <fpage>7155</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-warner2024smarter">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Warner</surname><given-names>Benjamin</given-names></name>
        <name><surname>Chaffin</surname><given-names>Antoine</given-names></name>
        <name><surname>Clavié</surname><given-names>Benjamin</given-names></name>
        <name><surname>Weller</surname><given-names>Orion</given-names></name>
        <name><surname>Hallström</surname><given-names>Oskar</given-names></name>
        <name><surname>Taghadouini</surname><given-names>Said</given-names></name>
        <name><surname>Gallagher</surname><given-names>Alexis</given-names></name>
        <name><surname>Biswas</surname><given-names>Raja</given-names></name>
        <name><surname>Ladhak</surname><given-names>Faisal</given-names></name>
        <name><surname>Aarsen</surname><given-names>Tom</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference</article-title>
      <source>arXiv preprint arXiv:2412.13663</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2412.13663</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-touvron2023llama">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Touvron</surname><given-names>Hugo</given-names></name>
        <name><surname>Lavril</surname><given-names>Thibaut</given-names></name>
        <name><surname>Izacard</surname><given-names>Gautier</given-names></name>
        <name><surname>Martinet</surname><given-names>Xavier</given-names></name>
        <name><surname>Lachaux</surname><given-names>Marie-Anne</given-names></name>
        <name><surname>Lacroix</surname><given-names>Timothée</given-names></name>
        <name><surname>Rozière</surname><given-names>Baptiste</given-names></name>
        <name><surname>Goyal</surname><given-names>Naman</given-names></name>
        <name><surname>Hambro</surname><given-names>Eric</given-names></name>
        <name><surname>Azhar</surname><given-names>Faisal</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Llama: Open and efficient foundation language models</article-title>
      <source>arXiv preprint arXiv:2302.13971</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2302.13971</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-brown2020language">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Tom</given-names></name>
        <name><surname>Mann</surname><given-names>Benjamin</given-names></name>
        <name><surname>Ryder</surname><given-names>Nick</given-names></name>
        <name><surname>Subbiah</surname><given-names>Melanie</given-names></name>
        <name><surname>Kaplan</surname><given-names>Jared D</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
        <name><surname>Neelakantan</surname><given-names>Arvind</given-names></name>
        <name><surname>Shyam</surname><given-names>Pranav</given-names></name>
        <name><surname>Sastry</surname><given-names>Girish</given-names></name>
        <name><surname>Askell</surname><given-names>Amanda</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Language models are few-shot learners</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <pub-id pub-id-type="doi">10.48550/arXiv.2005.14165</pub-id>
      <fpage>1877</fpage>
      <lpage>1901</lpage>
    </element-citation>
  </ref>
  <ref id="ref-devlin2019bert">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Devlin</surname><given-names>Jacob</given-names></name>
        <name><surname>Chang</surname><given-names>Ming-Wei</given-names></name>
        <name><surname>Lee</surname><given-names>Kenton</given-names></name>
        <name><surname>Toutanova</surname><given-names>Kristina</given-names></name>
      </person-group>
      <article-title>Bert: Pre-training of deep bidirectional transformers for language understanding</article-title>
      <source>Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1810.04805</pub-id>
      <fpage>4171</fpage>
      <lpage>4186</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pang2024cehr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pang</surname><given-names>Chao</given-names></name>
        <name><surname>Jiang</surname><given-names>Xinzhuo</given-names></name>
        <name><surname>Pavinkurve</surname><given-names>Nishanth Parameshwar</given-names></name>
        <name><surname>Kalluri</surname><given-names>Krishna S</given-names></name>
        <name><surname>Minto</surname><given-names>Elise L</given-names></name>
        <name><surname>Patterson</surname><given-names>Jason</given-names></name>
        <name><surname>Zhang</surname><given-names>Linying</given-names></name>
        <name><surname>Hripcsak</surname><given-names>George</given-names></name>
        <name><surname>Gürsoy</surname><given-names>Gamze</given-names></name>
        <name><surname>Elhadad</surname><given-names>Noémie</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>CEHR-GPT: Generating electronic health records with chronological patient timelines</article-title>
      <source>arXiv preprint arXiv:2402.04400</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2402.04400</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pang2021cehr">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Pang</surname><given-names>Chao</given-names></name>
        <name><surname>Jiang</surname><given-names>Xinzhuo</given-names></name>
        <name><surname>Kalluri</surname><given-names>Krishna S</given-names></name>
        <name><surname>Spotnitz</surname><given-names>Matthew</given-names></name>
        <name><surname>Chen</surname><given-names>RuiJun</given-names></name>
        <name><surname>Perotte</surname><given-names>Adler</given-names></name>
        <name><surname>Natarajan</surname><given-names>Karthik</given-names></name>
      </person-group>
      <article-title>CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks</article-title>
      <source>Machine learning for health</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2111.08585</pub-id>
      <fpage>239</fpage>
      <lpage>260</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gu2023mamba">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gu</surname><given-names>Albert</given-names></name>
        <name><surname>Dao</surname><given-names>Tri</given-names></name>
      </person-group>
      <article-title>Mamba: Linear-time sequence modeling with selective state spaces</article-title>
      <source>arXiv preprint arXiv:2312.00752</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2312.00752</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-rasmy2021med">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rasmy</surname><given-names>Laila</given-names></name>
        <name><surname>Xiang</surname><given-names>Yang</given-names></name>
        <name><surname>Xie</surname><given-names>Ziqian</given-names></name>
        <name><surname>Tao</surname><given-names>Cui</given-names></name>
        <name><surname>Zhi</surname><given-names>Degui</given-names></name>
      </person-group>
      <article-title>Med-BERT: Pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</article-title>
      <source>NPJ digital medicine</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>4</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41746-021-00455-y</pub-id>
      <fpage>86</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-pytorch_2019">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huggingface_transformers">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <string-name>Hugging Face</string-name>
      </person-group>
      <article-title>Transformers</article-title>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-09-25">2025</year><month>09</month><day>25</day></date-in-citation>
      <uri>https://huggingface.co/docs/transformers/en/index</uri>
    </element-citation>
  </ref>
  <ref id="ref-mlflow">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <string-name>MLflow</string-name>
      </person-group>
      <article-title>Mlflow.azureml</article-title>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-09-25">2025</year><month>09</month><day>25</day></date-in-citation>
      <uri>https://mlflow.org/docs/1.22.0/python_api/mlflow.azureml.html</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
