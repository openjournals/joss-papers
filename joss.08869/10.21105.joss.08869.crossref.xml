<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20251020142039-04b29a0bcc570c540350a56eb0fd3a4d51f6e8c3</doi_batch_id>
    <timestamp>20251020142039</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>10</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>114</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>BONSAI: A framework for processing and analysing Electronic Health Records (EHR) data using transformer-based models</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Maria Elkjær</given_name>
            <surname>Montgomery</surname>
            <affiliations>
              <institution><institution_name>University of Copenhagen, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0000-3809-0513</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Kiril</given_name>
            <surname>Klein</surname>
            <affiliations>
              <institution><institution_name>University of Copenhagen, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-8779-0209</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Mikkel</given_name>
            <surname>Odgaard</surname>
            <affiliations>
              <institution><institution_name>University of Copenhagen, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-9072-0874</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Stephan Sloth</given_name>
            <surname>Lorenzen</surname>
            <affiliations>
              <institution><institution_name>University of Copenhagen, Denmark</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0001-6701-3752</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Zahra</given_name>
            <surname>Sobhaninia</surname>
            <affiliations>
              <institution><institution_name>University of Copenhagen, Denmark</institution_name></institution>
            </affiliations>
          </person_name>
        </contributors>
        <publication_date>
          <month>10</month>
          <day>20</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>8869</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.08869</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.17277882</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/8869</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.08869</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.08869</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.08869.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="odgaard2024core">
            <article_title>Core-BEHRT: A carefully optimized and rigorously evaluated BEHRT</article_title>
            <author>Odgaard</author>
            <journal_title>arXiv preprint arXiv:2404.15201</journal_title>
            <doi>10.48550/arXiv.2404.15201</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Odgaard, M., Klein, K. V., Thysen, S. M., Jimenez-Solem, E., Sillesen, M., &amp; Nielsen, M. (2024). Core-BEHRT: A carefully optimized and rigorously evaluated BEHRT. arXiv Preprint arXiv:2404.15201. https://doi.org/10.48550/arXiv.2404.15201</unstructured_citation>
          </citation>
          <citation key="kolo2024meds">
            <article_title>MEDS decentralized, extensible validation (MEDS-DEV) benchmark: Establishing reproducibility and comparability in ML for health</article_title>
            <author>Kolo</author>
            <cYear>2024</cYear>
            <unstructured_citation>Kolo, A., Pang, C., Choi, E., Steinberg, E., Jeong, H., Gallifant, J., Fries, J. A., Chiang, J. N., Oh, J., Xu, J., &amp; others. (2024). MEDS decentralized, extensible validation (MEDS-DEV) benchmark: Establishing reproducibility and comparability in ML for health. https://openreview.net/pdf?id=DExp3tRRel</unstructured_citation>
          </citation>
          <citation key="li2020behrt">
            <article_title>BEHRT: Transformer for electronic health records</article_title>
            <author>Li</author>
            <journal_title>Scientific reports</journal_title>
            <issue>1</issue>
            <volume>10</volume>
            <doi>10.1038/s41598-020-62922-y</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Li, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu, Y., Rahimi, K., &amp; Salimi-Khorshidi, G. (2020). BEHRT: Transformer for electronic health records. Scientific Reports, 10(1), 7155. https://doi.org/10.1038/s41598-020-62922-y</unstructured_citation>
          </citation>
          <citation key="warner2024smarter">
            <article_title>Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference</article_title>
            <author>Warner</author>
            <journal_title>arXiv preprint arXiv:2412.13663</journal_title>
            <doi>10.48550/arXiv.2412.13663</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Warner, B., Chaffin, A., Clavié, B., Weller, O., Hallström, O., Taghadouini, S., Gallagher, A., Biswas, R., Ladhak, F., Aarsen, T., &amp; others. (2024). Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv Preprint arXiv:2412.13663. https://doi.org/10.48550/arXiv.2412.13663</unstructured_citation>
          </citation>
          <citation key="touvron2023llama">
            <article_title>Llama: Open and efficient foundation language models</article_title>
            <author>Touvron</author>
            <journal_title>arXiv preprint arXiv:2302.13971</journal_title>
            <doi>10.48550/arXiv.2302.13971</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., &amp; others. (2023). Llama: Open and efficient foundation language models. arXiv Preprint arXiv:2302.13971. https://doi.org/10.48550/arXiv.2302.13971</unstructured_citation>
          </citation>
          <citation key="brown2020language">
            <article_title>Language models are few-shot learners</article_title>
            <author>Brown</author>
            <journal_title>Advances in neural information processing systems</journal_title>
            <volume>33</volume>
            <doi>10.48550/arXiv.2005.14165</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., &amp; others. (2020). Language models are few-shot learners. Advances in Neural Information Processing Systems, 33, 1877–1901. https://doi.org/10.48550/arXiv.2005.14165</unstructured_citation>
          </citation>
          <citation key="devlin2019bert">
            <article_title>Bert: Pre-training of deep bidirectional transformers for language understanding</article_title>
            <author>Devlin</author>
            <journal_title>Proceedings of the 2019 conference of the north american chapter of the association for computational linguistics: Human language technologies, volume 1 (long and short papers)</journal_title>
            <doi>10.48550/arXiv.1810.04805</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171–4186. https://doi.org/10.48550/arXiv.1810.04805</unstructured_citation>
          </citation>
          <citation key="pang2024cehr">
            <article_title>CEHR-GPT: Generating electronic health records with chronological patient timelines</article_title>
            <author>Pang</author>
            <journal_title>arXiv preprint arXiv:2402.04400</journal_title>
            <doi>10.48550/arXiv.2402.04400</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Pang, C., Jiang, X., Pavinkurve, N. P., Kalluri, K. S., Minto, E. L., Patterson, J., Zhang, L., Hripcsak, G., Gürsoy, G., Elhadad, N., &amp; others. (2024). CEHR-GPT: Generating electronic health records with chronological patient timelines. arXiv Preprint arXiv:2402.04400. https://doi.org/10.48550/arXiv.2402.04400</unstructured_citation>
          </citation>
          <citation key="pang2021cehr">
            <article_title>CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks</article_title>
            <author>Pang</author>
            <journal_title>Machine learning for health</journal_title>
            <doi>10.48550/arXiv.2111.08585</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Pang, C., Jiang, X., Kalluri, K. S., Spotnitz, M., Chen, R., Perotte, A., &amp; Natarajan, K. (2021). CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks. Machine Learning for Health, 239–260. https://doi.org/10.48550/arXiv.2111.08585</unstructured_citation>
          </citation>
          <citation key="gu2023mamba">
            <article_title>Mamba: Linear-time sequence modeling with selective state spaces</article_title>
            <author>Gu</author>
            <journal_title>arXiv preprint arXiv:2312.00752</journal_title>
            <doi>10.48550/arXiv.2312.00752</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Gu, A., &amp; Dao, T. (2023). Mamba: Linear-time sequence modeling with selective state spaces. arXiv Preprint arXiv:2312.00752. https://doi.org/10.48550/arXiv.2312.00752</unstructured_citation>
          </citation>
          <citation key="rasmy2021med">
            <article_title>Med-BERT: Pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction</article_title>
            <author>Rasmy</author>
            <journal_title>NPJ digital medicine</journal_title>
            <issue>1</issue>
            <volume>4</volume>
            <doi>10.1038/s41746-021-00455-y</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Rasmy, L., Xiang, Y., Xie, Z., Tao, C., &amp; Zhi, D. (2021). Med-BERT: Pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. NPJ Digital Medicine, 4(1), 86. https://doi.org/10.1038/s41746-021-00455-y</unstructured_citation>
          </citation>
          <citation key="pytorch_2019">
            <article_title>PyTorch: An imperative style, high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing systems 32</journal_title>
            <doi>10.48550/arXiv.1912.01703</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S. (2019). PyTorch: An imperative style, high-performance deep learning library. In Advances in neural information processing systems 32 (pp. 8024–8035). Curran Associates, Inc. https://doi.org/10.48550/arXiv.1912.01703</unstructured_citation>
          </citation>
          <citation key="huggingface_transformers">
            <article_title>Transformers</article_title>
            <author>Hugging Face</author>
            <cYear>2025</cYear>
            <unstructured_citation>Hugging Face. (2025). Transformers. https://huggingface.co/docs/transformers/en/index</unstructured_citation>
          </citation>
          <citation key="mlflow">
            <article_title>Mlflow.azureml</article_title>
            <author>MLflow</author>
            <cYear>2025</cYear>
            <unstructured_citation>MLflow. (2025). Mlflow.azureml. https://mlflow.org/docs/1.22.0/python_api/mlflow.azureml.html</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
