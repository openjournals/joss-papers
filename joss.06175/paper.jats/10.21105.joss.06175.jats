<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6175</article-id>
<article-id pub-id-type="doi">10.21105/joss.06175</article-id>
<title-group>
<article-title><monospace>Taweret</monospace>: a Python package for
Bayesian model mixing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5369-0493</contrib-id>
<name>
<surname>Ingles</surname>
<given-names>K.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Liyanage</surname>
<given-names>D.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2354-1523</contrib-id>
<name>
<surname>Semposki</surname>
<given-names>A. C.</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Yannotty</surname>
<given-names>J. C.</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Illinois Center for Advanced Study of the Universe &amp;
Department of Physics, University of Illinois Urbana-Champaign,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Physics, The Ohio State University,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Physics and Astronomy &amp; Institute of
Nuclear and Particle Physics, Ohio University, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Department of Statistics, The Ohio State Univeristy,
USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-10-30">
<day>30</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>97</issue>
<fpage>6175</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>bayesian statistics</kwd>
<kwd>heavy-ion collision</kwd>
<kwd>nuclear theory</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Uncertainty quantification using Bayesian methods is a growing area
  of research. Bayesian model mixing (BMM) is a recent development which
  combines the predictions from multiple models such that the fidelity
  of each model is preserved in the final result. Practical tools and
  analysis suites that facilitate such methods are therefore needed.
  <monospace>Taweret</monospace><xref ref-type="fn" rid="fn1">1</xref>
  introduces BMM to existing Bayesian uncertainty quantification
  efforts. Currently, <monospace>Taweret</monospace> contains three
  individual Bayesian model mixing techniques, each pertaining to a
  different type of problem structure; we encourage the future inclusion
  of user-developed mixing methods. <monospace>Taweret</monospace>‚Äôs
  first use case is in nuclear physics, but the package has been
  structured such that it should be adaptable to any research engaged in
  model comparison or model mixing.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In physics applications, multiple models with different physics
  assumptions can be used to describe an underlying system of interest.
  Though each model may have similar predictive accuracy on average, the
  fidelity of the approximation across a subset of the domain may differ
  drastically for each of the models under consideration. In such cases,
  inference and prediction based on a single model may be unreliable.
  One strategy for improving accuracy is to combine, or ‚Äúmix&quot;, the
  predictions from each model using a linear combination or weighted
  average with input-dependent weights. This approach is intended to
  improve reliability of inference and prediction and properly quantify
  model uncertainties. When operating under a Bayesian framework, this
  technique is referred to as Bayesian model mixing (BMM). In general,
  model mixing techniques are designed to combine the individual mean
  predictions or density estimates from the
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  models under consideration. For example, <italic>mean-mixing</italic>
  techniques predict the underlying system by
  <disp-formula><alternatives>
  <tex-math><![CDATA[E[Y~|~x] = \sum_{k = 1}^K w_k(x)\; f_k(x),]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="0.278em"></mml:mspace><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[E[Y~|~x]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>Y</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  denotes the mean of <inline-formula><alternatives>
  <tex-math><![CDATA[Y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
  given the vector of input parameters <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[f_k(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the mean prediction under the <inline-formula><alternatives>
  <tex-math><![CDATA[k^\mathrm{th}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  model <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{M}_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[w_k(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the corresponding weight function. The
  <italic>density-mixing</italic> approach estimates the underlying
  predictive density by <disp-formula><alternatives>
  <tex-math><![CDATA[p(Y_0 \mid x_0,Y) = \sum_{k = 1}^K w_k(x_0)\;p(Y_0 \mid x_0,Y, \mathcal{M}_k),]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>‚à£</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="0.278em"></mml:mspace><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>‚à£</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[p(Y_0 \mid x_0, Y, \mathcal{M}_k)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>‚à£</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  represents the predictive density of a future observation
  <inline-formula><alternatives>
  <tex-math><![CDATA[Y_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Y</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  with respect to the <inline-formula><alternatives>
  <tex-math><![CDATA[k^\mathrm{th}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  model <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{M}_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  at a new input <inline-formula><alternatives>
  <tex-math><![CDATA[x_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>.
  In either BMM setup, a key challenge is defining
  <inline-formula><alternatives>
  <tex-math><![CDATA[w_k(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>‚Äîthe
  functional relationship between the inputs and the weights.</p>
  <fig id="figU003Abmm_schematic">
    <caption><p>Schematic of Bayesian model mixing. Each model has
    region of parameter space where it has a high fidelity, but all
    models are meant to describe the same phenomenon. To obtain a model
    that works well for all of parameter space, we combine them using
    Bayesian model mixing methods.</p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="bmm_schematic.pdf" />
  </fig>
  <p>This work introduces <monospace>Taweret</monospace>, a Python
  package for Bayesian model mixing that includes three novel approaches
  for combining models, each of which defines the weight function in a
  distinct way (see Table¬†1 for a comparison of the methods). This
  package has been developed as an integral piece of the Bayesian
  Analysis of Nuclear Dynamics (BAND) collaboration‚Äôs software. BAND is
  a multi-institutional effort to build a cyber-infrastructure framework
  for use in the nuclear physics community
  (<xref alt="Beyer et al., 2023" rid="ref-bandframework" ref-type="bibr">Beyer
  et al., 2023</xref>;
  <xref alt="Phillips et al., 2021" rid="ref-PhillipsU003A2020dmw" ref-type="bibr">Phillips
  et al., 2021</xref>). The software is designed to lower the barrier
  for researchers to employ uncertainty quantification in their data
  analysis and/or theoretical modeling, and to integrate, as best as
  possible, with the community‚Äôs current standards concerning coding
  style (<monospace>pep8</monospace>). Bayesian model mixing is one of
  BAND‚Äôs four central pillars in this framework (the others being
  emulation, calibration, and experimental design).</p>
  <p>In addition to this need, we are aware of several other fields
  outside of physics that use techniques such as model stacking and
  Bayesian model averaging (BMA)
  (<xref alt="Fragoso et al., 2018" rid="ref-Fragoso2018" ref-type="bibr">Fragoso
  et al., 2018</xref>), e.g., statistics
  (<xref alt="Yao et al., 2018" rid="ref-Yao2018" ref-type="bibr">Yao et
  al., 2018</xref>,
  <xref alt="2022" rid="ref-Yao2022" ref-type="bibr">2022</xref>),
  meteorology
  (<xref alt="Sloughter et al., 2007" rid="ref-Sloughter2007" ref-type="bibr">Sloughter
  et al., 2007</xref>), and neuroscience
  (<xref alt="FitzGerald et al., 2014" rid="ref-FitzGerald2014" ref-type="bibr">FitzGerald
  et al., 2014</xref>). It is expected that the Bayesian model mixing
  methods implemented in <monospace>Taweret</monospace> can also be
  applied to use cases within these fields. Statisticians have developed
  several versatile BMA/stacking packages, e.g.
  (<xref alt="Raftery et al., 2022" rid="ref-BMA_R" ref-type="bibr">Raftery
  et al., 2022</xref>;
  <xref alt="Vehtari et al., 2017" rid="ref-loo" ref-type="bibr">Vehtari
  et al., 2017</xref>). However, the only BMM-based package available is
  <monospace>SAMBA</monospace>‚Äîa BAND collaboration effort that was
  developed for testing BMM methods on a toy model
  (<xref alt="Semposki et al., 2022a" rid="ref-SemposkiU003A2022gcp" ref-type="bibr">Semposki
  et al., 2022a</xref>). <monospace>Taweret</monospace>‚Äôs increased
  functionality, user-friendly structure, and diverse selection of
  mixing methods make it a marked improvement over
  <monospace>SAMBA</monospace>.</p>
</sec>
<sec id="structure">
  <title>Structure</title>
  <sec id="overview-of-methods">
    <title>Overview of methods</title>
    <table-wrap>
      <caption>
        <p>A summary of the three BMM approaches currently implemented
        in <monospace>Taweret</monospace>. Note that
        <inline-formula><alternatives>
        <tex-math><![CDATA[K\geq 2]]></tex-math>
        <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>K</mml:mi><mml:mo>‚â•</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
        Following the method name and the type of mixing model, the
        <italic>Number of inputs</italic> column details the dimensions
        of the parameter which the mixing weights depend on (e.g., in
        heavy-ion collisions this is the centrality bin); the
        <italic>Number of outputs</italic> details how many observables
        the models themselves can have to compute the model likelihood
        (e.g., in heavy-ion collisions this can include charge
        multiplicities, transverse momentum distributions, transverse
        momentum fluctuations, etc.); the <italic>Number of
        models</italic> column details how many models the mixing method
        can combine, and the <italic>Weight functions</italic> column
        describes the available parameterization of how the mixing
        weights depend on the input parameter. </p>
      </caption>
      <table>
        <colgroup>
          <col width="23%" />
          <col width="11%" />
          <col width="13%" />
          <col width="13%" />
          <col width="13%" />
          <col width="23%" />
        </colgroup>
        <thead>
          <tr>
            <th align="center">Method</th>
            <th align="center">Type</th>
            <th align="center">Number of inputs</th>
            <th align="center">Number of outputs</th>
            <th align="center">Number of models</th>
            <th align="center">Weight functions</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="center">Bivariate linear mixing</td>
            <td align="center">Mean &amp; Density</td>
            <td align="center">1</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\geq 1]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>‚â•</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">2</td>
            <td align="center"><list list-type="bullet">
              <list-item>
                <p>Step,</p>
              </list-item>
              <list-item>
                <p>Sigmoid,</p>
              </list-item>
              <list-item>
                <p>Asymmetric 2-step</p>
              </list-item>
            </list></td>
          </tr>
          <tr>
            <td align="center">Multivariate mixing</td>
            <td align="center">Mean</td>
            <td align="center">1</td>
            <td align="center">1</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[K]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">Precision weighting</td>
          </tr>
          <tr>
            <td align="center">BART mixing</td>
            <td align="center">Mean</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[\geq 1]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>‚â•</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">1</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[K]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula></td>
            <td align="center">Regression trees</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <sec id="bivariate-linear-mixing">
      <title>Bivariate linear mixing</title>
      <p>The full description of this mixing method and several of its
      applications in relativistic heavy-ion collision physics can be
      found in the Ph.D.¬†thesis of D. Liyanage
      (<xref alt="Liyanage, 2023" rid="ref-Liyanage_thesis" ref-type="bibr">Liyanage,
      2023</xref>). The bivariate linear mixing method can mix two
      models either using a density-mixing or a mean-mixing strategy.
      Currently, this is the only mixing method in
      <monospace>Taweret</monospace> that can also calibrate the models
      while mixing. Each physics-based model we consider may have
      unknown parameters that have physical meaning. In this context,
      Bayesian calibration corresponds to the process of using
      observational data to learn the values (and more generally, the
      posterior distributions) of this these unknown parameters. Most
      approaches in model mixing and model averaging use a two-step
      approach: first, fit individual models using a subset of the data;
      second, mix the predictions from each model (obtained from the
      previous step) using the other subset of the data to learn the
      weights. This method, employing simultaneous calibration and
      mixing, enables doing both steps at once.</p>
      <p>The user may choose among the following mixing functions:</p>
      <list list-type="bullet">
        <list-item>
          <p>step: <inline-formula><alternatives>
          <tex-math><![CDATA[\Theta(\beta_0-x)]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œò</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p>
        </list-item>
        <list-item>
          <p>sigmoid: <inline-formula><alternatives>
          <tex-math><![CDATA[\exp\left[(x-\beta_0)/\beta_1\right]]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>exp</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>‚àí</mml:mo><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>/</mml:mi><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula></p>
        </list-item>
        <list-item>
          <p>asymmetric 2-step: <inline-formula><alternatives>
          <tex-math><![CDATA[\alpha \Theta(\beta_0-x) + (1-\alpha)\Theta(\beta_1-x)]]></tex-math>
          <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ±</mml:mi><mml:mi>Œò</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>‚àí</mml:mo><mml:mi>Œ±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>Œò</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>‚àí</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
        </list-item>
      </list>
      <p>Here <inline-formula><alternatives>
      <tex-math><![CDATA[\Theta]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œò</mml:mi></mml:math></alternatives></inline-formula>
      denotes the Heaviside step function,
      <inline-formula><alternatives>
      <tex-math><![CDATA[\beta_0]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[\beta_1]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Œ≤</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
      determine the shape of the weight function and are inferred from
      the experimental data, and <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>
      is the model input parameter (which is expected to be
      1-dimensional for this mixing method).</p>
    </sec>
    <sec id="multivariate-model-mixing">
      <title>Multivariate model mixing</title>
      <p>Another Bayesian model mixing method incorporated into
      <monospace>Taweret</monospace> was originally published in
      (<xref alt="Semposki et al., 2022a" rid="ref-SemposkiU003A2022gcp" ref-type="bibr">Semposki
      et al., 2022a</xref>), and was the focus of the BMM Python package
      <monospace>SAMBA</monospace>
      (<xref alt="Semposki et al., 2022b" rid="ref-SAMBA" ref-type="bibr">Semposki
      et al., 2022b</xref>). It can be described as combining models by
      weighting each of them by their precision, defined as the inverse
      of their respective variances. The posterior predictive
      distribution (PPD) of the mixed model is a Gaussian and can be
      expressed as
      <named-content id="eqU003Amulti_mm_gaussian" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[

         \mathcal M_\dagger \sim {\mathcal N(f_\dagger, Z_P^{-1})}:
          \quad
          f_{\dagger} = \frac{1}{Z_P}\sum_{k=1}^{K} \frac{1}{\sigma^{2}_k}f_k,
          \quad Z_P \equiv \sum_{k=1}^{K}\frac{1}{\sigma^{2}_k},]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msub><mml:mo>‚àº</mml:mo><mml:mrow><mml:mi>ùí©</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>Z</mml:mi><mml:mi>P</mml:mi><mml:mrow><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:msub><mml:mi>f</mml:mi><mml:mo>‚Ä†</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>Z</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>œÉ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:msub><mml:mi>Z</mml:mi><mml:mi>P</mml:mi></mml:msub><mml:mo>‚â°</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:msubsup><mml:mi>œÉ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></named-content>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[\mathcal N(\mu, \sigma^2)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí©</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œº</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      is a normal distribution with mean <inline-formula><alternatives>
      <tex-math><![CDATA[\mu]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œº</mml:mi></mml:math></alternatives></inline-formula>
      and variance <inline-formula><alternatives>
      <tex-math><![CDATA[\sigma^2]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>œÉ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives>
      <tex-math><![CDATA[Z_{P}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Z</mml:mi><mml:mi>P</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is the precision of the models, and each individual model is
      assumed to possess a Gaussian form such as
      <disp-formula><alternatives>
      <tex-math><![CDATA[\mathcal M_{k} \sim \mathcal N(f_{k}(x),\sigma^2_{k}(x)).]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>‚Ñ≥</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>‚àº</mml:mo><mml:mi>ùí©</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>œÉ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
      Here, <inline-formula><alternatives>
      <tex-math><![CDATA[f_{k}(x)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      is the mean of the model <inline-formula><alternatives>
      <tex-math><![CDATA[k]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>,
      and <inline-formula><alternatives>
      <tex-math><![CDATA[\sigma^{2}_{k}(x)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mi>œÉ</mml:mi><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      its variance, both at input parameter
      <inline-formula><alternatives>
      <tex-math><![CDATA[x]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>.</p>
      <p>In this method, the software receives the one-dimensional input
      space <inline-formula><alternatives>
      <tex-math><![CDATA[X]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>,
      the mean of the <inline-formula><alternatives>
      <tex-math><![CDATA[k]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
      models at each point <inline-formula><alternatives>
      <tex-math><![CDATA[x \in X]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      (hence it is a mean-based mixing procedure), and the variances of
      the models at each point <inline-formula><alternatives>
      <tex-math><![CDATA[x \in X]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
      Each model is assumed to have been calibrated prior to being
      included in the mix. The ignorance of this mixing method with
      respect to how each model was generated allows for any model to be
      used, including Bayesian Machine Learning tools such as Gaussian
      Processes
      (<xref alt="Semposki et al., 2022a" rid="ref-SemposkiU003A2022gcp" ref-type="bibr">Semposki
      et al., 2022a</xref>) and Bayesian Neural Networks
      (<xref alt="Kronheim et al., 2022" rid="ref-KronheimU003A2020dmp" ref-type="bibr">Kronheim
      et al., 2022</xref>).</p>
    </sec>
    <sec id="model-mixing-using-bayesian-additive-regression-trees">
      <title>Model mixing using Bayesian additive regression
      trees</title>
      <p>A third BMM approach implemented in
      <monospace>Taweret</monospace> adopts a mean-mixing strategy which
      models the weight functions using Bayesian Additive Regression
      Trees (BART) conditional on the mean predictions from a set of
      <inline-formula><alternatives>
      <tex-math><![CDATA[K]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
      models
      (<xref alt="Yannotty et al., 2023" rid="ref-yannotty2023model" ref-type="bibr">Yannotty
      et al., 2023</xref>). This approach enables the weight functions
      to be adaptively learned using tree bases and avoids the need for
      user-specified basis functions (such as a generalized linear
      model). Formally, the weight functions are defined by
      <disp-formula><alternatives>
      <tex-math><![CDATA[w_k(x) = \sum_{j = 1}^m g_k(x; T_j, M_j), \quad \text{for}\ k=1,\ldots,K]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>m</mml:mi></mml:munderover><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mtext mathvariant="normal">for</mml:mtext><mml:mspace width="0.222em"></mml:mspace><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[g_k(x;T_j,M_j)]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
      defines the <inline-formula><alternatives>
      <tex-math><![CDATA[k^\text{th}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>k</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
      output of the <inline-formula><alternatives>
      <tex-math><![CDATA[j^\text{th}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>j</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
      tree, <inline-formula><alternatives>
      <tex-math><![CDATA[T_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>T</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
      using the associated set of parameters,
      <inline-formula><alternatives>
      <tex-math><![CDATA[M_j]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>M</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.
      Each weight function is implicitly regularized via a prior to
      prefer the interval <inline-formula><alternatives>
      <tex-math><![CDATA[[0,1]]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
      Furthermore, the weight functions are not required to sum to one
      and can take values outside of the range of
      <inline-formula><alternatives>
      <tex-math><![CDATA[[0,1]]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
      This regularization approach is designed to maintain the
      flexibility of the model while also encouraging the weight
      functions to take values which preserve desired inferential
      properties.</p>
      <p>This BART-based approach is implemented in
      <monospace>C++</monospace> with the <monospace>trees</monospace>
      module in <monospace>Taweret</monospace> acting as a Python
      interface. The <monospace>C++</monospace> back-end implements
      Bayesian tree models and originates from the <italic>Open Bayesian
      Trees Project</italic> (<monospace>OpenBT</monospace>)
      (<xref alt="Pratola et al., 2023" rid="ref-OpenBT_MTP" ref-type="bibr">Pratola
      et al., 2023</xref>). This module serves as an example for how
      existing code bases can be integrated into the
      <monospace>Taweret</monospace> framework.</p>
    </sec>
  </sec>
  <sec id="overview-of-package-structure">
    <title>Overview of package structure</title>
    <fig id="figU003Acodediagram">
      <caption><p>Diagram depicting the base classes, their methods
      (functions) and their properties (data).</p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="base-classes.pdf" />
    </fig>
    <p><monospace>Taweret</monospace> uses abstract base classes to
    ensure compatibility and uniformity of models and mixing methods.
    The two base classes are <monospace>BaseModel</monospace> and
    <monospace>BaseMixer</monospace> located in the
    <monospace>core</monospace> folder (see
    Fig.¬†<xref alt="1" rid="figU003Acodediagram">1</xref> for a
    schematic); any model mixing method developed with
    <monospace>Taweret</monospace> is required to inherit from these.
    The former represents physics-based models that may include
    parameters which need to be determined by Bayesian inference. The
    latter, <monospace>BaseMixer</monospace>, represents a mixing method
    used to combine the predictions from the physics-based models using
    Bayesian model mixing.</p>
    <p>The design philosophy for <monospace>Taweret</monospace> is to
    make it easy to switch between mixing methods without having to
    rewrite an analysis script. Thus, the base classes prescribe which
    functions need to be present for interoperability between mixing
    methods, and in particular, the models being called in the method.
    The functions required by <monospace>BaseModel</monospace> are</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>evaluate</monospace> - gives a point prediction
        for the model;</p>
      </list-item>
      <list-item>
        <p><monospace>log_likelihood_elementwise</monospace> -
        calculates the log-likelihood, reducing along the last axis of
        an array if the input array has multiple axes;</p>
      </list-item>
      <list-item>
        <p><monospace>set_prior</monospace> - sets priors for parameters
        in the model.</p>
      </list-item>
    </list>
    <p>The functions required by <monospace>BaseMixer</monospace>
    are</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>evaluate</monospace> - gives point prediction for
        the mixed model given a set of parameters;</p>
      </list-item>
      <list-item>
        <p><monospace>evaluate_weights</monospace> - gives point
        prediction for the weights given a set of parameters;</p>
      </list-item>
      <list-item>
        <p><monospace>map</monospace> - returns the maximum <italic>a
        posteriori</italic> estimate for the parameters of the mixed
        model (which includes both the weights and any model
        parameters);</p>
      </list-item>
      <list-item>
        <p><monospace>posterior</monospace> - returns the chains of the
        sampled parameters from the mixed model;</p>
      </list-item>
      <list-item>
        <p><monospace>predict</monospace> - returns the posterior
        predictive distribution for the mixed model;</p>
      </list-item>
      <list-item>
        <p><monospace>predict_weights</monospace> - returns the
        posterior predictive distribution for the model weights;</p>
      </list-item>
      <list-item>
        <p><monospace>prior</monospace> - returns the prior
        distributions (typically objects, not arrays);</p>
      </list-item>
      <list-item>
        <p><monospace>prior_predict</monospace> - returns the prior
        predictive distribution for the mixed model;</p>
      </list-item>
      <list-item>
        <p><monospace>set_prior</monospace> - sets the prior
        distributions for the mixing method;</p>
      </list-item>
      <list-item>
        <p><monospace>train</monospace> - executes the Bayesian model
        mixing step.</p>
      </list-item>
    </list>
    <p>Following our design philosophy, the general workflow for an
    analysis using <monospace>Taweret</monospace> is described in
    Fig.¬†<xref alt="3" rid="figU003Ataweret_workflow">3</xref>. From
    this, one can see three sources of information are generally
    required for an analysis: a selected mixing method, a model set, and
    training data. Each of these sources are connected through the
    training phase, which is where the mixing weights are learned. This
    leads into the prediction phase, where final predictions are
    obtained for the overall system and the weight functions. This
    process is summarized in the code snippet below. This workflow is
    preserved across the various methods implemented in
    <monospace>Taweret</monospace> and is intended to be maintained for
    future mixing methods included in this work.</p>
    <code language="python">from mix.mix_method import MixMethod
from models.my_model import MyModel

mixer = MixMethod(models={'model_1': MyModel(...), ...})
mixer.set_prior(...)
mixer.train(...)
mixer.predict(...)
mixer.predict_weights(...)</code>
    <p>Extending <monospace>Taweret</monospace> with a custom class or
    model simply requires that you inherit from the base classes and
    implement the required functions.</p>
    <fig id="figU003Ataweret_workflow">
      <caption><p>The general workflow for an analysis using
      <monospace>Taweret</monospace>. (Blue) The user must define each
      of the <inline-formula><alternatives>
      <tex-math><![CDATA[K]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
      models as a class inherited from <monospace>BaseModel</monospace>.
      (Green) The user can select an existing mixing method from
      <monospace>Taweret</monospace> (solid) or contribute a new method
      (dashed). (Purple) The model is trained using a set of training
      data (red), the model set (blue), and the selected mixing method
      (green). Predictions and uncertainty quantification follow from
      the training process.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="Taweret_JOSS.png" />
    </fig>
  </sec>
</sec>
<sec id="taweret-moving-forward">
  <title>Taweret moving forward</title>
  <p>There are certainly many improvements that can be made to
  <monospace>Taweret</monospace>. An obvious one is a generalization of
  the bivariate linear mixing; this could be the mixing of an arbitrary
  number of models at the density level. Complementary to this density
  mixing method is a stochastic, mean-mixing method of arbitrary number
  <inline-formula><alternatives>
  <tex-math><![CDATA[K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>
  of models. An extension of the Multivariate Mixing method to
  multi-dimensional input and output spaces, correlated models, as well
  as calibration during mixing, is anticipated in future releases.
  Lastly, to facilitate the utilization of this growing framework, we
  hope to enable continuous integration routines for contributing
  individuals and create docker images that will run
  <monospace>Taweret</monospace>.</p>
</sec>
<sec id="contributions">
  <title>Contributions</title>
  <p>All authors have contributed to the development of the Taweret
  framework, while individual mixing methods were developed by D.
  Liyanage (bivariate linear mixing), A. Semposki (multivariate model
  mixing), and J. Yanotty (additive regression trees). Ongoing work by
  K. Ingles will be included in the next version release of
  <monospace>Taweret</monospace>.</p>
</sec>
<sec id="disclosure-statement">
  <title>Disclosure statement</title>
  <p>The authors of this work are not aware of any conflicts of interest
  that would affect this research.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We thank Daniel R. Phillips, Ulrich Heinz, Matt Pratola, Kyle
  Godbey, Stefan Wild, Sunil Jaiswal, Jared O‚ÄôNeal, and all other BAND
  members for crucial feedback and discussion during the development
  stage of this package. This work is supported by the CSSI program
  Award OAC-2004601 (DL, ACS, JCY). ACS also acknowledges support from
  the Department of Energy (contract no. DE-FG02-93ER40756).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Fragoso2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fragoso</surname><given-names>Tiago M.</given-names></name>
        <name><surname>Bertoli</surname><given-names>Wesley</given-names></name>
        <name><surname>Louzada</surname><given-names>Francisco</given-names></name>
      </person-group>
      <article-title>Bayesian model averaging: A systematic review and conceptual classification</article-title>
      <source>International Statistical Review</source>
      <year iso-8601-date="2018">2018</year>
      <volume>86</volume>
      <issue>1</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12243</uri>
      <pub-id pub-id-type="doi">10.1111/insr.12243</pub-id>
      <fpage>1</fpage>
      <lpage>28</lpage>
    </element-citation>
  </ref>
  <ref id="ref-loo">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
        <name><surname>Gabry</surname><given-names>Jonah</given-names></name>
      </person-group>
      <article-title>loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models</article-title>
      <year iso-8601-date="2017">2017</year>
      <uri>https://mc-stan.org/loo/</uri>
    </element-citation>
  </ref>
  <ref id="ref-Yao2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yao</surname><given-names>Yuling</given-names></name>
        <name><surname>Pir≈°</surname><given-names>Gregor</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Bayesian hierarchical stacking: some models are (somewhere) useful</article-title>
      <source>Bayesian Analysis</source>
      <publisher-name>International Society for Bayesian Analysis</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>17</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.1214/21-BA1287</uri>
      <pub-id pub-id-type="doi">10.1214/21-BA1287</pub-id>
      <fpage>1043 </fpage>
      <lpage> 1071</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SAMBA">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Semposki</surname><given-names>A. C.</given-names></name>
        <name><surname>Furnstahl</surname><given-names>Richard J.</given-names></name>
        <name><surname>Phillips</surname><given-names>Daniel R.</given-names></name>
      </person-group>
      <article-title>SAMBA: SAndbox for Mixing using Bayesian Analysis</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/asemposki/SAMBA</uri>
    </element-citation>
  </ref>
  <ref id="ref-bandframework">
    <element-citation publication-type="report">
      <person-group person-group-type="author">
        <name><surname>Beyer</surname><given-names>Kyle</given-names></name>
        <name><surname>Buskirk</surname><given-names>Landon</given-names></name>
        <name><surname>Chan</surname><given-names>Moses Y-H.</given-names></name>
        <name><surname>Chang</surname><given-names>Tyler H.</given-names></name>
        <name><surname>DeBoer</surname><given-names>Richard James</given-names></name>
        <name><surname>Furnstahl</surname><given-names>Richard J.</given-names></name>
        <name><surname>Giuliani</surname><given-names>Pablo</given-names></name>
        <name><surname>Godbey</surname><given-names>Kyle</given-names></name>
        <name><surname>Ingles</surname><given-names>Kevin</given-names></name>
        <name><surname>Liyanage</surname><given-names>Dananjaya</given-names></name>
        <name><surname>Nunes</surname><given-names>Filomena M.</given-names></name>
        <name><surname>Odell</surname><given-names>Daniel</given-names></name>
        <name><surname>Phillips</surname><given-names>Daniel R.</given-names></name>
        <name><surname>Plumlee</surname><given-names>Matthew</given-names></name>
        <name><surname>Pratola</surname><given-names>Matthew T.</given-names></name>
        <name><surname>Semposki</surname><given-names>Alexandra C.</given-names></name>
        <name><surname>S√ºrer</surname><given-names>√ñzge</given-names></name>
        <name><surname>Wild</surname><given-names>Stefan M.</given-names></name>
        <name><surname>Yannotty</surname><given-names>John C.</given-names></name>
      </person-group>
      <article-title>BANDFramework: an open-source framework for Bayesian analysis of nuclear dynamics</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/bandframework/bandframework</uri>
    </element-citation>
  </ref>
  <ref id="ref-Liyanage_thesis">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Liyanage</surname><given-names>Dananjaya</given-names></name>
      </person-group>
      <article-title>Multifaceted study of ultrarelativistic heavy ion collisions</article-title>
      <publisher-name>Ohio State University; The Ohio State University</publisher-name>
      <year iso-8601-date="2023-08">2023</year><month>08</month>
      <uri>http://rave.ohiolink.edu/etdc/view?acc_num=osu1689508609203123</uri>
    </element-citation>
  </ref>
  <ref id="ref-SemposkiU003A2022gcp">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Semposki</surname><given-names>A. C.</given-names></name>
        <name><surname>Furnstahl</surname><given-names>R. J.</given-names></name>
        <name><surname>Phillips</surname><given-names>D. R.</given-names></name>
      </person-group>
      <article-title>Interpolating between small- and large-g expansions using Bayesian model mixing</article-title>
      <source>Physical Review C</source>
      <year iso-8601-date="2022">2022</year>
      <volume>106</volume>
      <issue>4</issue>
      <uri>https://arxiv.org/abs/2206.04116</uri>
      <pub-id pub-id-type="doi">10.1103/PhysRevC.106.044002</pub-id>
      <fpage>044002</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-PhillipsU003A2020dmw">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Phillips</surname><given-names>D. R.</given-names></name>
        <name><surname>Furnstahl</surname><given-names>R. J.</given-names></name>
        <name><surname>Heinz</surname><given-names>U.</given-names></name>
        <name><surname>Maiti</surname><given-names>T.</given-names></name>
        <name><surname>Nazarewicz</surname><given-names>W.</given-names></name>
        <name><surname>Nunes</surname><given-names>F. M.</given-names></name>
        <name><surname>Plumlee</surname><given-names>M.</given-names></name>
        <name><surname>Pratola</surname><given-names>M. T.</given-names></name>
        <name><surname>Pratt</surname><given-names>S.</given-names></name>
        <name><surname>Viens</surname><given-names>F. G.</given-names></name>
        <name><surname>Wild</surname><given-names>S. M.</given-names></name>
      </person-group>
      <article-title>Get on the BAND wagon: a Bayesian framework for quantifying model uncertainties in nuclear dynamics</article-title>
      <source>Journal of Physics G</source>
      <year iso-8601-date="2021">2021</year>
      <volume>48</volume>
      <issue>7</issue>
      <uri>https://arxiv.org/abs/2012.07704</uri>
      <pub-id pub-id-type="doi">10.1088/1361-6471/abf1df</pub-id>
      <fpage>072001</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-yannotty2023model">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yannotty</surname><given-names>John C.</given-names></name>
        <name><surname>Santner</surname><given-names>Thomas J.</given-names></name>
        <name><surname>Furnstahl</surname><given-names>Richard J.</given-names></name>
        <name><surname>Pratola</surname><given-names>Matthew T.</given-names></name>
      </person-group>
      <article-title>Model mixing using Bayesian additive regression trees</article-title>
      <source>Technometrics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://doi.org/10.1080/00401706.2023.2257765</uri>
      <pub-id pub-id-type="doi">10.1080/00401706.2023.2257765</pub-id>
      <fpage>1</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Yao2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yao</surname><given-names>Yuling</given-names></name>
        <name><surname>Vehtari</surname><given-names>Aki</given-names></name>
        <name><surname>Simpson</surname><given-names>Daniel</given-names></name>
        <name><surname>Gelman</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Using stacking to average bayesian predictive distributions (with discussion)</article-title>
      <source>Bayesian Analysis, 13 (3) 917 - 1007</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1214/17-BA1091</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Sloughter2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sloughter</surname><given-names>J. McLean</given-names></name>
        <name><surname>Raftery</surname><given-names>Adrian E.</given-names></name>
        <name><surname>Gneiting</surname><given-names>Tilmann</given-names></name>
        <name><surname>Fraley</surname><given-names>Chris</given-names></name>
      </person-group>
      <article-title>Probabilistic quantitative precipitation forecasting using Bayesian model averaging</article-title>
      <source>Monthly Weather Review, 135, 3209‚Äì3220</source>
      <year iso-8601-date="2007">2007</year>
      <pub-id pub-id-type="doi">10.1175/MWR3441.1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-FitzGerald2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>FitzGerald</surname><given-names>Thomas H. B.</given-names></name>
        <name><surname>Dolan</surname><given-names>Raymond J.</given-names></name>
        <name><surname>Friston</surname><given-names>Karl J.</given-names></name>
      </person-group>
      <article-title>Model averaging, optimal inference, and habit formation</article-title>
      <source>Frontiers in Human Neuroscience, 8:457</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.3389/fnhum.2014.00457</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-OpenBT_MTP">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Pratola</surname><given-names>M. T.</given-names></name>
        <name><surname>McCulloch</surname><given-names>R. E.</given-names></name>
        <name><surname>Chipman</surname><given-names>H. A.</given-names></name>
        <name><surname>Horiguchi</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Open Bayesian Trees project</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://bitbucket.org/mpratola/openbt/wiki/Home</uri>
    </element-citation>
  </ref>
  <ref id="ref-KronheimU003A2020dmp">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kronheim</surname><given-names>Braden</given-names></name>
        <name><surname>Kuchera</surname><given-names>Michelle</given-names></name>
        <name><surname>Prosper</surname><given-names>Harrison</given-names></name>
      </person-group>
      <article-title>TensorBNN: Bayesian inference for neural networks using TensorFlow</article-title>
      <source>Computer Physics Communications</source>
      <year iso-8601-date="2022">2022</year>
      <volume>270</volume>
      <uri>https://arxiv.org/abs/2009.14393</uri>
      <pub-id pub-id-type="doi">10.1016/j.cpc.2021.108168</pub-id>
      <fpage>108168</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-BMA_R">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Raftery</surname><given-names>Adrian</given-names></name>
        <name><surname>Hoeting</surname><given-names>Jennifer</given-names></name>
        <name><surname>Volinsky</surname><given-names>Chris</given-names></name>
        <name><surname>Painter</surname><given-names>Ian</given-names></name>
        <name><surname>Yeung</surname><given-names>Ka Yee</given-names></name>
      </person-group>
      <source>BMA: Bayesian model averaging</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://CRAN.R-project.org/package=BMA</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Taweret is the Egyptian goddess, known as the
    protector of children and women, whose body is a fusion of a
    hippopotamus, lion and crocodile which represent her ferocity.
    Similarly, <monospace>Taweret</monospace>, the package, seeks to
    fuse models together to represent observed phenomena.</p>
  </fn>
</fn-group>
</back>
</article>
