<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4747</article-id>
<article-id pub-id-type="doi">10.21105/joss.04747</article-id>
<title-group>
<article-title>LenslessPiCam: A Hardware and Software Platform for
Lensless Computational Imaging with a Raspberry Pi</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4837-5031</contrib-id>
<name>
<surname>Bezzam</surname>
<given-names>Eric</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0735-371X</contrib-id>
<name>
<surname>Kashani</surname>
<given-names>Sepand</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6122-1216</contrib-id>
<name>
<surname>Vetterli</surname>
<given-names>Martin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4927-3697</contrib-id>
<name>
<surname>Simeoni</surname>
<given-names>Matthieu</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>École Polytechnique Fédérale de Lausanne
(EPFL)</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-03-17">
<day>17</day>
<month>3</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>4747</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>lensless imaging</kwd>
<kwd>inverse problems</kwd>
<kwd>Raspberry Pi</kwd>
<kwd>Python</kwd>
<kwd>computational imaging</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Lensless imaging seeks to replace or remove the lens in a
  conventional imaging system. The earliest cameras were in fact
  lensless, relying on long exposure times to form images on the other
  end of a small aperture in a darkened room/container (<italic>camera
  obscura</italic>). The introduction of a lens allowed for more light
  throughput and therefore shorter exposure times, while retaining sharp
  focus. The incorporation of digital sensors readily enabled the use of
  computational imaging techniques to post-process and enhance raw
  images (e.g. via deblurring, inpainting, denoising, sharpening).
  Recently, imaging scientists have started leveraging computational
  imaging as an integral part of lensless imaging systems, allowing them
  to form viewable images from the highly multiplexed raw measurements
  of lensless cameras (see
  (<xref alt="Boominathan et al., 2022" rid="ref-boominathan2022recent" ref-type="bibr">Boominathan
  et al., 2022</xref>) and references therein for a comprehensive
  treatment of lensless imaging). This represents a real paradigm shift
  in camera system design as there is more flexibility to cater the
  hardware to the application at hand (e.g., lightweight or flat
  designs). This increased flexibility comes, however, at the price of a
  more demanding post-processing of the raw digital recordings and a
  tighter integration of sensing and computation, often difficult to
  achieve in practice due to inefficient interactions between the
  various communities of scientists involved. With
  <monospace>LenslessPiCam</monospace>, we provide an easily accessible
  hardware and software framework to enable researchers, hobbyists, and
  students to implement and explore practical and computational aspects
  of lensless imaging.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Being at the interface of hardware, software, and algorithm design,
  the field of lensless imaging necessitates a broad array of
  competences that might deter newcomers to the field. The purpose of
  <monospace>LenslessPiCam</monospace> is to provide a complete toolkit
  with low-cost, accessible hardware designs and open-source software,
  to quickly enable the exploration of novel ideas for hardware,
  software, and algorithm design.</p>
  <p>The DiffuserCam tutorial
  (<xref alt="Biscarrat et al., 2018" rid="ref-diffusercam" ref-type="bibr">Biscarrat
  et al., 2018</xref>) served as a great starting point to the present
  toolkit as it demonstrates that a working lensless camera can be built
  with cheap hardware: a Raspberry Pi, the
  <ext-link ext-link-type="uri" xlink:href="https://www.raspberrypi.com/products/camera-module-v2">Camera
  Module 2</ext-link>, and double-sided tape. The authors also provide
  Python implementations of two image reconstruction algorithms:
  variants of gradient descent (GD) with a non-negativity constraint;
  and the alternating direction method of multipliers (ADMM)
  (<xref alt="Boyd et al., 2011" rid="ref-boyd2011distributed" ref-type="bibr">Boyd
  et al., 2011</xref>) with an additional total variation (TV)
  prior.</p>
  <p>The resolution and quality of the reconstructed images for the
  DiffuserCam tutorial is poor and the processing pipeline is limited to
  grayscale reconstruction. With <monospace>LenslessPiCam</monospace>,
  we improve the reconstruction by using the newer
  <ext-link ext-link-type="uri" xlink:href="https://www.raspberrypi.com/products/raspberry-pi-high-quality-camera/">HQ
  camera</ext-link> as well as a more versatile and generic RGB
  computational imaging pipeline. See
  <xref alt="[fig:compare_cams]" rid="figU003Acompare_cams">[fig:compare_cams]</xref>
  for a comparison between the two cameras.</p>
  <fig id="figU003Acompare_cams">
    <caption><p>ADMM reconstruction of (a) an image of thumbs-up on a
    phone 40 cm away for (b) the original DiffuserCam tutorial
    (<xref alt="Biscarrat et al., 2018" rid="ref-diffusercam" ref-type="bibr">Biscarrat
    et al., 2018</xref>) and (c) our camera with RGB
    support.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/compare_cams.png" />
  </fig>
  <p>Similar to
  (<xref alt="Biscarrat et al., 2018" rid="ref-diffusercam" ref-type="bibr">Biscarrat
  et al., 2018</xref>), the core image reconstruction functionality of
  <monospace>LenslessPiCam</monospace> depends on NumPy
  (<xref alt="Harris et al., 2020" rid="ref-numpy" ref-type="bibr">Harris
  et al., 2020</xref>) and SciPy
  (<xref alt="Virtanen et al., 2020" rid="ref-scipy" ref-type="bibr">Virtanen
  et al., 2020</xref>). Moreover, <monospace>LenslessPiCam</monospace>
  also provides support for Pycsou
  (<xref alt="Simeoni, 2021" rid="ref-pycsou" ref-type="bibr">Simeoni,
  2021</xref>), a universal and reusable software environment providing
  key computational imaging functionalities and tools with great
  modularity and interoperability. This results in a more flexible
  reconstruction workflow, allowing for the quick prototyping of
  advanced post-processing schemes with more sophisticated image priors.
  PyTorch
  (<xref alt="Paszke et al., 2017" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2017</xref>) support also enables the use of GPUs for faster
  reconstruction, and the use of deep learning for image
  reconstruction.</p>
  <p><monospace>LenslessPiCam</monospace> is designed to be used by
  researchers, hobbyists, and students. In the past, we have found such
  open-source hardware and software platforms to be a valuable resource
  for researchers
  (<xref alt="Bezzam et al., 2017" rid="ref-bezzam2017hardware" ref-type="bibr">Bezzam
  et al., 2017</xref>) and students alike
  (<xref alt="Bezzam et al., 2019" rid="ref-bezzam2019teaching" ref-type="bibr">Bezzam
  et al., 2019</xref>). Moreover, we have used
  <monospace>LenslessPiCam</monospace> in our own work for performing
  measurements, simulating data, and image reconstruction
  (<xref alt="Bezzam et al., 2022" rid="ref-bezzam2022privacy" ref-type="bibr">Bezzam
  et al., 2022</xref>), and in our graduate-level signal processing
  course as a
  <ext-link ext-link-type="uri" xlink:href="https://infoscience.epfl.ch/search?ln=en&amp;rm=&amp;ln=en&amp;sf=&amp;so=d&amp;rg=10&amp;c=Infoscience%2FArticle&amp;c=Infoscience%2FBook&amp;c=Infoscience%2FChapter&amp;c=Infoscience%2FConference&amp;c=Infoscience%2FDataset&amp;c=Infoscience%2FLectures&amp;c=Infoscience%2FPatent&amp;c=Infoscience%2FPhysical%20objects&amp;c=Infoscience%2FPoster&amp;c=Infoscience%2FPresentation&amp;c=Infoscience%2FProceedings&amp;c=Infoscience%2FReport&amp;c=Infoscience%2FReview&amp;c=Infoscience%2FStandard&amp;c=Infoscience%2FStudent&amp;c=Infoscience%2FThesis&amp;c=Infoscience%2FWorking%20papers&amp;c=Media&amp;c=Other%20doctypes&amp;c=Work%20done%20outside%20EPFL&amp;c=&amp;of=hb&amp;fct__2=LCAV&amp;p=diffusercam">final
  project</ext-link>.
  <xref alt="[fig:screen_examples]" rid="figU003Ascreen_examples">[fig:screen_examples]</xref>
  demonstrates reconstructed images of our students using
  <monospace>LenslessPiCam</monospace> and images that were projected on
  a monitor 40cm away; the figure is adapted from
  <ext-link ext-link-type="uri" xlink:href="https://infoscience.epfl.ch/record/291501?ln=en">this
  report</ext-link>.</p>
  <fig id="figU003Ascreen_examples">
    <caption><p>Reconstructions of images displayed on a monitor.
    “Original” is displayed on the monitor, and each subsequent row
    represents a reconstruction with (1) an L2 data fidelity between the
    measurement and the propagated image estimate (using a measured PSF)
    and (2) different priors/regularizers on the image estimate: L2
    sparsity, non-negativity and total variation (TV), non-negativity,
    and L1 sparsity.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/screen_examples.png" />
  </fig>
  <p>As opposed to
  <xref alt="[fig:compare_cams]" rid="figU003Acompare_cams">[fig:compare_cams]</xref>
  and
  <xref alt="[fig:screen_examples]" rid="figU003Ascreen_examples">[fig:screen_examples]</xref>,
  which show reconstructions of back-illuminated objects,
  <xref alt="[fig:in_the_wild]" rid="figU003Ain_the_wild">[fig:in_the_wild]</xref>
  demonstrates reconstructions of objects illuminated with an external
  source.</p>
  <fig id="figU003Ain_the_wild">
    <caption><p>Reconstruction of objects 25cm away, illuminated with a
    lamp.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/in_the_wild.png" />
  </fig>
  <p><xref alt="[fig:tape_fov]" rid="figU003Atape_fov">[fig:tape_fov]</xref>
  demonstrates the field-of-view (FOV) of the proposed system with
  double-sided tape. Objects are 40cm away.</p>
  <fig id="figU003Atape_fov">
    <caption><p>Experimental field-of-view (FOV) of tape-based
    LenslessPiCam.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/tape_fov.png" />
  </fig>
</sec>
<sec id="contributions">
  <title>Contributions</title>
  <p>With respect to the DiffuserCam tutorial
  (<xref alt="Biscarrat et al., 2018" rid="ref-diffusercam" ref-type="bibr">Biscarrat
  et al., 2018</xref>), we have made the following contributions. In
  terms of hardware, as shown in
  <xref alt="[fig:hardware]" rid="figU003Ahardware">[fig:hardware]</xref>,
  we:</p>
  <list list-type="bullet">
    <list-item>
      <p>make use of the HQ camera sensor ($50): 4056 x 3040 pixels
      (12.3 MP) and 7.9 mm sensor diagonal, compared to 3280 × 2464
      pixels (8.1 MP) and 4.6 mm sensor diagonal for the Camera Module 2
      ($30). A tutorial for building our proposed camera can be found on
      <ext-link ext-link-type="uri" xlink:href="https://medium.com/@bezzam/building-a-diffusercam-with-the-raspberry-hq-camera-cardboard-and-tape-896b6020aff6">Medium</ext-link>;</p>
    </list-item>
    <list-item>
      <p>provide the design and firmware for a cheap point source
      generator (needed for calibration), which consists of an Arduino,
      a white LED, and a cardboard box. A tutorial for building this
      system can be found on
      <ext-link ext-link-type="uri" xlink:href="https://medium.com/@bezzam/measuring-an-optical-psf-with-an-arduino-an-led-and-a-cardboard-box-2f3ddac660c1">Medium</ext-link>.</p>
    </list-item>
  </list>
  <fig id="figU003Ahardware">
    <caption><p>(a) LenslessPiCam, (b) point source generator (inside)
    and (c) (outside).</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/hardware.png" />
  </fig>
  <p>With respect to reconstruction algorithms, we:</p>
  <list list-type="bullet">
    <list-item>
      <p>provide significantly faster implementations of GD and
      ADMM;</p>
    </list-item>
    <list-item>
      <p>extend the above reconstructions to RGB;</p>
    </list-item>
    <list-item>
      <p>provide PyTorch / GPU support;</p>
    </list-item>
    <list-item>
      <p>provide an object-oriented structure that is easy to extend for
      exploring new algorithms;</p>
    </list-item>
    <list-item>
      <p>provide an object-oriented interface to Pycsou for solving
      lensless imaging inverse problems. Pycsou is a Python package for
      solving inverse problems of the form
      <named-content id="eqU003Afourier" content-type="equation"><disp-formula><alternatives>
      <tex-math><![CDATA[
      \min_{\mathbf{x}\in\mathbb{R}^N} \,F(\mathbf{y}, \mathbf{G} \mathbf{x})\quad+\quad \lambda\mathcal{R}(\mathbf{x}),]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:munder><mml:mo>min</mml:mo><mml:mrow><mml:mi>𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mi>N</mml:mi></mml:msup></mml:mrow></mml:munder><mml:mspace width="0.167em"></mml:mspace><mml:mi>F</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐲</mml:mi><mml:mo>,</mml:mo><mml:mi>𝐆</mml:mi><mml:mi>𝐱</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="1.0em"></mml:mspace><mml:mo>+</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mi>λ</mml:mi><mml:mi>ℛ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>𝐱</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></named-content>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[F]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>F</mml:mi></mml:math></alternatives></inline-formula>
      is a data-fidelity term between the observed and predicted
      measurements <inline-formula><alternatives>
      <tex-math><![CDATA[\mathbf{y}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>𝐲</mml:mi></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[\mathbf{G}\mathbf{x}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>𝐆</mml:mi><mml:mi>𝐱</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
      respectively, <inline-formula><alternatives>
      <tex-math><![CDATA[\mathcal{R}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ℛ</mml:mi></mml:math></alternatives></inline-formula>
      is a regularization component (could consist of more than one
      prior), and <inline-formula><alternatives>
      <tex-math><![CDATA[\lambda >0]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      controls the amount of regularization.</p>
    </list-item>
  </list>
  <p>We also provide functionalities to:</p>
  <list list-type="bullet">
    <list-item>
      <p>remotely display data on an external monitor (as done for
      <xref alt="[fig:screen_examples]" rid="figU003Ascreen_examples">[fig:screen_examples]</xref>),
      which can be used to automate raw data measurements to, e.g.,
      gather a dataset;</p>
    </list-item>
    <list-item>
      <p>simulate measurements, given a point spread function (PSF) of a
      lensless camera. A tutorial can be found on
      <ext-link ext-link-type="uri" xlink:href="https://medium.com/@bezzam/simulating-camera-measurements-through-wave-optics-with-pytorch-support-faf3fa620789">Medium</ext-link>;</p>
    </list-item>
    <list-item>
      <p>evaluate reconstructions on a variety of metrics: mean squared
      error (MSE), peak signal-to-noise ratio (PSNR), structural
      similarity index measure (SSIM), and learned perceptual image
      patch similarity (LPIPS). A tutorial can be found on
      <ext-link ext-link-type="uri" xlink:href="https://medium.com/@bezzam/image-similarity-metrics-applied-to-diffusercam-21998967af8d">Medium</ext-link>;</p>
    </list-item>
    <list-item>
      <p>quantitatively evaluate the PSF of the lensless camera.</p>
    </list-item>
  </list>
  <p>As previous noted, we have written a set of Medium articles to
  guide users through the process of building and using the proposed
  lensless camera. An overview of these articles can be found
  <ext-link ext-link-type="uri" xlink:href="https://medium.com/@bezzam/a-complete-lensless-imaging-tutorial-hardware-software-and-algorithms-8873fa81a660">here</ext-link>.
  A
  <ext-link ext-link-type="uri" xlink:href="https://lensless.readthedocs.io">ReadTheDocs</ext-link>
  page also provides an overview of all the available features.</p>
  <p>In the following sections, we describe some of these contributions,
  and quantify them (where appropriate).</p>
</sec>
<sec id="high-level-modular-functionality-for-reconstructions">
  <title>High-level, modular functionality for reconstructions</title>
  <p>The core algorithmic component of
  <monospace>LenslessPiCam</monospace> is the abstract class
  <monospace>lensless.ReconstructionAlgorithm</monospace>. The three
  reconstruction strategies available in
  <monospace>LenslessPiCam</monospace> derive from this class:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>lensless.GradientDescient</monospace>: projected GD
      with a non-negativity constraint. Two accelerated approaches are
      also available:
      <monospace>lensless.NesterovGradientDescent</monospace>
      (<xref alt="Nesterov, 1983" rid="ref-nesterov1983method" ref-type="bibr">Nesterov,
      1983</xref>) and <monospace>lensless.FISTA</monospace>
      (<xref alt="Beck &amp; Teboulle, 2009" rid="ref-beck2009fast" ref-type="bibr">Beck
      &amp; Teboulle, 2009</xref>);</p>
    </list-item>
    <list-item>
      <p><monospace>lensless.ADMM</monospace>: ADMM with a
      non-negativity constraint and a TV regularizer;</p>
    </list-item>
    <list-item>
      <p><monospace>lensless.APGD</monospace>: accelerated proximal GD
      with Pycsou as a backend. Any differentiable or proximal operator
      can be used as long as it is compatible with Pycsou, namely
      derives from one of
      <ext-link ext-link-type="uri" xlink:href="https://github.com/matthieumeo/pycsou/blob/a74b714192821501371c89dbd44eac15a5456a0f/src/pycsou/abc/operator.py#L980"><monospace>DiffFunc</monospace></ext-link>
      or
      <ext-link ext-link-type="uri" xlink:href="https://github.com/matthieumeo/pycsou/blob/a74b714192821501371c89dbd44eac15a5456a0f/src/pycsou/abc/operator.py#L741"><monospace>ProxFunc</monospace></ext-link>.</p>
    </list-item>
  </list>
  <p>One major advantage of deriving from
  <monospace>lensless.ReconstructionAlgorithm</monospace> is that code
  duplication across algorithm can be minimized as it handles most of
  the common functionality, i.e., efficiently computing 2D Fourier
  transforms (needed to solve
  <xref alt="Equation 1" rid="eqU003Afourier">Equation 1</xref>),
  iteration logic, saving intermediate outputs, and visualization. Using
  a reconstruction algorithm that derives from it boils down to three
  steps:</p>
  <list list-type="order">
    <list-item>
      <p>Creating an instance of the reconstruction algorithm.</p>
    </list-item>
    <list-item>
      <p>Setting the data.</p>
    </list-item>
    <list-item>
      <p>Applying the algorithm.</p>
    </list-item>
  </list>
  <p>For example, for ADMM (full example in
  <monospace>scripts/recon/admm.py</monospace>):</p>
  <code language="python">    recon = ADMM(psf)
    recon.set_data(data)
    res = recon.apply(n_iter=n_iter)</code>
  <p>Example reconstruction scripts can be found in
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/tree/main/scripts/recon"><monospace>scripts/recon</monospace></ext-link>.
  The Hydra framework
  (<xref alt="Yadan, 2019" rid="ref-Yadan2019Hydra" ref-type="bibr">Yadan,
  2019</xref>) is used to configure the various reconstruction
  algorithms, with the default configuration
  (<monospace>defaults_recon.yaml</monospace>) and others located in the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/tree/main/configs"><monospace>configs</monospace></ext-link>
  folder.</p>
</sec>
<sec id="efficient-reconstruction">
  <title>Efficient reconstruction</title>
  <p>In Table 1, we compare the processing time of DiffuserCam’s and
  <monospace>LenslessPiCam</monospace>’s implementations for grayscale
  reconstruction of:</p>
  <list list-type="order">
    <list-item>
      <p>GD using FISTA with a non-negativity constraint.</p>
    </list-item>
    <list-item>
      <p>ADMM with a non-negativity constraint and a TV regularizer.</p>
    </list-item>
  </list>
  <p>The DiffuserCam implementations can be found
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Waller-Lab/DiffuserCam-Tutorial">here</ext-link>,
  while <monospace>lensless.APGD</monospace> and
  <monospace>lensless.ADMM</monospace> are used for
  <monospace>LenslessPiCam</monospace>. The comparison is done on a Dell
  Precision 5820 Tower X-Series (08B1) machine with an Intel i9-10900X
  3.70 GHz processor (10 cores, 20 threads), running Ubuntu 20.04.5 LTS
  and (when applicable) an NVIDIA RTX A5000 GPU. </p>
  <table-wrap>
    <caption>
      <p>Benchmark grayscale reconstruction. 300 iterations for gradient
      descent (GD) and 5 iterations for alternating direction method of
      multipliers (ADMM).</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="center"></th>
          <th align="center">GD</th>
          <th align="center">ADMM</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center">DiffuserCam</td>
          <td align="center">246 s</td>
          <td align="center">6.81 s</td>
        </tr>
        <tr>
          <td align="center"><monospace>LenslessPiCam</monospace>
          (<monospace>numpy</monospace>)</td>
          <td align="center">21.1 s</td>
          <td align="center">1.26 s</td>
        </tr>
        <tr>
          <td align="center"><monospace>LenslessPiCam</monospace>
          (<monospace>torch</monospace>, CPU)</td>
          <td align="center">4.32 s</td>
          <td align="center">272 ms</td>
        </tr>
        <tr>
          <td align="center"><monospace>LenslessPiCam</monospace>
          (<monospace>torch</monospace>, GPU)</td>
          <td align="center">274 ms</td>
          <td align="center">2.88 ms</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>In Table 1, we observe an 11.7x reduction in computation time for
  GD and a 2.4x reduction for ADMM. This comes from:</p>
  <list list-type="bullet">
    <list-item>
      <p>our object-oriented implementation of the algorithms, which
      allocates all the necessary memory beforehand and pre-computes
      data-independent terms, such as forward operators from the point
      spread function (PSF);</p>
    </list-item>
    <list-item>
      <p>our use of the real-valued fast Fourier transform (FFT), which
      is possible since we are working with image intensities. Our
      convolver/deconvolver is implemented as an object -
      <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/e31c9a7c8c87d30d4881b1123d849e7667b2e335/lensless/rfft_convolve.py#L16"><monospace>RealFFTConvolve2D</monospace></ext-link>
      - that pre-computes the FFT of the PSF and supports SciPy and
      PyTorch backends.</p>
    </list-item>
  </list>
  <p>When using a GPU (through PyTorch), we observe a significant
  reduction in computation time: 898x and 2360x reduction for GD and
  ADMM, respectively.</p>
</sec>
<sec id="quantifying-performance">
  <title>Quantifying performance</title>
  <p>To methodically compare different reconstruction approaches, it is
  necessary to quantify the performance. To this end,
  <monospace>LenslessPiCam</monospace> provides functionality to extract
  regions of interest from the reconstruction and compare them with the
  original image via multiple metrics:</p>
  <list list-type="bullet">
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean-squared
      error</ext-link> (MSE), where lower is better and the minimum
      value is 0;</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">Peak
      signal-to-noise ratio</ext-link> (PSNR), where higher is better
      with values given in decibels (dB);</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://en.wikipedia.org/wiki/Structural_similarity">Structural
      similarity index measure</ext-link> (SSIM), where values are
      within [-1, 1] and higher is better;</p>
    </list-item>
    <list-item>
      <p>Learned perceptual image patch similarity (LPIPS)
      (<xref alt="Zhang et al., 2018a" rid="ref-zhang2018perceptual" ref-type="bibr">Zhang
      et al., 2018a</xref>), where values are within [0, 1] and lower is
      better.</p>
    </list-item>
  </list>
  <p>MSE, PSNR, and SSIM are computed using
  <monospace>skimage.metrics</monospace>
  (<xref alt="Van der Walt et al., 2014" rid="ref-van2014scikit" ref-type="bibr">Van
  der Walt et al., 2014</xref>), while LPIPS is computed using
  <monospace>lpips</monospace>
  (<xref alt="Zhang et al., 2018b" rid="ref-lpips" ref-type="bibr">Zhang
  et al., 2018b</xref>). MSE and PNSR compare images pixel-wise, while
  SSIM and LPIPS compare images patch-wise.</p>
  <p><xref alt="[fig:metric]" rid="figU003Ametric">[fig:metric]</xref>
  and Table 2 show how a reconstruction can be evaluated against an
  original image, using
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/main/scripts/compute_metrics_from_original.py"><monospace>scripts/compute_metrics_from_original.py</monospace></ext-link>.</p>
  <fig id="figU003Ametric">
    <caption><p>Comparing lensless reconstruction (left) with original
    image displayed on a screen (right).</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/metric.png" />
  </fig>
  <table-wrap>
    <caption>
      <p>Metrics for
      <xref alt="[fig:metric]" rid="figU003Ametric">[fig:metric]</xref>.</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="center">MSE</th>
          <th>PSNR</th>
          <th align="center">SSIM</th>
          <th align="center">LPIPS</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center">0.164</td>
          <td>7.85</td>
          <td align="center">0.405</td>
          <td align="center">0.645</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>One limitation with comparing the reconstructed image (from
  measurements) directly with the original image is that the lighting
  during measurement can lead to rather poor results on the metrics,
  even though the content is visually similar (as in
  <xref alt="[fig:metric]" rid="figU003Ametric">[fig:metric]</xref>). To
  mitigate this difference, we can compare the reconstructed image with
  the image displayed on the screen, but captured with a lensed camera.
  In the next section, we describe the functionalities
  <monospace>LenslessPiCam</monospace> provides for collecting such data
  and using existing datasets. Alternatively, simulation can be used to
  compare reconstruction algorithms without having to collect data.</p>
</sec>
<sec id="measured-and-simulated-data">
  <title>Measured and simulated data</title>
  <p>Sometimes it may be of interest to perform an exhaustive evaluation
  on a large dataset. <monospace>LenslessPiCam</monospace> could be used
  for collecting such a dataset with the proposed camera by using the
  remote display and capture scripts,
  i.e. <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/main/scripts/remote_display.py"><monospace>scripts/remote_display.py</monospace></ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/main/scripts/remote_capture.py"><monospace>scripts/remote_capture.py</monospace></ext-link>,
  respectively.</p>
  <p>Moreover, the authors of
  (<xref alt="Monakhova et al., 2019b" rid="ref-monakhova2019learned" ref-type="bibr">Monakhova
  et al., 2019b</xref>) have already collected a dataset of 25,000
  parallel measurements, namely 25,000 pairs of DiffuserCam and lensed
  camera images
  (<xref alt="Monakhova et al., 2019a" rid="ref-diffusercamdataset" ref-type="bibr">Monakhova
  et al., 2019a</xref>). <monospace>LenslessPiCam</monospace> offers
  functionality to evaluate a reconstruction algorithm on the full
  dataset (100 GB), or a
  <ext-link ext-link-type="uri" xlink:href="https://drive.switch.ch/index.php/s/vmAZzryGI8U8rcE">subset</ext-link>
  of 200 files (725 MB) that we have prepared. Note that this dataset is
  collected with a different lensless camera, but is nonetheless useful
  for exploring reconstruction techniques.</p>
  <p>Table 3 shows the average metric results after applying 100
  iterations of ADMM to the subset we have prepared, using
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/main/scripts/evaluate_mirflickr_admm.py"><monospace>scripts/evaluate_mirflickr_admm.py</monospace></ext-link>.</p>
  <table-wrap>
    <caption>
      <p>Average metrics for 100 iterations of ADMM on a subset (200
      files) of the DiffuserCam Lensless Mirflickr Dataset.</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="center">MSE</th>
          <th>PSNR</th>
          <th align="center">SSIM</th>
          <th align="center">LPIPS</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center">0.0797</td>
          <td>12.7</td>
          <td align="center">0.535</td>
          <td align="center">0.585</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>One can also visualize the performance on a single file of the
  dataset, e.g., by using
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/blob/main/scripts/apply_admm_single_mirflickr.py"><monospace>scripts/apply_admm_single_mirflickr.py</monospace></ext-link>
  to show how the reconstruction changes as the number of iterations
  increase.[^9] The final reconstruction and outputed metrics are shown
  in
  <xref alt="[fig:dataset_single_file]" rid="figU003Adataset_single_file">[fig:dataset_single_file]</xref>
  and Table 4.</p>
  <fig id="figU003Adataset_single_file">
    <caption><p>Visualizing performance of ADMM (100 iterations) on a
    single file of the DiffuserCam Lensless Mirflickr
    Dataset.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/dataset_single_file.png" />
  </fig>
  <table-wrap>
    <caption>
      <p>Metrics for
      <xref alt="[fig:dataset_single_file]" rid="figU003Adataset_single_file">[fig:dataset_single_file]</xref>.</p>
    </caption>
    <table>
      <thead>
        <tr>
          <th align="center">MSE</th>
          <th>PSNR</th>
          <th align="center">SSIM</th>
          <th align="center">LPIPS</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td align="center">0.0682</td>
          <td>11.7</td>
          <td align="center">0.486</td>
          <td align="center">0.504</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>Scripts inside
  <ext-link ext-link-type="uri" xlink:href="https://github.com/LCAV/LenslessPiCam/tree/main/scripts/sim"><monospace>scripts/sim</monospace></ext-link>
  show how to simulate lensless camera measurements given a PSF, most
  notably with PyTorch compatibility for easy integration with machine
  learning tasks. This approach can be used to conveniently compare
  reconstruction algorithms without having to collect data.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>In summary, <monospace>LenslessPiCam</monospace> provides all the
  necessary hardware designs and software to build, use, and evaluate a
  lensless camera with low-cost and accessible components. Furthermore,
  a simulation framework allows users to experiment with lensless
  imaging without having to build the camera. As we continue to use
  <monospace>LenslessPiCam</monospace> as a research and educational
  platform, we hope to investigate and incorporate:</p>
  <list list-type="bullet">
    <list-item>
      <p>computational refocusing and 3D imaging;</p>
    </list-item>
    <list-item>
      <p>video reconstruction;</p>
    </list-item>
    <list-item>
      <p>on-device reconstruction;</p>
    </list-item>
    <list-item>
      <p>programmable masks;</p>
    </list-item>
    <list-item>
      <p>data-driven, machine learning reconstruction techniques.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements-and-disclosure-of-funding">
  <title>Acknowledgements and disclosure of funding</title>
  <p>We acknowledge feedback from Julien Fageot and the students during
  the first iteration of this project in our graduate course.</p>
  <p>This work was in part funded by the Swiss National Science
  Foundation (SNSF) under grants CRSII5 193826 “AstroSignals - A New
  Window on the Universe, with the New Generation of Large
  Radio-Astronomy Facilities” (M. Simeoni), 200 021 181 978/1 “SESAM -
  Sensing and Sampling: Theory and Algorithms” (E. Bezzam) and CRSII5
  180232 “FemtoLippmann - Digital twin for multispectral imaging” (S.
  Kashani).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-boominathan2022recent">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boominathan</surname><given-names>Vivek</given-names></name>
        <name><surname>Robinson</surname><given-names>Jacob T.</given-names></name>
        <name><surname>Waller</surname><given-names>Laura</given-names></name>
        <name><surname>Veeraraghavan</surname><given-names>Ashok</given-names></name>
      </person-group>
      <article-title>Recent advances in lensless imaging</article-title>
      <source>Optica</source>
      <publisher-name>OSA</publisher-name>
      <year iso-8601-date="2022-01">2022</year><month>01</month>
      <volume>9</volume>
      <issue>1</issue>
      <uri>http://opg.optica.org/optica/abstract.cfm?URI=optica-9-1-1</uri>
      <pub-id pub-id-type="doi">10.1364/OPTICA.431361</pub-id>
      <fpage>1</fpage>
      <lpage>16</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bezzam2017hardware">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bezzam</surname><given-names>Eric</given-names></name>
        <name><surname>Scheibler</surname><given-names>Robin</given-names></name>
        <name><surname>Azcarreta</surname><given-names>Juan</given-names></name>
        <name><surname>Pan</surname><given-names>Hanjie</given-names></name>
        <name><surname>Simeoni</surname><given-names>Matthieu</given-names></name>
        <name><surname>Beuchat</surname><given-names>Rene</given-names></name>
        <name><surname>Hurley</surname><given-names>Paul</given-names></name>
        <name><surname>Bruneau</surname><given-names>Basile</given-names></name>
        <name><surname>Ferry</surname><given-names>Corentin</given-names></name>
        <name><surname>Kashani</surname><given-names>Sepand</given-names></name>
      </person-group>
      <article-title>Hardware and software for reproducible research in audio array signal processing</article-title>
      <source>2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
      <year iso-8601-date="2017">2017</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/ICASSP.2017.8005297</pub-id>
      <fpage>6591</fpage>
      <lpage>6592</lpage>
    </element-citation>
  </ref>
  <ref id="ref-diffusercam">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Biscarrat</surname><given-names>C.</given-names></name>
        <name><surname>Parthasarathy</surname><given-names>S.</given-names></name>
        <name><surname>Kuo</surname><given-names>G.</given-names></name>
        <name><surname>Antipa</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>Build your own DiffuserCam: Tutorial</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://waller-lab.github.io/DiffuserCam/tutorial.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-diffusercamdataset">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Monakhova</surname><given-names>Kristina</given-names></name>
        <name><surname>Yurtsever</surname><given-names>Joshua</given-names></name>
        <name><surname>Kuo</surname><given-names>Grace</given-names></name>
        <name><surname>Antipa</surname><given-names>Nick</given-names></name>
        <name><surname>Yanny</surname><given-names>Kyrollos</given-names></name>
        <name><surname>Waller</surname><given-names>Laura</given-names></name>
      </person-group>
      <article-title>DiffuserCam lensless Mirflickr dataset</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://waller-lab.github.io/LenslessLearning/dataset.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-bezzam2019teaching">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bezzam</surname><given-names>Eric</given-names></name>
        <name><surname>Hoffet</surname><given-names>Adrien</given-names></name>
        <name><surname>Prandoni</surname><given-names>Paolo</given-names></name>
      </person-group>
      <article-title>Teaching practical DSP with off-the-shelf hardware and free software</article-title>
      <source>ICASSP 2019 - 2019 IEEE international conference on acoustics, speech and signal processing (ICASSP)</source>
      <year iso-8601-date="2019">2019</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/ICASSP.2019.8682923</pub-id>
      <fpage>7660</fpage>
      <lpage>7664</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bezzam2022privacy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezzam</surname><given-names>Eric</given-names></name>
        <name><surname>Vetterli</surname><given-names>Martin</given-names></name>
        <name><surname>Simeoni</surname><given-names>Matthieu</given-names></name>
      </person-group>
      <article-title>Privacy-enhancing optical embeddings for lensless classification</article-title>
      <source>arXiv preprint arXiv:2211.12864</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-pycsou">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Simeoni</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Pycsou</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/matthieumeo/pycsou</uri>
    </element-citation>
  </ref>
  <ref id="ref-boyd2011distributed">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boyd</surname><given-names>Stephen</given-names></name>
        <name><surname>Parikh</surname><given-names>Neal</given-names></name>
        <name><surname>Chu</surname><given-names>Eric</given-names></name>
        <name><surname>Peleato</surname><given-names>Borja</given-names></name>
        <name><surname>Eckstein</surname><given-names>Jonathan</given-names></name>
      </person-group>
      <article-title>Distributed optimization and statistical learning via the alternating direction method of multipliers</article-title>
      <source>Found. Trends Mach. Learn.</source>
      <publisher-name>Now Publishers Inc.</publisher-name>
      <publisher-loc>Hanover, MA, USA</publisher-loc>
      <year iso-8601-date="2011-01">2011</year><month>01</month>
      <volume>3</volume>
      <issue>1</issue>
      <issn>1935-8237</issn>
      <uri>https://doi.org/10.1561/2200000016</uri>
      <pub-id pub-id-type="doi">10.1561/2200000016</pub-id>
      <fpage>1</fpage>
      <lpage>122</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhang2018perceptual">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Richard</given-names></name>
        <name><surname>Isola</surname><given-names>Phillip</given-names></name>
        <name><surname>Efros</surname><given-names>Alexei A.</given-names></name>
        <name><surname>Shechtman</surname><given-names>Eli</given-names></name>
        <name><surname>Wang</surname><given-names>Oliver</given-names></name>
      </person-group>
      <article-title>The unreasonable effectiveness of deep features as a perceptual metric</article-title>
      <source>2018 IEEE/CVF conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2018">2018</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/CVPR.2018.00068</pub-id>
      <fpage>586</fpage>
      <lpage>595</lpage>
    </element-citation>
  </ref>
  <ref id="ref-beck2009fast">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Beck</surname><given-names>Amir</given-names></name>
        <name><surname>Teboulle</surname><given-names>Marc</given-names></name>
      </person-group>
      <article-title>A fast iterative shrinkage-thresholding algorithm for linear inverse problems</article-title>
      <source>SIAM Journal on Imaging Sciences</source>
      <year iso-8601-date="2009">2009</year>
      <volume>2</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1137/080716542</uri>
      <pub-id pub-id-type="doi">10.1137/080716542</pub-id>
      <fpage>183</fpage>
      <lpage>202</lpage>
    </element-citation>
  </ref>
  <ref id="ref-monakhova2019learned">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Monakhova</surname><given-names>Kristina</given-names></name>
        <name><surname>Yurtsever</surname><given-names>Joshua</given-names></name>
        <name><surname>Kuo</surname><given-names>Grace</given-names></name>
        <name><surname>Antipa</surname><given-names>Nick</given-names></name>
        <name><surname>Yanny</surname><given-names>Kyrollos</given-names></name>
        <name><surname>Waller</surname><given-names>Laura</given-names></name>
      </person-group>
      <article-title>Learned reconstructions for practical mask-based lensless imaging</article-title>
      <source>Optics Express</source>
      <year iso-8601-date="2019-09">2019</year><month>09</month>
      <volume>27</volume>
      <issue>20</issue>
      <uri>https://arxiv.org/abs/1908.11502</uri>
      <pub-id pub-id-type="doi">10.1364/OE.27.028075</pub-id>
      <fpage>28075</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-nesterov1983method">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nesterov</surname><given-names>Yurii E.</given-names></name>
      </person-group>
      <article-title>A method for solving the convex programming problem with convergence rate O(1/k^2)</article-title>
      <source>Dokl. Akad. Nauk SSSR</source>
      <year iso-8601-date="1983">1983</year>
      <volume>269</volume>
      <fpage>543</fpage>
      <lpage>547</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Yadan2019Hydra">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Yadan</surname><given-names>Omry</given-names></name>
      </person-group>
      <article-title>Hydra - a framework for elegantly configuring complex applications</article-title>
      <publisher-name>Github</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/facebookresearch/hydra</uri>
    </element-citation>
  </ref>
  <ref id="ref-numpy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R</given-names></name>
        <name><surname>Millman</surname><given-names>K Jarrod</given-names></name>
        <name><surname>Van Der Walt</surname><given-names>Stéfan J</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scipy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in python</article-title>
      <source>Nature methods</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <issue>3</issue>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
      </person-group>
      <article-title>Automatic differentiation in pytorch</article-title>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-van2014scikit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Van der Walt</surname><given-names>Stefan</given-names></name>
        <name><surname>Schönberger</surname><given-names>Johannes L</given-names></name>
        <name><surname>Nunez-Iglesias</surname><given-names>Juan</given-names></name>
        <name><surname>Boulogne</surname><given-names>François</given-names></name>
        <name><surname>Warner</surname><given-names>Joshua D</given-names></name>
        <name><surname>Yager</surname><given-names>Neil</given-names></name>
        <name><surname>Gouillart</surname><given-names>Emmanuelle</given-names></name>
        <name><surname>Yu</surname><given-names>Tony</given-names></name>
      </person-group>
      <article-title>Scikit-image: Image processing in python</article-title>
      <source>PeerJ</source>
      <publisher-name>PeerJ Inc.</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>2</volume>
      <fpage>e453</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lpips">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Richard</given-names></name>
        <name><surname>Isola</surname><given-names>Phillip</given-names></name>
        <name><surname>Efros</surname><given-names>Alexei A.</given-names></name>
        <name><surname>Shechtman</surname><given-names>Eli</given-names></name>
        <name><surname>Wang</surname><given-names>Oliver</given-names></name>
      </person-group>
      <article-title>Perceptual similarity metric and dataset</article-title>
      <publisher-name>Github</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/richzhang/PerceptualSimilarity</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
