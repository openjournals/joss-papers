<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4869</article-id>
<article-id pub-id-type="doi">10.21105/joss.04869</article-id>
<title-group>
<article-title>EnsembleKalmanProcesses.jl: Derivative-free
ensemble-based model calibration</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0001-7374-0382</contrib-id>
<name>
<surname>Dunbar</surname>
<given-names>Oliver R. A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">0000-0002-7255-5895</contrib-id>
<name>
<surname>Lopez-Gomez</surname>
<given-names>Ignacio</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3279-619X</contrib-id>
<name>
<surname>Garbuno-Iñigo</surname>
<given-names>Alfredo</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6072-9352</contrib-id>
<name>
<surname>Huang</surname>
<given-names>Daniel Zhengyu</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9725-0203</contrib-id>
<name>
<surname>Bach</surname>
<given-names>Eviatar</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7438-4228</contrib-id>
<name>
<surname>Wu</surname>
<given-names>Jin-long</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Division of Geological and Planetary Sciences, California
Institute of Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Statistics, Mexico Autonomous Institute of
Technology</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Mechanical Engineering, University of
Wisconsin-Madison</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-09-26">
<day>26</day>
<month>9</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>80</issue>
<fpage>4869</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>julia</kwd>
<kwd>optimization</kwd>
<kwd>bayesian</kwd>
<kwd>data assimilation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>EnsembleKalmanProcesses.jl</monospace> is a Julia-based
  toolbox that can be used for a broad class of black-box gradient-free
  optimization problems. Specifically, the tools enable the
  optimization, or calibration, of parameters within a computer model in
  order to best match user-defined outputs of the model with available
  observed data
  (<xref alt="Kennedy &amp; O’Hagan, 2001" rid="ref-kennedy_ohagan_2001" ref-type="bibr">Kennedy
  &amp; O’Hagan, 2001</xref>). Some of the tools can also approximately
  quantify parametric uncertainty
  (<xref alt="Huang, Huang, et al., 2022" rid="ref-HuangU003A2022b" ref-type="bibr">Huang,
  Huang, et al., 2022</xref>). Though the package is written in Julia
  (<xref alt="Bezanson et al., 2017" rid="ref-julia" ref-type="bibr">Bezanson
  et al., 2017</xref>), a read–write TOML-file interface is provided so
  that the tools can be applied to computer models implemented in any
  language. Furthermore, the calibration tools are non-intrusive,
  relying only on the ability of users to compute an output of their
  model given a parameter value.</p>
  <p>As the package name suggests, the tools are inspired by the
  well-established class of ensemble Kalman methods. Ensemble Kalman
  filters are currently one of the only practical ways to assimilate
  large volumes of observational data into models for operational
  weather forecasting
  (<xref alt="Evensen, 1994" rid="ref-EvensenU003A1994" ref-type="bibr">Evensen,
  1994</xref>;
  <xref alt="Houtekamer &amp; Mitchell, 1998" rid="ref-HoutekamerU003A1998" ref-type="bibr">Houtekamer
  &amp; Mitchell, 1998</xref>,
  <xref alt="2001" rid="ref-HoutekamerU003A2001" ref-type="bibr">2001</xref>).
  In the data assimilation setting, a computational weather model is
  integrated for a short time over a collection, or ensemble, of initial
  conditions, and the ensemble is updated frequently by a variety of
  atmospheric observations, allowing the forecasts to keep track of the
  real system.</p>
  <p>The workflow is similar for ensemble Kalman processes. Here, a
  computer code is run (in parallel) for an ensemble of different values
  of the parameters that require calibration, producing an ensemble of
  outputs. This ensemble of outputs is then compared to observed data,
  and the parameters are updated to a new set of values which reduce the
  output–data misfit. The process is iterated until a user-defined
  criterion of convergence is met. Optimality of the update is
  guaranteed for linear models and Gaussian uncertainties, but good
  performance is observed outside of these settings, see Schillings
  &amp; Stuart
  (<xref alt="2017" rid="ref-Schillings2017" ref-type="bibr">2017</xref>).
  Optimal values are selected from statistics of the final ensemble.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The task of estimating parameters of a computer model or simulator
  such that its outputs fit with data is ubiquitous in science and
  engineering, coming under many names such as calibration, inverse
  problems, and parameter estimation. In statistics and machine
  learning, when closed-form estimators of parameters of a model are
  unavailable, similar approaches may need to be employed to fit the
  model to data. There is a wide variety of algorithms to suit these
  applications; however, there are many bottlenecks in the practical
  application of such methods to computer codes:</p>
  <list list-type="bullet">
    <list-item>
      <p>Legacy codes: Often code is old, and written in different
      languages than the packages implementing the calibration
      algorithms, requiring elaborate interfaces.</p>
    </list-item>
    <list-item>
      <p>Complex codes: Often large complex codes are difficult to
      change, so application of intrusive calibration tools to models
      can be challenging.</p>
    </list-item>
    <list-item>
      <p>Derivatives: When derivatives of a model output can be taken
      with respect to parameters, they can often improve the rate of
      convergence. But in many practical cases, these
      parameter-to-output maps are not differentiable; they may be
      chaotic or stochastic. Here one should not – or cannot – apply
      gradient-based methods.</p>
    </list-item>
    <list-item>
      <p>Lack of parallelism: There is now widespread access to
      high-performance computing clusters, cloud computing, and local
      multi-threading, and such facilities should be exploited where
      possible.</p>
    </list-item>
  </list>
  <p><monospace>EnsembleKalmanProcesses.jl</monospace> aims to provide a
  flexible and comprehensive solution to address these challenges:</p>
  <list list-type="order">
    <list-item>
      <p>It is embarrassingly parallel with respect to the ensemble;
      therefore, all computer model evaluations within an ensemble can
      happen simultaneously within an iteration.</p>
    </list-item>
    <list-item>
      <p>It is derivative-free, and so is appropriate for computer codes
      for which derivatives are not available. The optimal updates are
      robust to noise.</p>
    </list-item>
    <list-item>
      <p>It is non-intrusive and so can be applied to black-box computer
      codes written in any language or style, or to computer models for
      which the source code is not available to the user.</p>
    </list-item>
    <list-item>
      <p>With scalability enhancements, such as the ones provided by the
      <monospace>Localizer</monospace> structure, it can be applied to
      high-dimensional problems.</p>
    </list-item>
  </list>
  <sec id="state-of-the-field">
    <title>State of the field</title>
    <p>Many gradient-based optimizers have been implemented in Julia,
    collected in <monospace>Optim.jl</monospace>
    (<xref alt="Mogensen &amp; Riseth, 2018" rid="ref-optimjl" ref-type="bibr">Mogensen
    &amp; Riseth, 2018</xref>) and
    <monospace>JuliaSmoothOptimizers.jl</monospace>, for example. Some
    gradient-free optimization tools, better suited for
    non-deterministic or noisy optimization, are collected within
    packages such as <monospace>BlackBoxOptim.jl</monospace> and
    <monospace>Metaheuristics.jl</monospace>
    (<xref alt="Mejía-de-Dios &amp; Mezura-Montes, 2022" rid="ref-metaheuristicsjl" ref-type="bibr">Mejía-de-Dios
    &amp; Mezura-Montes, 2022</xref>). Although these packages feature a
    number of ensemble-based approaches, none utilize Kalman-based
    statistical updates, and instead rely on heuristic algorithms
    inspired from biological processes such as natural selection
    (Genetic Algorithms) or swarming (Particle Swarm Optimization). A
    related class of methods to calibrate black-box computer codes are
    based on Bayesian inference, such as (Markov Chain) Monte Carlo,
    implemented in <monospace>Turing.jl</monospace>
    (<xref alt="Ge et al., 2018" rid="ref-ge2018t" ref-type="bibr">Ge et
    al., 2018</xref>), for example. Such methods provide the posterior
    distribution of parameters, from which the optimum is taken as the
    summary statistic. However, they are far more computationally
    expensive.</p>
    <p><monospace>EnsembleKalmanProcesses.jl</monospace> fills the need
    for computationally inexpensive, gradient-free,
    mathematically-grounded, ensemble-based calibration algorithms.
    Ensemble Kalman processes are provably optimal in simple settings,
    and have a large literature of extensions to complex problems.
    Although implementations of Kalman filters exist in Julia
    (<monospace>EnKF.jl</monospace>; <monospace>Kalman.jl</monospace>;
    <monospace>GaussianFilters.jl</monospace>),
    <monospace>EnsembleKalmanProcesses.jl</monospace> is the only
    package to implement ensemble-based updates for parameter
    estimation; other packages focus on state estimation from sequential
    data.</p>
  </sec>
</sec>
<sec id="features">
  <title>Features</title>
  <p>There are different ensemble Kalman algorithms in the literature,
  which differ in the way that the ensemble update is performed. The
  following ensemble Kalman processes are implemented tools in our
  package, and we provide published references for detailed descriptions
  and evidence of their efficacy:</p>
  <list list-type="bullet">
    <list-item>
      <p>Ensemble Kalman Inversion (EKI, Iglesias et al.
      (<xref alt="2013" rid="ref-IglesiasU003A2013" ref-type="bibr">2013</xref>)),</p>
    </list-item>
    <list-item>
      <p>Ensemble Kalman Sampler (EKS, Garbuno-Inigo, Hoffmann, et al.
      (<xref alt="2020" rid="ref-Garbuno-InigoU003A2020a" ref-type="bibr">2020</xref>);
      Garbuno-Inigo, Nüsken, et al.
      (<xref alt="2020" rid="ref-Garbuno-InigoU003A2020b" ref-type="bibr">2020</xref>)),</p>
    </list-item>
    <list-item>
      <p>Unscented Kalman Inversion (UKI, Huang, Schneider, et al.
      (<xref alt="2022" rid="ref-HuangU003A2022a" ref-type="bibr">2022</xref>)),</p>
    </list-item>
    <list-item>
      <p>Sparse Ensemble Kalman Inversion (SEKI, Schneider, Stuart, et
      al.
      (<xref alt="2022" rid="ref-SchneiderU003A2022" ref-type="bibr">2022</xref>)).</p>
    </list-item>
  </list>
  <p>The package also implements some features to improve robustness and
  flexibility of the ensemble algorithms:</p>
  <list list-type="bullet">
    <list-item>
      <p>The <monospace>ParameterDistribution</monospace> structure
      allows users to perform calibrations for parameters with known
      constraints. It does so by defining transformation maps
      under-the-hood from the constrained space to an unconstrained
      space where the optimization problem can be suitably defined.
      Constrained optimization using this framework has been
      successfully demonstrated in a variety of settings
      (<xref alt="Dunbar et al., 2022" rid="ref-Dunbar2022" ref-type="bibr">Dunbar
      et al., 2022</xref>;
      <xref alt="Lopez-Gomez et al., 2022" rid="ref-Lopez-GomezU003A2022" ref-type="bibr">Lopez-Gomez
      et al., 2022</xref>;
      <xref alt="Schneider, Dunbar, et al., 2022" rid="ref-Schneider2022covid" ref-type="bibr">Schneider,
      Dunbar, et al., 2022</xref>).</p>
    </list-item>
    <list-item>
      <p>The <monospace>FailureHandler</monospace> structure allows
      calibrations to continue when several ensemble members fail.
      Common reasons for failure could be, for instance, simulation
      blow-up for certain parameter configurations, user termination of
      slow computations, data corruption, or bad nodes in a
      high-performance computing facility. This methodology is
      demonstrated in Lopez-Gomez et al.
      (<xref alt="2022" rid="ref-Lopez-GomezU003A2022" ref-type="bibr">2022</xref>).</p>
    </list-item>
    <list-item>
      <p>The <monospace>Localizer</monospace> structure allows users to
      overcome the restriction of the solution of the calibration to the
      linear span of the initial ensemble, and to reduce sampling errors
      due to the finite size of the ensemble. Various such localization
      and sampling error correction methods are implemented in
      <monospace>EnsembleKalmanProcesses.jl</monospace>
      (<xref alt="Lee, 2021" rid="ref-Lee2021" ref-type="bibr">Lee,
      2021</xref>;
      <xref alt="Tong &amp; Morzfeld, 2022" rid="ref-Tong2022" ref-type="bibr">Tong
      &amp; Morzfeld, 2022</xref>).</p>
    </list-item>
    <list-item>
      <p>The TOML-file interface defined in the
      <monospace>TOMLInterface</monospace> module allows non-intrusive
      use of <monospace>EnsembleKalmanProcesses.jl</monospace> through
      files, which are widely used for configuration files and easily
      read in any programming language. Given the computer model to
      calibrate and prior distributions on the parameters,
      <monospace>EnsembleKalmanProcesses.jl</monospace> reads these
      distributions from a file and, after an iteration of the ensemble
      Kalman algorithm, writes each member of the updated ensemble to a
      parameter file. Each of these parameter files can be then read
      individually to initiate the ensemble of the computer model for
      the next iteration.</p>
    </list-item>
  </list>
</sec>
<sec id="pedagogical-example">
  <title>Pedagogical example</title>
  <p>In this example, the computer code simulates a sine curve
  <disp-formula><alternatives>
  <tex-math><![CDATA[f(A,v) = A\sin(t+\varphi) + v, \ \forall t \in [0,2\pi],]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mo>∀</mml:mo><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  with a random phase shift <inline-formula><alternatives>
  <tex-math><![CDATA[\varphi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>φ</mml:mi></mml:math></alternatives></inline-formula>
  applied to every evaluation. We define the observable map
  <disp-formula><alternatives>
  <tex-math><![CDATA[G(A,v) = [\max f(A,v) - \min f(A,v), \mathrm{mean} f(A,v)].]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mo>max</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mo>min</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle mathvariant="normal"><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi></mml:mstyle><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
  We treat <inline-formula><alternatives>
  <tex-math><![CDATA[\varphi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>φ</mml:mi></mml:math></alternatives></inline-formula>
  as a “nuisance parameter” that we are not interested in estimating,
  thus the observable map <inline-formula><alternatives>
  <tex-math><![CDATA[G(A,v)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is chosen independent of <inline-formula><alternatives>
  <tex-math><![CDATA[\varphi]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>φ</mml:mi></mml:math></alternatives></inline-formula>.
  We are given one sample measurement of <inline-formula><alternatives>
  <tex-math><![CDATA[G]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>G</mml:mi></mml:math></alternatives></inline-formula>,
  polluted by Gaussian noise <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{N}(0,\Gamma)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>𝒩</mml:mi></mml:mstyle><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>Γ</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and call this <inline-formula><alternatives>
  <tex-math><![CDATA[y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>.
  Our task is to deduce the most likely amplitude
  <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>
  and vertical shift <inline-formula><alternatives>
  <tex-math><![CDATA[v]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>
  of the curve that produced the sample <inline-formula><alternatives>
  <tex-math><![CDATA[y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>y</mml:mi></mml:math></alternatives></inline-formula>.</p>
  <p>We encode information into prior distributions over the
  parameters:</p>
  <code language="julia"># A is positive, has likely value 2 with standard deviation 1
# v has likely value 0 with standard deviation 5
prior_A = constrained_gaussian(&quot;amplitude&quot;, 2, 1, 0, Inf)
prior_v = constrained_gaussian(&quot;vert_shift&quot;, 0, 5, -Inf, Inf)
prior = combine_distributions([prior_A, prior_v])</code>
  <p>To use a basic ensemble method we need to specify the size of the
  ensemble, which we take to be <monospace>N_ensemble = 5</monospace>.
  We now initialize the problem, by drawing the initial ensemble from
  the prior and selecting the <monospace>Inversion()</monospace> tool to
  perform ensemble Kalman inversion:</p>
  <code language="julia">initial_ensemble = construct_initial_ensemble(prior, N_ensemble)
ensemble_kalman_inversion =
    EnsembleKalmanProcess(initial_ensemble, y, Γ, Inversion())</code>
  <p>Then we run the algorithm iteratively. In this case, we choose to
  perform 5 iterations:</p>
  <code language="julia">N_iterations = 5
for i in 1:N_iterations`
    # get the latest parameter ensemble
    params_i = get_phi_final(prior, ensemble_kalman_process)
    # run a simulation for each parameter in the ensemble
    G_ens = hcat([G(params_i[:, i]) for i in 1:N_ensemble]...)
    # perform the Kalman update, producing a new ensemble
    update_ensemble!(ensemble_kalman_process, G_ens)
end</code>
  <p>The initial and final ensembles are shown in
  <xref alt="Figure 1" rid="figU003Asinusoid">Figure 1</xref>, by
  evaluating <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
  at these parameters. We observe that the final sinusoid ensemble has
  greatly reduced the error in amplitude and vertical shift, despite the
  presence of the random phase shifts.</p>
  <fig>
    <caption><p>Sinusoids produced from the initial and final ensembles,
    and the sine curve that generated the data (Truth).
    <styled-content id="figU003Asinusoid"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/sinusoid_output.pdf" xlink:title="" />
  </fig>
  <p>This final ensemble determines the problem solution; for ensemble
  Kalman inversion, a best estimate of the parameters is taken as the
  mean of the final ensemble:</p>
  <code language="julia">best_parameter_estimate = get_phi_mean_final(prior, ensemble_kalman_process)</code>
  <p>The Julia code and further explanation of this example is provided
  in the documentation.</p>
</sec>
<sec id="research-projects-using-the-package">
  <title>Research projects using the package</title>
  <list list-type="bullet">
    <list-item>
      <p><monospace>EnsembleKalmanProcesses.jl</monospace> has been used
      to train physics-based and machine-learning models of atmospheric
      turbulence and convection, implemented using
      <monospace>Flux.jl</monospace> and
      <monospace>TurbulenceConvection.jl</monospace>
      (<xref alt="Lopez-Gomez et al., 2022" rid="ref-Lopez-GomezU003A2022" ref-type="bibr">Lopez-Gomez
      et al., 2022</xref>). In this application, the available model
      outputs are not differentiable with respect to the learnable
      parameters, so gradient-based optimization was not an option. In
      addition, the unscented Kalman inversion algorithm was used to
      approximately quantify parameter uncertainty.</p>
    </list-item>
    <list-item>
      <p><monospace>EnsembleKalmanProcesses.jl</monospace> features
      within Calibrate-Emulate-Sample (CES, Cleary et al.
      (<xref alt="2021" rid="ref-Cleary2021" ref-type="bibr">2021</xref>)),
      a pipeline used to accelerate parameter uncertainty quantification
      (by a factor of <inline-formula><alternatives>
      <tex-math><![CDATA[10^3]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
      - <inline-formula><alternatives>
      <tex-math><![CDATA[10^4]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:math></alternatives></inline-formula>
      with respect to Monte Carlo methods) by using statistical
      emulators. <monospace>EnsembleKalmanProcesses.jl</monospace> is
      used to choose training points for these emulators. The training
      points are naturally concentrated by the ensemble Kalman processes
      into areas of high posterior probability mass. Within CES, the
      trained emulators are used to sample this probability
      distribution, and by design are most accurate where they need to
      be. CES has been successfully used to quantify parameter
      uncertainty within the moist convection scheme of a simplified
      climate model
      (<xref alt="Dunbar et al., 2021" rid="ref-Dunbar2021" ref-type="bibr">Dunbar
      et al., 2021</xref>,
      <xref alt="2022" rid="ref-Dunbar2022" ref-type="bibr">2022</xref>;
      <xref alt="Howland et al., 2022" rid="ref-Howland2022" ref-type="bibr">Howland
      et al., 2022</xref>), within a droplet collision-coalescence
      scheme for cloud microphyiscs
      (<xref alt="Bieli et al., 2022" rid="ref-BieliU003A2022" ref-type="bibr">Bieli
      et al., 2022</xref>), and within boundary layer turbulence schemes
      for ocean modeling
      (<xref alt="Hillier, 2022" rid="ref-Hillier2022" ref-type="bibr">Hillier,
      2022</xref>).</p>
    </list-item>
    <list-item>
      <p><monospace>EnsembleKalmanProcesses.jl</monospace> has been used
      to learn hyperparameters within a machine learning tool known as
      Random Features within the Julia package
      <monospace>RandomFeatures.jl</monospace>. Here, the
      hyperparameters characterize an infinite family of functions, from
      which a finite sample is drawn to use as a basis in regression
      problems. The objective for learning the parameters is noisy and
      non-differentiable due to the random sampling, so ensemble Kalman
      processes naturally perform well in this setting.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from several others who played a role
  in the evolution of this package. These include Jake Bolewski, Navid
  Constantinou, Gregory L. Wagner, Thomas Jackson, Michael Howland,
  Melanie Bieli, and Adeline Hillier. The development of this package
  was supported by the generosity of Eric and Wendy Schmidt by
  recommendation of the Schmidt Futures program, and by the Defense
  Advanced Research Projects Agency (Agreement No. HR00112290030).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-metaheuristicsjl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mejía-de-Dios</surname><given-names>Jesús-Adolfo</given-names></name>
        <name><surname>Mezura-Montes</surname><given-names>Efrén</given-names></name>
      </person-group>
      <article-title>Metaheuristics: A Julia package for single- and multi-objective optimization</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>78</issue>
      <uri>https://doi.org/10.21105/joss.04723</uri>
      <pub-id pub-id-type="doi">10.21105/joss.04723</pub-id>
      <fpage>4723</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-optimjl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mogensen</surname><given-names>Patrick K.</given-names></name>
        <name><surname>Riseth</surname><given-names>Asbjørn N.</given-names></name>
      </person-group>
      <article-title>Optim: A mathematical optimization package for Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>24</issue>
      <uri>https://doi.org/10.21105/joss.00615</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00615</pub-id>
      <fpage>615</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM Review</source>
      <publisher-name>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>59</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1137%2F141000671</uri>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ge2018t">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ge</surname><given-names>Hong</given-names></name>
        <name><surname>Xu</surname><given-names>Kai</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
      </person-group>
      <article-title>Turing: A language for flexible probabilistic inference</article-title>
      <source>International conference on artificial intelligence and statistics, AISTATS 2018, 9-11 april 2018, playa blanca, lanzarote, canary islands, spain</source>
      <year iso-8601-date="2018">2018</year>
      <uri>http://proceedings.mlr.press/v84/ge18b.html</uri>
      <fpage>1682</fpage>
      <lpage>1690</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Lopez-GomezU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lopez-Gomez</surname><given-names>Ignacio</given-names></name>
        <name><surname>Christopoulos</surname><given-names>Costa</given-names></name>
        <name><surname>Langeland Ervik</surname><given-names>Haakon Ludvig</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Cohen</surname><given-names>Yair</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
      </person-group>
      <article-title>Training physics-based machine-learning parameterizations with gradient-free ensemble Kalman methods</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1029/2022MS003105</pub-id>
      <fpage>e2022MS003105</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HuangU003A2022a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Daniel Zhengyu</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Iterated Kalman methodology for inverse problems</article-title>
      <source>Journal of Computational Physics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>463</volume>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2022.111262</pub-id>
      <fpage>111262</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HuangU003A2022b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Daniel Zhengyu</given-names></name>
        <name><surname>Huang</surname><given-names>Jiaoyang</given-names></name>
        <name><surname>Reich</surname><given-names>Sebastian</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Efficient derivative-free Bayesian inference for large-scale inverse problems</article-title>
      <source>Inverse Problems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>38</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1088/1361-6420/ac99fa</pub-id>
      <fpage>125006</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HoutekamerU003A1998">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Houtekamer</surname><given-names>P L</given-names></name>
        <name><surname>Mitchell</surname><given-names>Herschel L</given-names></name>
      </person-group>
      <article-title>Data assimilation using an ensemble Kalman filter technique</article-title>
      <source>Monthly Weather Review</source>
      <publisher-name>American Meteorological Society</publisher-name>
      <year iso-8601-date="1998">1998</year>
      <volume>126</volume>
      <pub-id pub-id-type="doi">10.1175/1520-0493(1998)126&lt;0796:DAUAEK&gt;2.0.CO;2</pub-id>
      <fpage>796</fpage>
      <lpage>811</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EvensenU003A1994">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Evensen</surname><given-names>Geir</given-names></name>
      </person-group>
      <article-title>Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics</article-title>
      <source>Journal of Geophysical Research: Oceans</source>
      <publisher-name>John Wiley &amp; Sons, Ltd</publisher-name>
      <year iso-8601-date="1994">1994</year>
      <volume>99</volume>
      <issn>0148-0227</issn>
      <pub-id pub-id-type="doi">10.1029/94JC00572</pub-id>
      <fpage>10143</fpage>
      <lpage>10162</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HoutekamerU003A2001">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Houtekamer</surname><given-names>P L</given-names></name>
        <name><surname>Mitchell</surname><given-names>Herschel L</given-names></name>
      </person-group>
      <article-title>A sequential ensemble Kalman filter for atmospheric data assimilation</article-title>
      <source>Monthly Weather Review</source>
      <year iso-8601-date="2001">2001</year>
      <volume>129</volume>
      <pub-id pub-id-type="doi">10.1175/1520-0493(2001)129&lt;0123:ASEKFF&gt;2.0.CO;2</pub-id>
      <fpage>123</fpage>
      <lpage>137</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Garbuno-InigoU003A2020a">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Hoffmann</surname><given-names>Franca</given-names></name>
        <name><surname>Li</surname><given-names>Wuchen</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Interacting Langevin diffusions: Gradient structure and ensemble Kalman sampler</article-title>
      <source>SIAM Journal on Applied Dynamical Systems</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>19</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1137/19M1251655</pub-id>
      <fpage>412</fpage>
      <lpage>441</lpage>
    </element-citation>
  </ref>
  <ref id="ref-IglesiasU003A2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Iglesias</surname><given-names>Marco A</given-names></name>
        <name><surname>Law</surname><given-names>Kody JH</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Ensemble Kalman methods for inverse problems</article-title>
      <source>Inverse Problems</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>29</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1088/0266-5611/29/4/045001</pub-id>
      <fpage>045001</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Garbuno-InigoU003A2020b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Nüsken</surname><given-names>Nikolas</given-names></name>
        <name><surname>Reich</surname><given-names>Sebastian</given-names></name>
      </person-group>
      <article-title>Affine invariant interacting Langevin dynamics for Bayesian inference</article-title>
      <source>SIAM Journal on Applied Dynamical Systems</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>19</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1137/19M1304891</pub-id>
      <fpage>1633</fpage>
      <lpage>1658</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SchneiderU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
        <name><surname>Wu</surname><given-names>Jin-Long</given-names></name>
      </person-group>
      <article-title>Ensemble Kalman inversion for sparse learning of dynamical systems from time-averaged data</article-title>
      <source>Journal of Computational Physics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2022.111559</pub-id>
      <fpage>111559</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kennedy_ohagan_2001">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kennedy</surname><given-names>Marc</given-names></name>
        <name><surname>O’Hagan</surname><given-names>Anthony</given-names></name>
      </person-group>
      <article-title>Bayesian calibration of computer models</article-title>
      <source>Journal of the Royal Statistical Society Series B</source>
      <year iso-8601-date="2001">2001</year>
      <volume>63</volume>
      <pub-id pub-id-type="doi">10.1111/1467-9868.00294</pub-id>
      <fpage>425</fpage>
      <lpage>464</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Dunbar2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Howland</surname><given-names>Michael F</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M</given-names></name>
      </person-group>
      <article-title>Ensemble-based experimental design for targeting data acquisition to inform climate models</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <publisher-name>John Wiley &amp; Sons, Ltd</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>9</issue>
      <issn>1942-2466</issn>
      <pub-id pub-id-type="doi">10.1029/2022MS002997</pub-id>
      <fpage>e2022MS002997</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Dunbar2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Calibration and uncertainty quantification of convective parameters in an idealized GCM</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>13</volume>
      <issue>9</issue>
      <pub-id pub-id-type="doi">10.1029/2020MS002454</pub-id>
      <fpage>e2020MS002454</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Howland2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Howland</surname><given-names>Michael F.</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
      </person-group>
      <article-title>Parameter uncertainty quantification in an idealized GCM with a seasonal cycle</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1029/2021MS002735</pub-id>
      <fpage>e2021MS002735</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Schneider2022covid">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Wu</surname><given-names>Jinlong</given-names></name>
        <name><surname>Böttcher</surname><given-names>Lucas</given-names></name>
        <name><surname>Burov</surname><given-names>Dmitry</given-names></name>
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Wagner</surname><given-names>Gregory L.</given-names></name>
        <name><surname>Pei</surname><given-names>Sen</given-names></name>
        <name><surname>Daraio</surname><given-names>Chiara</given-names></name>
        <name><surname>Ferrari</surname><given-names>Raffaele</given-names></name>
        <name><surname>Shaman</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>Epidemic management and control through risk-dependent individual contact interventions</article-title>
      <source>PLOS Computational Biology</source>
      <person-group person-group-type="editor">
        <name><surname>Moreno</surname><given-names>Yamir</given-names></name>
      </person-group>
      <publisher-name>Public Library of Science</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>18</volume>
      <issue>6</issue>
      <issn>1553-7358</issn>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1010171</pub-id>
      <fpage>e1010171</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Lee2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Yoonsang</given-names></name>
      </person-group>
      <article-title>Sampling error correction in ensemble Kalman inversion</article-title>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arxiv.2105.11341</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Tong2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tong</surname><given-names>Xin T.</given-names></name>
        <name><surname>Morzfeld</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>Localization in ensemble Kalman inversion</article-title>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2201.10821</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-BieliU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bieli</surname><given-names>Melanie</given-names></name>
        <name><surname>Dunbar</surname><given-names>Oliver R. A.</given-names></name>
        <name><surname>Jong</surname><given-names>Emily K. de</given-names></name>
        <name><surname>Jaruga</surname><given-names>Anna</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Bischoff</surname><given-names>Tobias</given-names></name>
      </person-group>
      <article-title>An efficient Bayesian approach to learning droplet collision kernels: Proof of concept using “Cloudy,” a new n-moment bulk microphysics scheme</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1029/2022MS002994</pub-id>
      <fpage>e2022MS002994</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cleary2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cleary</surname><given-names>Emmet</given-names></name>
        <name><surname>Garbuno-Inigo</surname><given-names>Alfredo</given-names></name>
        <name><surname>Lan</surname><given-names>Shiwei</given-names></name>
        <name><surname>Schneider</surname><given-names>Tapio</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Calibrate, emulate, sample</article-title>
      <source>Journal of Computational Physics</source>
      <year iso-8601-date="2021">2021</year>
      <volume>424</volume>
      <issn>0021-9991</issn>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2020.109716</pub-id>
      <fpage>109716</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Hillier2022">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Hillier</surname><given-names>Adeline</given-names></name>
      </person-group>
      <article-title>Supervised calibration and uncertainty quantification of subgrid closure parameters using ensemble Kalman inversion</article-title>
      <publisher-name>Massachusetts Institute of Technology. Department of Electrical Engineering; Computer Science</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://hdl.handle.net/1721.1/145140</uri>
    </element-citation>
  </ref>
  <ref id="ref-Schillings2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schillings</surname><given-names>Claudia</given-names></name>
        <name><surname>Stuart</surname><given-names>Andrew M.</given-names></name>
      </person-group>
      <article-title>Analysis of the ensemble Kalman filter for inverse problems</article-title>
      <source>SIAM Journal on Numerical Analysis</source>
      <year iso-8601-date="2017">2017</year>
      <volume>55</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1137/16M105959X</pub-id>
      <fpage>1264</fpage>
      <lpage>1290</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
