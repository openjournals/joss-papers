<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20231120T173622-f5cd4f87feb07d150e64a08e4dfe057a8a799847</doi_batch_id>
    <timestamp>20231120173622</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>11</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>91</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Software Design and User Interface of ESPnet-SE++:
Speech Enhancement for Robust Speech Processing</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Yen-Ju</given_name>
            <surname>Lu</surname>
            <ORCID>https://orcid.org/0000-0001-8400-4188</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Xuankai</given_name>
            <surname>Chang</surname>
            <ORCID>https://orcid.org/0000-0002-5221-5412</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Chenda</given_name>
            <surname>Li</surname>
            <ORCID>https://orcid.org/0000-0003-0299-9914</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Wangyou</given_name>
            <surname>Zhang</surname>
            <ORCID>https://orcid.org/0000-0003-4500-3515</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Samuele</given_name>
            <surname>Cornell</surname>
            <ORCID>https://orcid.org/0000-0002-5358-1844</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Zhaoheng</given_name>
            <surname>Ni</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yoshiki</given_name>
            <surname>Masuyama</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Brian</given_name>
            <surname>Yan</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Robin</given_name>
            <surname>Scheibler</surname>
            <ORCID>https://orcid.org/0000-0002-5205-8365</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Zhong-Qiu</given_name>
            <surname>Wang</surname>
            <ORCID>https://orcid.org/0000-0002-4204-9430</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yu</given_name>
            <surname>Tsao</surname>
            <ORCID>https://orcid.org/0000-0001-6956-0418</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yanmin</given_name>
            <surname>Qian</surname>
            <ORCID>https://orcid.org/0000-0002-0314-3790</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Shinji</given_name>
            <surname>Watanabe</surname>
            <ORCID>https://orcid.org/0000-0002-5970-8631</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>11</month>
          <day>20</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5403</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05403</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.10048174</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5403</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05403</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05403</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05403.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="Li:2021">
            <article_title>ESPnet-SE: End-to-end speech enhancement and
separation toolkit designed for ASR integration</article_title>
            <author>Li</author>
            <journal_title>2021 IEEE spoken language technology workshop
(SLT)</journal_title>
            <doi>10.1109/slt48900.2021.9383615</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Li, C., Shi, J., Zhang, W.,
Subramanian, A. S., Chang, X., Kamo, N., Hira, M., Hayashi, T.,
Boeddeker, C., &amp; Chen, S., Z. Watanabe. (2021). ESPnet-SE:
End-to-end speech enhancement and separation toolkit designed for ASR
integration. 2021 IEEE Spoken Language Technology Workshop (SLT),
785–792.
https://doi.org/10.1109/slt48900.2021.9383615</unstructured_citation>
          </citation>
          <citation key="Hershey:2016">
            <article_title>Deep clustering: Discriminative embeddings
for segmentation and separation</article_title>
            <author>Hershey</author>
            <journal_title>2016 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp.2016.7471631</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Hershey, J. R., Chen, Z., Le Roux,
J., &amp; Watanabe, S. (2016). Deep clustering: Discriminative
embeddings for segmentation and separation. 2016 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), 31–35.
https://doi.org/10.1109/icassp.2016.7471631</unstructured_citation>
          </citation>
          <citation key="Chen:2017">
            <article_title>Deep attractor network for single-microphone
speaker separation</article_title>
            <author>Chen</author>
            <journal_title>2017 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp.2017.7952155</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Chen, Z., Luo, Y., &amp; Mesgarani,
N. (2017). Deep attractor network for single-microphone speaker
separation. 2017 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP), 246–250.
https://doi.org/10.1109/icassp.2017.7952155</unstructured_citation>
          </citation>
          <citation key="Hu:2020">
            <article_title>DCCRN: Deep complex convolution recurrent
network for phase-aware speech enhancement</article_title>
            <author>Hu</author>
            <journal_title>Proceedings of interspeech</journal_title>
            <doi>10.21437/interspeech.2020-2537</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Hu, Y., Liu, Y., Lv, S., Xing, M.,
Zhang, S., Fu, Y., Wu, J., Zhang, B., &amp; Xie, L. (2020). DCCRN: Deep
complex convolution recurrent network for phase-aware speech
enhancement. Proceedings of Interspeech, 2472–2476.
https://doi.org/10.21437/interspeech.2020-2537</unstructured_citation>
          </citation>
          <citation key="Tan:2021">
            <article_title>Deep learning based real-time speech
enhancement for dual-microphone mobile phones</article_title>
            <author>Tan</author>
            <journal_title>IEEE/ACM Transactions on Audio, Speech, and
Language Processing</journal_title>
            <volume>29</volume>
            <doi>10.1109/taslp.2021.3082318</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Tan, K., Zhang, X., &amp; Wang, D.
(2021). Deep learning based real-time speech enhancement for
dual-microphone mobile phones. IEEE/ACM Transactions on Audio, Speech,
and Language Processing, 29, 1853–1863.
https://doi.org/10.1109/taslp.2021.3082318</unstructured_citation>
          </citation>
          <citation key="Li:2022">
            <article_title>SkiM: Skipping memory lstm for low-latency
real-time continuous speech separation</article_title>
            <author>Li</author>
            <journal_title>2022 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp43922.2022.9746372</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Li, C., Yang, L., Wang, W., &amp;
Qian, Y. (2022). SkiM: Skipping memory lstm for low-latency real-time
continuous speech separation. 2022 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 681–685.
https://doi.org/10.1109/icassp43922.2022.9746372</unstructured_citation>
          </citation>
          <citation key="Dang:2022">
            <article_title>DPT-FSNet: Dual-path transformer based
full-band and sub-band fusion network for speech
enhancement</article_title>
            <author>Dang</author>
            <journal_title>2022 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp43922.2022.9746171</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Dang, F., Chen, H., &amp; Zhang, P.
(2022). DPT-FSNet: Dual-path transformer based full-band and sub-band
fusion network for speech enhancement. 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
6857–6861.
https://doi.org/10.1109/icassp43922.2022.9746171</unstructured_citation>
          </citation>
          <citation key="Takahashi:2019">
            <article_title>Recursive speech separation for unknown
number of speakers</article_title>
            <author>Takahashi</author>
            <journal_title>Interspeech 2019</journal_title>
            <doi>10.21437/interspeech.2019-1550</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Takahashi, N., Parthasaarathy, S.,
Goswami, N., &amp; Mitsufuji, Y. (2019). Recursive speech separation for
unknown number of speakers. Interspeech 2019, 1348–1352.
https://doi.org/10.21437/interspeech.2019-1550</unstructured_citation>
          </citation>
          <citation key="Luo:2019">
            <article_title>FaSNet: Low-latency adaptive beamforming for
multi-microphone audio processing</article_title>
            <author>Luo</author>
            <journal_title>2019 IEEE automatic speech recognition and
understanding workshop (ASRU)</journal_title>
            <doi>10.1109/asru46091.2019.9003849</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Luo, Y., Han, C., Mesgarani, N.,
Ceolini, E., &amp; Liu, S. (2019). FaSNet: Low-latency adaptive
beamforming for multi-microphone audio processing. 2019 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU), 260–267.
https://doi.org/10.1109/asru46091.2019.9003849</unstructured_citation>
          </citation>
          <citation key="Lu:2022a">
            <article_title>Towards low-distortion multi-channel speech
enhancement: The ESPNET-se submission to the L3DAS22
challenge</article_title>
            <author>Lu</author>
            <journal_title>2022 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp43922.2022.9747146</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Lu, Y. J., Cornell, S., Chang, X.,
Zhang, W., Li, C., Ni, Z., Wang, Z., &amp; Watanabe, S. (2022). Towards
low-distortion multi-channel speech enhancement: The ESPNET-se
submission to the L3DAS22 challenge. 2022 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), 9201–9205.
https://doi.org/10.1109/icassp43922.2022.9747146</unstructured_citation>
          </citation>
          <citation key="Luo:2018">
            <article_title>TaSNet: Time-domain audio separation network
for real-time, single-channel speech separation</article_title>
            <author>Luo</author>
            <journal_title>2018 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp.2018.8462116</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Luo, Y., &amp; Mesgarani, N. (2018).
TaSNet: Time-domain audio separation network for real-time,
single-channel speech separation. 2018 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP), 696–700.
https://doi.org/10.1109/icassp.2018.8462116</unstructured_citation>
          </citation>
          <citation key="Le:2019">
            <article_title>SDR half-baked or well done?</article_title>
            <author>Le Roux</author>
            <journal_title>2019 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp.2019.8683855</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Le Roux, J., Wisdom, S., Erdogan, H.,
&amp; Hershey, J. R. (2019). SDR half-baked or well done? 2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 626–630.
https://doi.org/10.1109/icassp.2019.8683855</unstructured_citation>
          </citation>
          <citation key="Boeddeker:2021">
            <article_title>Convolutive transfer function invariant SDR
training criteria for multi-channel reverberant speech
separation</article_title>
            <author>Boeddeker</author>
            <journal_title>2021 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp39728.2021.9414661</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Boeddeker, C., Zhang, W., Nakatani,
T., Kinoshita, K., Ochiai, T., Delcroix, M., Kamo, N., Qian, Y., &amp;
Haeb-Umbach, R. (2021). Convolutive transfer function invariant SDR
training criteria for multi-channel reverberant speech separation. 2021
IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), 8428–8432.
https://doi.org/10.1109/icassp39728.2021.9414661</unstructured_citation>
          </citation>
          <citation key="Scheibler:2022">
            <article_title>SDR medium rare with fast
computations</article_title>
            <author>Scheibler</author>
            <journal_title>2022 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp43922.2022.9747473</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Scheibler, R. (2022). SDR medium rare
with fast computations. 2022 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), 701–705.
https://doi.org/10.1109/icassp43922.2022.9747473</unstructured_citation>
          </citation>
          <citation key="Lu:2022b">
            <article_title>ESPnet-SE++: Speech enhancement for robust
speech recognition, translation, and understanding</article_title>
            <author>Lu</author>
            <journal_title>Proceedings of interspeech</journal_title>
            <doi>10.21437/interspeech.2022-10727</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Lu, Y. J., Chang, X., Li, C., Zhang,
W., Cornell, S., Ni, Z., Masuyama, Y., Yan, B., Scheibler, R., Wang, Z.
Q., Tsao, Y., &amp; Qian Y. Watanabe, S. (2022). ESPnet-SE++: Speech
enhancement for robust speech recognition, translation, and
understanding. Proceedings of Interspeech, 5458–5462.
https://doi.org/10.21437/interspeech.2022-10727</unstructured_citation>
          </citation>
          <citation key="Hayashi:2020">
            <article_title>ESPnet-TTS: Unified, reproducible, and
integratable open source end-to-end text-to-speech
toolkit</article_title>
            <author>Hayashi</author>
            <journal_title>2020 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp40776.2020.9053512</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Hayashi, T., Yamamoto, R., Inoue, K.,
Yoshimura, T., Watanabe, S., Toda, T., Takeda, K., &amp; Zhang, X., Y.
Tan. (2020). ESPnet-TTS: Unified, reproducible, and integratable open
source end-to-end text-to-speech toolkit. 2020 IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP),
7654–7658.
https://doi.org/10.1109/icassp40776.2020.9053512</unstructured_citation>
          </citation>
          <citation key="Inaguma:2020">
            <article_title>ESPnet-ST: All-in-one speech translation
toolkit</article_title>
            <author>Inaguma</author>
            <journal_title>Proceedings of the 58th annual meeting of the
association for computational linguistics: System
demonstrations</journal_title>
            <doi>10.18653/v1/2020.acl-demos.34</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Inaguma, H., Kiyono, S., Duh, K.,
Karita, S., Soplin, N. E. Y., Hayashi, T., &amp; Watanabe, S. (2020).
ESPnet-ST: All-in-one speech translation toolkit. Proceedings of the
58th Annual Meeting of the Association for Computational Linguistics:
System Demonstrations, 302–311.
https://doi.org/10.18653/v1/2020.acl-demos.34</unstructured_citation>
          </citation>
          <citation key="Arora:2022">
            <article_title>ESPnet-SLU: Advancing spoken language
understanding through ESPnet</article_title>
            <author>Arora</author>
            <journal_title>2022 IEEE international conference on
acoustics, speech and signal processing (ICASSP)</journal_title>
            <doi>10.1109/icassp43922.2022.9747674</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Arora, S., Dalmia, S., Denisov, P.,
Chang, X., Ueda, Y., Peng, Y., Zhang, Y., Kumar, S., Ganesan, K., &amp;
Yan, W., B. (2022). ESPnet-SLU: Advancing spoken language understanding
through ESPnet. 2022 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), 7167–7171.
https://doi.org/10.1109/icassp43922.2022.9747674</unstructured_citation>
          </citation>
          <citation key="Watanabe:2018">
            <article_title>ESPnet: End-to-end speech processing
toolkit</article_title>
            <author>Watanabe</author>
            <journal_title>Proceedings of interspeech</journal_title>
            <doi>10.21437/interspeech.2018-1456</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Watanabe, S., Hori, T., Karita, S.,
Hayashi, T., Nishitoba, J., Unno, Y., Soplin, N. E. Y., Heymann, J.,
Wiesner, M., Chen, N., Renduchintala, A., &amp; Ochiai, T. (2018).
ESPnet: End-to-end speech processing toolkit. Proceedings of
Interspeech, 2207–2211.
https://doi.org/10.21437/interspeech.2018-1456</unstructured_citation>
          </citation>
          <citation key="Manilow:2018">
            <article_title>The northwestern university source separation
library.</article_title>
            <author>Manilow</author>
            <journal_title>International society for music information
retrieval (ISMIR)</journal_title>
            <doi>10.1163/1872-9037_afco_asc_1322</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Manilow, E., Seetharaman, P., &amp;
Pardo, B. (2018). The northwestern university source separation library.
International Society for Music Information Retrieval (ISMIR), 297–305.
https://doi.org/10.1163/1872-9037_afco_asc_1322</unstructured_citation>
          </citation>
          <citation key="Ni:2019">
            <article_title>ONSSEN: An open-source speech separation and
enhancement library</article_title>
            <author>Ni</author>
            <journal_title>arXiv preprint
arXiv:1911.00982</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Ni, M. I., Zhaoheng Mandel. (2019).
ONSSEN: An open-source speech separation and enhancement library. arXiv
Preprint arXiv:1911.00982.</unstructured_citation>
          </citation>
          <citation key="Pariente:2020">
            <article_title>Asteroid: The PyTorch-based audio source
separation toolkit for researchers</article_title>
            <author>Pariente</author>
            <journal_title>Proceedings of interspeech</journal_title>
            <doi>10.21437/interspeech.2020-1673</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Pariente, M., Cornell, S., Cosentino,
J., Sivasankaran, S., Tzinis, E., Heitkaemper, J., Olvera, M., Stöter,
F. R., Hu, M., Martı́n-Doñas, J. M., Ditter, D., Frank, A., Deleforge,
A., &amp; Vincent, E. (2020). Asteroid: The PyTorch-based audio source
separation toolkit for researchers. Proceedings of Interspeech,
2637–2641.
https://doi.org/10.21437/interspeech.2020-1673</unstructured_citation>
          </citation>
          <citation key="Ravanelli:2021">
            <article_title>SpeechBrain: A general-purpose speech
toolkit</article_title>
            <author>Ravanelli</author>
            <journal_title>arXiv preprint
arXiv:2106.04624</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Ravanelli, M., Parcollet, T.,
Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C.,
Dawalatabad, N., Heba, A., Zhong, J., Chou, J. C., Yeh, S. L., Fu, S.
W., Liao, C. F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao,
Y., &amp; Mori R. D. Bengio, Y. (2021). SpeechBrain: A general-purpose
speech toolkit. arXiv Preprint arXiv:2106.04624.</unstructured_citation>
          </citation>
          <citation key="Povey:2011">
            <article_title>The Kaldi speech recognition
toolkit</article_title>
            <author>Povey</author>
            <journal_title>IEEE 2011 workshop on automatic speech
recognition and understanding</journal_title>
            <doi>10.15199/48.2016.11.70</doi>
            <cYear>2011</cYear>
            <unstructured_citation>Povey, D., Ghoshal, A., Boulianne,
G., Burget, L., Glembek, O., Goel, N., Hannemann, M., Motlicek, P.,
Qian, Y., Schwarz, P., Silovsky´, J., &amp; Stemmer, K., G. Vesely.
(2011). The Kaldi speech recognition toolkit. IEEE 2011 Workshop on
Automatic Speech Recognition and Understanding.
https://doi.org/10.15199/48.2016.11.70</unstructured_citation>
          </citation>
          <citation key="Taal:2011">
            <article_title>An algorithm for intelligibility prediction
of time–frequency weighted noisy speech</article_title>
            <author>Taal</author>
            <journal_title>IEEE Transactions on Audio, Speech, and
Language Processing</journal_title>
            <issue>7</issue>
            <volume>19</volume>
            <doi>10.1109/tasl.2011.2114881</doi>
            <cYear>2011</cYear>
            <unstructured_citation>Taal, C. H., Hendriks, R. C.,
Heusdens, R., &amp; Jensen, J. (2011). An algorithm for intelligibility
prediction of time–frequency weighted noisy speech. IEEE Transactions on
Audio, Speech, and Language Processing, 19(7), 2125–2136.
https://doi.org/10.1109/tasl.2011.2114881</unstructured_citation>
          </citation>
          <citation key="Rix:2001">
            <article_title>Perceptual evaluation of speech quality
(PESQ)-a new method for speech quality assessment of telephone networks
and codecs</article_title>
            <author>Rix</author>
            <journal_title>2001 IEEE international conference on
acoustics, speech, and signal processing. Proceedings (cat. No.
01CH37221)</journal_title>
            <volume>2</volume>
            <doi>10.1109/icassp.2001.941023</doi>
            <cYear>2001</cYear>
            <unstructured_citation>Rix, A. W., Beerends, J. G., Hollier,
M. P., &amp; Hekstra, A. P. (2001). Perceptual evaluation of speech
quality (PESQ)-a new method for speech quality assessment of telephone
networks and codecs. 2001 IEEE International Conference on Acoustics,
Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221), 2,
749–752.
https://doi.org/10.1109/icassp.2001.941023</unstructured_citation>
          </citation>
          <citation key="Towns:2014">
            <article_title>XSEDE: Accelerating scientific
discovery</article_title>
            <author>Towns</author>
            <journal_title>Computing in Science &amp;
Engineering</journal_title>
            <issue>5</issue>
            <volume>16</volume>
            <doi>10.1109/mcse.2014.80</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Towns, J., Cockerill, T., Dahan, M.,
Foster, I., Gaither, K., Grimshaw, A., Hazlewood, V., Lathrop, S.,
Lifka, D., Peterson, G. D., Roskies, R., Scott, J. R., &amp;
Wilkins-Diehr, N. (2014). XSEDE: Accelerating scientific discovery.
Computing in Science &amp; Engineering, 16(5), 62–74.
https://doi.org/10.1109/mcse.2014.80</unstructured_citation>
          </citation>
          <citation key="Nystrom:2015">
            <article_title>Bridges: A uniquely flexible HPC resource for
new communities and data analytics</article_title>
            <author>Nystrom</author>
            <journal_title>Proceedings of the 2015 XSEDE conference:
Scientific advancements enabled by enhanced
cyberinfrastructure</journal_title>
            <doi>10.1145/2792745.2792775</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Nystrom, N. A., Levine, M. J.,
Roskies, R. Z., &amp; Scott, J. R. (2015). Bridges: A uniquely flexible
HPC resource for new communities and data analytics. Proceedings of the
2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced
Cyberinfrastructure, 1–8.
https://doi.org/10.1145/2792745.2792775</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
