<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3804</article-id>
<article-id pub-id-type="doi">10.21105/joss.03804</article-id>
<title-group>
<article-title>mctreesearch4j: A Monte Carlo Tree Search Implementation
for the JVM</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9375-1035</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Larkin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2681-8922</contrib-id>
<name>
<surname>Luo</surname>
<given-names>Jun Tao</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Toronto</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Carnegie Mellon University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-08-30">
<day>30</day>
<month>8</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>70</issue>
<fpage>3804</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Java Virtual Machine</kwd>
<kwd>Monte Carlo Tree Search</kwd>
<kwd>Markov Decision Process</kwd>
<kwd>Software Design</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>We introduce <italic>mctreesearch4j</italic>, a Monte Carlo Tree
  Search (MCTS) implementation written as a standard JVM (Java Virtual
  Machine) library following key object oriented programming design
  principles. This modular and extensibile implementation of MCTS
  provides a powerful tool to enable the discovery of approximate
  solutions to complex planning problems via rapid experimentation.
  <italic>mctreesearch4j</italic> utilizes class inheritance and generic
  types to standardize custom algorithm definitions. In addition, key
  class abstractions are designed for the library to flexibly adapt to
  any well-defined Markov Decision Process (MDP) or turn-based
  adversarial games. Furthermore, <italic>mctreesearch4j</italic> is
  capable of customization across a variety of MDP domains, consequently
  enabling the adoption of MCTS heuristics and customization into the
  core library with ease.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Open source implementations of Monte Carlo Tree Search exist, but
  have not gained widespread adoption. Implementations of MCTS have been
  written for Java Virtual Machine (JVM), C/C++, and Python, yet many do
  not provide easy access to heuristics, nor do they implement
  extensible and modular state action space abstractions
  (<xref alt="Cowling et al., 2012" rid="ref-CowlingU003A2012" ref-type="bibr">Cowling
  et al., 2012</xref>)
  (<xref alt="Lucas, 2018" rid="ref-LucasU003A2018" ref-type="bibr">Lucas,
  2018</xref>). <italic>mctreesearch4j</italic> aims to build on the
  performance advantages of a compiled language families, such as JVM,
  while simultaneously being extensible and modular. Therefore, many
  improvements and research experiments could be performed by extending
  and modifying the capabilities of the base software.
  <italic>mctreesearch4j</italic> is designed to enable researchers to
  experiment with various MCTS strategies while standardizing the
  functionality of the MCTS solver and ensuring reliability, where the
  experiments are reproducible and the solver is compatible with common
  JVM runtimes.</p>
  <p>In the domain of probabilistic planning, extensible, reproducible,
  and modular software packages do exist in the solution space of
  Reinforcement Learning. We take inspiration from software libraries
  such as Acme
  (<xref alt="Hoffman et al., 2020" rid="ref-acmeU003A2020" ref-type="bibr">Hoffman
  et al., 2020</xref>) and Tensorforce
  (<xref alt="Schaarschmidt et al., 2018" rid="ref-tensorforceU003A2018" ref-type="bibr">Schaarschmidt
  et al., 2018</xref>), notably from its design concept. Such libraries
  function as a domain independent software library to generate
  solutions for a variety of MDP’s. The MDP’s are defined separate from
  the core library, and communicate via a common interface. Such a
  design concept has also been incorporated to
  <italic>mctreesearch4j</italic> within the solution space of MCTS for
  probabilistic and deterministic planning.</p>
</sec>
<sec id="monte-carlo-tree-search">
  <title>Monte Carlo Tree Search</title>
  <p>Monte Carlo Tree Search primarily makes use of a deterministic
  selection of actions and stochastic simulations to estimate the reward
  function of well defined Markov Decision Process (MDP). MCTS is a tree
  search adaptation of the UCB1 Multi-Armed Bandit Strategy
  (<xref alt="Auer et al., 2002" rid="ref-AuerU003A2002" ref-type="bibr">Auer
  et al., 2002</xref>). In more detail, the MAB does not change state
  with each action taken, and is primarily focused on optimizing
  exploration - exploring new actions - versus exploitation - obtaining
  rewards from known actions. MCTS, is the extension, where we optimize
  for exploration vs. exploitation for dynamic states contingent on any
  actions taken in the action space. We present a more detailed
  discussion in
  (<xref alt="Liu &amp; Luo, 2021" rid="ref-LiuU003A2021" ref-type="bibr">Liu
  &amp; Luo, 2021</xref>).</p>
  <p>The MCTS algorithm is distinctly divided into 4-phases,
  <italic>Selection</italic>, <italic>Expansion</italic>,
  <italic>Simulation</italic>, and <italic>Backpropagation</italic>,
  which are illustrated in Fig.
  <xref alt="1" rid="figU003Amcts-diagram">1</xref>. In
  <italic>Selection</italic>, a policy deterministically selects which
  action to play, based on previously expanded states, guided by some
  exploration constant, <italic>C</italic>. In the
  <italic>Expansion</italic> phase, states that are unexplored,
  represented by a leaf node, are added to the search tree.
  Subsequently, in the <italic>Simulation</italic> phase, a simulation
  is stochastically played out. Finally,
  <italic>Backpropagation</italic> propagates the final reward of either
  a terminal state, or a node at a specified depth limit, back to the
  root node. This 4-phase process is repeated until a maximum number of
  iterations or a convergence criteria is established.</p>
  <fig>
    <caption><p>Monte Carlo Tree Search Key Mechanisms.
    (<xref alt="Browne et al., 2012" rid="ref-BrowneU003A2012" ref-type="bibr">Browne
    et al., 2012</xref>)
    <styled-content id="figU003Amcts-diagram"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/mcts-diagram-v2.png" xlink:title="Title" />
  </fig>
</sec>
<sec id="design-principles">
  <title>Design Principles</title>
  <p><italic>mctreesearch4j</italic> is designed follow three key design
  principles.</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Adaptability</bold>: Adaptability is defined as the
      ability for MDP domain to be easily integrated into the
      <italic>mctreesearch4j</italic> framework using provided class
      abstractions. Our implementation seeks to simplify the adoption of
      MCTS solutions for a variety of domains.</p>
    </list-item>
    <list-item>
      <p><bold>JVM Compatibility</bold>: We design a library that is
      fully compatible with the Java Virtual Machine (JVM), and
      consequently functional with any JVM languages, ie. Java, Scala,
      Kotlin etc.</p>
    </list-item>
    <list-item>
      <p><bold>Extensibility</bold> We design to achieve a high degree
      of extensibility and modularity within the framework.
      Extensibility is defined as the ability for key mechanisms to be
      reused, redefined, and enhanced, without sacrificing
      interoperability.</p>
    </list-item>
  </list>
  <p><italic>mctreesearch4j</italic> serves as a framework to
  standardize the development of Monte Carlo Tree Search development in
  a research setting, while simultaneously serving as a production-ready
  software for deployment of MCTS for the JVM ecosystem.
  <italic>mctreesearch4j</italic> is specifically designed to be
  extensible, standardized, and modular. This standardization is key to
  enabling scientists and engineers to better reproduce results of AI
  research, and furthermore reduce the iteration speed of research into
  novel algorithms in AI.</p>
</sec>
<sec id="domain-abstraction">
  <title>Domain Abstraction</title>
  <p>The main abstraction that is used to define an MDP problem is the
  abstract class defined in <italic>MDP Abstraction</italic>, using
  generic type variables. Each of the methods correspond to specific
  behaviour of a discrete MDP. In <italic>mctreesearch4j</italic> we use
  generic types <monospace>StateType</monospace> and
  <monospace>ActionType</monospace> to represent the MDP states and
  actions respectively. This abstract class has five members that must
  be implemented. These abstract class methods define the functionality
  of an MDP. The MDP abstraction will be used by MCTS solvers to compute
  the optimal policy. The MDP interface can be written in any JVM
  language, we use Kotlin and Scala for this paper, with the Scala
  implementation from
  (<xref alt="Liu, 2021" rid="ref-LiuU003A2021-connect4" ref-type="bibr">Liu,
  2021</xref>).</p>
  <sec id="mdp-abstraction">
    <title>MDP Abstraction</title>
    <code language="kotlin">abstract class MDP&lt;StateType, ActionType&gt; {
    abstract fun transition(StateType, ActionType) : StateType
    abstract fun reward(StateType, ActionType?, StateType) : Double
    abstract fun initialState() : StateType
    abstract fun isTerminal(StateType) : Boolean
    abstract fun actions(StateType) : Collection&lt;ActionType&gt;</code>
  </sec>
  <sec id="solver-design">
    <title>Solver Design</title>
    <p><italic>mctreesearch4j</italic> provides a default implementation
    known as <monospace>GenericSolver</monospace>, and an alternate
    <monospace>StatefulSolver</monospace>. The abstract
    <monospace>Solver</monospace> serves as the base class for both
    versions, and defines a set of functionalities that all solver
    implementations must provide as well as a set of extensible
    utilities. Similar to the MDP abstraction, the solver uses a set of
    type parameters to provide strongly typed methods that unify and
    simplify the solver implementation. The programmer is free to select
    the data type or data structure that best defines how states and
    actions are represented in their MDP domain.</p>
    <p>The difference between solvers lies in their respective memory
    utilization of abstract node types to track states during MCTS
    iterations. The default <monospace>GenericSolver</monospace>
    provides a leaner implementation, where actions are tracked and no
    explicit states are stored permanently. The states tracked with
    <monospace>GenericSolver</monospace> are dynamic and the MDP
    transitions must be rerun when traversing the search tree during
    selection in order to infer the state. The
    <monospace>StatefulSolver</monospace> keeps an explicit record of
    the visited states, and additional information, such as terminality
    and availability of downstream actions. The extra overhead of
    storing the state explicitly in the MCTS node, allows the algorithm
    to optimize its search using information from previously visited
    states. This is particularly useful for deterministic games, where a
    re-computation of the MDP transition is not necessary to determine
    the state of the agent after taking a specific action. This results
    in different implementations of the <italic>Selection</italic> step,
    while maintaining identical implementations of
    <italic>Expansion</italic>, <italic>Simulate</italic> and
    <italic>Backpropagation</italic>.</p>
    <fig>
      <caption><p>Software Architecture.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/software_design_mcts.png" xlink:title="Title" />
    </fig>
  </sec>
</sec>
<sec id="customization">
  <title>Customization</title>
  <p><italic>mctreesearch4j</italic> maintains a high degree of
  modularity. For example, the key mechanisms highlighted in Fig.
  <xref alt="1" rid="figU003Amcts-diagram">1</xref> can be rearranged,
  allowing scientists to experiment with different versions of MCTS.
  Furthermore, the internal functionality of each of the key mechanisms
  can be redefined. And complex meta heuristics and search algorithms
  can be implemented to add specific advantages to the solver for
  specific MDP domains. All of these features are important to give
  researchers the flexibility to design bespoke MCTS algorithms for
  their respective research.</p>
  <p>Though the default MCTS implementation works well in many
  scenarios, there are situations where knowledge about specific problem
  domains can be applied to improve the MCTS performance. Improvements
  to MCTS, such as heuristics driven simulation, exploit domain
  knowledge to improve solver performance. We demonstrate that a Reversi
  AI that uses heuristics derived from
  (<xref alt="Guenther &amp; Klinik, 2004" rid="ref-GuentherU003A2004" ref-type="bibr">Guenther
  &amp; Klinik, 2004</xref>) is able to outperform the basic MCTS
  implementation contained in <italic>mctreesearch4j</italic>. These
  heuristics are programmed via extensibility points in the
  <italic>mctreesearch4j</italic> solver implementation, where the key
  mechanisms can be altered or augmented. In our Heuristic
  Implementation Example, we introduce the
  <monospace>heuristicWeight</monospace> array, a 2D array storing
  domain specific ratings of every position on a Reversi board
  representing the desirability of that position on the board. The
  negative numbers represent a likely loss and positive numbers
  representing a likely win, again as represented in Fig.
  <xref alt="3" rid="figU003Areversi-heu">3</xref>.</p>
  <p>This value is taken into consideration when traversing down the
  simulation tree. The <monospace>heuristicWeight</monospace> array
  adjusts the propensity to explore any position for both agents based
  on the heurisitc’s belief about the desirability of the position. To
  alter the MCTS simulation phase we override the
  <monospace>simulate()</monospace> method and create a new definition
  for it. The application of this <monospace>heuristicWeight</monospace>
  only requires minor alterations to the
  <monospace>simulate()</monospace> method, as illustrated in
  <italic>Heuristic Implementation Example</italic>.</p>
  <sec id="heuristic-implementation-example">
    <title>Heuristic Implementation Example</title>
    <code language="kotlin">override fun simulate(node: NodeType): Double {
    /*... Original Simulation code ...*/
    while(true) {
        val validActions = mdp.actions(currentState)
        var bestActionScore = Int.MIN_VALUE // Begin heuristic
            var bestActions = mutableListOf&lt;Point&gt;()
            for (action in validActions) {
                val score = heuristicWeight[action.x][action.y]
                if (score &gt; bestActionScore) {
                    bestActionScore = score
                    bestActions = mutableListOf(action)
                }
                if (score == bestActionScore) {bestActions.add(action)}
            }
        val randomAction = bestActions.random() // End heuristic
        val newState = mdp.transition(currentState, randomAction)
        /*... Original Simulation code ...*/
    }
}</code>
    <fig>
      <caption><p>Reversi Heuristic.
      <styled-content id="figU003Areversi-heu"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/reversi-heu.png" xlink:title="Title" />
    </fig>
  </sec>
</sec>
<sec id="results">
  <title>Results</title>
  <p>When the MCTS solver is accurately selecting the optimal solutions,
  it will continue to compel the agent to explore in the optimal subset
  of the state space, and reduce its exploration in the non-optimal
  subset of the state space. We provide a quick example in the MDP
  Domain of GridWorld, detailed in
  (<xref alt="Liu &amp; Luo, 2021" rid="ref-LiuU003A2021" ref-type="bibr">Liu
  &amp; Luo, 2021</xref>). The cumulative number of visits corresponding
  to the optimal policy is proportionally increasing with respect to the
  number of MCTS iterations. Whereas for non-optimal solutions, the
  cumulative visits are significantly lower because the solver will
  avoid visiting the non-optimal state subspace.</p>
  <fig>
    <caption><p>Convergence of visits.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/gw_visits.png" xlink:title="Title" />
  </fig>
</sec>
<sec id="real-world-applications">
  <title>Real World Applications</title>
  <p><italic>mctreesearch4j</italic> is designed to run on the JVM, and
  therefore is useful in a research setting where JVM is intended to be
  used alongside either the implementation of the game or the core AI
  engine. Typically, for state-of-the-art AI, MCTS is used in
  combination with other Deep Learning algorithms, where MCTS drives the
  exploration of hypothetical games to be played out in order for the
  agent to make better decisions.</p>
  <p>MCTS is typically combined with Deep Learning strategies to achieve
  state-of-the-art performance for complex games whose solutions are
  typically intractable via standard tree-search methods. For example in
  AlphaGo [silver:2016], MCTS is combined with two types of neural
  networks, namely value networks and policy networks, to achieve
  state-of-the-art performance in the game of Go, never achieved before.
  Similarly, in the area of control theory, MCTS acting in combination
  with Value and Policy Networks to produce state and action space
  distributions, have shown impressive performance in probabilistic
  planning for self-driving cars, as demonstrated in Tesla’s AI day
  event [tesla:2021]. The utility of MCTS illustrates the acceptance of
  MCTS into the standard repetoire of both probablisitc and
  deterministic planning. And as the need for MCTS grows among industry
  and research, the impetus to create MCTS software libraries that are
  standardized, reliable, and can integrate well into general software
  ecosystems, such as JVM, is increasing.</p>
  <p>One key industry application of <italic>mctreesearch4j</italic> is
  the fact that it is a JVM software, which allows it to integrate into
  both legacy and modern JVM software. One key application can be in
  mobile gaming, which has a large developer base in Android, largely
  written in Kotlin within the JVM. <italic>mctreesearch4j</italic>’s
  lightweight design with guaranteed reliability and JVM compatibility,
  tested extensively on both Java 8 and Java 11, make it a prime
  candidate for robust AI in turn-based games on mobile devices.</p>
</sec>
<sec id="future-contributions">
  <title>Future Contributions</title>
  <p>Future work on <italic>mctreesearch4j</italic> could potentially
  involve further optimization, particularly of the memory consumption
  of the algorithm. Additional speed improvements can be improved via
  multi-threading and parallelization, which are fully supported on the
  JVM.</p>
  <p>The fact that <italic>mctreesearch4j</italic> is based in the JVM
  can however also serve as a downside, since Java based programming
  languages are not typically used for research problems within AI.
  That, combined with JVM language’s added complexity when compared to
  scripting languages such as Python, can result in
  <italic>mctreesearch4j</italic> experiencing a slow adoption in a pure
  research setting. Nevertheless, the key design principles that
  <italic>mctreesearch4j</italic> was designed on, could easily be
  ported into another scripting based language, more commonly found in
  AI research, namely Python. <italic>mctreesearch4j</italic> is
  designed to serve as a foundational template for research in MCTS for
  the JVM, and it can also serve as a reliable importable library for
  JVM applications which implement MCTS.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>In closing, <italic>mctreesearch4j</italic> presents a framework
  which enables programmers to adapt an MCTS solver to a variety of MDP
  domains. This is important because software application was a main
  focus of <italic>mctreesearch4j</italic>. Furthermore,
  <italic>mctreesearch4j</italic> is fully compatible with JVM, and this
  design decision was made due to the excellent support of class
  structure and generic variable typing in Kotlin, and other JVM
  languages, as well as support for mobile applications. Yet most
  importantly, <italic>mctreesearch4j</italic> is modular and
  extensible, the key mechanism of MCTS are broken down, and the
  programmer is able to inherit class members, redefine and/or
  re-implement certain sections of the algorithm while maintaining a
  high degree of MCTS standardization.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This research was conducted without external funding, resulting in
  no conflicts of interests. We would like to acknowledge the Maven
  Central Repository, for providing an important service to make our JVM
  artifacts accessible to all JVM projects.</p>
  <p><italic>mctreesearch4j</italic> is available here:
  <ext-link ext-link-type="uri" xlink:href="https://mvnrepository.com/artifact/ca.aqtech/mctreesearch4j">mvnrepository.com/artifact/ca.aqtech/mctreesearch4j</ext-link></p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-LiuU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Larkin</given-names></name>
        <name><surname>Luo</surname><given-names>Jun Tao</given-names></name>
      </person-group>
      <article-title>An extensible and modular design and implementation of monte carlo tree search for the JVM</article-title>
      <source>Preprint</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://www.preprints.org/manuscript/202107.0622/v1</uri>
      <pub-id pub-id-type="doi">doi:10.20944/preprints202107.0622.v1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-LiuU003A2021-connect4">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Larkin</given-names></name>
      </person-group>
      <article-title>Connect 4 implementation in scala</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/larkz/connect4-scala, commit = 4549c00398e7987814d83a2bd0760bbaedeb879b; GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-AuerU003A2002">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Auer</surname><given-names>Peter</given-names></name>
        <name><surname>Cesa-Bianchi</surname><given-names>Nicolò</given-names></name>
        <name><surname>Fischer</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>Finite-time analysis of the multiarmed bandit problem</article-title>
      <source>Mach. Learn.</source>
      <publisher-name>Kluwer Academic Publishers</publisher-name>
      <publisher-loc>Hingham, MA, USA</publisher-loc>
      <year iso-8601-date="2002-05">2002</year><month>05</month>
      <volume>47</volume>
      <issue>2-3</issue>
      <issn>0885-6125</issn>
      <uri>https://doi.org/10.1023/A:1013689704352</uri>
      <pub-id pub-id-type="doi">10.1023/A:1013689704352</pub-id>
      <fpage>235</fpage>
      <lpage>256</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GuentherU003A2004">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Guenther</surname><given-names>Mathias</given-names></name>
        <name><surname>Klinik</surname><given-names>Jobas</given-names></name>
      </person-group>
      <article-title>A new experience: O-thell-us – an AI project</article-title>
      <publisher-name>https://courses.cs.washington.edu/courses/cse573/04au/Project/mini1/O-Thell-Us/Othellus.pdf; University of Washington</publisher-name>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-BrowneU003A2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Browne</surname><given-names>Cameron</given-names></name>
        <name><surname>Powley</surname><given-names>Edward</given-names></name>
        <name><surname>Whitehouse</surname><given-names>Daniel</given-names></name>
        <name><surname>Lucas</surname><given-names>Simon</given-names></name>
        <name><surname>Cowling</surname><given-names>Peter I.</given-names></name>
        <name><surname>Tavener</surname><given-names>Stephen</given-names></name>
        <name><surname>Perez</surname><given-names>Diego</given-names></name>
        <name><surname>Samothrakis</surname><given-names>Spyridon</given-names></name>
        <name><surname>Colton</surname><given-names>Simon</given-names></name>
        <name><surname>al.</surname></name>
      </person-group>
      <article-title>A survey of monte carlo tree search methods</article-title>
      <source>IEEE TRANSACTIONS ON COMPUTATIONAL INTELLIGENCE AND AI</source>
      <year iso-8601-date="2012">2012</year>
    </element-citation>
  </ref>
  <ref id="ref-CowlingU003A2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cowling</surname><given-names>Peter</given-names></name>
        <name><surname>Powley</surname><given-names>Edward</given-names></name>
        <name><surname>Whitehouse</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>Information set monte carlo tree search</article-title>
      <source>IEEE Transactions on Computational Intelligence and Ai in Games</source>
      <year iso-8601-date="2012-06">2012</year><month>06</month>
      <volume>4</volume>
      <pub-id pub-id-type="doi">10.1109/TCIAIG.2012.2200894</pub-id>
      <fpage>120</fpage>
      <lpage>143</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LucasU003A2018">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lucas</surname><given-names>Simon</given-names></name>
      </person-group>
      <article-title>MonteCarloTS - java</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/DrumerJoe21/MonteCarloTS; GitHub</publisher-name>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-acmeU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matt</given-names></name>
        <name><surname>Shahriari</surname><given-names>Bobak</given-names></name>
        <name><surname>Aslanides</surname><given-names>John</given-names></name>
        <name><surname>Barth-Maron</surname><given-names>Gabriel</given-names></name>
        <name><surname>Behbahani</surname><given-names>Feryal</given-names></name>
        <name><surname>Norman</surname><given-names>Tamara</given-names></name>
        <name><surname>Abdolmaleki</surname><given-names>Abbas</given-names></name>
        <name><surname>Cassirer</surname><given-names>Albin</given-names></name>
        <name><surname>Yang</surname><given-names>Fan</given-names></name>
        <name><surname>Baumli</surname><given-names>Kate</given-names></name>
        <name><surname>Henderson</surname><given-names>Sarah</given-names></name>
        <name><surname>Novikov</surname><given-names>Alex</given-names></name>
        <name><surname>Colmenarejo</surname><given-names>Sergio Gómez</given-names></name>
        <name><surname>Cabi</surname><given-names>Serkan</given-names></name>
        <name><surname>Gulcehre</surname><given-names>Caglar</given-names></name>
        <name><surname>Paine</surname><given-names>Tom Le</given-names></name>
        <name><surname>Cowie</surname><given-names>Andrew</given-names></name>
        <name><surname>Wang</surname><given-names>Ziyu</given-names></name>
        <name><surname>Piot</surname><given-names>Bilal</given-names></name>
        <name><surname>Freitas</surname><given-names>Nando de</given-names></name>
      </person-group>
      <article-title>Acme: A research framework for distributed reinforcement learning</article-title>
      <source>arXiv preprint arXiv:2006.00979</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2006.00979</uri>
    </element-citation>
  </ref>
  <ref id="ref-tensorforceU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schaarschmidt</surname><given-names>Michael</given-names></name>
        <name><surname>Kuhnle</surname><given-names>Alexander</given-names></name>
        <name><surname>Ellis</surname><given-names>Ben</given-names></name>
        <name><surname>Fricke</surname><given-names>Kai</given-names></name>
        <name><surname>Gessert</surname><given-names>Felix</given-names></name>
        <name><surname>Yoneki</surname><given-names>Eiko</given-names></name>
      </person-group>
      <article-title>LIFT: Reinforcement learning in computer systems by learning from demonstrations</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2018">2018</year>
      <volume>abs/1808.07903</volume>
      <uri>http://arxiv.org/abs/1808.07903</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
