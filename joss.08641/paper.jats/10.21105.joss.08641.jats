<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8641</article-id>
<article-id pub-id-type="doi">10.21105/joss.08641</article-id>
<title-group>
<article-title>anvay: A Web-based Tool for Interpretive Topic Modelling
in Bengali</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0008-5623-1226</contrib-id>
<name>
<surname>Das Gupta</surname>
<given-names>Vinayak</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Shiv Nadar Institution of Eminence</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-04-23">
<day>23</day>
<month>4</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>118</issue>
<fpage>8641</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>topic modelling</kwd>
<kwd>bengali language</kwd>
<kwd>natural language processing</kwd>
<kwd>digital humanities</kwd>
<kwd>interpretability</kwd>
<kwd>pedagogy</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><italic>anvay</italic> is a web-based tool for topic modelling in
  Bengali, developed for exploratory reading and interpretive analysis.
  It provides a full pipeline for Latent Dirichlet Allocation (LDA)
  (<xref alt="Blei et al., 2003" rid="ref-blei2003lda" ref-type="bibr">Blei
  et al., 2003</xref>)—from corpus ingestion and preprocessing to model
  configuration and visual output—within a lightweight, browser-based
  interface. The tool foregrounds user interpretation: rather than
  providing coherence scores or fixed topic labels,
  <italic>anvay</italic> presents the model output to be read,
  interpreted, and adjusted by the user.</p>
  <p>Designed for literary, journalistic, and historical corpora in
  Bengali, <italic>anvay</italic> supports a range of language-specific
  preprocessing functions including lemmatisation, frequency filtering,
  and stopword pruning. The outputs, ranging from topic-word networks to
  document-level previews, are rendered with clarity and designed to
  enable close reading. Each topic is accessible through multiple
  lenses: top words, paragraph-level examples, document weights, and
  corpus-wide distribution.</p>
  <p>Beyond its technical function, <italic>anvay</italic> is an
  intervention in how we teach and understand computational methods
  within the humanities in low-resource contexts.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the Field</title>
  <p>Topic modelling is well established in the digital humanities and
  is used for analysing large text collections in literary studies,
  cultural analytics, and media research
  (<xref alt="Goldstone &amp; Underwood, 2014b" rid="ref-goldstone2014quiet" ref-type="bibr">Goldstone
  &amp; Underwood, 2014b</xref>;
  <xref alt="Jockers, 2013" rid="ref-jockers2013macroanalysis" ref-type="bibr">Jockers,
  2013</xref>). Scholars employ models to identify recurring themes and
  support exploratory reading.</p>
  <p>Gensim
  (<xref alt="Řehůřek &amp; Sojka, 2010" rid="ref-rehurek_lrec" ref-type="bibr">Řehůřek
  &amp; Sojka, 2010</xref>) and Mallet
  (<xref alt="McCallum, 2002" rid="ref-mccallum2002mallet" ref-type="bibr">McCallum,
  2002</xref>) are popular backend libraries. Several interfaces support
  this work, and <italic>anvay</italic> draws inspiration from many of
  these. Voyant Tools provides a general-purpose environment for text
  analysis, which includes an accessible topic-modelling component
  (<xref alt="Sinclair &amp; Rockwell, 2016" rid="ref-voyant" ref-type="bibr">Sinclair
  &amp; Rockwell, 2016</xref>). Termite, one of the earliest
  contributions of this type, offers a clear tabular display for
  comparing topic–term relations
  (<xref alt="Chuang et al., 2012" rid="ref-chuang2012termite" ref-type="bibr">Chuang
  et al., 2012</xref>). pyLDAvis supplies interactive views of topic
  distances and word relevance
  (<xref alt="Sievert &amp; Shirley, 2014" rid="ref-sievert-shirley-2014-ldavis" ref-type="bibr">Sievert
  &amp; Shirley, 2014</xref>). TopicWizard presents topic clusters,
  keywords and document-level patterns through a web interface
  (<xref alt="Kardos et al., 2025" rid="ref-kardos2025topicwizardmodernmodelagnostic" ref-type="bibr">Kardos
  et al., 2025</xref>). jsLDA demonstrates that browser-based topic
  modelling is feasible and useful in teaching contexts
  (<xref alt="Mimno, n.d." rid="ref-mimno_jslda" ref-type="bibr">Mimno,
  n.d.</xref>). Other platforms have made model outputs available to
  wider audiences. The Topic Modeling Tool provides a simple graphical
  front end to MALLET
  (<xref alt="Enderle, 2019" rid="ref-enderle2019topictool" ref-type="bibr">Enderle,
  2019</xref>). DFR-Browser supports exploratory reading of topic models
  within journal archives
  (<xref alt="Goldstone &amp; Underwood, 2014a" rid="ref-goldstone2014dfrbrowser" ref-type="bibr">Goldstone
  &amp; Underwood, 2014a</xref>).</p>
  <p>Recent systems such as BERTopic combine contextualised
  representations with neural topic models to improve coherence
  (<xref alt="Grootendorst, 2022" rid="ref-grootendorst2022bertopic" ref-type="bibr">Grootendorst,
  2022</xref>). Other methods, such as Top2Vec
  (<xref alt="Angelov, 2020" rid="ref-angelov2020top2vec" ref-type="bibr">Angelov,
  2020</xref>) and CombinedTM
  (<xref alt="Bianchi et al., 2021" rid="ref-bianchi-etal-2021-pre" ref-type="bibr">Bianchi
  et al., 2021</xref>) built on sentence transformers or contextualised
  embeddings, also aim to improve topic coherence and reduce dependence
  on bag-of-words features.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Most topic-modelling tools are designed for English and other
  high-resource languages. They rely on tokenisers, stemmers and
  embedding models that do not transfer well to Bengali. They might also
  require users to prepare their own pipelines or rely on
  English-centric defaults.</p>
  <p>Researchers and students working with Bengali texts face several
  difficulties. Tokenisation may produce malformed units, existing
  stopword lists are incomplete and lemmatisation resources are limited.
  In teaching settings, students often lack the technical background to
  run scripts or to interpret model output without guidance.</p>
  <p>Transformer-based topic-modelling systems, such as BERTopic,
  Top2Vec and contextual topic models, are not suitable for this
  project. They rely on pretrained embeddings that do not exist for many
  Bengali registers. They are slower and less lightweight than classical
  LDA, which limits accessibility in browser-based or workshop
  environments. They also behave as opaque models, which conflicts with
  the interpretive aims of the interface.</p>
  <p><italic>anvay</italic> addresses these problems by offering a full
  topic-modelling workflow tailored to Bengali. It provides appropriate
  tokenisation and lemmatisation, configurable model parameters and a
  set of visualisations that foreground interpretability. Users can
  explore top words, representative sentences and topic distributions
  directly in the browser. The tool also supplies documentation that
  helps students understand how topic models operate and how to read
  them critically.</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p><italic>anvay</italic> is written in Python and uses Flask, Gensim,
  and standard visualisation libraries. Its main features include:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Corpus upload and cleaning</bold>: Up to 800
      <monospace>.txt</monospace> files can be uploaded. Users can apply
      stopword filters with NLTK
      (<xref alt="Bird et al., 2009" rid="ref-bird2009natural" ref-type="bibr">Bird
      et al., 2009</xref>) or user stopword lists, stemming or
      lemmatisation, and token pruning.</p>
    </list-item>
    <list-item>
      <p><bold>Model training</bold>: Parameters such as passes,
      iterations, alpha, and chunk size can be adjusted. Models are
      trained on the server.</p>
    </list-item>
    <list-item>
      <p><bold>Bengali processing</bold>: Tokenisation avoids malformed
      output. Lemma data is drawn from public resources
      (<xref alt="Alam et al., 2021" rid="ref-alam2021review" ref-type="bibr">Alam
      et al., 2021</xref>;
      <xref alt="Chakrabarty et al., 2017" rid="ref-chakrabarty-etal-2017-context" ref-type="bibr">Chakrabarty
      et al., 2017</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Visualisations</bold>: Results are shown using:</p>
      <list list-type="bullet">
        <list-item>
          <p>Topic scatter plots (Plotly)</p>
        </list-item>
        <list-item>
          <p>Heatmaps (Seaborn)
          (<xref alt="Waskom, 2021" rid="ref-Waskom2021" ref-type="bibr">Waskom,
          2021</xref>)</p>
        </list-item>
        <list-item>
          <p>Bar and pie charts for topic-document relations</p>
        </list-item>
        <list-item>
          <p>Topic-word network graphs (NetworkX)
          (<xref alt="Hagberg et al., 2008" rid="ref-hagberg2008exploring" ref-type="bibr">Hagberg
          et al., 2008</xref>)</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p><bold>Interpretive tools</bold>: Users can see representative
      paragraphs, find key topics in each file, and compare topic
      strength. Topics that appear noisy or incoherent are flagged with
      a “Low Confidence” warning.</p>
    </list-item>
    <list-item>
      <p><bold>Report generation</bold>: Alongside visual outputs,
      <italic>anvay</italic> creates a structured report that prints the
      training configuration, dataset statistics, and top keywords per
      topic. This includes metrics like document and token counts,
      vocabulary size, topic prevalence, and topic weights per document.
      A representative sentence is also shown for each topic. These help
      users trace how the model was built and better understand its
      results.</p>
    </list-item>
    <list-item>
      <p><bold>Export and accessibility</bold>: The tool supports CSV
      and TXT downloads. It works in all major browsers, with responsive
      design and dark mode.</p>
    </list-item>
  </list>
</sec>
<sec id="research-and-pedagogical-use">
  <title>Research and Pedagogical Use</title>
  <p>The design of <italic>anvay</italic> is informed by research-led
  teaching practice. Topic modelling, while widely adopted in digital
  humanities, often remains inaccessible due to steep learning curves
  and underdeveloped interfaces. <italic>anvay</italic> was developed to
  lower these barriers and support new modes of engagement with Bengali
  textual corpora, especially where existing NLP tools fail to account
  for morphological variance, informal orthographies, or the diversity
  of textual registers in Bengali.</p>
  <p>In pedagogical contexts, <italic>anvay</italic> functions as a
  conceptual primer. It prompts students to ask: What is a topic? What
  assumptions shape a model’s output? How do visualisations shape
  interpretation? The tool has been tested in classroom environments
  with undergraduate and postgraduate students, many of whom were
  engaging with topic modelling for the first time. The feedback has
  been consistent: the visual design, language support, and
  document-level previews help to render the model’s assumptions
  legible.</p>
  <p>To support this, <italic>anvay</italic> includes extensive
  web-based documentation. Each section guides users through corpus
  preparation, parameter tuning, and result analysis, with annotated
  examples and embedded visual references. The documentation foregrounds
  conceptual understanding: users are encouraged to read models
  critically, experiment with settings, and reflect on how computational
  structure intersects with thematic interpretation. It is embedded
  directly in the interface and designed for both classroom and
  independent study.</p>
</sec>
<sec id="performance-and-limitations">
  <title>Performance and Limitations</title>
  <p><italic>anvay</italic> is designed for moderate-scale corpora,
  where interpretability and visual exploration are prioritised over
  throughput. In a benchmark run using <bold>800 Bengali
  <monospace>.txt</monospace> files</bold> (totalling
  <bold>21.9MB</bold>, ~<bold>940,000 tokens</bold>, and <bold>171,754
  unique vocabulary terms</bold>), the system successfully trained a
  10-topic LDA model with <bold>10 passes</bold> and <bold>50
  iterations</bold> in approximately <bold>62 seconds</bold> on a
  single-core setup. This corpus included highly variable document
  lengths, from <bold>79</bold> to <bold>86,099 tokens</bold> per file,
  demonstrating robustness against input heterogeneity.</p>
  <p>While the system is tuned for formal Bengali prose, there are
  limitations: informal or dialectal orthographies may lead to malformed
  tokens, and OCR artefacts or non-Unicode glyphs may interfere with
  tokenisation.</p>
  <p>The modelling backend is standard LDA; no coherence optimisation or
  neural alignment is included. As such, <italic>anvay</italic> is best
  used as an exploratory interface, for interpretive reading rather than
  automated evaluation.</p>
</sec>
<sec id="repository-and-license">
  <title>Repository and License</title>
  <p>The source code and documentation for <italic>anvay</italic> are
  hosted on GitHub: https://github.com/vinayakdasgupta/anvay
  The software is released under the MIT License.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-rehurek_lrec">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Řehůřek</surname><given-names>Radim</given-names></name>
        <name><surname>Sojka</surname><given-names>Petr</given-names></name>
      </person-group>
      <article-title>Software framework for topic modelling with large corpora</article-title>
      <source>Proceedings of the LREC 2010 workshop on New Challenges for NLP Frameworks</source>
      <publisher-name>ELRA</publisher-name>
      <publisher-loc>Valletta, Malta</publisher-loc>
      <year iso-8601-date="2010-05-22">2010</year><month>05</month><day>22</day>
      <fpage>45</fpage>
      <lpage>50</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chakrabarty-etal-2017-context">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chakrabarty</surname><given-names>Abhisek</given-names></name>
        <name><surname>Pandit</surname><given-names>Onkar Arun</given-names></name>
        <name><surname>Garain</surname><given-names>Utpal</given-names></name>
      </person-group>
      <article-title>Context sensitive lemmatization using two successive bidirectional gated recurrent networks</article-title>
      <source>Proceedings of the 55th annual meeting of the Association for Computational Linguistics (volume 1: Long papers)</source>
      <person-group person-group-type="editor">
        <name><surname>Barzilay</surname><given-names>Regina</given-names></name>
        <name><surname>Kan</surname><given-names>Min-Yen</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Vancouver, Canada</publisher-loc>
      <year iso-8601-date="2017-07">2017</year><month>07</month>
      <uri>https://aclanthology.org/P17-1136/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/P17-1136</pub-id>
      <fpage>1481</fpage>
      <lpage>1491</lpage>
    </element-citation>
  </ref>
  <ref id="ref-alam2021review">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alam</surname><given-names>Firoj</given-names></name>
        <name><surname>Hasan</surname><given-names>Md. Ashraful</given-names></name>
        <name><surname>Alam</surname><given-names>Tanvir</given-names></name>
        <name><surname>Khan</surname><given-names>Akib</given-names></name>
        <name><surname>Tajrin</surname><given-names>Jannatul</given-names></name>
        <name><surname>Khan</surname><given-names>Nayeem</given-names></name>
        <name><surname>Chowdhury</surname><given-names>Shammur Absar</given-names></name>
      </person-group>
      <article-title>A review of Bangla natural language processing tasks and the utility of transformer models</article-title>
      <source>arXiv preprint arXiv:2107.03844</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2107.03844</uri>
    </element-citation>
  </ref>
  <ref id="ref-voyant">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Sinclair</surname><given-names>Stéfan</given-names></name>
        <name><surname>Rockwell</surname><given-names>Geoffrey</given-names></name>
      </person-group>
      <article-title>Voyant Tools</article-title>
      <publisher-name>https://voyant-tools.org/</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-bianchi-etal-2021-pre">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bianchi</surname><given-names>Federico</given-names></name>
        <name><surname>Terragni</surname><given-names>Silvia</given-names></name>
        <name><surname>Hovy</surname><given-names>Dirk</given-names></name>
      </person-group>
      <article-title>Pre-training is a hot topic: Contextualized document embeddings improve topic coherence</article-title>
      <source>Proceedings of the 59th annual meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (volume 2: Short papers)</source>
      <person-group person-group-type="editor">
        <name><surname>Zong</surname><given-names>Chengqing</given-names></name>
        <name><surname>Xia</surname><given-names>Fei</given-names></name>
        <name><surname>Li</surname><given-names>Wenjie</given-names></name>
        <name><surname>Navigli</surname><given-names>Roberto</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021-08">2021</year><month>08</month>
      <uri>https://aclanthology.org/2021.acl-short.96/</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2021.acl-short.96</pub-id>
      <fpage>759</fpage>
      <lpage>766</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goldstone2014quiet">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goldstone</surname><given-names>Andrew</given-names></name>
        <name><surname>Underwood</surname><given-names>Ted</given-names></name>
      </person-group>
      <article-title>The quiet transformations of literary studies: What thirteen thousand scholars could tell us</article-title>
      <source>New Literary History</source>
      <year iso-8601-date="2014">2014</year>
      <volume>45</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1353/nlh.2014.0025</pub-id>
      <fpage>359</fpage>
      <lpage>384</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chuang2012termite">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chuang</surname><given-names>Jason</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher D.</given-names></name>
        <name><surname>Heer</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>Termite: Visualization techniques for assessing textual topic models</article-title>
      <source>Proceedings of the International Working Conference on Advanced Visual Interfaces</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2012">2012</year>
      <isbn>9781450312875</isbn>
      <uri>https://doi.org/10.1145/2254556.2254572</uri>
      <pub-id pub-id-type="doi">10.1145/2254556.2254572</pub-id>
      <fpage>74</fpage>
      <lpage>77</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blei2003lda">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Blei</surname><given-names>David M.</given-names></name>
        <name><surname>Ng</surname><given-names>Andrew Y.</given-names></name>
        <name><surname>Jordan</surname><given-names>Michael I.</given-names></name>
      </person-group>
      <article-title>Latent Dirichlet allocation</article-title>
      <source>Journal of Machine Learning Research</source>
      <publisher-name>JMLR.org</publisher-name>
      <year iso-8601-date="2003-03">2003</year><month>03</month>
      <volume>3</volume>
      <issn>1532-4435</issn>
      <fpage>993</fpage>
      <lpage>1022</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sievert-shirley-2014-ldavis">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sievert</surname><given-names>Carson</given-names></name>
        <name><surname>Shirley</surname><given-names>Kenneth</given-names></name>
      </person-group>
      <article-title>LDAvis: A method for visualizing and interpreting topics</article-title>
      <source>Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces</source>
      <person-group person-group-type="editor">
        <name><surname>Chuang</surname><given-names>Jason</given-names></name>
        <name><surname>Green</surname><given-names>Spence</given-names></name>
        <name><surname>Hearst</surname><given-names>Marti</given-names></name>
        <name><surname>Heer</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Koehn</surname><given-names>Philipp</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Baltimore, Maryland, USA</publisher-loc>
      <year iso-8601-date="2014-06">2014</year><month>06</month>
      <uri>https://aclanthology.org/W14-3110/</uri>
      <pub-id pub-id-type="doi">10.3115/v1/W14-3110</pub-id>
      <fpage>63</fpage>
      <lpage>70</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kardos2025topicwizardmodernmodelagnostic">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kardos</surname><given-names>Márton</given-names></name>
        <name><surname>Enevoldsen</surname><given-names>Kenneth C.</given-names></name>
        <name><surname>Nielbo</surname><given-names>Kristoffer Laigaard</given-names></name>
      </person-group>
      <article-title>topicwizard – a modern, model-agnostic framework for topic model visualization and interpretation</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://arxiv.org/abs/2505.13034</uri>
    </element-citation>
  </ref>
  <ref id="ref-mimno_jslda">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Mimno</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>JsLDA: In-browser topic modeling</article-title>
      <publisher-name>https://github.com/mimno/jsLDA</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-enderle2019topictool">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Enderle</surname><given-names>Scott</given-names></name>
      </person-group>
      <article-title>Topic modeling tool</article-title>
      <publisher-name>https://github.com/senderle/topic-modeling-tool</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-goldstone2014dfrbrowser">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Goldstone</surname><given-names>Andrew</given-names></name>
        <name><surname>Underwood</surname><given-names>Ted</given-names></name>
      </person-group>
      <article-title>DFR-browser</article-title>
      <publisher-name>https://github.com/agoldst/dfr-browser</publisher-name>
      <year iso-8601-date="2014">2014</year>
    </element-citation>
  </ref>
  <ref id="ref-grootendorst2022bertopic">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Grootendorst</surname><given-names>Maarten</given-names></name>
      </person-group>
      <article-title>BERTopic: Neural topic modeling with a class-based TF-IDF procedure</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2203.05794</uri>
    </element-citation>
  </ref>
  <ref id="ref-angelov2020top2vec">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Angelov</surname><given-names>Dimo</given-names></name>
      </person-group>
      <article-title>Top2Vec: Distributed representations of topics</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2008.09470</uri>
    </element-citation>
  </ref>
  <ref id="ref-mccallum2002mallet">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>McCallum</surname><given-names>Andrew K.</given-names></name>
      </person-group>
      <article-title>MALLET: A machine learning for language toolkit</article-title>
      <publisher-name>http://mallet.cs.umass.edu/</publisher-name>
      <year iso-8601-date="2002">2002</year>
    </element-citation>
  </ref>
  <ref id="ref-Waskom2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Waskom</surname><given-names>Michael L.</given-names></name>
      </person-group>
      <article-title>seaborn: Statistical data visualization</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>60</issue>
      <uri>https://doi.org/10.21105/joss.03021</uri>
      <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      <fpage>3021</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hagberg2008exploring">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hagberg</surname><given-names>Aric A.</given-names></name>
        <name><surname>Schult</surname><given-names>Daniel A.</given-names></name>
        <name><surname>Swart</surname><given-names>Pieter J.</given-names></name>
      </person-group>
      <article-title>Exploring network structure, dynamics, and function using NetworkX</article-title>
      <source>Proceedings of the 7th Python in Science Conference (SciPy2008)</source>
      <year iso-8601-date="2008">2008</year>
      <pub-id pub-id-type="doi">10.25080/tcwv9851</pub-id>
      <fpage>11</fpage>
      <lpage>15</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bird2009natural">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Bird</surname><given-names>Steven</given-names></name>
        <name><surname>Klein</surname><given-names>Ewan</given-names></name>
        <name><surname>Loper</surname><given-names>Edward</given-names></name>
      </person-group>
      <source>Natural language processing with Python</source>
      <publisher-name>O’Reilly Media, Inc.</publisher-name>
      <year iso-8601-date="2009">2009</year>
    </element-citation>
  </ref>
  <ref id="ref-jockers2013macroanalysis">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Jockers</surname><given-names>Matthew L.</given-names></name>
      </person-group>
      <source>Macroanalysis: Digital methods and literary history</source>
      <publisher-name>University of Illinois Press</publisher-name>
      <year iso-8601-date="2013">2013</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
