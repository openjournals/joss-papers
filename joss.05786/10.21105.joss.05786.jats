<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5786</article-id>
<article-id pub-id-type="doi">10.21105/joss.05786</article-id>
<title-group>
<article-title>SIRUS.jl: Interpretable Machine Learning via Rule
Extraction</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9445-8466</contrib-id>
<name>
<surname>Huijzer</surname>
<given-names>Rik</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6588-5079</contrib-id>
<name>
<surname>Blaauw</surname>
<given-names>Frank</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0094-8307</contrib-id>
<name>
<surname>den Hartigh</surname>
<given-names>Ruud J. R.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Groningen, Groningen, the
Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Researchable, Assen, the Netherlands</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-09-27">
<day>27</day>
<month>9</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>90</issue>
<fpage>5786</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>SIRUS.jl<xref ref-type="fn" rid="fn1">1</xref> is an implementation
  of the original Stable and Interpretable RUle Sets (SIRUS) algorithm
  in the Julia programming language
  (<xref alt="Bezanson et al., 2017" rid="ref-julia" ref-type="bibr">Bezanson
  et al., 2017</xref>). The SIRUS algorithm is a fully interpretable
  version of random forests, that is, it reduces thousands of trees in
  the forest to a much lower number of interpretable rules (e.g., 10 or
  20). With our Julia implementation, we aimed to reproduce the original
  C++ and R implementation in a high-level language to verify the
  algorithm as well as making the code easier to read. We show that the
  model performs well on classification tasks while retaining
  interpretability and stability. Furthermore, we made the code
  available under the permissive MIT license. In turn, this allows
  others to research the algorithm further or easily port it to
  production systems.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Many of the modern day machine learning models are noninterpretable
  models, also known as <italic>black box</italic> models. Well-known
  examples of noninterpretable models are random forests
  (<xref alt="Breiman, 2001" rid="ref-breiman2001random" ref-type="bibr">Breiman,
  2001</xref>) and neural networks. Such models are available in the
  Julia programming language via, for example, LightGBM.jl
  (<xref alt="Ke et al., 2017" rid="ref-ke2017lightgbm" ref-type="bibr">Ke
  et al., 2017</xref>), Flux.jl
  (<xref alt="Innes, 2018" rid="ref-innes2018flux" ref-type="bibr">Innes,
  2018</xref>), and BetaML.jl
  (<xref alt="Lobianco, 2021" rid="ref-lobianco2021betaml" ref-type="bibr">Lobianco,
  2021</xref>). Although these models can obtain high predictive
  performance and are commonly used, they can be problematic in high
  stakes domains where model decisions have real-world impact on
  individuals, such as suggesting treatments or selecting personnel. The
  reason is that noninterpretable models may lead to unsafe, unfair, or
  unreliable predictions
  (<xref alt="Barredo Arrieta et al., 2020" rid="ref-barredo2020explainable" ref-type="bibr">Barredo
  Arrieta et al., 2020</xref>;
  <xref alt="Doshi-Velez &amp; Kim, 2017" rid="ref-doshi2017towards" ref-type="bibr">Doshi-Velez
  &amp; Kim, 2017</xref>). Furthermore, interpretable models may allow
  researchers to learn more from the model, which in turn may allow
  researchers to make better model decisions and achieve a higher
  predictive performance.</p>
  <p>However, the set of interpretable models is often limited to
  ordinary and generalized regression models, decision trees, RuleFit,
  naive Bayes classification, and k-nearest neighbors
  (<xref alt="Molnar, 2022" rid="ref-molnar2022interpretable" ref-type="bibr">Molnar,
  2022</xref>). For these models, however, predictive performance can be
  poor for certain tasks. Linear models, for instance, may perform
  poorly when features are correlated and can be sensitive to the choice
  of hyperparameters. For decision trees, predictive performance is poor
  compared to random forests
  (<xref alt="James et al., 2013" rid="ref-james2013introduction" ref-type="bibr">James
  et al., 2013</xref>). RuleFit is not available in Julia and is
  <italic>unstable</italic>
  (<xref alt="Bénard, Biau, Veiga, et al., 2021" rid="ref-benard2021sirus" ref-type="bibr">Bénard,
  Biau, Veiga, et al., 2021</xref>), meaning sensitive to small changes
  in data. Naive Bayes, available in Julia as
  NaiveBayes.jl<xref ref-type="fn" rid="fn2">2</xref>, is often
  overlooked and can be a suitable solution, but only if the features
  are independent
  (<xref alt="Ashari et al., 2013" rid="ref-ashari2013performance" ref-type="bibr">Ashari
  et al., 2013</xref>).</p>
  <p>Researchers have attempted to make the random forest models more
  interpretable. Model interpretation techniques, such as SHAP
  (<xref alt="Lundberg &amp; Lee, 2017" rid="ref-lundberg2017unified" ref-type="bibr">Lundberg
  &amp; Lee, 2017</xref>) or Shapley, available via
  Shapley.jl<xref ref-type="fn" rid="fn3">3</xref>, have been used to
  visualize the fitted model. However, the disadvantage of these
  techniques are that they convert the complex model to a simplified
  representation. This causes the simplified representation to be
  different from the complex model and may therefore hide biases and
  issues related to safety and reliability
  (<xref alt="Barredo Arrieta et al., 2020" rid="ref-barredo2020explainable" ref-type="bibr">Barredo
  Arrieta et al., 2020</xref>).</p>
  <p>The SIRUS algorithm solves this by simplifying the complex model
  and by then using the simplified model for predictions. This ensures
  that the same model is used for interpretation and prediction.
  However, the original SIRUS algorithm was implemented in about 10k
  lines of C++ and 2k lines of R
  code<xref ref-type="fn" rid="fn4">4</xref> which makes it hard to
  inspect and extend due to the combination of two languages. Our
  implementation is written in about 2k lines of pure Julia code. This
  allows researchers to more easily verify the algorithm and investigate
  further improvements. Furthermore, the original algorithm was covered
  by the GPL-3 copyleft license meaning that copies are required to be
  made freely available. A more permissive license makes it easier to
  port the code to other languages or production systems.</p>
</sec>
<sec id="interpretability">
  <title>Interpretability</title>
  <p>To show that the algorithm is fully interpretable, we fit an
  example on the Haberman’s Survival Dataset
  (<xref alt="Haberman, 1999" rid="ref-haberman1999survival" ref-type="bibr">Haberman,
  1999</xref>). The dataset contains survival data on patients who had
  undergone surgery for breast cancer and contains three features,
  namely the number of axillary <italic>nodes</italic> that were
  detected, the <italic>age</italic> of the patient at the time of the
  operation, and the patient’s <italic>year</italic> of operation. For
  this example, we have set the hyperparameters for the maximum number
  of rules to 8 since this is a reasonable trade-off between predictive
  performance and interpretability. Generally, a higher maximum number
  of rules will yield a higher predictive performance. We have also set
  the maximum depth hyperparameter to 2. This hyperparameter means that
  the random forests inside the algorithm are not allowed to have a
  depth higher than 2. In turn, this means that rules contain at most 2
  clauses (<monospace>if A &amp; B</monospace>). When the maximum depth
  is set to 1, then the rules contain at most 1 clause
  (<monospace>if A</monospace>). Most rule-based models, including
  SIRUS, are restricted to depth of 1 or 2
  (<xref alt="Bénard, Biau, Veiga, et al., 2021" rid="ref-benard2021sirus" ref-type="bibr">Bénard,
  Biau, Veiga, et al., 2021</xref>).</p>
  <p>The output for the fitted model looks as follows (see Section
  <italic>Code Example</italic> for the code):</p>
  <preformat>StableRules model with 8 rules:
 if X[i, :nodes] &lt; 7.0 then 0.238 else 0.046 +
 if X[i, :nodes] &lt; 2.0 then 0.183 else 0.055 +
 if X[i, :age] ≥ 62.0 &amp; X[i, :year] &lt; 1959.0 then 0.0 else 0.001 +
 if X[i, :year] &lt; 1959.0 &amp; X[i, :nodes] ≥ 2.0 then 0.0 else 0.006 +
 if X[i, :nodes] ≥ 7.0 &amp; X[i, :age] ≥ 62.0 then 0.0 else 0.008 +
 if X[i, :year] &lt; 1959.0 &amp; X[i, :nodes] ≥ 7.0 then 0.0 else 0.003 +
 if X[i, :year] ≥ 1966.0 &amp; X[i, :age] &lt; 42.0 then 0.0 else 0.008 +
 if X[i, :nodes] ≥ 7.0 &amp; X[i, :age] ≥ 42.0 then 0.014 else 0.045
and 2 classes: [0, 1].</preformat>
  <p>This shows that the model contains 8 rules where the first rule,
  for example, can be interpreted as:</p>
  <p><italic>If the number of detected axillary nodes is lower than 7,
  then take 0.238, otherwise take 0.046.</italic></p>
  <p>This calculation is done for all 8 rules and the score is summed to
  get a prediction. In essence, the first rule says that if there are
  less than 8 axillary nodes detected, then the patient is more likely
  to survive (<monospace>class == 1</monospace>). Put differently, the
  model states that if there are many axillary nodes detected, then it
  is, unfortunately, less likely that the patient will survive. This
  model is fully interpretable because the model contains a few dozen
  rules which can all be interpreted in isolation and together.</p>
</sec>
<sec id="stability">
  <title>Stability</title>
  <p>Another problem that the SIRUS algorithm addresses is that of model
  stability. A stable model is defined as a model which leads to similar
  conclusions for small changes to data
  (<xref alt="Yu, 2020" rid="ref-yu2020veridical" ref-type="bibr">Yu,
  2020</xref>). Unstable models can be difficult to apply in practice as
  they might require processes to constantly change. This also makes
  such models appear less trustworthy. Put differently, an unstable
  model by definition leads to different conclusions for small changes
  to the data and, hence, small changes to the data could cause a sudden
  drop in predictive performance. One model which suffers from a low
  stability is a decision tree, available via DecisionTree.jl
  (<xref alt="Sadeghi et al., 2022" rid="ref-sadeghi2022decisiontree" ref-type="bibr">Sadeghi
  et al., 2022</xref>), because it will first create the root node of
  the tree, so a small change in the data can cause the root, and
  therefore the rest, of the tree to be completely different
  (<xref alt="Molnar, 2022" rid="ref-molnar2022interpretable" ref-type="bibr">Molnar,
  2022</xref>). Similarly, linear models can be highly sensitive to
  correlated data and, in the case of regularized linear models, the
  choice of hyperparameters. The aforementioned RuleFit algorithm also
  suffers from stability issues due to the unstable combination of tree
  fitting and rule extraction
  (<xref alt="Bénard, Biau, Veiga, et al., 2021" rid="ref-benard2021sirus" ref-type="bibr">Bénard,
  Biau, Veiga, et al., 2021</xref>). The SIRUS algorithm solves this
  problem by stabilizing the trees inside the forest, and the original
  authors have proven the correctness of this stabilization
  mathematically
  (<xref alt="Bénard, Biau, Veiga, et al., 2021" rid="ref-benard2021sirus" ref-type="bibr">Bénard,
  Biau, Veiga, et al., 2021</xref>). In the rest of this paper, we will
  compare the predictive performance of SIRUS.jl to the performance of
  decision trees
  (<xref alt="Sadeghi et al., 2022" rid="ref-sadeghi2022decisiontree" ref-type="bibr">Sadeghi
  et al., 2022</xref>), linear models, XGBoost
  (<xref alt="Chen &amp; Guestrin, 2016" rid="ref-chen2016xgboost" ref-type="bibr">Chen
  &amp; Guestrin, 2016</xref>), and the original (C++/R) SIRUS
  implementation
  (<xref alt="Bénard, Biau, Veiga, et al., 2021" rid="ref-benard2021sirus" ref-type="bibr">Bénard,
  Biau, Veiga, et al., 2021</xref>). The interpretability and stability
  are summarized in Table
  <xref alt="[tab:is]" rid="tabU003Ais">[tab:is]</xref>.</p>
  <boxed-text id="tabU003Ais">
    <table-wrap>
      <caption>
        <p>Summary of interpretability and stability for various
        models.</p>
      </caption>
      <table>
        <thead>
          <tr>
            <th align="left"></th>
            <th align="center"><bold>Decision Tree</bold></th>
            <th align="center"><bold>Linear Model</bold></th>
            <th align="center"><bold>XGBoost</bold></th>
            <th align="center"><bold>SIRUS</bold></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left"><bold>Interpretability</bold></td>
            <td align="center">High</td>
            <td align="center">High</td>
            <td align="center">Medium</td>
            <td align="center">High</td>
          </tr>
          <tr>
            <td align="left"><bold>Stability</bold></td>
            <td align="center">Low</td>
            <td align="center">Medium</td>
            <td align="center">High</td>
            <td align="center">High</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </boxed-text>
</sec>
<sec id="predictive-performance">
  <title>Predictive Performance</title>
  <p>The SIRUS model is based on random forests and therefore well
  suited for settings where the number of variables is comparatively
  large to the number of datapoints
  (<xref alt="Biau &amp; Scornet, 2016" rid="ref-biau2016random" ref-type="bibr">Biau
  &amp; Scornet, 2016</xref>). To make the random forests interpretable,
  the large number of trees are converted to a small number of rules.
  The conversion works by converting each tree to a set of rules and
  then pruning the rules by removing simple duplicates and linearly
  dependent duplicates, see the SIRUS.jl documentation or the original
  paper
  (<xref alt="Bénard, Biau, Da Veiga, et al., 2021" rid="ref-benard2021interpretable" ref-type="bibr">Bénard,
  Biau, Da Veiga, et al., 2021</xref>) for details. In practice, this
  trade-off between between model complexity and interpretability comes
  at a small performance cost.</p>
  <p>To show the performance, we compared SIRUS to a decision tree,
  linear model, XGBoost, and the original (C++/R) SIRUS algorithm;
  similar to Table
  <xref alt="[tab:is]" rid="tabU003Ais">[tab:is]</xref>. We have used
  Julia version 1.9.3 with SIRUS version 1.3.3 (at commit
  <monospace>5c87eda</monospace>), 10-fold cross-validation, and we will
  present variability as <inline-formula><alternatives>
  <tex-math><![CDATA[1.96 * \text{standard error}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>1.96</mml:mn><mml:mo>*</mml:mo><mml:mtext mathvariant="normal">standard error</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>
  for all evaluations with respectively the following datasets, outcome
  variable type, and measures: Haberman’s Survival Dataset
  (<xref alt="Haberman, 1999" rid="ref-haberman1999survival" ref-type="bibr">Haberman,
  1999</xref>) binary classification dataset with AUC, Titanic
  (<xref alt="Eaton &amp; Haas, 1995" rid="ref-eaton1995titanic" ref-type="bibr">Eaton
  &amp; Haas, 1995</xref>) binary classification dataset with Area Under
  the Curve (AUC), Breast Cancer Wisconsin
  (<xref alt="Wolberg &amp; Street, 1995" rid="ref-wolberg1995breast" ref-type="bibr">Wolberg
  &amp; Street, 1995</xref>) binary classification dataset with AUC,
  Pima Indians Diabetes
  (<xref alt="Smith et al., 1988" rid="ref-smith1988using" ref-type="bibr">Smith
  et al., 1988</xref>) binary classification dataset with AUC, Iris
  (<xref alt="Fisher, 1936" rid="ref-fisher1936use" ref-type="bibr">Fisher,
  1936</xref>) multiclass classification dataset with accuracy, and
  Boston Housing
  (<xref alt="Harrison &amp; Rubinfeld, 1978" rid="ref-harrison1978hedonic" ref-type="bibr">Harrison
  &amp; Rubinfeld, 1978</xref>) regression dataset with
  <inline-formula><alternatives>
  <tex-math><![CDATA[\text{R}^2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mtext mathvariant="normal">R</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:math></alternatives></inline-formula>;
  see Table <xref alt="[tab:perf]" rid="tabU003Aperf">[tab:perf]</xref>.
  For full details, see
  <ext-link ext-link-type="uri" xlink:href="https://github.com/rikhuijzer/SIRUS.jl/blob/5c87eda4d0c50e0b78d12d6bd2c4387f5a83f518/test/mlj.jl"><monospace>test/mlj.jl</monospace></ext-link>.
  The performance scores were taken from the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/rikhuijzer/SIRUS.jl/actions/runs/6429413860/attempts/1#summary-17458424403">SIRUS.jl
  test job</ext-link> that ran following commit
  <monospace>5c873da</monospace> using GitHub Actions. The result for
  the Iris dataset for the original SIRUS algorithm is missing because
  the original algorithm has not implemented multiclass
  classification.</p>
  <boxed-text id="tabU003Aperf">
    <table-wrap>
      <caption>
        <p>Predictive performance estimates.</p>
      </caption>
      <table>
        <tbody>
          <tr>
            <td align="left"><bold>Dataset</bold></td>
            <td align="center"><bold>Decision</bold></td>
            <td align="center"><bold>Linear</bold></td>
            <td align="center"><bold>XGBoost</bold></td>
            <td align="center"><bold>XGBoost</bold></td>
            <td align="center"><bold>Original</bold></td>
            <td align="center"><bold>SIRUS.jl</bold></td>
          </tr>
          <tr>
            <td align="left"></td>
            <td align="center"><bold>Tree</bold></td>
            <td align="center"><bold>Model</bold></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><bold>SIRUS</bold></td>
            <td align="center"></td>
          </tr>
          <tr>
            <td align="left"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><bold>max depth:
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathbb{\infty}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi mathvariant="double-struck">∞</mml:mi></mml:math></alternatives></inline-formula></bold></td>
            <td align="center"><bold>max depth: 2</bold></td>
            <td align="center"><bold>max depth: 2</bold></td>
            <td align="center"><bold>max depth: 2</bold></td>
          </tr>
          <tr>
            <td align="left"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"></td>
            <td align="center"><bold>max rules: 10</bold></td>
            <td align="center"><bold>max rules: 10</bold></td>
          </tr>
          <tr>
            <td align="left">Haberman</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.54 \pm 0.06]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.54</mml:mn><mml:mo>±</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.69 \pm 0.06]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.69</mml:mn><mml:mo>±</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.65 \pm 0.04]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.65</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.63 \pm 0.04]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.63</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">0.66</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.67 \pm 0.06]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.67</mml:mn><mml:mo>±</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="left">Titanic</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.76 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.76</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.84 \pm 0.02]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.84</mml:mn><mml:mo>±</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.86 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.86</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.87 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.87</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">0.81</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.83 \pm 0.02]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.83</mml:mn><mml:mo>±</mml:mo><mml:mn>0.02</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="left">Cancer</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.92 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.92</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.98 \pm 0.01]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.98</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.99 \pm 0.00]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.99</mml:mn><mml:mo>±</mml:mo><mml:mn>0.00</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.99 \pm 0.00]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.99</mml:mn><mml:mo>±</mml:mo><mml:mn>0.00</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">0.96</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.98 \pm 0.01]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.98</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="left">Diabetes</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.67 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.67</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.70 \pm 0.06]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.70</mml:mn><mml:mo>±</mml:mo><mml:mn>0.06</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.80 \pm 0.04]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.80</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.82 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.82</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">0.80</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.75 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.75</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="left">Iris</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.95 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.95</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.97 \pm 0.03]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.97</mml:mn><mml:mo>±</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.94 \pm 0.04]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.94</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.93 \pm 0.04]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.93</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.77 \pm 0.08]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.77</mml:mn><mml:mo>±</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td align="left">Boston</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.74 \pm 0.11]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.74</mml:mn><mml:mo>±</mml:mo><mml:mn>0.11</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.70 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.70</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.87 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.87</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.86 \pm 0.05]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.86</mml:mn><mml:mo>±</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
            <td align="center">0.63</td>
            <td align="center"><inline-formula><alternatives>
            <tex-math><![CDATA[0.61 \pm 0.09]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mn>0.61</mml:mn><mml:mo>±</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></alternatives></inline-formula></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </boxed-text>
  <p>At the time of writing, SIRUS’s predictive performance is
  comparable to the linear model and XGBoost on the binary
  classification datasets, that is, Haberman, Titanic, Breast Cancer,
  and Diabetes. The best performance occurs at the Diabetes dataset
  where both XGBoost and the SIRUS models outperform the linear model.
  The reason for this could be that negative effects are often nonlinear
  for fragile systems
  (<xref alt="Taleb, 2020" rid="ref-taleb2020statistical" ref-type="bibr">Taleb,
  2020</xref>). For example, it could be that an increase in oral
  glucose tolerance increases the chance of diabetes exponentially. In
  such cases, the hard cutoff points chosen by tree-based models, such
  as XGBoost and SIRUS, may fit the data better.</p>
  <p>For the multiclass Iris classification and the Boston Housing
  regression datasets, the performance was worse than the other
  non-SIRUS models. It could be that this is caused by a bug in the
  implementation or because this is a fundamental issue in the
  algorithm. Further work is needed to find the root cause or
  workarounds for these low scores. One possible solution would be to
  add SymbolicRegression.jl
  (<xref alt="Cranmer, 2023" rid="ref-cranmer2023interpretable" ref-type="bibr">Cranmer,
  2023</xref>) as a secondary back end for regression tasks. Similar to
  SIRUS.jl, SymbolicRegression.jl can fit expressions of a pre-defined
  form to data albeit with more free parameters, which might fit better
  but also might cause overfitting, depending on the data. This achieves
  performance that is similar to XGBoost
  (<xref alt="Hanson, 2023" rid="ref-hanson2023discourse" ref-type="bibr">Hanson,
  2023</xref>).</p>
  <p>In conclusion, interpretability and stability are often required in
  high-stakes decision making contexts such as personnel or treatment
  selection. In such contexts and when the task is classification,
  SIRUS.jl obtains a reasonable predictive performance, while retaining
  model stability and interpretability.</p>
</sec>
<sec id="code-example">
  <title>Code Example</title>
  <p>The model can be used via the Machine Learning Julia (MLJ)
  (<xref alt="Blaom et al., 2020" rid="ref-blaom2020mlj" ref-type="bibr">Blaom
  et al., 2020</xref>) interface. The following code, for example, was
  used to obtain the fitted model for the Haberman example at the start
  of this paper, and is also available in the SIRUS.jl
  docs<xref ref-type="fn" rid="fn5">5</xref>.</p>
  <p>We first load the dependencies: </p>
  <code language="julia">using CategoricalArrays: categorical
using CSV: CSV
using DataDeps: DataDeps, DataDep, @datadep_str
using DataFrames
using MLJ
using StableRNGs: StableRNG
using SIRUS: StableRulesClassifier</code>
  <p>And specify the Haberman dataset via DataDeps.jl, which allows data
  verification via the checksum and enables caching: </p>
  <code language="julia">function register_haberman()
    name = &quot;Haberman&quot;
    message = &quot;Haberman's Survival Data Set&quot;
    remote_path = &quot;https://github.com/rikhuijzer/haberman-survival-dataset/
        releases/download/v1.0.0/haberman.csv&quot;
    checksum = &quot;a7e9aeb249e11ac17c2b8ea4fdafd5c9392219d27cb819ffaeb8a869eb727a0f&quot;
    DataDeps.register(DataDep(name, message, remote_path, checksum))
end</code>
  <p>Next, we load the data into a <monospace>DataFrame</monospace>:
  </p>
  <code language="julia">function load_haberman()::DataFrame
    register_haberman()
    path = joinpath(datadep&quot;Haberman&quot;, &quot;haberman.csv&quot;)
    df = CSV.read(path, DataFrame)
    df[!, :survival] = categorical(df.survival)
    return df
end</code>
  <p>We split the data into features (<monospace>X</monospace>) and
  outcomes (<monospace>y</monospace>):</p>
  <code language="julia">data = load_haberman()
X = select(data, Not(:survival))
y = data.survival</code>
  <p>We define the model that we want to use with some reasonable
  hyperparameters for this small dataset: </p>
  <code language="julia">model = StableRulesClassifier(; rng=StableRNG(1), q=4, max_depth=2, max_rules=8)</code>
  <p>Finally, we fit the model to the data via MLJ and show the fitted
  model: </p>
  <code language="julia">mach = let
    mach = machine(model, X, y)
    MLJ.fit!(mach)
end

mach.fitresult</code>
  <p>Resulting in the fitresult that was presented at the start of this
  paper.</p>
</sec>
<sec id="funding">
  <title>Funding</title>
  <p>This research was supported by the Ministry of Defence, the
  Netherlands.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>We thank Clément Bénard for his help in re-implementing the SIRUS
  algorithm. Furthermore, we thank Anthony Blaom and Dávid Hanák (Cursor
  Insight) for respectively doing code reviews and finding a critical
  bug.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-julia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bezanson</surname><given-names>Jeff</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
        <name><surname>Karpinski</surname><given-names>Stefan</given-names></name>
        <name><surname>Shah</surname><given-names>Viral B.</given-names></name>
      </person-group>
      <article-title>Julia: A fresh approach to numerical computing</article-title>
      <source>SIAM Review</source>
      <publisher-name>Society for Industrial &amp; Applied Mathematics (SIAM)</publisher-name>
      <year iso-8601-date="2017-01">2017</year><month>01</month>
      <volume>59</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1137%2F141000671</uri>
      <pub-id pub-id-type="doi">10.1137/141000671</pub-id>
      <fpage>65</fpage>
      <lpage>98</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ashari2013performance">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ashari</surname><given-names>Ahmad</given-names></name>
        <name><surname>Paryudi</surname><given-names>Iman</given-names></name>
        <name><surname>Tjoa</surname><given-names>A Min</given-names></name>
      </person-group>
      <article-title>Performance comparison between naı̈ve Bayes, decision tree and k-nearest neighbor in searching alternative design in an energy simulation tool</article-title>
      <source>International Journal of Advanced Computer Science and Applications</source>
      <publisher-name>Citeseer</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>4</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.14569/IJACSA.2013.041105</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-barredo2020explainable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barredo Arrieta</surname><given-names>Alejandro</given-names></name>
        <name><surname>Díaz-Rodríguez</surname><given-names>Natalia</given-names></name>
        <name><surname>Del Ser</surname><given-names>Javier</given-names></name>
        <name><surname>Bennetot</surname><given-names>Adrien</given-names></name>
        <name><surname>Tabik</surname><given-names>Siham</given-names></name>
        <name><surname>Barbado</surname><given-names>Alberto</given-names></name>
        <name><surname>García</surname><given-names>Salvador</given-names></name>
        <name><surname>Gil-López</surname><given-names>Sergio</given-names></name>
        <name><surname>Molina</surname><given-names>Daniel</given-names></name>
        <name><surname>Benjamins</surname><given-names>Richard</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI</article-title>
      <source>Information fusion</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>58</volume>
      <pub-id pub-id-type="doi">10.1016/j.inffus.2019.12.012</pub-id>
      <fpage>82</fpage>
      <lpage>115</lpage>
    </element-citation>
  </ref>
  <ref id="ref-benard2021sirus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bénard</surname><given-names>Clément</given-names></name>
        <name><surname>Biau</surname><given-names>Gérard</given-names></name>
        <name><surname>Veiga</surname><given-names>Sébastien Da</given-names></name>
        <name><surname>Scornet</surname><given-names>Erwan</given-names></name>
      </person-group>
      <article-title>SIRUS: Stable and Interpretable RUle Set for classification</article-title>
      <source>Electronic Journal of Statistics</source>
      <publisher-name>Institute of Mathematical Statistics; Bernoulli Society</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>15</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1214/20-EJS1792</uri>
      <pub-id pub-id-type="doi">10.1214/20-EJS1792</pub-id>
      <fpage>427 </fpage>
      <lpage> 505</lpage>
    </element-citation>
  </ref>
  <ref id="ref-benard2021interpretable">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bénard</surname><given-names>Clément</given-names></name>
        <name><surname>Biau</surname><given-names>Gérard</given-names></name>
        <name><surname>Da Veiga</surname><given-names>Sébastien</given-names></name>
        <name><surname>Scornet</surname><given-names>Erwan</given-names></name>
      </person-group>
      <article-title>Interpretable random forests via rule extraction</article-title>
      <source>International conference on artificial intelligence and statistics</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <fpage>937</fpage>
      <lpage>945</lpage>
    </element-citation>
  </ref>
  <ref id="ref-biau2016random">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Biau</surname><given-names>Gérard</given-names></name>
        <name><surname>Scornet</surname><given-names>Erwan</given-names></name>
      </person-group>
      <article-title>A random forest guided tour</article-title>
      <source>Test</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>25</volume>
      <pub-id pub-id-type="doi">10.1007/s11749-016-0481-7</pub-id>
      <fpage>197</fpage>
      <lpage>227</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blaom2020mlj">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Blaom</surname><given-names>Anthony D.</given-names></name>
        <name><surname>Kiraly</surname><given-names>Franz</given-names></name>
        <name><surname>Lienart</surname><given-names>Thibaut</given-names></name>
        <name><surname>Simillides</surname><given-names>Yiannis</given-names></name>
        <name><surname>Arenas</surname><given-names>Diego</given-names></name>
        <name><surname>Vollmer</surname><given-names>Sebastian J.</given-names></name>
      </person-group>
      <article-title>MLJ: A Julia package for composable machine learning</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>55</issue>
      <pub-id pub-id-type="doi">10.21105/joss.02704</pub-id>
      <fpage>2704</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-breiman2001random">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Breiman</surname><given-names>Leo</given-names></name>
      </person-group>
      <article-title>Random forests</article-title>
      <source>Machine learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2001">2001</year>
      <volume>45</volume>
      <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      <fpage>5</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chen2016xgboost">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Tianqi</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
      </person-group>
      <article-title>XGBoost: A scalable tree boosting system</article-title>
      <source>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1145/2939672.2939785</pub-id>
      <fpage>785</fpage>
      <lpage>794</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cranmer2023interpretable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cranmer</surname><given-names>Miles</given-names></name>
      </person-group>
      <article-title>Interpretable machine learning for science with PySR and SymbolicRegression.jl</article-title>
      <source>arXiv preprint arXiv:2305.01582</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2305.01582</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-doshi2017towards">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Doshi-Velez</surname><given-names>Finale</given-names></name>
        <name><surname>Kim</surname><given-names>Been</given-names></name>
      </person-group>
      <article-title>Towards a rigorous science of interpretable machine learning</article-title>
      <source>arXiv preprint arXiv:1702.08608</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1702.08608</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-eaton1995titanic">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Eaton</surname><given-names>John P</given-names></name>
        <name><surname>Haas</surname><given-names>Charles</given-names></name>
      </person-group>
      <source>Titanic: Triumph and tragedy</source>
      <publisher-name>WW Norton &amp; Company</publisher-name>
      <year iso-8601-date="1995">1995</year>
    </element-citation>
  </ref>
  <ref id="ref-fisher1936use">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fisher</surname><given-names>Ronald A</given-names></name>
      </person-group>
      <article-title>The use of multiple measurements in taxonomic problems</article-title>
      <source>Annals of eugenics</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1936">1936</year>
      <volume>7</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1111/j.1469-1809.1936.tb02137.x</pub-id>
      <fpage>179</fpage>
      <lpage>188</lpage>
    </element-citation>
  </ref>
  <ref id="ref-haberman1999survival">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Haberman</surname><given-names>S</given-names></name>
      </person-group>
      <article-title>Habermanś Survival</article-title>
      <publisher-name>UCI Machine Learning Repository</publisher-name>
      <year iso-8601-date="1999">1999</year>
      <uri>https://doi.org/10.24432/C5XK51</uri>
      <pub-id pub-id-type="doi">10.24432/C5XK51</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-hanson2023discourse">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hanson</surname><given-names>Eric</given-names></name>
      </person-group>
      <article-title>[ANN] SIRUS.jl v1.2: Interpretable machine learning via rule extraction</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://discourse.julialang.org/t/ann-sirus-jl-v1-2-interpretable-machine-learning-via-rule-extraction/100932/3</uri>
    </element-citation>
  </ref>
  <ref id="ref-harrison1978hedonic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harrison</surname><given-names>David</given-names></name>
        <name><surname>Rubinfeld</surname><given-names>Daniel L</given-names></name>
      </person-group>
      <article-title>Hedonic housing prices and the demand for clean air</article-title>
      <source>Journal of environmental economics and management</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="1978">1978</year>
      <volume>5</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1016/0095-0696(78)90006-2</pub-id>
      <fpage>81</fpage>
      <lpage>102</lpage>
    </element-citation>
  </ref>
  <ref id="ref-james2013introduction">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>James</surname><given-names>Gareth</given-names></name>
        <name><surname>Witten</surname><given-names>Daniela</given-names></name>
        <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <source>An introduction to statistical learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <volume>112</volume>
      <pub-id pub-id-type="doi">10.1007/978-1-0716-1418-1</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lundberg2017unified">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lundberg</surname><given-names>Scott M</given-names></name>
        <name><surname>Lee</surname><given-names>Su-In</given-names></name>
      </person-group>
      <article-title>A unified approach to interpreting model predictions</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
    </element-citation>
  </ref>
  <ref id="ref-molnar2022interpretable">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Molnar</surname><given-names>Christoph</given-names></name>
      </person-group>
      <source>Interpretable machine learning</source>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-sadeghi2022decisiontree">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Sadeghi</surname><given-names>Ben</given-names></name>
        <name><surname>Chiarawongse</surname><given-names>Poom</given-names></name>
        <name><surname>Squire</surname><given-names>Kevin</given-names></name>
        <name><surname>Jones</surname><given-names>Daniel C.</given-names></name>
        <name><surname>Noack</surname><given-names>Andreas</given-names></name>
        <name><surname>St-Jean</surname><given-names>Cédric</given-names></name>
        <name><surname>Huijzer</surname><given-names>Rik</given-names></name>
        <name><surname>Schätzle</surname><given-names>Roland</given-names></name>
        <name><surname>Butterworth</surname><given-names>Ian</given-names></name>
        <name><surname>Peng</surname><given-names>Yu-Fong</given-names></name>
        <name><surname>Blaom</surname><given-names>Anthony</given-names></name>
      </person-group>
      <article-title>DecisionTree.jl - a Julia implementation of the CART Decision Tree and Random Forest algorithms</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-11">2022</year><month>11</month>
      <pub-id pub-id-type="doi">10.5281/zenodo.7359268</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-smith1988using">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Smith</surname><given-names>Jack W</given-names></name>
        <name><surname>Everhart</surname><given-names>James E</given-names></name>
        <name><surname>Dickson</surname><given-names>WC</given-names></name>
        <name><surname>Knowler</surname><given-names>William C</given-names></name>
        <name><surname>Johannes</surname><given-names>Robert Scott</given-names></name>
      </person-group>
      <article-title>Using the ADAP learning algorithm to forecast the onset of diabetes mellitus</article-title>
      <source>Proceedings of the annual symposium on computer application in medical care</source>
      <publisher-name>American Medical Informatics Association</publisher-name>
      <year iso-8601-date="1988">1988</year>
      <fpage>261</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-taleb2020statistical">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Taleb</surname><given-names>Nassim Nicholas</given-names></name>
      </person-group>
      <article-title>Statistical consequences of fat tails: Real world preasymptotics, epistemology, and applications</article-title>
      <source>arXiv preprint arXiv:2001.10488</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2001.10488</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-innes2018flux">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Innes</surname><given-names>Mike</given-names></name>
      </person-group>
      <article-title>Flux: Elegant machine learning with Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>25</issue>
      <uri>https://doi.org/10.21105/joss.00602</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00602</pub-id>
      <fpage>602</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ke2017lightgbm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ke</surname><given-names>Guolin</given-names></name>
        <name><surname>Meng</surname><given-names>Qi</given-names></name>
        <name><surname>Finley</surname><given-names>Thomas</given-names></name>
        <name><surname>Wang</surname><given-names>Taifeng</given-names></name>
        <name><surname>Chen</surname><given-names>Wei</given-names></name>
        <name><surname>Ma</surname><given-names>Weidong</given-names></name>
        <name><surname>Ye</surname><given-names>Qiwei</given-names></name>
        <name><surname>Liu</surname><given-names>Tie-Yan</given-names></name>
      </person-group>
      <article-title>LightGBM: A highly efficient gradient boosting decision tree</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
    </element-citation>
  </ref>
  <ref id="ref-lobianco2021betaml">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lobianco</surname><given-names>Antonello</given-names></name>
      </person-group>
      <article-title>BetaML: The beta machine learning toolkit, a self-contained repository of machine learning algorithms in Julia</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>60</issue>
      <uri>https://doi.org/10.21105/joss.02849</uri>
      <pub-id pub-id-type="doi">10.21105/joss.02849</pub-id>
      <fpage>2849</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-wolberg1995breast">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wolberg</surname><given-names>Mangasarian</given-names><suffix>William</suffix></name>
        <name><surname>Street</surname><given-names>W.</given-names></name>
      </person-group>
      <article-title>Breast Cancer Wisconsin (Diagnostic)</article-title>
      <publisher-name>UCI Machine Learning Repository</publisher-name>
      <year iso-8601-date="1995">1995</year>
      <pub-id pub-id-type="doi">10.24432/C5DW2B</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-yu2020veridical">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yu</surname><given-names>Bin</given-names></name>
      </person-group>
      <article-title>Veridical data science</article-title>
      <source>Proceedings of the 13th international conference on web search and data mining</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1073/pnas.1901326117</pub-id>
      <fpage>4</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Source code available at
    <ext-link ext-link-type="uri" xlink:href="https://github.com/rikhuijzer/SIRUS.jl">https://github.com/rikhuijzer/SIRUS.jl</ext-link>.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Source code available at
    <ext-link ext-link-type="uri" xlink:href="https://github.com/dfdx/NaiveBayes.jl">https://github.com/dfdx/NaiveBayes.jl</ext-link>.</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>Source code available at
    <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/ExpandingMan/Shapley.jl">https://gitlab.com/ExpandingMan/Shapley.jl</ext-link>.</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>Source code available at
    <ext-link ext-link-type="uri" xlink:href="https://gitlab.com/drti/sirus">https://gitlab.com/drti/sirus</ext-link>.</p>
  </fn>
  <fn id="fn5">
    <label>5</label><p><ext-link ext-link-type="uri" xlink:href="https://sirus.jl.huijzer.xyz/dev/basic-example/">https://sirus.jl.huijzer.xyz/dev/basic-example/</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
