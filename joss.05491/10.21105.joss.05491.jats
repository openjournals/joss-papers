<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5491</article-id>
<article-id pub-id-type="doi">10.21105/joss.05491</article-id>
<title-group>
<article-title>ViMag: A Visual Vibration Analysis
Toolbox</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6421-7351</contrib-id>
<name>
<surname>Lado-Roigé</surname>
<given-names>Ricard</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4140-1823</contrib-id>
<name>
<surname>Pérez</surname>
<given-names>Marco A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>IQS School of Engineering, Universitat Ramon Llull, Via
Augusta 390, 08017 Barcelona, Spain</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-11-28">
<day>28</day>
<month>11</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>87</issue>
<fpage>5491</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>deep-learning</kwd>
<kwd>modal analysis</kwd>
<kwd>condition monitoring</kwd>
<kwd>structural health monitoring</kwd>
<kwd>motion magnification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Recent developments in computer vision have brought about a new set
  of techniques called Video Motion Magnification, which are capable of
  identifying and magnifying eye-imperceptible movements in video data.
  These techniques have proved effective in applications such as
  producing visual representations of an object’s operating deflection
  shapes or recovering sound from a room behind soundproof glass. Our
  research explores the new possibilities of motion magnification
  applied to Structural Health Monitoring (SHM) and vibration testing,
  harnessing the latest advances in deep-learning to achieve
  state-of-the-art results.</p>
  <p>Vision-based damage detection techniques can reduce sensor
  deployment costs while providing accurate, useful, and full-field
  readings of structural behavior. We present a new video processing
  approach that allows the treatment of video data to obtain vibrational
  signatures of complex structures. This approach enables the
  identification of very light structural damage in a controlled lab
  environment. The presented software is based on the use of
  state-of-the-art deep-learning video motion magnification techniques
  to offer an easy-to-use, effective, full-field tool for SHM at a
  fraction of the cost of contact-based techniques.</p>
</sec>
<sec id="related-work">
  <title>Related work</title>
  <p>This work is based on the method developed by Lado-Roigé et al.
  (<xref alt="2022" rid="ref-LADOROIGE2022112218" ref-type="bibr">2022</xref>)
  for vibration-based damage detection and on the Swin Transformer Based
  Video Motion Magnification (STB-VMM) method
  (<xref alt="Lado-Roigé &amp; Pérez, 2023" rid="ref-lado2022_STB-VMM" ref-type="bibr">Lado-Roigé
  &amp; Pérez, 2023</xref>), which improves on the previous motion
  magnification backend
  (<xref alt="Oh et al., 2018" rid="ref-oh_learning-based_2018" ref-type="bibr">Oh
  et al., 2018</xref>) in terms of image quality.</p>
  <p>Other researchers have used similar techniques for vibration
  testing
  (<xref alt="Eitner et al., 2021" rid="ref-EITNER2021106995" ref-type="bibr">Eitner
  et al., 2021</xref>;
  <xref alt="Molina-Viedma et al., 2018" rid="ref-MOLINAVIEDMA2018245" ref-type="bibr">Molina-Viedma
  et al., 2018</xref>). However, to the authors’ knowledge, non have
  released a software tool to go along with their publications. ViMag
  offers a simple interface to replicate some of these experiments using
  state-of-the-art learning-based video motion magnification.</p>
  <fig>
    <caption><p>Video sequence to signal pipeline
    <styled-content id="figU003Apipeline"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/pipeline_chart.png" />
  </fig>
  <p>Motion magnification is a video processing technique that consists
  on the transformation of input frames to exaggerate motion. The goal
  of these algorithms is to amplify subtle motions in a video sequence,
  allowing the visualization of vibrations and deformations that would
  otherwise be invisible. Video motion magnification was first developed
  by C. Liu et al.
  (<xref alt="2005" rid="ref-liu_motion_2005" ref-type="bibr">2005</xref>)
  and opened a new range of possibilities for research, however, this
  first approach produced numerous visual artifacts on top of being
  computationally expensive. Years later,further developments by Wu et
  al.
  (<xref alt="2012" rid="ref-wu_eulerian_2012" ref-type="bibr">2012</xref>)
  introduced a novel Eulerian approach to magnification that produced
  much cleaner results with less computational cost, paving the way for
  newer and more refined algorithms that produced increasingly better
  results such as
  (<xref alt="Wadhwa et al., 2014" rid="ref-wadhwa_riesz_2014" ref-type="bibr">Wadhwa
  et al., 2014</xref>),
  (<xref alt="Oh et al., 2018" rid="ref-oh_learning-based_2018" ref-type="bibr">Oh
  et al., 2018</xref>) or
  (<xref alt="Lado-Roigé &amp; Pérez, 2023" rid="ref-lado2022_STB-VMM" ref-type="bibr">Lado-Roigé
  &amp; Pérez, 2023</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>ViMag provides an easy-to-use graphical user interface aimed at
  extracting time-series signals of vibrating machinery and structure
  videos. This software enables the visualization of videos, selection
  of magnification area, and signal processing. Consequently, it
  facilitates and automates the technique developed by Lado-Roigé et al.
  (<xref alt="2022" rid="ref-LADOROIGE2022112218" ref-type="bibr">2022</xref>)
  and allows machine learning layman to obtain reliable results without
  having to apply a manual multistage image processing pipeline.
  Therefore, this software facilitates the use of a camera as a
  functional replacement for an accelerometer by employing STB-VMM as
  the motion magnification backend.</p>
  <p>The intended use of ViMag is to support the assessment of
  mechanical systems’ performance, such as machines or structures.
  Researchers and engineers should consider employing condition
  monitoring or SHM methodologies on the outcomes yielded by ViMag. Such
  techniques are defined as the set of analysis and assessment tools
  applied to autonomously determine the integrity and durability of
  engineering structures. These techniques are aimed at tracking the
  operational status, assessing the condition, and alerting to the
  changes in the geometric or material properties that can affect a
  structure’s overall performance, safety, reliability, and operational
  life
  (<xref alt="Cosenza &amp; Manfredi, 2000" rid="ref-cosenza_damage_2000" ref-type="bibr">Cosenza
  &amp; Manfredi, 2000</xref>;
  <xref alt="Frangopol &amp; Curley, 1987" rid="ref-frangopol_effects_1987" ref-type="bibr">Frangopol
  &amp; Curley, 1987</xref>).</p>
  <p>However, the use for ViMag might not be constrained to mechanical
  engineering exclusively, and some other interesting applications could
  also benefit from the software, such as medical applications
  (<xref alt="Janatka et al., 2020" rid="ref-10.1007U002F978-3-030-59716-0_34" ref-type="bibr">Janatka
  et al., 2020</xref>) or miscellaneous technical demos like recovering
  sound from video
  (<xref alt="Davis et al., 2014" rid="ref-10.1145U002F2601097.2601119" ref-type="bibr">Davis
  et al., 2014</xref>).</p>
</sec>
<sec id="video-processing-workflow">
  <title>Video processing workflow</title>
  <p><xref alt="[fig:pipeline]" rid="figU003Apipeline">[fig:pipeline]</xref>
  presents a graph depicting the process of converting a video sequence
  to a discrete signal. To begin the signal extraction process, the user
  is asked to select a linear region of interest, preferably on a
  high-contrast area of the frame. Then, the area surrounding the
  selected region of interest is magnified using STB-VMM throughout the
  target video sequence’s length. The motion-magnified result is then
  converted into a single image that represents movement in the temporal
  domain, achieved by extracting the selected linear region in each of
  the frames and stacking them horizontally as shown in
  <xref alt="[fig:slice_gen]" rid="figU003Aslice_gen">[fig:slice_gen]</xref>.
  Finally, an edge detection algorithm is run over the temporal slice to
  determine the discrete temporal signal and convert it into an array of
  values over time. From this point on, existing signal processing
  techniques, such as the Fourier transform, can be used to extract
  further information.</p>
  <fig>
    <caption><p>Video sequence transformation to temporal slice
    <styled-content id="figU003Aslice_gen"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/slice_gen.png" />
  </fig>
  <p>Motion magnification acts like a microscope for motion, magnifying
  tiny movements on video sequences to retrieve seemingly invisible or
  almost imperceptible movements. Consequently, motion magnification may
  allow the naked eye to see a structure’s operating deflection shapes
  as they happened in real operating conditions. The STB-VMM model
  consists of three main functional blocks that extract features from
  frames, manipulate those features and finally reconstruct the frames.
  Implemented in PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-NEURIPS2019_9015" ref-type="bibr">Paszke
  et al., 2019</xref>), STB-VMM borrows ideas from Dosovitskiy et al.
  (<xref alt="2020" rid="ref-ViT" ref-type="bibr">2020</xref>), Vaswani
  et al.
  (<xref alt="2017" rid="ref-Vaswani2017AttentionIA" ref-type="bibr">2017</xref>),
  and Z. Liu et al.
  (<xref alt="2021" rid="ref-SWIN" ref-type="bibr">2021</xref>) to
  improve the image quality offered by prior motion magnification
  methods at the cost of some performance. The lack of temporal
  filtering and the higher image quality offered by STB-VMM play an
  important role in applications that require precise magnification for
  vibration monitoring, as less-noisy images produce clearer signals
  that highlight abnormal behaviors sooner.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors would like to gratefully acknowledge the support and
  funding of the Catalan Agency for Business Competitiveness (ACCIÓ)
  through the project INNOTEC ISAPREF 2021. Additionally, the first
  author would like to acknowledge a Doctoral Scholarship from IQS.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-LADOROIGE2022112218">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lado-Roigé</surname><given-names>Ricard</given-names></name>
        <name><surname>Font-Moré</surname><given-names>Josep</given-names></name>
        <name><surname>Pérez</surname><given-names>Marco A.</given-names></name>
      </person-group>
      <article-title>Learning-based video motion magnification approach for vibration-based damage detection</article-title>
      <source>Measurement</source>
      <year iso-8601-date="2022">2022</year>
      <issn>0263-2241</issn>
      <pub-id pub-id-type="doi">10.1016/j.measurement.2022.112218</pub-id>
      <fpage>112218</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lado2022_STB-VMM">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lado-Roigé</surname><given-names>Ricard</given-names></name>
        <name><surname>Pérez</surname><given-names>Marco A.</given-names></name>
      </person-group>
      <article-title>STB-VMM: Swin transformer based video motion magnification</article-title>
      <source>Knowledge-Based Systems</source>
      <year iso-8601-date="2023">2023</year>
      <issn>0950-7051</issn>
      <pub-id pub-id-type="doi">10.1016/j.knosys.2023.110493</pub-id>
      <fpage>110493</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-oh_learning-based_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Oh</surname><given-names>Tae-Hyun</given-names></name>
        <name><surname>Jaroensri</surname><given-names>Ronnachai</given-names></name>
        <name><surname>Kim</surname><given-names>Changil</given-names></name>
        <name><surname>Elgharib</surname><given-names>Mohamed</given-names></name>
        <name><surname>Durand</surname><given-names>Frédo</given-names></name>
        <name><surname>Freeman</surname><given-names>William T.</given-names></name>
        <name><surname>Matusik</surname><given-names>Wojciech</given-names></name>
      </person-group>
      <article-title>Learning-based video motion magnification</article-title>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1804.02684</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-MOLINAVIEDMA2018245">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Molina-Viedma</surname><given-names>A. J.</given-names></name>
        <name><surname>Felipe-Sesé</surname><given-names>L.</given-names></name>
        <name><surname>López-Alba</surname><given-names>E.</given-names></name>
        <name><surname>Díaz</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>High frequency mode shapes characterisation using digital image correlation and phase-based motion magnification</article-title>
      <source>Mech. Syst. Sig. Process.</source>
      <year iso-8601-date="2018">2018</year>
      <volume>102</volume>
      <issn>0888-3270</issn>
      <pub-id pub-id-type="doi">10.1016/j.ymssp.2017.09.019</pub-id>
      <fpage>245</fpage>
      <lpage>261</lpage>
    </element-citation>
  </ref>
  <ref id="ref-EITNER2021106995">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eitner</surname><given-names>Marc</given-names></name>
        <name><surname>Miller</surname><given-names>Benjamin</given-names></name>
        <name><surname>Sirohi</surname><given-names>Jayant</given-names></name>
        <name><surname>Tinney</surname><given-names>Charles</given-names></name>
      </person-group>
      <article-title>Effect of broad-band phase-based motion magnification on modal parameter estimation</article-title>
      <source>Mechanical Systems and Signal Processing</source>
      <year iso-8601-date="2021">2021</year>
      <volume>146</volume>
      <issn>0888-3270</issn>
      <pub-id pub-id-type="doi">10.1016/j.ymssp.2020.106995</pub-id>
      <fpage>106995</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F2601097.2601119">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Davis</surname><given-names>Abe</given-names></name>
        <name><surname>Rubinstein</surname><given-names>Michael</given-names></name>
        <name><surname>Wadhwa</surname><given-names>Neal</given-names></name>
        <name><surname>Mysore</surname><given-names>Gautham J.</given-names></name>
        <name><surname>Durand</surname><given-names>Frédo</given-names></name>
        <name><surname>Freeman</surname><given-names>William T.</given-names></name>
      </person-group>
      <article-title>The visual microphone: Passive recovery of sound from video</article-title>
      <source>ACM Trans. Graph.</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2014-07">2014</year><month>07</month>
      <volume>33</volume>
      <issue>4</issue>
      <issn>0730-0301</issn>
      <uri>https://doi.org/10.1145/2601097.2601119</uri>
      <pub-id pub-id-type="doi">10.1145/2601097.2601119</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-frangopol_effects_1987">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Frangopol</surname><given-names>Dan M.</given-names></name>
        <name><surname>Curley</surname><given-names>James P.</given-names></name>
      </person-group>
      <article-title>Effects of damage and redundancy on structural reliability</article-title>
      <source>J. Struct. Eng.</source>
      <year iso-8601-date="1987">1987</year>
      <volume>113</volume>
      <issue>7</issue>
      <issn>0733-9445</issn>
      <pub-id pub-id-type="doi">10.1061/(ASCE)0733-9445(1987)113:7(1533)</pub-id>
      <fpage>1533</fpage>
      <lpage>1549</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cosenza_damage_2000">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cosenza</surname><given-names>Edoardo</given-names></name>
        <name><surname>Manfredi</surname><given-names>Gaetano</given-names></name>
      </person-group>
      <article-title>Damage indices and damage measures</article-title>
      <source>Prog. in Struct. Eng. Mater.s</source>
      <year iso-8601-date="2000">2000</year>
      <volume>2</volume>
      <issue>1</issue>
      <issn>1365-0556</issn>
      <pub-id pub-id-type="doi">10.1002/(SICI)1528-2716(200001/03)2:1&lt;50::AID-PSE7&gt;3.0.CO;2-S</pub-id>
      <fpage>50</fpage>
      <lpage>59</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NEURIPS2019_9015">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ViT">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dosovitskiy</surname><given-names>Alexey</given-names></name>
        <name><surname>Beyer</surname><given-names>Lucas</given-names></name>
        <name><surname>Kolesnikov</surname><given-names>Alexander</given-names></name>
        <name><surname>Weissenborn</surname><given-names>Dirk</given-names></name>
        <name><surname>Zhai</surname><given-names>Xiaohua</given-names></name>
        <name><surname>Unterthiner</surname><given-names>Thomas</given-names></name>
        <name><surname>Dehghani</surname><given-names>Mostafa</given-names></name>
        <name><surname>Minderer</surname><given-names>Matthias</given-names></name>
        <name><surname>Heigold</surname><given-names>Georg</given-names></name>
        <name><surname>Gelly</surname><given-names>Sylvain</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Houlsby</surname><given-names>Neil</given-names></name>
      </person-group>
      <article-title>An image is worth 16x16 words: Transformers for image recognition at scale</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2010.11929</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Vaswani2017AttentionIA">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N.</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention is all you need</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.48550/ARXIV.1706.03762</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-SWIN">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Ze</given-names></name>
        <name><surname>Lin</surname><given-names>Yutong</given-names></name>
        <name><surname>Cao</surname><given-names>Yue</given-names></name>
        <name><surname>Hu</surname><given-names>Han</given-names></name>
        <name><surname>Wei</surname><given-names>Yixuan</given-names></name>
        <name><surname>Zhang</surname><given-names>Zheng</given-names></name>
        <name><surname>Lin</surname><given-names>Stephen</given-names></name>
        <name><surname>Guo</surname><given-names>Baining</given-names></name>
      </person-group>
      <article-title>Swin transformer: Hierarchical vision transformer using shifted windows</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2103.14030</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-10.1007U002F978-3-030-59716-0_34">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Janatka</surname><given-names>Mirek</given-names></name>
        <name><surname>Marcus</surname><given-names>Hani J.</given-names></name>
        <name><surname>Dorward</surname><given-names>Neil L.</given-names></name>
        <name><surname>Stoyanov</surname><given-names>Danail</given-names></name>
      </person-group>
      <article-title>Surgical video motion magnification with suppression of instrument artefacts</article-title>
      <source>Medical image computing and computer assisted intervention – MICCAI 2020</source>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2020">2020</year>
      <isbn>978-3-030-59716-0</isbn>
      <pub-id pub-id-type="doi">10.48550/arXiv.2009.07432</pub-id>
      <fpage>353</fpage>
      <lpage>363</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liu_motion_2005">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Ce</given-names></name>
        <name><surname>Torralba</surname><given-names>Antonio</given-names></name>
        <name><surname>Freeman</surname><given-names>William T.</given-names></name>
        <name><surname>Durand</surname><given-names>Frédo</given-names></name>
        <name><surname>Adelson</surname><given-names>Edward H.</given-names></name>
      </person-group>
      <article-title>Motion magnification</article-title>
      <source>ACM SIGGRAPH 2005 papers</source>
      <publisher-loc>Los Angeles, California</publisher-loc>
      <year iso-8601-date="2005">2005</year>
      <isbn>9781450378253</isbn>
      <pub-id pub-id-type="doi">10.1145/1186822.1073223</pub-id>
      <fpage>519</fpage>
      <lpage>526</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wu_eulerian_2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Hao-Yu</given-names></name>
        <name><surname>Rubinstein</surname><given-names>Michael</given-names></name>
        <name><surname>Shih</surname><given-names>Eugene</given-names></name>
        <name><surname>Guttag</surname><given-names>J.</given-names></name>
        <name><surname>Durand</surname><given-names>F.</given-names></name>
        <name><surname>Freeman</surname><given-names>W.</given-names></name>
      </person-group>
      <article-title>Eulerian video magnification for revealing subtle changes in the world</article-title>
      <source>ACM Trans. Graph.</source>
      <year iso-8601-date="2012">2012</year>
      <pub-id pub-id-type="doi">10.1145/2185520.2185561</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wadhwa_riesz_2014">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wadhwa</surname><given-names>Neal</given-names></name>
        <name><surname>Rubinstein</surname><given-names>Michael</given-names></name>
        <name><surname>Durand</surname><given-names>Fredo</given-names></name>
        <name><surname>Freeman</surname><given-names>William</given-names></name>
      </person-group>
      <article-title>Riesz pyramids for fast phase-based video magnification</article-title>
      <source>2014 IEEE ICCP</source>
      <year iso-8601-date="2014">2014</year>
      <isbn>978-1-4799-5188-8</isbn>
      <pub-id pub-id-type="doi">10.1109/ICCPHOT.2014.6831820</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
