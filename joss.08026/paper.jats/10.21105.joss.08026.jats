<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8026</article-id>
<article-id pub-id-type="doi">10.21105/joss.08026</article-id>
<title-group>
<article-title>Calzone: A Python package for measuring calibration of
probabilistic models for classification</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8246-4751</contrib-id>
<name>
<surname>Fan</surname>
<given-names>Kwok Lung</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9779-1165</contrib-id>
<name>
<surname>Pennello</surname>
<given-names>Gene</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4053-4213</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Qi</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5167-8899</contrib-id>
<name>
<surname>Petrick</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6661-4801</contrib-id>
<name>
<surname>Samala</surname>
<given-names>Ravi K.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1130-0303</contrib-id>
<name>
<surname>Samuelson</surname>
<given-names>Frank W.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9653-7707</contrib-id>
<name>
<surname>Thompson</surname>
<given-names>Yee Lam Elim</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3900-0433</contrib-id>
<name>
<surname>Cao</surname>
<given-names>Qian</given-names>
</name>
<email>qian.cao@fda.hhs.gov</email>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>U.S. Food and Drug Administration</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email>qian.cao@fda.hhs.gov</email></corresp>
</author-notes>
<volume>10</volume>
<issue>114</issue>
<fpage>8026</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Machine Learning</kwd>
<kwd>Artificial Intelligence</kwd>
<kwd>Calibration</kwd>
<kwd>Probabilistic models</kwd>
<kwd>Metric</kwd>
<kwd>Evaluation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Calzone</monospace> is a Python package for evaluating
  the calibration of probabilistic outputs of classifier models. It
  provides a set of functions for visualizing calibration and computing
  calibration metrics given a representative dataset with the model’s
  predictions and the true class labels. The metrics provided in
  <monospace>Calzone</monospace> include: expected calibration error
  (ECE), maximum calibration error (MCE), Hosmer-Lemeshow (HL) test,
  integrated calibration index (ICI), Spiegelhalter’s Z-statistic and
  Cox’s calibration slope/intercept. The package is designed with
  versatility in mind. Many metrics allow users to adjust binning
  schemes and choose between top-class or class-wise calculations.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Classification is one of the most common applications in machine
  learning. Metrics associated with discrimination performance
  (resolution), such as area under the curve (AUC), sensitivity (Se,
  true positive rate), and specificity (Sp, 1 - false positive rate) are
  typically used to characterize classification performance Hastie et
  al.
  (<xref alt="2001" rid="ref-statistical_learning" ref-type="bibr">2001</xref>).
  These metrics may be sufficient if the outputs of the model are not
  meant to be interpreted as a probability.</p>
  <p>However, Diamond
  (<xref alt="1992" rid="ref-DIAMOND199285" ref-type="bibr">1992</xref>)
  showed that the resolution (i.e., high performance) of a model does
  not indicate the reliability/calibration of the model. Calibration is
  the agreement between predicted and true probabilities,
  <inline-formula><alternatives>
  <tex-math><![CDATA[P(D=1|\hat{p}=p) = p]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mover><mml:mi>p</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  defined as moderate calibration by Van Calster &amp; Steyerberg
  (<xref alt="2018" rid="ref-Calster_weak_cal" ref-type="bibr">2018</xref>),
  also known as model reliability. Bröcker
  (<xref alt="2009" rid="ref-Brocker_decompose" ref-type="bibr">2009</xref>)
  later showed that any proper scoring rule can be decomposed into the
  resolution and reliability. Thus, a model with high resolution may
  still lack reliability. In high-risk medical applications such as
  computer-aided diagnosis, reliability enables the correct
  interpretation of model output, and for downstream treatment
  decisions.</p>
  <p>While existing libraries such as
  <monospace>scikit-learn</monospace> include basic tools like
  reliability diagrams and expected calibration error, they lack support
  for more comprehensive and flexible evaluation metrics—such as
  reliability diagrams with error bars, class-conditional calibration
  error, different binning schemes or statistical significance testing
  for miscalibration. This is also the case with other
  calibration-focused libraries, such as
  <monospace>ml-calibration</monospace>,
  <monospace>uncertainty-toolbox</monospace>, and
  <monospace>pycaleva</monospace>. For example,
  <monospace>ml-calibration</monospace> provides advanced controls for
  plotting reliability diagrams and computing smooth expected
  calibration error but does not include statistical tests for
  miscalibration
  (<xref alt="Blasiok &amp; Nakkiran, 2024" rid="ref-ml-calibration" ref-type="bibr">Blasiok
  &amp; Nakkiran, 2024</xref>). The
  <monospace>uncertainty-toolbox</monospace> focuses on calibration
  methods rather than assessment
  (<xref alt="Chung et al., 2021" rid="ref-uncertaintyToolbox" ref-type="bibr">Chung
  et al., 2021</xref>). The <monospace>pycaleva</monospace> package
  overlaps with many functionalities in <monospace>Calzone</monospace>,
  but it does not support Cox’s calibration analysis, Wald intervals for
  reliability, or custom curve fitting methods for expected calibration
  error
  (<xref alt="Martin Weigl, 2022" rid="ref-pycaleva" ref-type="bibr">Martin
  Weigl, 2022</xref>). In contrast, <monospace>Calzone</monospace>
  emphasizes the evaluation of calibration. It features a comprehensive
  set of calibration metrics, statistical tests (e.g., hypothesis
  testing for miscalibration), and visualization tools tailored for many
  types of classification tasks (e.g., multi-class metrics). The package
  is designed to help users not only visualize miscalibration but also
  quantify and statistically validate it in a consistent and
  interpretable way.</p>
</sec>
<sec id="software-description">
  <title>Software description</title>
  <sec id="input-data">
    <title>Input data</title>
    <p>To evaluate the calibration of a model, users need a
    representative dataset from the intended population. The dataset
    should contain the true class labels and the model’s predicted
    probabilities. In <monospace>Calzone</monospace>, the dataset can be
    a CSV file or two NumPy arrays containing the true labels and
    predicted probabilities.</p>
  </sec>
  <sec id="reliability-diagram">
    <title>Reliability Diagram</title>
    <p>The reliability diagram is a graphical representation of the
    calibration
    (<xref alt="Bröcker &amp; Smith, 2007" rid="ref-Brocker_reldia" ref-type="bibr">Bröcker
    &amp; Smith, 2007</xref>;
    <xref alt="Murphy &amp; Winkler, 1977" rid="ref-Murphy_reliability" ref-type="bibr">Murphy
    &amp; Winkler, 1977</xref>). It groups the predicted probabilities
    into bins and plots the mean predicted probability against the
    empirical frequency in each bin. The reliability diagram can be used
    to qualitatively assess the calibration of the model. The confidence
    intervals of the empirical frequency are calculated using Wilson’s
    score interval
    (<xref alt="Wilson, 1927" rid="ref-wilson_interval" ref-type="bibr">Wilson,
    1927</xref>).</p>
    <code language="python">from calzone.utils import reliability_diagram
from calzone.vis import plot_reliability_diagram
reliability, confidence, bin_edges, bin_counts = reliability_diagram(
    labels,
    probs,
    num_bins=15,
    class_to_plot=1
) 

plot_reliability_diagram(
    reliability,
    confidence,
    bin_counts,
    error_bar=True,
    title='Reliability diagram'
    
)</code>
    <fig>
      <caption><p>Reliability Diagram for class 1 with simulated data.
      <styled-content id="figU003Areldia"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="reliability_diagram_calzonepaper.png" />
    </fig>
  </sec>
  <sec id="calibration-metrics">
    <title>Calibration metrics</title>
    <p><monospace>Calzone</monospace> provides functions to compute
    various calibration metrics, including methods to compute expected
    calibration error and statistical tests to assess calibration. These
    functions provide quantitative metrics for users to evaluate the
    calibration performance of the model. The
    <monospace>CalibrationMetrics()</monospace> class allows the user to
    compute the calibration metrics in a more convenient way. The
    following are metrics that are currently supported in
    <monospace>Calzone</monospace>:</p>
    <sec id="expected-calibration-error-ece-and-maximum-calibration-error-mce">
      <title>Expected Calibration Error (ECE) and Maximum Calibration
      Error (MCE)</title>
      <p>Expected calibration error (ECE) and maximum calibration error
      (MCE)
      (<xref alt="Guo et al., 2017" rid="ref-guo_calibration" ref-type="bibr">Guo
      et al., 2017</xref>;
      <xref alt="Pakdaman Naeini et al., 2015" rid="ref-Naeini_ece" ref-type="bibr">Pakdaman
      Naeini et al., 2015</xref>) measure the average and maximum
      deviation between predicted and true probabilities.
      <monospace>Calzone</monospace> supports two binning strategies for
      ECE: equal-width binning (ECE-H), which divides the probability
      range [0, 1] into bins of equal width, and equal-count binning
      (ECE-C), which divides predictions into bins containing
      approximately the same number of samples. Users can compute these
      metrics for the top-class (highest probability) or
      class-of-interest (one-vs-rest classification).</p>
    </sec>
    <sec id="hosmer-lemeshow-statistic-hl">
      <title>Hosmer-Lemeshow statistic (HL)</title>
      <p>The Hosmer-Lemeshow (HL) test
      (<xref alt="Hosmer &amp; Lemesbow, 1980" rid="ref-hl_test" ref-type="bibr">Hosmer
      &amp; Lemesbow, 1980</xref>) evaluates model calibration using a
      chi-square test comparing observed and expected events in bins.
      The null hypothesis is that the model is well calibrated.
      <monospace>Calzone</monospace> supports equal-width (ECE-H) and
      equal-count (ECE-C) binning. The test statistic is:
      <disp-formula><alternatives>
      <tex-math><![CDATA[
      \text{HL} = \sum_{m=1}^{M} \frac{(O_{1,m}-E_{1,m})^2}{E_{1,m}\left(1-\frac{E_{1,m}}{N_m}\right)}  \sim \chi^2_{M-2}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">HL</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfrac><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>∼</mml:mo><mml:msubsup><mml:mi>χ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></alternatives></disp-formula>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[E_{1,m}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[O_{1,m}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>O</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math></alternatives></inline-formula>
      are the expected and observed events in the
      <inline-formula><alternatives>
      <tex-math><![CDATA[m^{th}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
      bin, <inline-formula><alternatives>
      <tex-math><![CDATA[N_m]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      is the total observations in the bin, and
      <inline-formula><alternatives>
      <tex-math><![CDATA[M]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>
      is the number of bins. For validation sets, the degrees of freedom
      change from <inline-formula><alternatives>
      <tex-math><![CDATA[M-2]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      to <inline-formula><alternatives>
      <tex-math><![CDATA[M]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>
      (<xref alt="Hosmer Jr et al., 2013" rid="ref-hosmer2013applied" ref-type="bibr">Hosmer
      Jr et al., 2013</xref>). The increase in degree of freedom for
      validation samples has often been overlooked but it is crucial for
      the test to maintain the correct Type I error rate. In
      <monospace>Calzone</monospace>, the default is
      <inline-formula><alternatives>
      <tex-math><![CDATA[M-2]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
      adjustable via the <monospace>df</monospace> parameter.</p>
    </sec>
    <sec id="coxs-calibration-slopeintercept">
      <title>Cox’s calibration slope/intercept</title>
      <p>Cox’s calibration slope/intercept assesses model calibration
      without binning
      (<xref alt="Cox, 1958" rid="ref-Cox" ref-type="bibr">Cox,
      1958</xref>). A logistic regression is fit with predicted odds
      (<inline-formula><alternatives>
      <tex-math><![CDATA[\frac{p}{1-p}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mfrac><mml:mi>p</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:mfrac></mml:math></alternatives></inline-formula>)
      as the independent variable and the outcome as the dependent
      variable. Perfect calibration is indicated by a slope of 1 and
      intercept of 0. To test calibration, fit the intercept with slope
      fixed at 1; if the intercept differs from 0, the model is not
      calibrated. Similarly, fit the slope with intercept fixed at 0; if
      the slope differs from 1, the model is not calibrated.
      Alternatively, fit both simultaneously using a bivariate
      distribution
      (<xref alt="McCullagh &amp; Nelder, 1989" rid="ref-McCullaghU003A1989" ref-type="bibr">McCullagh
      &amp; Nelder, 1989</xref>). This feature is not in
      <monospace>Calzone</monospace>, but users can manually test using
      the covariance matrix.</p>
      <p>A slope &gt;1 indicates overconfidence at high probabilities
      and underconfidence at low probabilities, while a slope &lt;1
      indicates the opposite. A positive intercept indicates general
      overconfidence. Even with ideal slope and intercept, non-linear
      miscalibration may still exist.</p>
    </sec>
    <sec id="integrated-calibration-index-ici">
      <title>Integrated calibration index (ICI)</title>
      <p>The integrated calibration index (ICI) measures the average
      deviation between predicted and true probabilities using curve
      smoothing techniques
      (<xref alt="Austin &amp; Steyerberg, 2019" rid="ref-ICI_austin" ref-type="bibr">Austin
      &amp; Steyerberg, 2019</xref>). It is calculated as:
      <disp-formula><alternatives>
      <tex-math><![CDATA[
      \text{ICI} = \frac{1}{n}\sum_{i=1}^{n} |f(p_i)-p_i|
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">ICI</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">|</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">|</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
      where <inline-formula><alternatives>
      <tex-math><![CDATA[f]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
      is the fitting function and <inline-formula><alternatives>
      <tex-math><![CDATA[p]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>
      is the predicted probability. Typically, Locally Weighted
      Scatterplot Smoothing (LOWESS) is used, but any curve fitting
      method can be applied. <monospace>Calzone</monospace> supports
      both Cox regression–based ICI and LOWESS-based ICI, allowing users
      to choose their preferred method. Users should visualize the
      fitting results to avoid overfitting or underfitting, as flexible
      methods like LOWESS are sensitive to span and delta
      parameters.</p>
    </sec>
    <sec id="spiegelhalters-z-test">
      <title>Spiegelhalter’s Z-test</title>
      <p>Spiegelhalter’s Z-test is a test of calibration proposed by
      Spiegelhalter in 1986
      (<xref alt="Spiegelhalter, 1986" rid="ref-spiegelhalter_z" ref-type="bibr">Spiegelhalter,
      1986</xref>). It uses the fact that the Brier score can be
      decomposed into: <disp-formula><alternatives>
      <tex-math><![CDATA[
      B = \frac{1}{N} \sum_{i=1}^N (x_i - p_i)^2 = \frac{1}{N} \sum_{i=1}^N (x_i - p_i)(1-2p_i) + \frac{1}{N} \sum_{i=1}^N p_i(1-p_i)
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
      And the test statistic (TS) of Z test is defined as:
      <disp-formula><alternatives>
      <tex-math><![CDATA[
      Z = \frac{B - E(B)}{\sqrt{\text{Var}(B)}} = \frac{ \sum_{i=1}^N (x_i - p_i)(1-2p_i)}{\sum_{i=1}^N (1-2p_i)^2 p_i (1-p_i)}
      ]]></tex-math>
      <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mtext mathvariant="normal">Var</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>B</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></alternatives></disp-formula>
      and it is asymptotically distributed as a standard normal
      distribution.</p>
    </sec>
    <sec id="metrics-class">
      <title>Metrics class</title>
      <p><monospace>Calzone</monospace> also provides a class called
      <monospace>CalibrationMetrics()</monospace> to calculate all the
      metrics mentioned above. The function will return a dictionary
      containing the metrics’ names and their values. The metrics can be
      specified as a list of strings. The string ‘all’ can be used to
      calculate all the metrics.</p>
      <code language="python">from calzone.metrics import CalibrationMetrics

metrics = CalibrationMetrics(class_to_calculate=1)

metrics.calculate_metrics(
    labels,
    probs,
    metrics='all'
)</code>
    </sec>
  </sec>
</sec>
<sec id="other-features">
  <title>Other features</title>
  <sec id="confidence-intervals">
    <title>Confidence intervals</title>
    <p><monospace>Calzone</monospace> also provides functionality to
    compute confidence intervals for all metrics using bootstrapping.
    The user can specify the number of bootstrap samples and the
    confidence level.</p>
    <code language="python">from calzone.metrics import CalibrationMetrics

metrics = CalibrationMetrics(class_to_calculate=1)

CalibrationMetrics.bootstrap(
    labels,
    probs,
    metrics='all',
    n_samples=1000
)</code>
    <p>and a structured NumPy array will be returned.</p>
  </sec>
  <sec id="subgroup-analysis">
    <title>Subgroup analysis</title>
    <p><monospace>Calzone</monospace> will perform subgroup analysis by
    default in the command line user interface. If the user input CSV
    file contains a subgroup column, the program will compute metrics
    for the entire dataset and for each subgroup. A detailed description
    of the input format can be found in the documentation.</p>
  </sec>
  <sec id="prevalence-adjustment">
    <title>Prevalence adjustment</title>
    <p><monospace>Calzone</monospace> offers prevalence adjustment to
    correct for differences in disease prevalence between training and
    testing data. Calibration is based on posterior probability, so a
    shift in prevalence can cause miscalibration. The adjusted
    probability is calculated as: <disp-formula><alternatives>
    <tex-math><![CDATA[
    P'(D=1|\hat{p}=p) = \frac{\eta'/(1-\eta')}{(1/p-1)(\eta/(1-\eta))} = p'
    ]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>P</mml:mi><mml:mi>′</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mover><mml:mi>p</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>η</mml:mi><mml:mi>′</mml:mi><mml:mi>/</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mi>′</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>η</mml:mi><mml:mi>/</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></disp-formula>
    where <inline-formula><alternatives>
    <tex-math><![CDATA[\eta]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>η</mml:mi></mml:math></alternatives></inline-formula>
    is the testing data prevalence, <inline-formula><alternatives>
    <tex-math><![CDATA[\eta']]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>η</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
    is the training data prevalence, and <inline-formula><alternatives>
    <tex-math><![CDATA[p]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>p</mml:mi></mml:math></alternatives></inline-formula>
    is the predicted probability. The optimal
    <inline-formula><alternatives>
    <tex-math><![CDATA[\eta']]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>η</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
    is found by minimizing cross-entropy loss, or users can specify
    <inline-formula><alternatives>
    <tex-math><![CDATA[\eta']]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>η</mml:mi><mml:mi>′</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
    directly if known
    (<xref alt="Chen et al., 2018" rid="ref-weijie_prevalence_adjustment" ref-type="bibr">Chen
    et al., 2018</xref>;
    <xref alt="Gu &amp; Pepe, 2010" rid="ref-gu_likelihod_ratio" ref-type="bibr">Gu
    &amp; Pepe, 2010</xref>;
    <xref alt="Horsch et al., 2008" rid="ref-Prevalence_HORSCH" ref-type="bibr">Horsch
    et al., 2008</xref>;
    <xref alt="Tian et al., 2020" rid="ref-prevalence_shift" ref-type="bibr">Tian
    et al., 2020</xref>).</p>
  </sec>
  <sec id="multiclass-extension">
    <title>Multiclass extension</title>
    <p><monospace>Calzone</monospace> supports multiclass classification
    using a 1-vs-rest approach or top-class calibration. In top-class
    calibration, class 1 probability is the highest predicted
    probability, and class 0 is 1 minus this probability. Metrics
    interpretation may change in this transformation.</p>
  </sec>
  <sec id="verification-of-methods">
    <title>Verification of methods</title>
    <p>To ensure the accuracy and reliability of the metrics implemented
    in <monospace>Calzone</monospace>, we performed comprehensive
    validation against established external packages. Reliability
    diagrams were compared with
    <monospace>sklearn.calibration.calibration_curve()</monospace>
    (<xref alt="Pedregosa et al., 2011" rid="ref-scikit" ref-type="bibr">Pedregosa
    et al., 2011</xref>), top-class ECE and Spiegelhalter’s Z scores
    were validated against <monospace>MAPIE</monospace>
    (<xref alt="Taquet et al., 2022" rid="ref-taquet2022mapie" ref-type="bibr">Taquet
    et al., 2022</xref>), and the Hosmer-Lemeshow statistic was checked
    against <monospace>ResourceSelection</monospace>
    (<xref alt="Lele et al., 2024" rid="ref-ResourceSelection" ref-type="bibr">Lele
    et al., 2024</xref>) in R. Additional tests were conducted using the
    <monospace>relplot</monospace> and <monospace>pycaleva</monospace>
    Python packages to further confirm metric consistency. All
    differences were within 0.1%, demonstrating strong agreement. These
    validation tests are documented in
    <monospace>test_results.py</monospace>. Furthermore, synthetic data
    tests (see <monospace>test_metrics.py</monospace>) were used to
    confirm the expected behavior of the calibration metrics under
    controlled conditions.</p>
  </sec>
  <sec id="command-line-interface">
    <title>Command line interface</title>
    <p><monospace>Calzone</monospace> offers a command line interface
    for visualizing calibration curves, calculating metrics, and
    confidence intervals. Run
    <monospace>python cal_metrics.py -h</monospace> for help.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The mention of commercial products, their sources, or their use in
  connection with material reported herein is not to be construed as
  either an actual or implied endorsement of such products by the
  Department of Health and Human Services. This is a contribution of the
  U.S. Food and Drug Administration and is not subject to copyright.</p>
  <p>The authors acknowledge the Research Participation Program at the
  Center for Devices and Radiological Health, administered by the Oak
  Ridge Institute for Science and Education through an interagency
  agreement between the U.S. Department of Energy and the U.S. Food and
  Drug Administration.</p>
</sec>
<sec id="conflicts-of-interest">
  <title>Conflicts of interest</title>
  <p>The authors declare no conflicts of interest.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Naeini_ece">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pakdaman Naeini</surname><given-names>Mahdi</given-names></name>
        <name><surname>Cooper</surname><given-names>Gregory</given-names></name>
        <name><surname>Hauskrecht</surname><given-names>Milos</given-names></name>
      </person-group>
      <article-title>Obtaining well calibrated probabilities using Bayesian binning</article-title>
      <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
      <year iso-8601-date="2015-02">2015</year><month>02</month>
      <volume>29</volume>
      <issue>1</issue>
      <uri>https://ojs.aaai.org/index.php/AAAI/article/view/9602</uri>
      <pub-id pub-id-type="doi">10.1609/aaai.v29i1.9602</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wilson_interval">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wilson</surname><given-names>Edwin B</given-names></name>
      </person-group>
      <article-title>Probable inference, the law of succession, and statistical inference</article-title>
      <source>Journal of the American Statistical Association</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1927">1927</year>
      <volume>22</volume>
      <issue>158</issue>
      <pub-id pub-id-type="doi">10.2307/2276774</pub-id>
      <fpage>209</fpage>
      <lpage>212</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Brocker_reldia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bröcker</surname><given-names>Jochen</given-names></name>
        <name><surname>Smith</surname><given-names>Leonard A.</given-names></name>
      </person-group>
      <article-title>Increasing the reliability of reliability diagrams</article-title>
      <source>Weather and Forecasting</source>
      <publisher-name>American Meteorological Society</publisher-name>
      <publisher-loc>Boston MA, USA</publisher-loc>
      <year iso-8601-date="2007">2007</year>
      <volume>22</volume>
      <issue>3</issue>
      <uri>https://journals.ametsoc.org/view/journals/wefo/22/3/waf993_1.xml</uri>
      <pub-id pub-id-type="doi">10.1175/WAF993.1</pub-id>
      <fpage>651</fpage>
      <lpage>661</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ICI_austin">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Austin</surname><given-names>Peter C.</given-names></name>
        <name><surname>Steyerberg</surname><given-names>Ewout W.</given-names></name>
      </person-group>
      <article-title>The integrated calibration index (ICI) and related metrics for quantifying the calibration of logistic regression models</article-title>
      <source>Statistics in Medicine</source>
      <year iso-8601-date="2019">2019</year>
      <volume>38</volume>
      <issue>21</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8281</uri>
      <pub-id pub-id-type="doi">10.1002/sim.8281</pub-id>
      <fpage>4051</fpage>
      <lpage>4065</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Murphy_reliability">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Murphy</surname><given-names>Allan H.</given-names></name>
        <name><surname>Winkler</surname><given-names>Robert L.</given-names></name>
      </person-group>
      <article-title>Reliability of subjective probability forecasts of precipitation and temperature</article-title>
      <source>Journal of the Royal Statistical Society. Series C (Applied Statistics)</source>
      <publisher-name>Royal Statistical Society</publisher-name>
      <year iso-8601-date="1977">1977</year>
      <volume>26</volume>
      <issue>1</issue>
      <uri>http://www.jstor.org/stable/2346866</uri>
      <pub-id pub-id-type="doi">10.2307/2346866</pub-id>
      <fpage>41</fpage>
      <lpage>47</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Prevalence_HORSCH">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Horsch</surname><given-names>Karla</given-names></name>
        <name><surname>Giger</surname><given-names>Maryellen L.</given-names></name>
        <name><surname>Metz</surname><given-names>Charles E.</given-names></name>
      </person-group>
      <article-title>Prevalence scaling: Applications to an intelligent workstation for the diagnosis of breast cancer</article-title>
      <source>Academic Radiology</source>
      <year iso-8601-date="2008">2008</year>
      <volume>15</volume>
      <issue>11</issue>
      <issn>1076-6332</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S1076633208002936</uri>
      <pub-id pub-id-type="doi">10.1016/j.acra.2008.04.022</pub-id>
      <fpage>1446</fpage>
      <lpage>1457</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hl_test">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hosmer</surname><given-names>David W.</given-names></name>
        <name><surname>Lemesbow</surname><given-names>Stanley</given-names></name>
      </person-group>
      <article-title>Goodness of fit tests for the multiple logistic regression model</article-title>
      <source>Communications in Statistics - Theory and Methods</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1980">1980</year>
      <volume>9</volume>
      <issue>10</issue>
      <uri>https://www.tandfonline.com/doi/abs/10.1080/03610928008827941</uri>
      <pub-id pub-id-type="doi">10.1080/03610928008827941</pub-id>
      <fpage>1043</fpage>
      <lpage>1069</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hosmer2013applied">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Hosmer Jr</surname><given-names>David W</given-names></name>
        <name><surname>Lemeshow</surname><given-names>Stanley</given-names></name>
        <name><surname>Sturdivant</surname><given-names>Rodney X</given-names></name>
      </person-group>
      <source>Applied logistic regression</source>
      <publisher-name>John Wiley &amp; Sons</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <isbn>9781118548387</isbn>
    </element-citation>
  </ref>
  <ref id="ref-Brocker_decompose">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bröcker</surname><given-names>Jochen</given-names></name>
      </person-group>
      <article-title>Reliability, sufficiency, and the decomposition of proper scores</article-title>
      <source>Quarterly Journal of the Royal Meteorological Society</source>
      <year iso-8601-date="2009">2009</year>
      <volume>135</volume>
      <issue>643</issue>
      <uri>https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.456</uri>
      <pub-id pub-id-type="doi">10.1002/qj.456</pub-id>
      <fpage>1512</fpage>
      <lpage>1519</lpage>
    </element-citation>
  </ref>
  <ref id="ref-McCullaghU003A1989">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>McCullagh</surname><given-names>P.</given-names></name>
        <name><surname>Nelder</surname><given-names>J. A.</given-names></name>
      </person-group>
      <source>Generalized linear models</source>
      <publisher-name>Chapman &amp; Hall / CRC</publisher-name>
      <publisher-loc>London</publisher-loc>
      <year iso-8601-date="1989">1989</year>
      <isbn>978-0412317606</isbn>
      <pub-id pub-id-type="doi">10.1201/9781439891148-8</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Calster_weak_cal">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Van Calster</surname><given-names>Ben</given-names></name>
        <name><surname>Steyerberg</surname><given-names>Ewout W.</given-names></name>
      </person-group>
      <article-title>Calibration of prognostic risk scores</article-title>
      <source>Wiley StatsRef: Statistics reference online</source>
      <publisher-name>John Wiley &amp; Sons, Ltd</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <isbn>9781118445112</isbn>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat08078</uri>
      <pub-id pub-id-type="doi">10.1002/9781118445112.stat08078</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-weijie_prevalence_adjustment">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Weijie</given-names></name>
        <name><surname>Sahiner</surname><given-names>Berkman</given-names></name>
        <name><surname>Samuelson</surname><given-names>Frank</given-names></name>
        <name><surname>Pezeshk</surname><given-names>Aria</given-names></name>
        <name><surname>Petrick</surname><given-names>Nicholas</given-names></name>
      </person-group>
      <article-title>Calibration of medical diagnostic classifier scores to the probability of disease</article-title>
      <source>Statistical Methods in Medical Research</source>
      <year iso-8601-date="2018">2018</year>
      <volume>27</volume>
      <issue>5</issue>
      <uri>https://doi.org/10.1177/0962280216661371</uri>
      <pub-id pub-id-type="doi">10.1177/0962280216661371</pub-id>
      <fpage>1394</fpage>
      <lpage>1409</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DIAMOND199285">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Diamond</surname><given-names>George A.</given-names></name>
      </person-group>
      <article-title>What price perfection? Calibration and discrimination of clinical prediction models</article-title>
      <source>Journal of Clinical Epidemiology</source>
      <year iso-8601-date="1992">1992</year>
      <volume>45</volume>
      <issue>1</issue>
      <issn>0895-4356</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/089543569290192P</uri>
      <pub-id pub-id-type="doi">10.1016/0895-4356(92)90192-P</pub-id>
      <fpage>85</fpage>
      <lpage>89</lpage>
    </element-citation>
  </ref>
  <ref id="ref-taquet2022mapie">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Taquet</surname><given-names>Vianney</given-names></name>
        <name><surname>Blot</surname><given-names>Vincent</given-names></name>
        <name><surname>Morzadec</surname><given-names>Thomas</given-names></name>
        <name><surname>Lacombe</surname><given-names>Louis</given-names></name>
        <name><surname>Brunel</surname><given-names>Nicolas</given-names></name>
      </person-group>
      <article-title>MAPIE: An open-source library for distribution-free uncertainty quantification</article-title>
      <source>arXiv preprint arXiv:2207.12274</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2207.12274</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-uncertaintyToolbox">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chung</surname><given-names>Youngseog</given-names></name>
        <name><surname>Char</surname><given-names>Ian</given-names></name>
        <name><surname>Guo</surname><given-names>Han</given-names></name>
        <name><surname>Schneider</surname><given-names>Jeff</given-names></name>
        <name><surname>Neiswanger</surname><given-names>Willie</given-names></name>
      </person-group>
      <article-title>Uncertainty toolbox: An open-source library for assessing, visualizing, and improving uncertainty quantification</article-title>
      <source>arXiv preprint arXiv:2109.10254</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2109.10254</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ResourceSelection">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Lele</surname><given-names>Subhash R.</given-names></name>
        <name><surname>Keim</surname><given-names>Jonah L.</given-names></name>
        <name><surname>Solymos</surname><given-names>Peter</given-names></name>
      </person-group>
      <source>ResourceSelection: Resource selection (probability) functions for use-availability data</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/psolymos/ResourceSelection</uri>
      <pub-id pub-id-type="doi">10.32614/cran.package.resourceselection</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-scikit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Michel</surname><given-names>Vincent</given-names></name>
        <name><surname>Thirion</surname><given-names>Bertrand</given-names></name>
        <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
        <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
        <name><surname>Weiss</surname><given-names>Ron</given-names></name>
        <name><surname>Dubourg</surname><given-names>Vincent</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of machine learning research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <issue>Oct</issue>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cox">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cox</surname><given-names>D. R.</given-names></name>
      </person-group>
      <article-title>Two further applications of a model for binary regression</article-title>
      <source>Biometrika</source>
      <year iso-8601-date="1958-12">1958</year><month>12</month>
      <volume>45</volume>
      <issue>3-4</issue>
      <issn>0006-3444</issn>
      <uri>https://doi.org/10.1093/biomet/45.3-4.562</uri>
      <pub-id pub-id-type="doi">10.1093/biomet/45.3-4.562</pub-id>
      <fpage>562</fpage>
      <lpage>565</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gu_likelihod_ratio">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gu</surname><given-names>Wen</given-names></name>
        <name><surname>Pepe</surname><given-names>Margaret Sullivan</given-names></name>
      </person-group>
      <article-title>Estimating the diagnostic likelihood ratio of a continuous marker</article-title>
      <source>Biostatistics</source>
      <year iso-8601-date="2010-07">2010</year><month>07</month>
      <volume>12</volume>
      <issue>1</issue>
      <issn>1465-4644</issn>
      <uri>https://doi.org/10.1093/biostatistics/kxq045</uri>
      <pub-id pub-id-type="doi">10.1093/biostatistics/kxq045</pub-id>
      <fpage>87</fpage>
      <lpage>101</lpage>
    </element-citation>
  </ref>
  <ref id="ref-guo_calibration">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Guo</surname><given-names>Chuan</given-names></name>
        <name><surname>Pleiss</surname><given-names>Geoff</given-names></name>
        <name><surname>Sun</surname><given-names>Yu</given-names></name>
        <name><surname>Weinberger</surname><given-names>Kilian Q.</given-names></name>
      </person-group>
      <article-title>On calibration of modern neural networks</article-title>
      <source>Proceedings of the 34th international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>Precup</surname><given-names>Doina</given-names></name>
        <name><surname>Teh</surname><given-names>Yee Whye</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>70</volume>
      <uri>https://proceedings.mlr.press/v70/guo17a.html</uri>
      <fpage>1321</fpage>
      <lpage>1330</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pycaleva">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Martin Weigl</surname><given-names>Mark Aron Szulyovszky</given-names></name>
      </person-group>
      <article-title>Pycaleva</article-title>
      <publisher-name>https://github.com/MartinWeigl/pycaleva</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-ml-calibration">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Blasiok</surname><given-names>Jaroslaw</given-names></name>
        <name><surname>Nakkiran</surname><given-names>Preetum</given-names></name>
      </person-group>
      <article-title>Smooth ECE: Principled reliability diagrams via kernel smoothing</article-title>
      <source>The twelfth international conference on learning representations, ICLR 2024, vienna, austria, may 7-11, 2024</source>
      <publisher-name>OpenReview.net</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <uri>https://openreview.net/forum?id=XwiA1nDahv</uri>
    </element-citation>
  </ref>
  <ref id="ref-statistical_learning">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Hastie</surname><given-names>Trevor</given-names></name>
        <name><surname>Tibshirani</surname><given-names>Robert</given-names></name>
        <name><surname>Friedman</surname><given-names>Jerome</given-names></name>
      </person-group>
      <source>The elements of statistical learning</source>
      <publisher-name>Springer New York Inc.</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2001">2001</year>
      <isbn>978-0387848570</isbn>
    </element-citation>
  </ref>
  <ref id="ref-spiegelhalter_z">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Spiegelhalter</surname><given-names>David J</given-names></name>
      </person-group>
      <article-title>Probabilistic prediction in patient management and clinical trials</article-title>
      <source>Statistics in medicine</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1986">1986</year>
      <volume>5</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1002/sim.4780050506</pub-id>
      <fpage>421</fpage>
      <lpage>433</lpage>
    </element-citation>
  </ref>
  <ref id="ref-prevalence_shift">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tian</surname><given-names>Junjiao</given-names></name>
        <name><surname>Liu</surname><given-names>Yen-Cheng</given-names></name>
        <name><surname>Glaser</surname><given-names>Nathaniel</given-names></name>
        <name><surname>Hsu</surname><given-names>Yen-Chang</given-names></name>
        <name><surname>Kira</surname><given-names>Zsolt</given-names></name>
      </person-group>
      <article-title>Posterior re-calibration for imbalanced datasets</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Ranzato</surname><given-names>M.</given-names></name>
        <name><surname>Hadsell</surname><given-names>R.</given-names></name>
        <name><surname>Balcan</surname><given-names>M. F.</given-names></name>
        <name><surname>Lin</surname><given-names>H.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2020/file/5ca359ab1e9e3b9c478459944a2d9ca5-Paper.pdf</uri>
      <fpage>8101</fpage>
      <lpage>8113</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
