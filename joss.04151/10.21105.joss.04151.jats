<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4151</article-id>
<article-id pub-id-type="doi">10.21105/joss.04151</article-id>
<title-group>
<article-title>DeepSynth: Scaling Neural Program Synthesis with
Distribution-based Search</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5043-3221</contrib-id>
<name>
<surname>Matricon</surname>
<given-names>Théo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6576-4680</contrib-id>
<name>
<surname>Fijalkow</surname>
<given-names>Nathanaël</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lagarde</surname>
<given-names>Guillaume</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Ellis</surname>
<given-names>Kevin</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>CNRS, LaBRI and Université de Bordeaux,
France</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>The Alan Turing Institute of data science, United
Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Cornell University, United States</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-01-03">
<day>3</day>
<month>1</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>78</issue>
<fpage>4151</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>program synthesis</kwd>
<kwd>programming by example</kwd>
<kwd>neuro-symbolic methods</kwd>
<kwd>parallel search procedures</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Writing software is tedious, error-prone, and accessible only to a
  small share of the population – yet coding grows increasingly
  important as the digital world plays larger and larger roles in
  peoples’ lives. Programming by example seeks to make programming more
  reliable and accessible by allowing non-technical users to specify
  programs only from pairs of input-output examples.
  <monospace>DeepSynth</monospace> is a general purpose programming by
  example tool, which combines predictions from neural networks with
  symbolic methods to generate programs.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>DeepSynth</monospace> was used for the experiments of
  the recently published Fijalkow et al.
  (<xref alt="2022" rid="ref-FijalkowU003A2021" ref-type="bibr">2022</xref>).
  The main purpose was to evaluate and compare different search
  algorithms and their integration with neural predictions. Beyond the
  publication Fijalkow et al.
  (<xref alt="2022" rid="ref-FijalkowU003A2021" ref-type="bibr">2022</xref>),
  the goal of <monospace>DeepSynth</monospace> is to serve as a
  reference implementation of existing neuro-symbolic methods for
  programming by example. Indeed, it includes different options for
  different parts of the pipeline, focussing on neural predictions and
  search algorithms.</p>
</sec>
<sec id="how-it-works">
  <title>How it works</title>
  <p><monospace>DeepSynth</monospace> is parameterised by a domain
  specific language (DSL), which is the programming language chosen for
  solving a program synthesis task. The user first specifies a DSL by
  giving a set of primitives together with their types and semantics, as
  well as semantic and syntactic constraints (such as program depth).
  The compilation phase constructs a Context Free Grammar (CFG)
  representing the set of programs.</p>
  <p>The second ingredient is the prediction model.
  <monospace>DeepSynth</monospace> leverages PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>): a neural network reads the examples and outputs
  predictions to guide the search towards likely programs.
  <monospace>DeepSynth</monospace> includes end to end procedures for
  training a neural network that makes predictions from examples. The
  predictions are given as distributions on the derivation rules of
  CFG.</p>
  <p><xref alt="Figure 1" rid="figU003Adescription">Figure 1</xref>
  illustrates the machine learning pipeline on a toy DSL describing
  integer list manipulating programs.</p>
  <fig>
    <caption><p>Pipeline for neural predictions for syntax guided
    program
    synthesis.<styled-content id="figU003Adescription"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/sygus.png" xlink:title="" />
  </fig>
  <p>Once the DSL is specified and the prediction model trained,
  <monospace>DeepSynth</monospace> can be used as follows: the end user
  gives a few input-output examples, and
  <monospace>DeepSynth</monospace> searches for a program matching the
  examples. <monospace>DeepSynth</monospace> includes a number of
  predictions-guided search algorithms; the most efficient is
  <monospace>HeapSearch</monospace>. An optional parallelisation
  framework allows the user to split the search on multiple CPUs by
  partitioning the progam space.</p>
  <p>We refer to the <monospace>readme</monospace> for example uses of
  <monospace>DeepSynth</monospace>. The full technical details are
  described in Fijalkow et al.
  (<xref alt="2022" rid="ref-FijalkowU003A2021" ref-type="bibr">2022</xref>).</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Programming by example has been intensively investigated in the
  past years both in academia and in industry, spearheaded by the
  success of FlashFill in Microsoft Excel (see Gulwani
  (<xref alt="2011" rid="ref-GulwaniU003A2011" ref-type="bibr">2011</xref>)),
  allowing users to synthesize spreadsheet programs by giving examples.
  FlashFill is now integrated into the larger and more ambitious project
  PROSE by Microsoft Research (see the
  <ext-link ext-link-type="uri" xlink:href="https://www.microsoft.com/en-us/research/group/prose/">website</ext-link>).
  PROSE is a general purpose program synthesis tool (see the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/microsoft/prose/">GitHub
  repository</ext-link>). It is based on constraint programming: in
  addition to the DSL, the end-user needs to specify so-called witness
  functions, which are used for specifying a programming by example
  instance in a SAT or SMT solver.</p>
  <p>Other tools have emerged recently exploiting the programming by
  example paradigm, for instance
  <ext-link ext-link-type="uri" xlink:href="https://ai.googleblog.com/2014/10/smart-autofill-harnessing-predictive.html">SmartFill</ext-link>
  for Google Sheets, and the TF-Coder for
  <ext-link ext-link-type="uri" xlink:href="https://blog.tensorflow.org/2020/08/introducing-tensorflow-coder-tool.html">TensorFlow</ext-link>
  (see Shi et al.
  (<xref alt="2020" rid="ref-ShiU003A2020" ref-type="bibr">2020</xref>)).
  Unlike PROSE, they are tailored to address a single domain: for
  SmartFill spreadsheet programs, and for TF-Coder tensorflow programs.
  A number of other prototype tools have been developed in the academic
  world: DeepCoder was the first leveraging deep learning techniques
  (see Balog et al.
  (<xref alt="2017" rid="ref-Balog2017" ref-type="bibr">2017</xref>)),
  PC-Coder (see Zohar &amp; Wolf
  (<xref alt="2018" rid="ref-ZoharW18" ref-type="bibr">2018</xref>)),
  and recently the general-purpose DreamCoder (see Ellis et al.
  (<xref alt="2021" rid="ref-EllisWNSMHCST21" ref-type="bibr">2021</xref>)).</p>
  <p>The recent release of the GitHub Copilot (see GitHub
  (<xref alt="2021" rid="ref-copilot" ref-type="bibr">2021</xref>)),
  powered by the Codex large language model from OpenAI, shows the wide
  applicability of the program synthesis: Copilot is presented as
  <monospace>your AI pair programmer</monospace>, meaning that it
  assists developers by autocompleting pieces of code in an interactive
  fashion. The key difference is that specifications in Copilot are
  given in natural language, which may or may not include examples.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p>This package has the following capabilities:</p>
  <list list-type="bullet">
    <list-item>
      <p>Create a DSL from syntactic constraints and semantics
      functions;</p>
    </list-item>
    <list-item>
      <p>Transform this DSL into a Context Free Grammar (CFG);</p>
    </list-item>
    <list-item>
      <p>Transform this CFG into a Probabilistic CFG (PCFG);</p>
    </list-item>
    <list-item>
      <p>Sample programs from a PCFG;</p>
    </list-item>
    <list-item>
      <p>Enumerate programs in a PCFG with different algorithms
      including <monospace>HeapSearch</monospace>;</p>
    </list-item>
    <list-item>
      <p>A grammar splitter that enables to split the search into
      <monospace>n</monospace> independent searches, enabling parallel
      search scaling linearly with the number of CPUs;</p>
    </list-item>
    <list-item>
      <p>A neural network architecture to predict probabilities of a CFG
      given pairs of input-output examples and its automatic training
      procedure from a DSL that supports <monospace>int</monospace>,
      <monospace>bool</monospace> and <monospace>list</monospace>
      inputs.</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-EllisWNSMHCST21">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ellis</surname><given-names>Kevin</given-names></name>
        <name><surname>Wong</surname><given-names>Catherine</given-names></name>
        <name><surname>Nye</surname><given-names>Maxwell I.</given-names></name>
        <name><surname>Sablé-Meyer</surname><given-names>Mathias</given-names></name>
        <name><surname>Morales</surname><given-names>Lucas</given-names></name>
        <name><surname>Hewitt</surname><given-names>Luke B.</given-names></name>
        <name><surname>Cary</surname><given-names>Luc</given-names></name>
        <name><surname>Solar-Lezama</surname><given-names>Armando</given-names></name>
        <name><surname>Tenenbaum</surname><given-names>Joshua B.</given-names></name>
      </person-group>
      <article-title>DreamCoder: Bootstrapping inductive program synthesis with wake-sleep library learning</article-title>
      <source>International conference on programming language design and implementation, PLDI</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://doi.org/10.1145/3453483.3454080</uri>
      <pub-id pub-id-type="doi">10.1145/3453483.3454080</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ZoharW18">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zohar</surname><given-names>A.</given-names></name>
        <name><surname>Wolf</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>Automatic program synthesis of long programs with a learned garbage collector</article-title>
      <source>Neural information processing systems, NeurIPS</source>
      <year iso-8601-date="2018">2018</year>
      <uri>https://proceedings.neurips.cc/paper/2018/hash/390e982518a50e280d8e2b535462ec1f-Abstract.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-Balog2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Balog</surname><given-names>Matej</given-names></name>
        <name><surname>Gaunt</surname><given-names>Alexander L.</given-names></name>
        <name><surname>Brockschmidt</surname><given-names>Marc</given-names></name>
        <name><surname>Nowozin</surname><given-names>Sebastian</given-names></name>
        <name><surname>Tarlow</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>DeepCoder: Learning to write programs</article-title>
      <source>International conference on learning representations, ICLR</source>
      <year iso-8601-date="2017">2017</year>
      <uri>https://openreview.net/forum?id=ByldLrqlx</uri>
    </element-citation>
  </ref>
  <ref id="ref-ShiU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shi</surname><given-names>Kensen</given-names></name>
        <name><surname>Bieber</surname><given-names>David</given-names></name>
        <name><surname>Singh</surname><given-names>Rishabh</given-names></name>
      </person-group>
      <article-title>TF-coder: Program synthesis for tensor manipulations</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2020">2020</year>
      <volume>abs/2003.09040</volume>
      <uri>https://arxiv.org/abs/2003.09040</uri>
      <pub-id pub-id-type="doi">10.1145/3517034</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-GulwaniU003A2011">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gulwani</surname><given-names>Sumit</given-names></name>
      </person-group>
      <article-title>Automating string processing in spreadsheets using input-output examples</article-title>
      <source>ACM SIGPLAN-SIGACT symposium on principles of programming languages, POPL</source>
      <year iso-8601-date="2011">2011</year>
      <uri>https://doi.org/10.1145/1926385.1926423</uri>
      <pub-id pub-id-type="doi">10.1145/1926385.1926423</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-copilot">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>GitHub</surname></name>
      </person-group>
      <article-title>GitHub copilot</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://copilot.github.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-FijalkowU003A2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fijalkow</surname><given-names>Nathanaël</given-names></name>
        <name><surname>Lagarde</surname><given-names>Guillaume</given-names></name>
        <name><surname>Matricon</surname><given-names>Théo</given-names></name>
        <name><surname>Ellis</surname><given-names>Kevin</given-names></name>
        <name><surname>Ohlmann</surname><given-names>Pierre</given-names></name>
        <name><surname>Potta</surname><given-names>Akarsh Nayan</given-names></name>
      </person-group>
      <article-title>Scaling neural program synthesis with distribution-based search</article-title>
      <source>Proceedings of the AAAI Conference on Artificial Intelligence</source>
      <year iso-8601-date="2022">2022</year>
      <volume>36</volume>
      <issue>6</issue>
      <uri>https://ojs.aaai.org/index.php/AAAI/article/view/20616</uri>
      <pub-id pub-id-type="doi">10.1609/aaai.v36i6.20616</pub-id>
      <fpage>6623</fpage>
      <lpage>6630</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
