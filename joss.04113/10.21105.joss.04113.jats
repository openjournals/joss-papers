<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4113</article-id>
<article-id pub-id-type="doi">10.21105/joss.04113</article-id>
<title-group>
<article-title>SGMCMCJax: a lightweight JAX library for stochastic
gradient Markov chain Monte Carlo algorithms</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-7032-3425</contrib-id>
<name>
<surname>Coullon</surname>
<given-names>Jeremie</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9084-3866</contrib-id>
<name>
<surname>Nemeth</surname>
<given-names>Christopher</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Cervest, London, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Lancaster University, UK</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-01-17">
<day>17</day>
<month>1</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>72</issue>
<fpage>4113</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>JAX</kwd>
<kwd>MCMC</kwd>
<kwd>SGMCMC</kwd>
<kwd>Bayesian inference</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In Bayesian inference, the <italic>posterior distribution</italic>
  is the probability distribution over the model parameters resulting
  from the prior distribution and the likelihood. One can compute
  integrals over this distribution to obtain quantities of interest,
  such as the posterior mean and variance, or credible uncertainty
  regions. However, as these integrals are often intractable for
  problems of interest they require numerical methods to approximate
  them.</p>
  <p>Markov Chain Monte Carlo (MCMC) is currently the gold standard for
  approximating integrals needed in Bayesian inference. However, as
  these algorithms become prohibitively expensive for large datasets,
  stochastic gradient MCMC (SGMCMC)
  (<xref alt="Ma et al., 2015" rid="ref-NIPS2015_Ma_complete_recipe" ref-type="bibr">Ma
  et al., 2015</xref>;
  <xref alt="Nemeth &amp; Fearnhead, 2021" rid="ref-nemeth2021stochastic" ref-type="bibr">Nemeth
  &amp; Fearnhead, 2021</xref>) is a popular approach to approximate
  these integrals in these cases. This class of scalable algorithms uses
  data subsampling techniques to approximate gradient based sampling
  algorithms, and are regularly used to fit statistical models or
  Bayesian neural networks (BNNs). The SGMCMC literature develops new
  algorithms by finding novel gradient estimation techniques, designing
  more efficient diffusions, and finding more stable numerical
  discretisations to these diffusions. SGMCMCJax is a lightweight
  library that is designed to allow the user to innovate along these
  lines or use one of the existing gradient-based SGMCMC algorithms
  already included in the library. This makes SGMCMCJax very well suited
  for both research purposes and practical applications.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>SGMCMCJax is a Python package written in the popular JAX library
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>). Although there are libraries for SGMCMC
  algorithms in other languages and automatic differentiation frameworks
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>;
  <xref alt="Baker et al., 2019" rid="ref-baker2019sgmcmc" ref-type="bibr">Baker
  et al., 2019</xref>), there is no mature library for the JAX
  ecosystem. However, as this has become a popular framework for machine
  learning and scientific computing, this gap has become more
  noticeable. As SGMCMC algorithms are a standard tool to train Bayesian
  neural networks as well as statistical models with large datasets, we
  have written this library of samplers to fill this gap.</p>
  <p>SGMCMCJax uses JAX to perform automatic differentiation and
  compilation to XLA. The use of JAX allows the SGMCMCJax library to
  effortlessly run on GPUs and TPUs, which is essential for large models
  such as BNNs. As a result, the library has an easy-to-use interface
  and provides very competitive speed performance. SGMCMCJax is designed
  in a modular framework allowing users to simply run one of its many
  algorithms or to create new algorithms for research purposes by using
  the existing algorithms as building blocks. Furthermore, SGMCMCJax can
  integrate easily with other codebases within the JAX ecosystem such as
  Flax, a neural network library for JAX. As SGMCMC algorithms are often
  used to train BNNs, which are usually written using frameworks such as
  Flax, having a library that can interact with these packages is
  essential to support research in the growing area of Bayesian neural
  network modelling.</p>
  <p>There also exists many probabilistic programming languagues (PPLs)
  such as NumPyro, Stan, or PyMC, which allow users to define models and
  sample from them using advanced MCMC algorithms. However, these PPLs
  do not usually include SGMCMC algorithms in their inference toolkit
  which are useful for dealing with large datasets. In contrast,
  SGMCMCJax has a wide range of standard and state-of-the-art SGMCMC
  algorithms, does not include a modelling language, and is designed in
  a modular way that allows users to develop new sampling algorithms. As
  a result, this lightweight library fills the need for these scalable
  algorithms, and can be used to sample from models defined using these
  PPLs.</p>
</sec>
<sec id="software-requirements-and-external-usage">
  <title>Software requirements and external usage</title>
  <p>SGMCMCJax is written using JAX
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>) as its main requirement. Although SGMCMCJax is a
  recent library it has already been used in a research paper
  (<xref alt="Coullon et al., 2021" rid="ref-coullon2021efficient" ref-type="bibr">Coullon
  et al., 2021</xref>) as well as used in the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/probml/pyprobml">code
  to accompany</ext-link> the book <italic>“Machine learning: a
  probabilistic perspective”</italic>
  (<xref alt="Murphy, 2023" rid="ref-murphy2023" ref-type="bibr">Murphy,
  2023</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The design of the codebase was inspired by JAX’s
  <ext-link ext-link-type="uri" xlink:href="https://github.com/google/jax/blob/main/jax/example_libraries/optimizers.py">optimizers
  module</ext-link> as well as the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/blackjax-devs/blackjax">Blackjax</ext-link>
  library of MCMC samplers. We give special thanks to Kevin Murphy, Remi
  Louf, Colin Carroll, Charles Matthews, and Sharad Vikram for code
  contributions and insightful discussions.</p>
  <p>The authors were supported by the Engineering and Physical Sciences
  Research Council grantEP/V022636/1.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-coullon2021efficient">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Coullon</surname><given-names>Jeremie</given-names></name>
        <name><surname>South</surname><given-names>Leah</given-names></name>
        <name><surname>Nemeth</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>Efficient and generalizable tuning strategies for stochastic gradient MCMC</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2105.13059</uri>
    </element-citation>
  </ref>
  <ref id="ref-nemeth2021stochastic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nemeth</surname><given-names>Christopher</given-names></name>
        <name><surname>Fearnhead</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>Stochastic gradient Markov chain Monte Carlo</article-title>
      <source>Journal of the American Statistical Association</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>116</volume>
      <issue>533</issue>
      <fpage>433</fpage>
      <lpage>450</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NIPS2015_Ma_complete_recipe">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ma</surname><given-names>Yi-An</given-names></name>
        <name><surname>Chen</surname><given-names>Tianqi</given-names></name>
        <name><surname>Fox</surname><given-names>Emily</given-names></name>
      </person-group>
      <article-title>A complete recipe for stochastic gradient MCMC</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Cortes</surname><given-names>C.</given-names></name>
        <name><surname>Lawrence</surname><given-names>N.</given-names></name>
        <name><surname>Lee</surname><given-names>D.</given-names></name>
        <name><surname>Sugiyama</surname><given-names>M.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <uri>https://proceedings.neurips.cc/paper/2015/file/9a4400501febb2a95e79248486a5f6d3-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-jax2018github">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <source>JAX: Composable transformations of Python+NumPy programs</source>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-baker2019sgmcmc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baker</surname><given-names>Jack</given-names></name>
        <name><surname>Fearnhead</surname><given-names>Paul</given-names></name>
        <name><surname>Fox</surname><given-names>Emily B.</given-names></name>
        <name><surname>Nemeth</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>sgmcmc: An R package for stochastic gradient Markov chain Monte Carlo</article-title>
      <source>Journal of Statistical Software</source>
      <publisher-name>University of California at Los Angeles</publisher-name>
      <year iso-8601-date="2019-10-31">2019</year><month>10</month><day>31</day>
      <volume>91</volume>
      <issue>3</issue>
      <issn>1548-7660</issn>
      <pub-id pub-id-type="doi">10.18637/jss.v091.i03</pub-id>
      <fpage>1</fpage>
      <lpage>27</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow2015-whitepaper">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martı́n</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Mané</surname><given-names>Dandelion</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://www.tensorflow.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-murphy2023">
    <element-citation publication-type="manuscript">
      <person-group person-group-type="author">
        <name><surname>Murphy</surname><given-names>Kevin</given-names></name>
      </person-group>
      <article-title>Probabilistic machine learning: Advanced topics</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://probml.github.io/pml-book/book2.html</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
