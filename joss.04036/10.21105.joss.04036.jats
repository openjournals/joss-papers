<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4036</article-id>
<article-id pub-id-type="doi">10.21105/joss.04036</article-id>
<title-group>
<article-title>sweater: Speedy Word Embedding Association Test and
Extras Using R</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6232-7530</contrib-id>
<name>
<surname>Chan</surname>
<given-names>Chung-hong</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Mannheimer Zentrum für Europäische Sozialforschung,
Universität Mannheim</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-01-28">
<day>28</day>
<month>1</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>72</issue>
<fpage>4036</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>word embedding</kwd>
<kwd>implicit bias</kwd>
<kwd>media bias</kwd>
<kwd>algorithmic accountability</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The goal of this R package is to detect associations among words in
  word embedding spaces. Word embeddings can capture how similar or
  different two words are in terms of implicit and explicit meanings.
  Using the example in Collobert et al.
  (<xref alt="2011" rid="ref-collobert2011natural" ref-type="bibr">2011</xref>),
  the word vector for “XBox” is close to that for “PlayStation”, as
  measured by a distance measure such as cosine distance. Word
  embeddings can also be used to study associations among words that are
  otherwise difficult to detect. For instance, Jing &amp; Ahn
  (<xref alt="2021" rid="ref-jing2021characterizing" ref-type="bibr">2021</xref>)
  used word embeddings to study how Democrats and Republicans are
  divided along party lines about COVID-19.</p>
  <p>The same technique can also be used to detect unwanted implicit
  associations, or biases. For example, Kroon, Trilling, &amp; Raats
  (<xref alt="2020" rid="ref-kroon2020guilty" ref-type="bibr">2020</xref>)
  measure how close the word vectors for various ethnic group names
  (e.g. “Dutch”, “Belgian” , and “Syrian”) are to those for various
  nouns related to threats (e.g. “terrorist”, “murderer”, and
  “gangster”). These biases in word embedding can be understood through
  the implicit social cognition model of media priming
  (<xref alt="Arendt, 2013" rid="ref-arendtU003A2013U003ADDM" ref-type="bibr">Arendt,
  2013</xref>). In this model, implicit stereotypes are defined as the
  “strength of the automatic association between a group concept (e.g.,
  minority group) and an attribute (e.g., criminal).”
  (<xref alt="Arendt, 2013, p. 832" rid="ref-arendtU003A2013U003ADDM" ref-type="bibr">Arendt,
  2013, p. 832</xref>) All of these bias detection methods are based on
  the strength of association between a concept (or a target) and an
  attribute in embedding spaces.</p>
  <p>The importance of detecting biases in word embeddings is twofold.
  First, pretrained, biased word embeddings deployed in real-life
  machine learning systems can pose fairness concerns
  (<xref alt="Boyarskaya, Olteanu, &amp; Crawford, 2020" rid="ref-boyarskaya2020overcoming" ref-type="bibr">Boyarskaya,
  Olteanu, &amp; Crawford, 2020</xref>;
  <xref alt="Packer, Mitchell, Guajardo-C’espedes, &amp; Halpern, 2018" rid="ref-packer2018text" ref-type="bibr">Packer,
  Mitchell, Guajardo-C’espedes, &amp; Halpern, 2018</xref>). Second,
  biases in word embeddings reflect the biases in the original training
  material. Social scientists, communication researchers included, have
  exploited these methods to quantify (implicit) media biases by
  extracting biases from word embeddings locally trained on large text
  corpora
  (<xref alt="Knoche, Popovi’c, Lemmerich, &amp; Strohmaier, 2019" rid="ref-knoche2019identifying" ref-type="bibr">Knoche,
  Popovi’c, Lemmerich, &amp; Strohmaier, 2019</xref>; e.g.
  <xref alt="Kroon et al., 2020" rid="ref-kroon2020guilty" ref-type="bibr">Kroon
  et al., 2020</xref>;
  <xref alt="Sales, Balby, &amp; Veloso, 2019" rid="ref-sales2019media" ref-type="bibr">Sales,
  Balby, &amp; Veloso, 2019</xref>).</p>
  <p>Previously, the software of these methods is only available
  piecemeal as the addendum of the original papers and was implemented
  in different languages (Java, Python, etc.).
  <monospace>sweater</monospace> provides several of these bias
  detection methods in one unified package with a consistent R interface
  (<xref alt="R Core Team, 2021" rid="ref-rcore" ref-type="bibr">R Core
  Team, 2021</xref>). Also, <monospace>sweater</monospace> provides
  several methods that are implemented in C++ for speed and interfaced
  to R using the <monospace>Rcpp</monospace> package
  (<xref alt="Eddelbuettel, 2013" rid="ref-eddelbuettelU003A2013U003ASRC" ref-type="bibr">Eddelbuettel,
  2013</xref>) <xref ref-type="fn" rid="fn1">1</xref>.</p>
</sec>
<sec id="related-work">
  <title>Related work</title>
  <p>The R package <monospace>cbn</monospace>
  (https://github.com/conjugateprior/cbn) by Will Lowe provides tools
  for replicating the study by Caliskan, Bryson, &amp; Narayanan
  (<xref alt="2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">2017</xref>).
  The Python package <monospace>wefe</monospace>
  (<xref alt="Badilla, Bravo-Marquez, &amp; P’erez, 2020" rid="ref-badilla2020wefe" ref-type="bibr">Badilla,
  Bravo-Marquez, &amp; P’erez, 2020</xref>) provides several methods for
  bias evaluation in a unified (Python) interface.</p>
</sec>
<sec id="usage">
  <title>Usage</title>
  <p>In this section, I demonstrate how the package can be used to
  detect biases and reproduce some published findings.</p>
  <sec id="word-embeddings">
    <title>Word Embeddings</title>
    <p>The input word embedding <inline-formula><alternatives>
    <tex-math><![CDATA[w]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>w</mml:mi></mml:math></alternatives></inline-formula>
    is a dense <inline-formula><alternatives>
    <tex-math><![CDATA[m\times n]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>m</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
    matrix, where <inline-formula><alternatives>
    <tex-math><![CDATA[m]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>m</mml:mi></mml:math></alternatives></inline-formula>
    is the total size of the vocabulary in the training corpus and
    <inline-formula><alternatives>
    <tex-math><![CDATA[n]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
    is the vector dimension size.</p>
    <p><monospace>sweater</monospace> supports input word embeddings,
    <inline-formula><alternatives>
    <tex-math><![CDATA[w]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>w</mml:mi></mml:math></alternatives></inline-formula>,
    in several formats. For locally trained word embeddings, output from
    the R packages <monospace>word2vec</monospace>
    (<xref alt="Wijffels, 2021" rid="ref-wijffelsword2vec" ref-type="bibr">Wijffels,
    2021</xref>), <monospace>rsparse</monospace>
    (<xref alt="Selivanov, 2020" rid="ref-rsparse" ref-type="bibr">Selivanov,
    2020</xref>) and <monospace>text2vec</monospace>
    (<xref alt="Selivanov, Bickel, &amp; Wang, 2020" rid="ref-selivanov2020tex2vec" ref-type="bibr">Selivanov,
    Bickel, &amp; Wang, 2020</xref>) can be used directly with the
    packages primary functions, such as <monospace>query</monospace>
    <xref ref-type="fn" rid="fn2">2</xref>. Pretrained word embeddings
    in the so-called “word2vec” file format, such as those obtained
    online <xref ref-type="fn" rid="fn3">3</xref>, can be converted to
    the dense numeric matrix format required with the
    <monospace>read_word2vec</monospace> function.</p>
    <p>The package also provides three trimmed word embeddings for
    experimentation: <monospace>googlenews</monospace>
    (<xref alt="Mikolov, Sutskever, Chen, Corrado, &amp; Dean, 2013" rid="ref-mikolov2013distributed" ref-type="bibr">Mikolov,
    Sutskever, Chen, Corrado, &amp; Dean, 2013</xref>),
    <monospace>glove_math</monospace>
    (<xref alt="Pennington et al., 2014" rid="ref-penningtonU003A2014U003AG" ref-type="bibr">Pennington
    et al., 2014</xref>) , and <monospace>small_reddit</monospace>
    (<xref alt="An, Kwak, &amp; Ahn, 2018" rid="ref-an2018semaxis" ref-type="bibr">An,
    Kwak, &amp; Ahn, 2018</xref>).</p>
  </sec>
  <sec id="query">
    <title>Query</title>
    <p><monospace>sweater</monospace> uses the concept of a
    <italic>query</italic>
    (<xref alt="Badilla et al., 2020" rid="ref-badilla2020wefe" ref-type="bibr">Badilla
    et al., 2020</xref>) to study associations in
    <inline-formula><alternatives>
    <tex-math><![CDATA[w]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>w</mml:mi></mml:math></alternatives></inline-formula>
    and the <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}\mathcal{T}\mathcal{A}\mathcal{B}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:mrow></mml:math></alternatives></inline-formula>
    notation from Brunet, Alkalay-Houlihan, Anderson, &amp; Zemel
    (<xref alt="2019" rid="ref-brunet2019understanding" ref-type="bibr">2019</xref>)
    to form a query. A query contains two or more sets of seed words
    (wordsets selected by the individual administering the test,
    sometimes called “seed lexicons” or “dictionaries”). Among these
    seed wordsets, there should be at least one set of <italic>target
    words</italic> and one set of <italic>attribute words</italic>.</p>
    <p>In the situation of bias detection, target words are words that
    <bold>should</bold> have no bias and usually represent the concept
    one would like to probe for biases. For instance, Garg, Schiebinger,
    Jurafsky, &amp; Zou
    (<xref alt="2018" rid="ref-gargU003A2018U003AW" ref-type="bibr">2018</xref>)
    investigated the “women bias” of occupation-related words and their
    target words contain “nurse”, “mathematician”, and “blacksmith”.
    These words can be used as target words because in an ideal world
    with no “women bias” associated with occupations, these
    occupation-related words should have no gender association.</p>
    <p>Target words are denoted as wordsets
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
    All methods require <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    while <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    is only required for Word Embedding Association Test (WEAT). For
    instance, the study of gender stereotypes in academic pursuits by
    Caliskan et al.
    (<xref alt="2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">2017</xref>)
    used <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S} = \{math, algebra, geometry, calculus, equations, ...\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>b</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}= \{poetry, art, dance, literature, novel, ...\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
    <p>In the situation of bias detection, attribute words are words
    that have known properties in relation to the bias. They are denoted
    as wordsets <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{A}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{B}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
    All methods require both wordsets except Mean Average Cosine
    Similarity
    (<xref alt="Manzini, Lim, Tsvetkov, &amp; Black, 2019" rid="ref-manzini2019black" ref-type="bibr">Manzini,
    Lim, Tsvetkov, &amp; Black, 2019</xref>). For instance, the study of
    gender stereotypes by Caliskan et al.
    (<xref alt="2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">2017</xref>)
    used <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{A} = \{he, son, his, him, ...\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{B} = \{she, daughter, hers, her, ...\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>g</mml:mi><mml:mi>h</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
    In some applications, popular off-the-shelf sentiment dictionaries
    can also be used as <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{A}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{B}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    (e.g.
    <xref alt="Sweeney &amp; Najafian, 2020" rid="ref-sweeney2020reducing" ref-type="bibr">Sweeney
    &amp; Najafian, 2020</xref>). That being said, it is up to the
    researchers to select and derive these seed words in a query.
    However, the selection of seed words has been shown to be the most
    consequential part of the entire analysis
    (<xref alt="Antoniak &amp; Mimno, 2021" rid="ref-antoniak2021bad" ref-type="bibr">Antoniak
    &amp; Mimno, 2021</xref>;
    <xref alt="Du, Fang, &amp; Nguyen, 2021" rid="ref-du2021assessing" ref-type="bibr">Du,
    Fang, &amp; Nguyen, 2021</xref>). Please read Antoniak &amp; Mimno
    (<xref alt="2021" rid="ref-antoniak2021bad" ref-type="bibr">2021</xref>)
    for recommendations.</p>
  </sec>
  <sec id="supported-methods">
    <title>Supported methods</title>
    <p>Table 1 lists all methods supported by sweater. The function
    <monospace>query</monospace> is used to conduct a query. The
    function <monospace>calculate_es</monospace> can be used for some
    methods to calculate the effect size representing the overall bias
    of <inline-formula><alternatives>
    <tex-math><![CDATA[w]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>w</mml:mi></mml:math></alternatives></inline-formula>
    from the query.</p>
    <table-wrap>
      <caption>
        <p>All methods supported by sweater</p>
      </caption>
      <table>
        <colgroup>
          <col width="52%" />
          <col width="24%" />
          <col width="24%" />
        </colgroup>
        <thead>
          <tr>
            <th>Method</th>
            <th>Target words</th>
            <th>Attribute words</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Mean Average Cosine Similarity
            (<xref alt="Manzini et al., 2019" rid="ref-manzini2019black" ref-type="bibr">Manzini
            et al., 2019</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>Relative Norm Distance
            (<xref alt="Garg et al., 2018" rid="ref-gargU003A2018U003AW" ref-type="bibr">Garg
            et al., 2018</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>Relative Negative Sentiment Bias
            (<xref alt="Sweeney &amp; Najafian, 2020" rid="ref-sweeney2020reducing" ref-type="bibr">Sweeney
            &amp; Najafian, 2020</xref>)
            <xref ref-type="fn" rid="fn4">4</xref></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>SemAxis
            (<xref alt="An et al., 2018" rid="ref-an2018semaxis" ref-type="bibr">An
            et al., 2018</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>Normalized Association Score
            (<xref alt="Caliskan et al., 2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">Caliskan
            et al., 2017</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>Embedding Coherence Test
            (<xref alt="Dev &amp; Phillips, 2019" rid="ref-dev2019attenuating" ref-type="bibr">Dev
            &amp; Phillips, 2019</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
          <tr>
            <td>Word Embedding Association Test
            (<xref alt="Caliskan et al., 2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">Caliskan
            et al., 2017</xref>)</td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{S}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{T}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
            <td><inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{A}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
            <inline-formula><alternatives>
            <tex-math><![CDATA[\mathcal{B}]]></tex-math>
            <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula></td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="example-1">
    <title>Example 1</title>
    <p>Relative Norm Distance (RND)
    (<xref alt="Garg et al., 2018" rid="ref-gargU003A2018U003AW" ref-type="bibr">Garg
    et al., 2018</xref>) is calculated with two sets of attribute words.
    The following analysis reproduces the calculation of “women bias”
    values in Garg et al.
    (<xref alt="2018" rid="ref-gargU003A2018U003AW" ref-type="bibr">2018</xref>).
    The publicly available word2vec word embeddings trained on the
    Google News corpus is used
    (<xref alt="Mikolov et al., 2013" rid="ref-mikolov2013distributed" ref-type="bibr">Mikolov
    et al., 2013</xref>). Words such as “nurse”, “midwife” and
    “librarian” are more associated with female, as indicated by the
    positive relative norm distance (Figure 1).</p>
    <code language="r script">library(sweater)</code>
    <code language="r script">S1 &lt;- c(&quot;janitor&quot;, &quot;statistician&quot;, &quot;midwife&quot;, &quot;bailiff&quot;, &quot;auctioneer&quot;,
       &quot;photographer&quot;, &quot;geologist&quot;, &quot;shoemaker&quot;, &quot;athlete&quot;, &quot;cashier&quot;,
       &quot;dancer&quot;, &quot;housekeeper&quot;, &quot;accountant&quot;, &quot;physicist&quot;, &quot;gardener&quot;,
       &quot;dentist&quot;, &quot;weaver&quot;, &quot;blacksmith&quot;, &quot;psychologist&quot;, &quot;supervisor&quot;,
       &quot;mathematician&quot;, &quot;surveyor&quot;, &quot;tailor&quot;, &quot;designer&quot;, &quot;economist&quot;,
       &quot;mechanic&quot;, &quot;laborer&quot;, &quot;postmaster&quot;, &quot;broker&quot;, &quot;chemist&quot;,
       &quot;librarian&quot;, &quot;attendant&quot;, &quot;clerical&quot;, &quot;musician&quot;, &quot;porter&quot;,
       &quot;scientist&quot;, &quot;carpenter&quot;, &quot;sailor&quot;, &quot;instructor&quot;, &quot;sheriff&quot;,
       &quot;pilot&quot;, &quot;inspector&quot;, &quot;mason&quot;, &quot;baker&quot;, &quot;administrator&quot;,
       &quot;architect&quot;, &quot;collector&quot;, &quot;operator&quot;, &quot;surgeon&quot;, &quot;driver&quot;,
       &quot;painter&quot;, &quot;conductor&quot;, &quot;nurse&quot;, &quot;cook&quot;, &quot;engineer&quot;, &quot;retired&quot;,
       &quot;sales&quot;, &quot;lawyer&quot;, &quot;clergy&quot;, &quot;physician&quot;, &quot;farmer&quot;, &quot;clerk&quot;,
       &quot;manager&quot;, &quot;guard&quot;, &quot;artist&quot;, &quot;smith&quot;, &quot;official&quot;, &quot;police&quot;,
       &quot;doctor&quot;, &quot;professor&quot;, &quot;student&quot;, &quot;judge&quot;, &quot;teacher&quot;, &quot;author&quot;,
       &quot;secretary&quot;, &quot;soldier&quot;)
A1 &lt;- c(&quot;he&quot;, &quot;son&quot;, &quot;his&quot;, &quot;him&quot;, &quot;father&quot;, &quot;man&quot;, &quot;boy&quot;, &quot;himself&quot;,
        &quot;male&quot;, &quot;brother&quot;, &quot;sons&quot;, &quot;fathers&quot;, &quot;men&quot;, &quot;boys&quot;, &quot;males&quot;,
        &quot;brothers&quot;, &quot;uncle&quot;, &quot;uncles&quot;, &quot;nephew&quot;, &quot;nephews&quot;)
B1 &lt;- c(&quot;she&quot;, &quot;daughter&quot;, &quot;hers&quot;, &quot;her&quot;, &quot;mother&quot;, &quot;woman&quot;, &quot;girl&quot;,
       &quot;herself&quot;, &quot;female&quot;, &quot;sister&quot;, &quot;daughters&quot;, &quot;mothers&quot;, &quot;women&quot;,
       &quot;girls&quot;, &quot;females&quot;, &quot;sisters&quot;, &quot;aunt&quot;, &quot;aunts&quot;, &quot;niece&quot;, &quot;nieces&quot;)
res_rnd_male &lt;- query(w = googlenews, S_words = S1,
                      A_words = A1, B_words= B1,
                      method = &quot;rnd&quot;)
plot(res_rnd_male)</code>
    <fig>
      <caption><p>Bias of words in the target wordset according to
      relative norm distance</p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="media/paper_files/figure-latex/rnd-1.pdf" xlink:title="" />
    </fig>
  </sec>
  <sec id="example-2">
    <title>Example 2</title>
    <p>Word Embedding Association Test (WEAT)
    (<xref alt="Caliskan et al., 2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">Caliskan
    et al., 2017</xref>) requires all four wordsets of
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{A}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>,
    and <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{B}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
    The method is modeled after the Implicit Association Test (IAT)
    (<xref alt="Nosek, Greenwald, &amp; Banaji, 2005" rid="ref-nosekU003A2005U003AUUI" ref-type="bibr">Nosek,
    Greenwald, &amp; Banaji, 2005</xref>) and it measures the relative
    strength of <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>’s
    association with <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{A}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒜</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    to <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{B}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>ℬ</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    against the same of <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.
    The effect sizes calculated from a large corpus, as shown by
    Caliskan et al.
    (<xref alt="2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">2017</xref>),
    are comparable to the published IAT effect sizes obtained from
    volunteers.</p>
    <p>In this example, the publicly available GLoVE embeddings made
    available by the original Stanford Team
    (<xref alt="Pennington et al., 2014" rid="ref-penningtonU003A2014U003AG" ref-type="bibr">Pennington
    et al., 2014</xref>) were used. In the following example, the
    calculation of “Math vs Arts” gender bias in Caliskan et al.
    (<xref alt="2017" rid="ref-caliskanU003A2017U003AS" ref-type="bibr">2017</xref>)
    is reproduced. In this example, the positive effect size indicates
    the words in the wordset <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{S}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒮</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>
    are more associated with males than are the words in wordset
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{T}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mstyle mathvariant="script"><mml:mi>𝒯</mml:mi></mml:mstyle></mml:math></alternatives></inline-formula>.</p>
    <code language="r script">S2 &lt;- c(&quot;math&quot;, &quot;algebra&quot;, &quot;geometry&quot;, &quot;calculus&quot;, &quot;equations&quot;,
        &quot;computation&quot;, &quot;numbers&quot;, &quot;addition&quot;)
T2 &lt;- c(&quot;poetry&quot;, &quot;art&quot;, &quot;dance&quot;, &quot;literature&quot;, &quot;novel&quot;, &quot;symphony&quot;,
        &quot;drama&quot;, &quot;sculpture&quot;)
A2 &lt;- c(&quot;male&quot;, &quot;man&quot;, &quot;boy&quot;, &quot;brother&quot;, &quot;he&quot;, &quot;him&quot;, &quot;his&quot;, &quot;son&quot;)
B2 &lt;- c(&quot;female&quot;, &quot;woman&quot;, &quot;girl&quot;, &quot;sister&quot;, &quot;she&quot;, &quot;her&quot;, &quot;hers&quot;,
        &quot;daughter&quot;)
sw &lt;- query(w = glove_math,
            S_words = S2, T_words = T2,
            A_words = A2, B_words = B2)
sw</code>
    <preformat>## </preformat>
    <preformat>## -- sweater object ------------------------------------------------------------------------------------------------------------------------------------------</preformat>
    <preformat>## Test type:  weat 
## Effect size:  1.055015</preformat>
    <preformat>## </preformat>
    <preformat>## -- Functions -----------------------------------------------------------------------------------------------------------------------------------------------</preformat>
    <preformat>## * `calculate_es()`: Calculate effect size</preformat>
    <preformat>## * `weat_resampling()`: Conduct statistical test</preformat>
    <p>The statistical significance of the effect size can be evaluated
    using the function <monospace>weat_resampling</monospace>.</p>
    <code language="r script">weat_resampling(sw)</code>
    <preformat>## 
##  Resampling approximation of the exact test in Caliskan et al. (2017)
## 
## data:  sw
## bias = 0.024865, p-value = 0.0171
## alternative hypothesis: true bias is greater than 7.245425e-05
## sample estimates:
##       bias 
## 0.02486533</preformat>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The development of this package was supported by the Federal
  Ministry for Family Affairs, Senior Citizens, Women and Youth
  (<italic>Bundesministerium für Familie, Senioren, Frauen und
  Jugend</italic>), the Federal Republic of Germany – Research project:
  “<italic>Erfahrungen von Alltagsrassismus und medienvermittelter
  Rassismus in der (politischen) Öffentlichkeit</italic>”.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-an2018semaxis">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>An</surname><given-names>Jisun</given-names></name>
        <name><surname>Kwak</surname><given-names>Haewoon</given-names></name>
        <name><surname>Ahn</surname><given-names>Yong-Yeol</given-names></name>
      </person-group>
      <article-title>Semaxis: A lightweight framework to characterize domain-specific word semantics beyond sentiment</article-title>
      <source>arXiv preprint arXiv:1806.05521</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.18653/v1/p18-1228</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-antoniak2021bad">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Antoniak</surname><given-names>Maria</given-names></name>
        <name><surname>Mimno</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Bad seeds: Evaluating lexical methods for bias measurement</article-title>
      <source>Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.18653/v1/2021.acl-long.148</pub-id>
      <fpage>1889</fpage>
      <lpage>1904</lpage>
    </element-citation>
  </ref>
  <ref id="ref-arendtU003A2013U003ADDM">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arendt</surname><given-names>Florian</given-names></name>
      </person-group>
      <article-title>Dose-dependent media priming effects of stereotypic newspaper articles on implicit and explicit stereotypes</article-title>
      <source>Journal of Communication</source>
      <publisher-name>Oxford University Press (OUP)</publisher-name>
      <year iso-8601-date="2013-09">2013</year><month>09</month>
      <volume>63</volume>
      <issue>5</issue>
      <issn>0021-9916</issn>
      <uri>http://dx.doi.org/10.1111/jcom.12056</uri>
      <pub-id pub-id-type="doi">10.1111/jcom.12056</pub-id>
      <fpage>830</fpage>
      <lpage>851</lpage>
    </element-citation>
  </ref>
  <ref id="ref-badilla2020wefe">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Badilla</surname><given-names>Pablo</given-names></name>
        <name><surname>Bravo-Marquez</surname><given-names>Felipe</given-names></name>
        <name><surname>P’erez</surname><given-names>Jorge</given-names></name>
      </person-group>
      <article-title>WEFE: The word embeddings fairness evaluation framework.</article-title>
      <source>IJCAI</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.24963/ijcai.2020/60</pub-id>
      <fpage>430</fpage>
      <lpage>436</lpage>
    </element-citation>
  </ref>
  <ref id="ref-boyarskaya2020overcoming">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Boyarskaya</surname><given-names>Margarita</given-names></name>
        <name><surname>Olteanu</surname><given-names>Alexandra</given-names></name>
        <name><surname>Crawford</surname><given-names>Kate</given-names></name>
      </person-group>
      <article-title>Overcoming Failures of Imagination in AI Infused System Development and Deployment</article-title>
      <source>arXiv preprint arXiv:2011.13416</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-brunet2019understanding">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Brunet</surname><given-names>Marc-Etienne</given-names></name>
        <name><surname>Alkalay-Houlihan</surname><given-names>Colleen</given-names></name>
        <name><surname>Anderson</surname><given-names>Ashton</given-names></name>
        <name><surname>Zemel</surname><given-names>Richard</given-names></name>
      </person-group>
      <article-title>Understanding the origins of bias in word embeddings</article-title>
      <source>International conference on machine learning</source>
      <year iso-8601-date="2019">2019</year>
      <fpage>803</fpage>
      <lpage>811</lpage>
    </element-citation>
  </ref>
  <ref id="ref-caliskanU003A2017U003AS">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Caliskan</surname><given-names>Aylin</given-names></name>
        <name><surname>Bryson</surname><given-names>Joanna J.</given-names></name>
        <name><surname>Narayanan</surname><given-names>Arvind</given-names></name>
      </person-group>
      <article-title>Semantics derived automatically from language corpora contain human-like biases</article-title>
      <source>Science</source>
      <publisher-name>American Association for the Advancement of Science (AAAS)</publisher-name>
      <year iso-8601-date="2017-04">2017</year><month>04</month>
      <volume>356</volume>
      <issue>6334</issue>
      <issn>1095-9203</issn>
      <uri>http://dx.doi.org/10.1126/science.aal4230</uri>
      <pub-id pub-id-type="doi">10.1126/science.aal4230</pub-id>
      <fpage>183</fpage>
      <lpage>186</lpage>
    </element-citation>
  </ref>
  <ref id="ref-du2021assessing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Du</surname><given-names>Yupei</given-names></name>
        <name><surname>Fang</surname><given-names>Qixiang</given-names></name>
        <name><surname>Nguyen</surname><given-names>Dong</given-names></name>
      </person-group>
      <article-title>Assessing the reliability of word embedding gender bias measures</article-title>
      <source>arXiv preprint arXiv:2109.04732</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.18653/v1/2021.emnlp-main.785</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gargU003A2018U003AW">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Garg</surname><given-names>Nikhil</given-names></name>
        <name><surname>Schiebinger</surname><given-names>Londa</given-names></name>
        <name><surname>Jurafsky</surname><given-names>Dan</given-names></name>
        <name><surname>Zou</surname><given-names>James</given-names></name>
      </person-group>
      <article-title>Word embeddings quantify 100 years of gender and ethnic stereotypes</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <publisher-name>Proceedings of the National Academy of Sciences</publisher-name>
      <year iso-8601-date="2018-04">2018</year><month>04</month>
      <volume>115</volume>
      <issue>16</issue>
      <issn>1091-6490</issn>
      <uri>http://dx.doi.org/10.1073/pnas.1720347115</uri>
      <pub-id pub-id-type="doi">10.1073/pnas.1720347115</pub-id>
      <fpage>E3635</fpage>
      <lpage>E3644</lpage>
    </element-citation>
  </ref>
  <ref id="ref-knoche2019identifying">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Knoche</surname><given-names>Markus</given-names></name>
        <name><surname>Popovi’c</surname><given-names>Radomir</given-names></name>
        <name><surname>Lemmerich</surname><given-names>Florian</given-names></name>
        <name><surname>Strohmaier</surname><given-names>Markus</given-names></name>
      </person-group>
      <article-title>Identifying biases in politically biased wikis through word embeddings</article-title>
      <source>Proceedings of the 30th ACM conference on hypertext and social media</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1145/3342220.3343658</pub-id>
      <fpage>253</fpage>
      <lpage>257</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kroon2020guilty">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kroon</surname><given-names>Anne C.</given-names></name>
        <name><surname>Trilling</surname><given-names>Damian</given-names></name>
        <name><surname>Raats</surname><given-names>Tamara</given-names></name>
      </person-group>
      <article-title>Guilty by association: Using word embeddings to measure ethnic stereotypes in news coverage</article-title>
      <source>Journalism &amp; Mass Communication Quarterly</source>
      <publisher-name>SAGE Publications Sage CA: Los Angeles, CA</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1177/1077699020932304</pub-id>
      <fpage>1077699020932304</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-manzini2019black">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Manzini</surname><given-names>Thomas</given-names></name>
        <name><surname>Lim</surname><given-names>Yao Chong</given-names></name>
        <name><surname>Tsvetkov</surname><given-names>Yulia</given-names></name>
        <name><surname>Black</surname><given-names>Alan W</given-names></name>
      </person-group>
      <article-title>Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings</article-title>
      <source>arXiv preprint arXiv:1904.04047</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.18653/v1/n19-1062</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mikolov2013distributed">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Mikolov</surname><given-names>Tomas</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Chen</surname><given-names>Kai</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S</given-names></name>
        <name><surname>Dean</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>Distributed representations of words and phrases and their compositionality</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2013">2013</year>
      <fpage>3111</fpage>
      <lpage>3119</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nosekU003A2005U003AUUI">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nosek</surname><given-names>Brian A.</given-names></name>
        <name><surname>Greenwald</surname><given-names>Anthony G.</given-names></name>
        <name><surname>Banaji</surname><given-names>Mahzarin R.</given-names></name>
      </person-group>
      <article-title>Understanding and Using the Implicit Association Test: II. Method Variables and Construct Validity</article-title>
      <source>Personality and Social Psychology Bulletin</source>
      <publisher-name>SAGE Publications</publisher-name>
      <year iso-8601-date="2005-02">2005</year><month>02</month>
      <volume>31</volume>
      <issue>2</issue>
      <issn>1552-7433</issn>
      <uri>http://dx.doi.org/10.1177/0146167204271418</uri>
      <pub-id pub-id-type="doi">10.1177/0146167204271418</pub-id>
      <fpage>166</fpage>
      <lpage>180</lpage>
    </element-citation>
  </ref>
  <ref id="ref-packer2018text">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Packer</surname><given-names>Ben</given-names></name>
        <name><surname>Mitchell</surname><given-names>M</given-names></name>
        <name><surname>Guajardo-C’espedes</surname><given-names>Mario</given-names></name>
        <name><surname>Halpern</surname><given-names>Yoni</given-names></name>
      </person-group>
      <article-title>Text embeddings contain bias. Here’s why that matters.</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-penningtonU003A2014U003AG">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pennington</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Socher</surname><given-names>Richard</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>Glove: Global vectors for word representation</article-title>
      <source>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <uri>http://dx.doi.org/10.3115/v1/d14-1162</uri>
      <pub-id pub-id-type="doi">10.3115/v1/d14-1162</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-rcore">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>R Core Team</string-name>
      </person-group>
      <source>R: A language and environment for statistical computing</source>
      <publisher-name>R Foundation for Statistical Computing</publisher-name>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <uri>https://www.R-project.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-rsparse">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Selivanov</surname><given-names>Dmitriy</given-names></name>
      </person-group>
      <source>Rsparse: Statistical learning on sparse matrices</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://CRAN.R-project.org/package=rsparse</uri>
    </element-citation>
  </ref>
  <ref id="ref-sales2019media">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sales</surname><given-names>Allan</given-names></name>
        <name><surname>Balby</surname><given-names>Leandro</given-names></name>
        <name><surname>Veloso</surname><given-names>Adriano</given-names></name>
      </person-group>
      <article-title>Media bias characterization in brazilian presidential elections</article-title>
      <source>Proceedings of the 30th ACM conference on hypertext and social media</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1145/3345645.3351107</pub-id>
      <fpage>231</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-selivanov2020tex2vec">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Selivanov</surname><given-names>Dmitriy</given-names></name>
        <name><surname>Bickel</surname><given-names>Manuel</given-names></name>
        <name><surname>Wang</surname><given-names>Qing</given-names></name>
      </person-group>
      <source>text2vec: Modern text mining framework for R</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://CRAN.R-project.org/package=text2vec</uri>
    </element-citation>
  </ref>
  <ref id="ref-sweeney2020reducing">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sweeney</surname><given-names>Chris</given-names></name>
        <name><surname>Najafian</surname><given-names>Maryam</given-names></name>
      </person-group>
      <article-title>Reducing sentiment polarity for demographic attributes in word embeddings using adversarial learning</article-title>
      <source>Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1145/3351095.3372837</pub-id>
      <fpage>359</fpage>
      <lpage>368</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wijffelsword2vec">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Wijffels</surname><given-names>Jan</given-names></name>
      </person-group>
      <source>word2vec: Distributed representations of words</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://CRAN.R-project.org/package=word2vec</uri>
    </element-citation>
  </ref>
  <ref id="ref-eddelbuettelU003A2013U003ASRC">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eddelbuettel</surname><given-names>Dirk</given-names></name>
      </person-group>
      <article-title>Seamless R and C++ Integration with Rcpp</article-title>
      <publisher-name>Springer New York</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <isbn>9781461468684</isbn>
      <uri>http://dx.doi.org/10.1007/978-1-4614-6868-4</uri>
      <pub-id pub-id-type="doi">10.1007/978-1-4614-6868-4</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-dev2019attenuating">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dev</surname><given-names>Sunipa</given-names></name>
        <name><surname>Phillips</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>Attenuating bias in word vectors</article-title>
      <source>The 22nd international conference on artificial intelligence and statistics</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <fpage>879</fpage>
      <lpage>887</lpage>
    </element-citation>
  </ref>
  <ref id="ref-collobert2011natural">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Collobert</surname><given-names>Ronan</given-names></name>
        <name><surname>Weston</surname><given-names>Jason</given-names></name>
        <name><surname>Bottou</surname><given-names>Léon</given-names></name>
        <name><surname>Karlen</surname><given-names>Michael</given-names></name>
        <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
        <name><surname>Kuksa</surname><given-names>Pavel</given-names></name>
      </person-group>
      <article-title>Natural language processing (almost) from scratch</article-title>
      <source>Journal of machine learning research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <issue>ARTICLE</issue>
      <fpage>2493</fpage>
      <lpage>2537</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jing2021characterizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jing</surname><given-names>Elise</given-names></name>
        <name><surname>Ahn</surname><given-names>Yong-Yeol</given-names></name>
      </person-group>
      <article-title>Characterizing partisan political narrative frameworks about COVID-19 on Twitter</article-title>
      <source>EPJ data science</source>
      <publisher-name>Springer Berlin Heidelberg</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>10</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1140/epjds/s13688-021-00308-4</pub-id>
      <fpage>53</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-benoit2018quanteda">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Benoit</surname><given-names>Kenneth</given-names></name>
        <name><surname>Watanabe</surname><given-names>Kohei</given-names></name>
        <name><surname>Wang</surname><given-names>Haiyan</given-names></name>
        <name><surname>Nulty</surname><given-names>Paul</given-names></name>
        <name><surname>Obeng</surname><given-names>Adam</given-names></name>
        <name><surname>Müller</surname><given-names>Stefan</given-names></name>
        <name><surname>Matsuo</surname><given-names>Akitaka</given-names></name>
      </person-group>
      <article-title>Quanteda: An R package for the quantitative analysis of textual data</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>30</issue>
      <pub-id pub-id-type="doi">10.21105/joss.00774</pub-id>
      <fpage>774</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>Compared with a pure R implementation, the C++
    implementation of Word Embedding Association Test in
    <monospace>sweater</monospace> is at least 7 times faster. See the
    benchmark
    <ext-link ext-link-type="uri" xlink:href="https://github.com/chainsawriot/sweater/blob/master/paper/benchmark.md">here</ext-link>.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>The vignette of <monospace>text2vec</monospace>
    provides a guide on how to locally train word embeddings using the
    GLoVE algorithm
    (<xref alt="Pennington, Socher, &amp; Manning, 2014" rid="ref-penningtonU003A2014U003AG" ref-type="bibr">Pennington,
    Socher, &amp; Manning, 2014</xref>).
    https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>For example, the
    <ext-link ext-link-type="uri" xlink:href="https://nlp.stanford.edu/projects/glove/">pretrained
    GLoVE word embeddings</ext-link>,
    <ext-link ext-link-type="uri" xlink:href="https://wikipedia2vec.github.io/wikipedia2vec/pretrained/">pretrained
    word2vec word embeddings</ext-link> and pretrained
    <ext-link ext-link-type="uri" xlink:href="https://fasttext.cc/docs/en/english-vectors.html">fastText
    word embeddings</ext-link>.</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>Experimental support for quanteda dictionaries
    (<xref alt="Benoit et al., 2018" rid="ref-benoit2018quanteda" ref-type="bibr">Benoit
    et al., 2018</xref>) is current available for this method. The
    support will be expanded to all methods later.</p>
  </fn>
</fn-group>
</back>
</article>
