<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20260121075307-d0afe77eb2409874eb8724c35068b2bc75ba312f</doi_batch_id>
    <timestamp>20260121075307</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2026</year>
        </publication_date>
        <journal_volume>
          <volume>11</volume>
        </journal_volume>
        <issue>117</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>PureML: a transparent NumPy-only deep learning framework for teaching and prototyping</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Yehor</given_name>
            <surname>Mishchyriak</surname>
            <affiliations>
              <institution><institution_name>Department of Mathematics and Computer Science, Wesleyan University, Middletown, CT, United States</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0001-8371-7159</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>21</day>
          <year>2026</year>
        </publication_date>
        <pages>
          <first_page>9631</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.09631</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.18277559</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/9631</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.09631</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.09631</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.09631.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="harris2020numpy">
            <article_title>Array programming with NumPy</article_title>
            <author>Harris</author>
            <journal_title>Nature</journal_title>
            <issue>7825</issue>
            <volume>585</volume>
            <doi>10.1038/s41586-020-2649-2</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Harris, C. R., Millman, K. J., Walt, S. J. van der, &amp; others. (2020). Array programming with NumPy. Nature, 585(7825), 357–362. https://doi.org/10.1038/s41586-020-2649-2</unstructured_citation>
          </citation>
          <citation key="zarrpython2025">
            <article_title>Zarr-developers/zarr-python: v3.1.5</article_title>
            <author>Miles</author>
            <doi>10.5281/zenodo.17672242</doi>
            <cYear>2025</cYear>
            <unstructured_citation>Miles, A., Kirkham, J., Stansby, D., Papadopoulos Orfanos, D., Hamman, J., &amp; others. (2025). Zarr-developers/zarr-python: v3.1.5 (Version v3.1.5). https://doi.org/10.5281/zenodo.17672242</unstructured_citation>
          </citation>
          <citation key="paszke2019pytorch">
            <article_title>PyTorch: An imperative style, high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in Neural Information Processing Systems</journal_title>
            <volume>32</volume>
            <doi>10.48550/arXiv.1912.01703</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F., &amp; others. (2019). PyTorch: An imperative style, high-performance deep learning library. Advances in Neural Information Processing Systems, 32. https://doi.org/10.48550/arXiv.1912.01703</unstructured_citation>
          </citation>
          <citation key="jax2018github">
            <article_title>JAX: Composable transformations of Python+NumPy programs</article_title>
            <author>Bradbury</author>
            <cYear>2018</cYear>
            <unstructured_citation>Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., Necula, G., Paszke, A., VanderPlas, J., Wanderman-Milne, S., &amp; Zhang, Q. (2018). JAX: Composable transformations of Python+NumPy programs (Version 0.3.13). http://github.com/jax-ml/jax</unstructured_citation>
          </citation>
          <citation key="Goodfellow-et-al-2016">
            <volume_title>Deep learning</volume_title>
            <author>Goodfellow</author>
            <cYear>2016</cYear>
            <unstructured_citation>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). Deep learning. MIT Press.</unstructured_citation>
          </citation>
          <citation key="kingma2015adam">
            <article_title>Adam: A method for stochastic optimization</article_title>
            <author>Kingma</author>
            <doi>10.48550/arXiv.1412.6980</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Kingma, D. P., &amp; Ba, J. (2015). Adam: A method for stochastic optimization. https://doi.org/10.48550/arXiv.1412.6980</unstructured_citation>
          </citation>
          <citation key="robbins1951stochastic">
            <article_title>A stochastic approximation method</article_title>
            <author>Robbins</author>
            <journal_title>The Annals of Mathematical Statistics</journal_title>
            <issue>3</issue>
            <volume>22</volume>
            <doi>10.1214/aoms/1177729586</doi>
            <cYear>1951</cYear>
            <unstructured_citation>Robbins, H., &amp; Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, 22(3), 400–407. https://doi.org/10.1214/aoms/1177729586</unstructured_citation>
          </citation>
          <citation key="duchi2011adagrad">
            <article_title>Adaptive subgradient methods for online learning and stochastic optimization</article_title>
            <author>Duchi</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <volume>12</volume>
            <cYear>2011</cYear>
            <unstructured_citation>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12, 2121–2159. http://jmlr.org/papers/v12/duchi11a.html</unstructured_citation>
          </citation>
          <citation key="tieleman2012rmsprop">
            <article_title>Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude</article_title>
            <author>Tieleman</author>
            <cYear>2012</cYear>
            <unstructured_citation>Tieleman, T., &amp; Hinton, G. (2012). Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. Coursera: Neural Networks for Machine Learning. http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</unstructured_citation>
          </citation>
          <citation key="loshchilov2016sgdr">
            <article_title>SGDR: Stochastic gradient descent with warm restarts</article_title>
            <author>Loshchilov</author>
            <doi>10.48550/arXiv.1608.03983</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Loshchilov, I., &amp; Hutter, F. (2016). SGDR: Stochastic gradient descent with warm restarts. https://doi.org/10.48550/arXiv.1608.03983</unstructured_citation>
          </citation>
          <citation key="lecun1998mnist">
            <article_title>Gradient-based learning applied to document recognition</article_title>
            <author>LeCun</author>
            <journal_title>Proceedings of the IEEE</journal_title>
            <issue>11</issue>
            <volume>86</volume>
            <doi>10.1109/5.726791</doi>
            <cYear>1998</cYear>
            <unstructured_citation>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278–2324. https://doi.org/10.1109/5.726791</unstructured_citation>
          </citation>
          <citation key="karpathy2020micrograd">
            <article_title>Micrograd</article_title>
            <author>Karpathy</author>
            <cYear>2020</cYear>
            <unstructured_citation>Karpathy, A. (2020). Micrograd. https://github.com/karpathy/micrograd</unstructured_citation>
          </citation>
          <citation key="bourgin2019numpyml">
            <article_title>Numpy-ml</article_title>
            <author>Bourgin</author>
            <cYear>2019</cYear>
            <unstructured_citation>Bourgin, D. (2019). Numpy-ml. https://github.com/ddbourgin/numpy-ml</unstructured_citation>
          </citation>
          <citation key="abadi2016tensorflow">
            <article_title>TensorFlow: A system for large-scale machine learning</article_title>
            <author>Abadi</author>
            <journal_title>OSDI</journal_title>
            <doi>10.48550/arXiv.1605.08695</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., &amp; others. (2016). TensorFlow: A system for large-scale machine learning. OSDI. https://doi.org/10.48550/arXiv.1605.08695</unstructured_citation>
          </citation>
          <citation key="chollet2015keras">
            <article_title>Keras</article_title>
            <author>Chollet</author>
            <cYear>2015</cYear>
            <unstructured_citation>Chollet, F., &amp; others. (2015). Keras. https://keras.io.</unstructured_citation>
          </citation>
          <citation key="tinygrad2024">
            <article_title>Tinygrad</article_title>
            <author>Hotz</author>
            <cYear>2024</cYear>
            <unstructured_citation>Hotz, G., &amp; contributors. (2024). Tinygrad. https://github.com/tinygrad/tinygrad</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
