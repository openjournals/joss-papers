<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9631</article-id>
<article-id pub-id-type="doi">10.21105/joss.09631</article-id>
<title-group>
<article-title>PureML: a transparent NumPy-only deep learning framework
for teaching and prototyping</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-8371-7159</contrib-id>
<name>
<surname>Mishchyriak</surname>
<given-names>Yehor</given-names>
</name>
<email>ymishchyriak@wesleyan.edu</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Mathematics and Computer Science, Wesleyan
University, Middletown, CT, United States</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-12-01">
<day>1</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>117</issue>
<fpage>9631</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>numpy</kwd>
<kwd>autodiff</kwd>
<kwd>machine-learning</kwd>
<kwd>deep-learning</kwd>
<kwd>education</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>PureML is a compact deep-learning framework implemented entirely in
  NumPy. It provides a tensor type with reverse-mode automatic
  differentiation, core neural-network layers and losses, optimizers and
  learning-rate schedulers, activations, training utilities, and
  persistence for model and optimizer states. A packaged MNIST dataset
  makes it easy to benchmark or teach end-to-end
  (<xref alt="LeCun et al., 1998" rid="ref-lecun1998mnist" ref-type="bibr">LeCun
  et al., 1998</xref>).</p>
  <p>PureML is for learners and instructors who need a small, auditable
  codebase to illustrate end-to-end training; researchers prototyping or
  auditing algorithms without the overhead of multi-language stacks; and
  CPU-only or minimal-dependency environments where installing PyTorch,
  TensorFlow/Keras, or JAX is impractical.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Modern deep-learning libraries such as PyTorch, TensorFlow/Keras,
  and JAX provide rich ecosystems but are conceptually and operationally
  heavy for teaching low-level ML theory, code reading, or CPU-only
  environments
  (<xref alt="Abadi et al., 2016" rid="ref-abadi2016tensorflow" ref-type="bibr">Abadi
  et al., 2016</xref>;
  <xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>;
  <xref alt="Chollet &amp; others, 2015" rid="ref-chollet2015keras" ref-type="bibr">Chollet
  &amp; others, 2015</xref>;
  <xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>). Pedagogical materials (including standard texts
  like <italic>Deep Learning</italic>
  (<xref alt="Goodfellow et al., 2016" rid="ref-Goodfellow-et-al-2016" ref-type="bibr">Goodfellow
  et al., 2016</xref>)) often rely on pseudo-code or small snippets that
  omit practical details: broadcasting semantics, batching, parameter
  persistence, computational graph construction, vectorization, gradient
  accumulation, checkpointing, etc. As a result, learners struggle to
  bridge the gap to real systems. At the other end of the spectrum,
  educational projects like <monospace>micrograd</monospace>
  (<xref alt="Karpathy, 2020" rid="ref-karpathy2020micrograd" ref-type="bibr">Karpathy,
  2020</xref>) purposefully keep the scope tiny, and minimalist systems
  like <monospace>tinygrad</monospace>
  (<xref alt="Hotz &amp; contributors, 2024" rid="ref-tinygrad2024" ref-type="bibr">Hotz
  &amp; contributors, 2024</xref>) assume a systems background:
  <monospace>tinygrad</monospace> is engineered like a compiler to
  optimize kernels and targets performance and low-level optimization
  over didactic readability, effectively teaching how to build PyTorch.
  Projects like <monospace>numpy-ml</monospace>
  (<xref alt="Bourgin, 2019" rid="ref-bourgin2019numpyml" ref-type="bibr">Bourgin,
  2019</xref>) offer a broad catalog of algorithms implemented in NumPy
  but are not built around an autodiff engine and are not aimed at
  performance; they serve as references rather than frameworks for
  training deep networks.</p>
  <p>PureML aims to sit between these extremes: small enough to audit
  end-to-end, but feature-complete enough for nontrivial models. The
  code remains transparent while supporting batch-vectorized
  computation, dynamic computational graphs, forward pass caching,
  persistence, and related functionality. It focuses on:</p>
  <list list-type="bullet">
    <list-item>
      <p>Explicit reverse-mode autodiff with readable vector-Jacobian
      products (VJPs) for every operation, so gradient flow is
      inspectable.</p>
    </list-item>
    <list-item>
      <p>Minimal runtime dependencies (NumPy and zarr) suitable for
      laptops, classrooms, and CPU-only servers
      (<xref alt="Harris et al., 2020" rid="ref-harris2020numpy" ref-type="bibr">Harris
      et al., 2020</xref>;
      <xref alt="Miles et al., 2025" rid="ref-zarrpython2025" ref-type="bibr">Miles
      et al., 2025</xref>).</p>
    </list-item>
    <list-item>
      <p>Ready-to-run MNIST example to demonstrate end-to-end training
      without additional downloads
      (<xref alt="LeCun et al., 1998" rid="ref-lecun1998mnist" ref-type="bibr">LeCun
      et al., 1998</xref>).</p>
    </list-item>
    <list-item>
      <p>Persistence utilities that round-trip models, optimizer slots,
      and data for reproducible exercises or small experiments.</p>
    </list-item>
    <list-item>
      <p>Intentional trade-offs: no GPU bindings, a single-file autodiff
      core with explicit VJPs, and an emphasis on readability and
      auditability. Despite the small surface area, operations are
      vectorized and efficient relying on NumPy.</p>
    </list-item>
  </list>
</sec>
<sec id="design-and-implementation">
  <title>Design and implementation</title>
  <p><bold>Core autograd.</bold> The <monospace>Tensor</monospace> type
  wraps NumPy arrays, records operations dynamically, and executes
  reverse-mode autodiff with explicit VJPs. Broadcasting-aware gradient
  reduction, slice/advanced indexing support, and graph teardown
  utilities (<monospace>zero_grad_graph</monospace>,
  <monospace>detach_graph</monospace>) mirror the behaviors found in
  larger frameworks while remaining short enough to audit. Safe exports
  (<monospace>Tensor.numpy</monospace>) discourage in-place mutation of
  parameter buffers.</p>
  <p><bold>Layers and losses.</bold> The library supplies
  <monospace>Affine</monospace>, <monospace>Dropout</monospace>,
  <monospace>BatchNorm1d</monospace>, and
  <monospace>Embedding</monospace> layers. Losses include mean squared
  error, binary cross-entropy (probabilities or logits), and categorical
  cross-entropy with optional label smoothing. Stable softmax and
  log-softmax implementations avoid overflow.</p>
  <p><bold>Optimization stack.</bold> Optimizers (SGD
  (<xref alt="Robbins &amp; Monro, 1951" rid="ref-robbins1951stochastic" ref-type="bibr">Robbins
  &amp; Monro, 1951</xref>) with momentum, AdaGrad
  (<xref alt="Duchi et al., 2011" rid="ref-duchi2011adagrad" ref-type="bibr">Duchi
  et al., 2011</xref>), RMSProp
  (<xref alt="Tieleman &amp; Hinton, 2012" rid="ref-tieleman2012rmsprop" ref-type="bibr">Tieleman
  &amp; Hinton, 2012</xref>), Adam/AdamW
  (<xref alt="Kingma &amp; Ba, 2015" rid="ref-kingma2015adam" ref-type="bibr">Kingma
  &amp; Ba, 2015</xref>)) share a common interface, support coupled or
  decoupled weight decay, and persist optimizer slots via
  <monospace>save_state</monospace>/<monospace>load_state</monospace>.
  Lightweight schedulers (step, exponential, cosine annealing
  (<xref alt="Loshchilov &amp; Hutter, 2016" rid="ref-loshchilov2016sgdr" ref-type="bibr">Loshchilov
  &amp; Hutter, 2016</xref>)) operate in-place on optimizer learning
  rates.</p>
  <p><bold>Data utilities and models.</bold> A
  <monospace>Dataset</monospace> protocol,
  <monospace>TensorDataset</monospace>, and
  <monospace>DataLoader</monospace> (with slicing fast paths and
  optional shuffling) simplify input pipelines. The bundled
  <monospace>MnistDataset</monospace> streams compressed images/labels
  from a packaged zarr archive
  (<xref alt="LeCun et al., 1998" rid="ref-lecun1998mnist" ref-type="bibr">LeCun
  et al., 1998</xref>). Example models include a small fully connected
  MNIST classifier (<monospace>MNIST_BEATER</monospace>) and a classical
  k-nearest neighbors classifier.</p>
  <p><bold>Persistence.</bold> The <monospace>ArrayStorage</monospace>
  abstraction wraps zarr v3 groups with Blosc compression and can
  compress to read-only zip archives
  (<xref alt="Miles et al., 2025" rid="ref-zarrpython2025" ref-type="bibr">Miles
  et al., 2025</xref>). Model parameters, buffers, and top-level
  literals can be round-tripped to a single
  <monospace>.pureml.zip</monospace> file for reproducibility.</p>
  <p><bold>Ecosystem and dependencies.</bold> PureML requires only NumPy
  and zarr at runtime
  (<xref alt="Harris et al., 2020" rid="ref-harris2020numpy" ref-type="bibr">Harris
  et al., 2020</xref>;
  <xref alt="Miles et al., 2025" rid="ref-zarrpython2025" ref-type="bibr">Miles
  et al., 2025</xref>), targets Python 3.11+, and is distributed on PyPI
  as <monospace>ym-pure-ml</monospace>. Logging utilities configure
  rotating file/console handlers for experiments.</p>
  <p>Project structure at a glance (code modules):</p>
  <preformat>pureml/
  machinery.py       # Tensor core, autograd graph/VJPs
  layers.py          # Affine, BatchNorm1d, Dropout, Embedding
  losses.py          # CCE, BCE, MSE
  activations.py     # relu, softmax, log-softmax, etc.
  optimizers.py      # SGD, Adam/AdamW, RMSProp, AdaGrad + schedulers
  training_utils.py  # DataLoader, batching/loop helpers
  datasets/
    MNIST/dataset.py # packaged MNIST reader (zarr)
  models/
    neural_networks/mnist_beater.py
    classical/knn.py
  util.py            # ArrayStorage (zarr persistence), helpers
  base.py            # NN base class (save/load, train/eval)
  evaluation.py      # metrics (accuracy)
  general_math.py    # math helpers
  logging_util.py    # logging setup</preformat>
</sec>
<sec id="quality-control">
  <title>Quality control</title>
  <p>The GitHub repository contains a unit test suite
  (<monospace>tests/</monospace>) consisting of 106 tests that cover
  autograd correctness (elementwise ops, broadcasting, slicing, matmul,
  reshaping), activation stability, layers and buffers (including bias
  toggles and seeding), optimizer and scheduler behavior, persistence
  round-trips, data utilities, and the MNIST dataset/model flow. The
  suite runs with
  <monospace>python -m unittest discover tests</monospace>.</p>
</sec>
<sec id="example-usage">
  <title>Example usage</title>
  <code language="python">from pureml import Tensor
from pureml.activations import relu
from pureml.layers import Affine
from pureml.base import NN
from pureml.datasets import MnistDataset
from pureml.optimizers import Adam
from pureml.losses import CCE
from pureml.training_utils import DataLoader
from pureml.evaluation import accuracy
import time

class MNIST_BEATER(NN):

    def __init__(self) -&gt; None:
        self.L1 = Affine(28*28, 256)
        self.L2 = Affine(256, 10)

    def predict(self, x: Tensor) -&gt; Tensor:
        x = x.flatten(sample_ndim=2) # passing 2 because imgs in MNIST are 2D
        x = relu(self.L1(x))
        x = self.L2(x)
        if self.training:
            return x
        return x.argmax(axis=x.ndim-1) # argmax over the feature dim

with MnistDataset(&quot;train&quot;) as train, MnistDataset(&quot;test&quot;) as test:
    model = MNIST_BEATER().train()
    opt = Adam(model.parameters, lr=1e-3, weight_decay=1e-2)
    start_time = time.perf_counter()
    for _ in range(5):
        for X, Y in DataLoader(train, batch_size=128, shuffle=True):
            opt.zero_grad()
            logits = model(X)
            loss = CCE(Y, logits, from_logits=True)
            loss.backward()
            opt.step()
    end_time = time.perf_counter()
    model.eval()
    acc = accuracy(model, test, batch_size=1024)
print(&quot;Time taken: &quot;, end_time - start_time, &quot; sec.&quot;)
print(f&quot;Test accuracy: {acc * 100}&quot;)</code>
  <sec id="example-usage-in-computational-biology">
    <title>Example usage in computational biology</title>
    <p><monospace>SAWNERGY</monospace> project builds its skip-gram
    embedding pipeline for amino acid interaction networks using PureML
    (<ext-link ext-link-type="uri" xlink:href="https://github.com/Yehor-Mishchyriak/SAWNERGY/blob/main/sawnergy/embedding/SGNS_pml.py">link</ext-link>).</p>
  </sec>
</sec>
<sec id="availability">
  <title>Availability</title>
  <p>Source code is available at
  https://github.com/Yehor-Mishchyriak/PureML.
  The package is published on PyPI at
  https://pypi.org/project/ym-pure-ml/.
  Documentation is available at
  https://ymishchyriak.com/docs/PUREML-DOCS.
  The license is Apache-2.0.</p>
</sec>
<sec id="future-directions">
  <title>Future directions</title>
  <p>Planned extensions include convolutional, recurrent, and
  message-passing layers, attention mechanisms, additional activation
  and loss functions, richer evaluation metrics, and related tooling to
  support a broader range of deep-learning experiments.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>I am grateful to Professor Kelly M. Thayer (Wesleyan University)
  for guidance and constructive feedback on this project. I also
  acknowledge the authors of the <italic>Deep Learning</italic>
  textbook, which informed the design and pedagogy of PureML
  (<xref alt="Goodfellow et al., 2016" rid="ref-Goodfellow-et-al-2016" ref-type="bibr">Goodfellow
  et al., 2016</xref>). This work was supported by the National Science
  Foundation under Grant No. CHE-2320718.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-harris2020numpy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R</given-names></name>
        <name><surname>Millman</surname><given-names>K Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>Stéfan J van der</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zarrpython2025">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Miles</surname><given-names>Alistair</given-names></name>
        <name><surname>Kirkham</surname><given-names>John</given-names></name>
        <name><surname>Stansby</surname><given-names>David</given-names></name>
        <name><surname>Papadopoulos Orfanos</surname><given-names>Dimitri</given-names></name>
        <name><surname>Hamman</surname><given-names>Joe</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Zarr-developers/zarr-python: v3.1.5</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://github.com/zarr-developers/zarr-python</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.17672242</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <uri>https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-jax2018github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/jax-ml/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-Goodfellow-et-al-2016">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Courville</surname><given-names>Aaron</given-names></name>
      </person-group>
      <source>Deep learning</source>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-kingma2015adam">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P</given-names></name>
        <name><surname>Ba</surname><given-names>Jimmy</given-names></name>
      </person-group>
      <article-title>Adam: A method for stochastic optimization</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/abs/1412.6980</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-robbins1951stochastic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Robbins</surname><given-names>Herbert</given-names></name>
        <name><surname>Monro</surname><given-names>Sutton</given-names></name>
      </person-group>
      <article-title>A stochastic approximation method</article-title>
      <source>The Annals of Mathematical Statistics</source>
      <year iso-8601-date="1951">1951</year>
      <volume>22</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1214/aoms/1177729586</pub-id>
      <fpage>400</fpage>
      <lpage>407</lpage>
    </element-citation>
  </ref>
  <ref id="ref-duchi2011adagrad">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Duchi</surname><given-names>John</given-names></name>
        <name><surname>Hazan</surname><given-names>Elad</given-names></name>
        <name><surname>Singer</surname><given-names>Yoram</given-names></name>
      </person-group>
      <article-title>Adaptive subgradient methods for online learning and stochastic optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <uri>http://jmlr.org/papers/v12/duchi11a.html</uri>
      <fpage>2121</fpage>
      <lpage>2159</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tieleman2012rmsprop">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tieleman</surname><given-names>Tijmen</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
      </person-group>
      <article-title>Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude</article-title>
      <publisher-name>Coursera: Neural Networks for Machine Learning</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <uri>http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-loshchilov2016sgdr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Loshchilov</surname><given-names>Ilya</given-names></name>
        <name><surname>Hutter</surname><given-names>Frank</given-names></name>
      </person-group>
      <article-title>SGDR: Stochastic gradient descent with warm restarts</article-title>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/abs/1608.03983</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1608.03983</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lecun1998mnist">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Bottou</surname><given-names>Leon</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Haffner</surname><given-names>Patrick</given-names></name>
      </person-group>
      <article-title>Gradient-based learning applied to document recognition</article-title>
      <source>Proceedings of the IEEE</source>
      <year iso-8601-date="1998">1998</year>
      <volume>86</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
      <fpage>2278</fpage>
      <lpage>2324</lpage>
    </element-citation>
  </ref>
  <ref id="ref-karpathy2020micrograd">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Karpathy</surname><given-names>Andrej</given-names></name>
      </person-group>
      <article-title>Micrograd</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/karpathy/micrograd</uri>
    </element-citation>
  </ref>
  <ref id="ref-bourgin2019numpyml">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bourgin</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Numpy-ml</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/ddbourgin/numpy-ml</uri>
    </element-citation>
  </ref>
  <ref id="ref-abadi2016tensorflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>TensorFlow: A system for large-scale machine learning</article-title>
      <source>OSDI</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1605.08695</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chollet2015keras">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Keras</article-title>
      <publisher-name>https://keras.io</publisher-name>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-tinygrad2024">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Hotz</surname><given-names>George</given-names></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>Tinygrad</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/tinygrad/tinygrad</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
