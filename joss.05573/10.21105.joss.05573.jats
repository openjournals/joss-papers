<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5573</article-id>
<article-id pub-id-type="doi">10.21105/joss.05573</article-id>
<title-group>
<article-title>brains-py, A framework to support research on
energy-efficient unconventional hardware for machine
learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5957-7945</contrib-id>
<name>
<surname>Alegre-Ibarra</surname>
<given-names>Unai</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Euler</surname>
<given-names>Hans-Christian Ruiz</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>A.Mollah</surname>
<given-names>Humaid</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Petrov</surname>
<given-names>Bozhidar P.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Sastry</surname>
<given-names>Srikumar S.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Boon</surname>
<given-names>Marcus N.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de Jong</surname>
<given-names>Michel P.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zolfagharinejad</surname>
<given-names>Mohamadreza</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Uitzetter</surname>
<given-names>Florentina M. J.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>van de Ven</surname>
<given-names>Bram</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>de Almeida</surname>
<given-names>António J. Sousa</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kinge</surname>
<given-names>Sachin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>van der Wiel</surname>
<given-names>Wilfred G.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>MESA+ Institute for Nanotechnology &amp; BRAINS Center for
Brain-Inspired Nano Systems, University of Twente,
Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Advanced Tech., Materials Engineering Div., Toyota Motor
Europe, Belgium</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-12-04">
<day>4</day>
<month>12</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>90</issue>
<fpage>5573</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Dopant network processing units (DNPUs)</kwd>
<kwd>Material Learning</kwd>
<kwd>Machine Learning Hardware design</kwd>
<kwd>Efficient Computing</kwd>
<kwd>Materials Science</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Projections about the limitations of digital computers for deep
  learning models are leading to a shift towards domain-specific
  hardware, where novel analogue components are sought after, due to
  their potential advantages in power consumption. This paper introduces
  brains-py, a generic framework to facilitate research on different
  sorts of disordered nano-material networks for natural and
  energy-efficient analogue computing. Mainly, it has been applied to
  the concept of dopant network processing units (DNPUs), a novel and
  promising CMOS-compatible nano-scale tunable system based on doped
  silicon with potentially very low-power consumption at the inference
  stage. The framework focuses on two material-learning-based
  approaches, for training DNPUs to compute supervised learning tasks:
  evolution-in-matter and surrogate models. While evolution-in-matter
  focuses on providing a quick exploration of newly manufactured single
  DNPUs, the surrogate model approach is used for the design and
  simulation of the interconnection between multiple DNPUs, enabling the
  exploration of their scalability. All simulation results can be
  seamlessly validated on hardware, saving time and costs associated
  with their reproduction. The framework is generic and can be reused
  for research on various materials with different design aspects,
  providing support for the most common tasks required for doing
  experiments with these novel materials.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The breakthroughs of deep learning come along with high energy
  costs, related to the high throughput data-movement requirements for
  computing them. The increasing computational demands of these models,
  along with the projected traditional hardware limitations, are
  shifting the paradigm towards innovative hardware solutions, where
  analogue components for application-specific integrated circuits are
  keenly
  sought (<xref alt="Kaspar et al., 2021" rid="ref-kaspar2021rise" ref-type="bibr">Kaspar
  et al., 2021</xref>). Dopant network processing units (DNPUs) are a
  novel concept consisting of a lightly doped semiconductor, edged with
  several electrodes, where hopping in dopant networks is the dominant
  charge transport mechanism Chen et al.
  (<xref alt="2021" rid="ref-chen20211" ref-type="bibr">2021</xref>)
  (See Figure 1). The output current of DNPUs is a non-linear function
  of the input voltages, which can be tuned for representing different
  complex functions. The process of finding adequate control voltages
  for a particular task is called training. Once the right voltage
  values are found, the same device can represent different complex
  functions on demand. The main advantages of these CMOS-compatible
  devices are the very low currents, their multi-scale tunability on a
  high dimensional parameter space, and their potential for extensive
  parallelism. Devices based on this concept enable the creation of
  potentially very low energy-consuming hardware for computing deep
  neural network (DNN) models, where each DNPU has a projected energy
  consumption of over 100 Tera-operations per second per
  Watt (<xref alt="Chen et al., 2020" rid="ref-chen2020classification" ref-type="bibr">Chen
  et al., 2020</xref>). This concept is still in its infancy, and it can
  be developed in diverse ways. From various types of materials (host
  and dopants) to distinct dopant concentrations, different device
  dimensions or the number of electrodes, as well as different
  combinations of data-input, control and readout electrodes. Also,
  there are different ways in which DNPU-based circuits could be scaled
  up, and having to create hardware circuits from scratch is an arduous
  and time-consuming process. For this reason, this paper introduces
  brains-py, a generic framework to facilitate research on different
  sorts of disordered nano-material networks for natural and
  energy-efficient analogue computing. To the extent of the knowledge of
  the authors, there is no other similar works on the area.</p>
</sec>
<sec id="framework-description">
  <title>Framework description</title>
  <p>The framework is composed of two main packages, brains-py (core)
  and brainspy-smg (surrogate model generator). The former package
  contains the whole structure for processors, managing the drivers for
  National Instruments devices that are connected to the DNPUs, and a
  set of utils functions that can be used for managing the signals,
  custom loss/fitness functions, linear transformations and i/o file
  management. The latter package contains the libraries required to
  prepare DNPU devices for data gathering (multiple and single IV
  curves), to gather information from multi-terminal devices in an
  efficient way
  (<xref alt="Ruiz Euler et al., 2020" rid="ref-ruiz2020deep" ref-type="bibr">Ruiz
  Euler et al., 2020</xref>), as well as training support for surrogate
  models, and consistency checks.</p>
</sec>
<sec id="finding-functionality-on-a-single-dnpu-circuit-design">
  <title>Finding functionality on a single DNPU circuit design</title>
  <p>There are two main flavours in which single DNPUs can be trained:
  <italic>Evolution in Matter</italic> or <italic>Surrogate model based
  gradient descent</italic>. The <italic>evolution in matter</italic>
  approach performs a stochastic search for suitable control voltages
  for a single DNPU unit, measured in hardware, for a given supervised
  task (<xref alt="Sivanandam &amp; Deepa, 2008" rid="ref-sivanandam2008genetic" ref-type="bibr">Sivanandam
  &amp; Deepa, 2008</xref>). It provides a quicker exploration (directly
  on hardware) of the usefulness of newly manufactured single DNPUs.
  Typically, common classification benchmarks are used, such as solving
  non-linear boolean
  gates (<xref alt="Chen et al., 2020" rid="ref-chen2020classification" ref-type="bibr">Chen
  et al., 2020</xref>) or measuring the Vapnik-Chervonenkis
  dimension (<xref alt="Ruiz-Euler et al., 2021" rid="ref-ruiz2021dopant" ref-type="bibr">Ruiz-Euler
  et al., 2021</xref>). On the other hand, the <italic>surrogate
  model</italic>
  approach (<xref alt="Ruiz Euler et al., 2020" rid="ref-ruiz2020deep" ref-type="bibr">Ruiz
  Euler et al., 2020</xref>) is better suited for studying the scaling
  of DNPU hardware. The process is as follows:</p>
  <list list-type="order">
    <list-item>
      <p>Generate a surrogate model: For this, the multi-dimensional
      input-output data of the device is densely sampled. The input
      consists of sinusoidal or triangular modulation functions, chosen
      in such a way that the ratios of all frequency combinations are
      irrational, guaranteeing a densely covered input-output space
      without repetitions. A DNN is trained for representing the
      function of this input-output data.</p>
    </list-item>
    <list-item>
      <p>Train for DNPU functionality: The weights of the trained DNN
      are frozen, and the control voltages are declared as learnable
      parameters of the surrogate model of the DNPU. The training for
      DNPU functionality is supported by the brains-py framework but is
      also possible to use customised user-created gradient
      descent (<xref alt="Dogo et al., 2018" rid="ref-dogo2018comparative" ref-type="bibr">Dogo
      et al., 2018</xref>) algorithms using PyTorch.</p>
    </list-item>
    <list-item>
      <p>Validate on hardware: Once satisfactory control voltages for a
      task are found, brains-py supports seamlessly validating them on
      hardware, without having to modify the code of the model, by
      simply changing the model to hardware-evaluation mode.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Scanning electron microscope image and its schematic
    cross section.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/dnpu.png" />
  </fig>
  <fig>
    <caption><p>Simplified class diagram for processors package in
    brains-py library.</p></caption>
    <graphic mimetype="image" mime-subtype="jpeg" xlink:href="media/figures/processor.jpg" />
  </fig>
</sec>
<sec id="finding-functionality-on-multi-dnpu-circuit-design">
  <title>Finding functionality on multi-DNPU circuit design</title>
  <p>One of the main aims of the framework is to explore different ways
  in which DNPU-based circuits can be scaled up. Developers can create
  experiments using several surrogate models in a custom PyTorch module,
  in a very similar way to how they would do it to create a custom
  neural network with PyTorch (It allows to create your own module that
  is a <italic>torch.nn.Module</italic> class child or
  <italic>lightning.LightningModule</italic> child from the Pytorch
  Lightning library). In this way, they can explore different ways of
  interconnecting DNPUs, and analyse their performance. Any experiment
  based on surrogate model simulations can then be validated on hardware
  with minor changes required on the code. In this way, early
  proof-of-concept hardware can be prototyped fast, avoiding having to
  develop again the experiments for hardware, which would otherwise be
  cumbersome to reproduce. Common programming tasks such as loading
  datasets, creating training loops, optimisers and loss functions
  required to implement the supervised learning task can be programmed
  in a very similar way to how it would be done with any regular
  gradient descent implementation in
  PyTorch (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>) and/or PyTorch
  Lightning (<xref alt="Falcon &amp; The Pytorch Lightning team, 2019" rid="ref-lightning" ref-type="bibr">Falcon
  &amp; The Pytorch Lightning team, 2019</xref>). For providing
  validation on hardware with small code modifications, the brains-py
  library leverages the concept of ‘<italic>Processor</italic>’, which
  allows changing the internal behaviour of the class to measure on
  simulations or hardware measurements, while maintaining an equivalent
  behaviour for the public methods. Internally, the
  <italic>Processor</italic> class also deals with the differences
  between input/output signals that are inherent to measuring in
  hardware or simulations (<italic>e.g.</italic>, noise or length of
  signal). This greatly facilitates the reuse of the same code for
  simulations and hardware measurements. Without this feature, the
  research on these materials becomes difficult and time-consuming to
  reproduce in hardware. The library also provides programmers with
  additional modules, which are associated to a
  <italic>Processor</italic> by an aggregation relationship, that can
  already replicate the scaling of DNPUs in ways that are known to work
  well (see Figure 2):</p>
  <list list-type="bullet">
    <list-item>
      <p><italic>DNPU</italic>: It is a very similar class to that of a
      <italic>Processor</italic>, but it contains the control voltages,
      declared as learnable parameters. Therefore, it only has the same
      input dimensions as the number of available data input electrodes.
      It is also a child of <italic>torch.nn.Module</italic>, and it
      allows for representing a layer of DNPUs in a time-multiplexing
      fashion (with the same Processor instance). It also enables
      applying linear transformations to the inputs before passing them
      to the processor.</p>
    </list-item>
    <list-item>
      <p><italic>DNPUBatchNorm</italic>: It is a child of the
      <italic>DNPU</italic> class, and it also facilitates the
      incorporation of a batch normalisation layer after the output,
      which has been shown to produce better
      results (<xref alt="Ruiz-Euler et al., 2021" rid="ref-ruiz2021dopant" ref-type="bibr">Ruiz-Euler
      et al., 2021</xref>). It also enables logging outputs before and
      after normalisation.</p>
    </list-item>
    <list-item>
      <p><italic>DNPUConv2d</italic>: It is a child of the
      <italic>DNPU</italic> class, and it enables the processing of the
      information in the same way as a convolution layer would do, for
      different kernel dimensions. In each case, a number of devices in
      parallel (time-multiplexed) will represent a kernel (e.g., for a
      3x3 convolution, a layer of three eight-electrode devices can be
      used, where each device has 3 data-input electrodes and a single
      output). This layer can be used to reproduce computation with
      DNPUs using convolutional neural network (CNN) layers, and
      replicate existing deep-learning models.</p>
    </list-item>
  </list>
</sec>
<sec id="conclusions-and-future-research-lines">
  <title>Conclusions and future research lines</title>
  <p>We introduce a framework for facilitating the characterisation of
  different materials that can be used for energy-efficient computations
  in the context of machine learning hardware development research. It
  supports developers during typical tasks required for the mentioned
  purpose, including preliminary direct measurements of devices, the
  gathering of data and training of surrogate models, and the
  possibility to seamlessly validate simulations of surrogate models in
  hardware. In this way, researchers can save a significant amount of
  energy and resources when exploring the abilities of different DNPU
  materials and designs for creating energy-efficient hardware. The
  libraries have been designed with reusability in mind.</p>
</sec>
<sec id="projects">
  <title>Projects</title>
  <p>The tool is primarily used in the Center for Brain-Inspired
  NanoSystems, which includes the MESA+ Institute for Nanotechnology,
  the Digital Society Institute and the Faculty of Behavioural,
  Management and Social sciences. It has been used for several
  projects:</p>
  <list list-type="order">
    <list-item>
      <p>HYBRAIN – Hybrid electronic-photonic architectures for
      brain-inspired computing</p>
    </list-item>
  </list>
  <p>        Project funded by the European Union’s Horizon Europe
  research and innovation         programme under Grant Agreement No
  101046878</p>
  <p>        <ext-link ext-link-type="uri" xlink:href="https://hybrain.eu/">https://hybrain.eu/</ext-link></p>
  <list list-type="order">
    <list-item>
      <label>2.</label>
      <p>Collaborative Research Centre on Intelligent Matter (CRC
      1459)</p>
    </list-item>
  </list>
  <p>        Project funded by the Deutsche Forschungsgemeinschaft (DFG,
  German Research         Foundation) through project 433682494 -
  <ext-link ext-link-type="uri" xlink:href="https://www.uni-muenster.de/SFB1459/">SFB
  1459 - Intelligent Matter - University of Münster</ext-link></p>
  <list list-type="order">
    <list-item>
      <label>3.</label>
      <p>Evolutionary Computing using Nanomaterial Networks</p>
    </list-item>
  </list>
  <p>        Project funded by the Dutch Research Council (NWO)
  Natuurkunde Projectruimte         Grant No. 680-91-114</p>
  <list list-type="order">
    <list-item>
      <label>4.</label>
      <p>Nanomaterial Networks for Artificial Intelligence in the
      Automotive Industry: NANO(AI)2</p>
    </list-item>
  </list>
  <p>        Project funded by the Dutch Research Council (NWO)
  Natuurkunde Projectruimte         Grant No. 16237 DFG, German Research
  Foundation) through project 433682494 –         SFB 1459</p>
  <p>Within the scope of the above projects, several PhDs and Master
  Students have developed and are developing code based on this
  library.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This project has received financial support from the University of
  Twente, the Dutch Research Council ( HTSM grant no. 16237 and
  Natuurkunde Projectruimte grant no. 680-91-114 ) as well as from
  Toyota Motor Europe N.V. We acknowledge financial support from the
  HYBRAIN project funded by the European Union’s Horizon Europe research
  and innovation programme under Grant Agreement No 101046878. This work
  was further funded by the Deutsche Forschungsgemeinschaft (DFG, German
  Research Foundation) through project 433682494 – SFB 1459.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-kaspar2021rise">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kaspar</surname><given-names>C</given-names></name>
        <name><surname>Ravoo</surname><given-names>BJ</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
        <name><surname>Wegner</surname><given-names>SV</given-names></name>
        <name><surname>Pernice</surname><given-names>WHP</given-names></name>
      </person-group>
      <article-title>The rise of intelligent matter</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>594</volume>
      <issue>7863</issue>
      <fpage>345</fpage>
      <lpage>355</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chen2020classification">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Tao</given-names></name>
        <name><surname>Gelder</surname><given-names>Jeroen van</given-names></name>
        <name><surname>Ven</surname><given-names>Bram van de</given-names></name>
        <name><surname>Amitonov</surname><given-names>Sergey V</given-names></name>
        <name><surname>Wilde</surname><given-names>Bram de</given-names></name>
        <name><surname>Euler</surname><given-names>Hans-Christian Ruiz</given-names></name>
        <name><surname>Broersma</surname><given-names>Hajo</given-names></name>
        <name><surname>Bobbert</surname><given-names>Peter A</given-names></name>
        <name><surname>Zwanenburg</surname><given-names>Floris A</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
      </person-group>
      <article-title>Classification with a disordered dopant-atom network in silicon</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>577</volume>
      <issue>7790</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-019-1901-0</pub-id>
      <fpage>341</fpage>
      <lpage>345</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ruiz2021dopant">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ruiz-Euler</surname><given-names>Hans-Christian</given-names></name>
        <name><surname>Alegre-Ibarra</surname><given-names>Unai</given-names></name>
        <name><surname>Ven</surname><given-names>Bram van de</given-names></name>
        <name><surname>Broersma</surname><given-names>Hajo</given-names></name>
        <name><surname>Bobbert</surname><given-names>Peter A</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
      </person-group>
      <article-title>Dopant network processing units: Towards efficient neural network emulators with high-capacity nanoelectronic nodes</article-title>
      <source>Neuromorphic Computing and Engineering</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2021-09">2021</year><month>09</month>
      <volume>1</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1088/2634-4386/ac1a7f</pub-id>
      <fpage>024002</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-sivanandam2008genetic">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Sivanandam</surname><given-names>SN</given-names></name>
        <name><surname>Deepa</surname><given-names>SN</given-names></name>
      </person-group>
      <article-title>Genetic algorithms</article-title>
      <source>Introduction to genetic algorithms</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <fpage>15</fpage>
      <lpage>37</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ruiz2020deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ruiz Euler</surname><given-names>Hans-Christian</given-names></name>
        <name><surname>Boon</surname><given-names>Marcus N</given-names></name>
        <name><surname>Wildeboer</surname><given-names>Jochem T</given-names></name>
        <name><surname>Ven</surname><given-names>Bram van de</given-names></name>
        <name><surname>Chen</surname><given-names>Tao</given-names></name>
        <name><surname>Broersma</surname><given-names>Hajo</given-names></name>
        <name><surname>Bobbert</surname><given-names>Peter A</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
      </person-group>
      <article-title>A deep-learning approach to realizing functionality in nanoelectronic devices</article-title>
      <source>Nature nanotechnology</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>15</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1038/s41565-020-00779-y</pub-id>
      <fpage>992</fpage>
      <lpage>998</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dogo2018comparative">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dogo</surname><given-names>EM</given-names></name>
        <name><surname>Afolabi</surname><given-names>OJ</given-names></name>
        <name><surname>Nwulu</surname><given-names>NI</given-names></name>
        <name><surname>Twala</surname><given-names>Bhekisipho</given-names></name>
        <name><surname>Aigbavboa</surname><given-names>CO</given-names></name>
      </person-group>
      <article-title>A comparative analysis of gradient descent-based optimization algorithms on convolutional neural networks</article-title>
      <source>2018 international conference on computational techniques, electronics and mechanical systems (CTEMS)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1109/CTEMS.2018.8769211</pub-id>
      <fpage>92</fpage>
      <lpage>99</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lightning">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Falcon</surname><given-names>William</given-names></name>
        <name><surname>The Pytorch Lightning team</surname><given-names>the</given-names></name>
      </person-group>
      <article-title>PyTorch lightning: The lightweight PyTorch wrapper for high-performance AI research.</article-title>
      <publisher-name>https://github.com/Lightning-AI/lightning</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.3828935</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-tertilt2022hopping">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tertilt</surname><given-names>Henri</given-names></name>
        <name><surname>Bakker</surname><given-names>Jesse</given-names></name>
        <name><surname>Becker</surname><given-names>Marlon</given-names></name>
        <name><surname>Wilde</surname><given-names>Bram de</given-names></name>
        <name><surname>Klanberg</surname><given-names>Indrek</given-names></name>
        <name><surname>Geurts</surname><given-names>Bernard J</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
        <name><surname>Heuer</surname><given-names>Andreas</given-names></name>
        <name><surname>Bobbert</surname><given-names>Peter A</given-names></name>
      </person-group>
      <article-title>Hopping-transport mechanism for reconfigurable logic in disordered dopant networks</article-title>
      <source>Physical Review Applied</source>
      <publisher-name>APS</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>17</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevApplied.17.064025</pub-id>
      <fpage>064025</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-chen20211">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Tao</given-names></name>
        <name><surname>Bobbert</surname><given-names>Peter A</given-names></name>
        <name><surname>Wiel</surname><given-names>Wilfred G van der</given-names></name>
      </person-group>
      <article-title>1/f noise and machine intelligence in a nonlinear dopant atom network</article-title>
      <source>Small Science</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>1</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1002/smsc.202000014</pub-id>
      <fpage>2000014</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
