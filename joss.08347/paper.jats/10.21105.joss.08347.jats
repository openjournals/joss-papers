<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8347</article-id>
<article-id pub-id-type="doi">10.21105/joss.08347</article-id>
<title-group>
<article-title>SBGM: Score-Based Generative Models in
JAX.</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-0985-1437</contrib-id>
<name>
<surname>Homer</surname>
<given-names>Jed</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University Observatory, Faculty for Physics,
Ludwig-Maximilians-UniversitÃ¤t MÃ¼nchen, Scheinerstrasse 1, MÃ¼nchen,
Deutschland.</institution>
<institution-id institution-id-type="ROR">00hx57361</institution-id>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Munich Center for Machine Learning.</institution>
<institution-id institution-id-type="ROR">00hx57361</institution-id>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-01-25">
<day>25</day>
<month>1</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>116</issue>
<fpage>8347</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Machine learning</kwd>
<kwd>Generative models</kwd>
<kwd>Diffusion models</kwd>
<kwd>Simulation-based inference</kwd>
<kwd>Emulators</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Diffusion models
  (<xref alt="Ho et al., 2020" rid="ref-ddpm" ref-type="bibr">Ho et al.,
  2020</xref>;
  <xref alt="Sohl-Dickstein et al., 2015" rid="ref-diffusion" ref-type="bibr">Sohl-Dickstein
  et al., 2015</xref>;
  <xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>) have emerged as the dominant
  paradigm for generative modeling based on performance in a variety of
  tasks
  (<xref alt="Peebles &amp; Xie, 2023" rid="ref-dit" ref-type="bibr">Peebles
  &amp; Xie, 2023</xref>;
  <xref alt="Rombach et al., 2022" rid="ref-ldms" ref-type="bibr">Rombach
  et al., 2022</xref>). The advantages of accurate density estimation
  and high-quality samples of normalising flows
  (<xref alt="Grathwohl et al., 2018" rid="ref-ffjord" ref-type="bibr">Grathwohl
  et al., 2018</xref>;
  <xref alt="Papamakarios et al., 2021" rid="ref-flows" ref-type="bibr">Papamakarios
  et al., 2021</xref>), VAEs
  (<xref alt="Diederik P. Kingma &amp; Welling, 2022" rid="ref-vaes" ref-type="bibr">Diederik
  P. Kingma &amp; Welling, 2022</xref>) and GANs
  (<xref alt="Goodfellow et al., 2014" rid="ref-gans" ref-type="bibr">Goodfellow
  et al., 2014</xref>) are subsumed into this method. Significant
  limitations exist on implicit and neural network based likelihood
  models with respect to modeling normalised probability distributions
  and sampling speed. Score-matching diffusion models are more efficient
  than previous generative model algorithms for these tasks. The
  diffusion process is agnostic to the data representation meaning
  different types of data such as audio, point-clouds, videos and images
  can be modelled.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Diffusion-based generative models
  (<xref alt="Ho et al., 2020" rid="ref-ddpm" ref-type="bibr">Ho et al.,
  2020</xref>;
  <xref alt="Sohl-Dickstein et al., 2015" rid="ref-diffusion" ref-type="bibr">Sohl-Dickstein
  et al., 2015</xref>) are a method for sampling from high-dimensional
  distributions. A sub-class of these models, score-based diffusion
  generative models (SBGMs,
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>)), permit exact-likelihood
  estimation via a change-of-variables associated with the forward
  diffusion process
  (<xref alt="Song, Durkan, et al., 2021" rid="ref-sde_ml" ref-type="bibr">Song,
  Durkan, et al., 2021</xref>). Diffusion models allow fitting
  generative models to high-dimensional data in a more efficient way
  than normalising flows, since only one neural network model
  parameterises the diffusion process, as opposed to a sequence of
  neural networks in typical normalising flow architectures. Whilst
  existing diffusion models
  (<xref alt="Ho et al., 2020" rid="ref-ddpm" ref-type="bibr">Ho et al.,
  2020</xref>;
  <xref alt="Diederik P. Kingma et al., 2023" rid="ref-vdms" ref-type="bibr">Diederik
  P. Kingma et al., 2023</xref>) allow for sampling, they are limited to
  inaccurate variational inference approaches for density estimation
  which limits their use for Bayesian inference. This code provides
  density estimation with diffusion models using GPU enabled ODE solvers
  in <monospace>jax</monospace>
  (<xref alt="Bradbury et al., 2018" rid="ref-jax" ref-type="bibr">Bradbury
  et al., 2018</xref>) and <monospace>diffrax</monospace>
  (<xref alt="Kidger, 2022" rid="ref-kidger" ref-type="bibr">Kidger,
  2022</xref>). Similar codes (e.g.
  (<xref alt="Rozet, 2024" rid="ref-azula" ref-type="bibr">Rozet,
  2024</xref>)) exist for diffusion models but they do not implement
  log-likelihood calculations, various network architectures, and
  parallelised computations for optimisation and SDE/ODE-sampling.</p>
  <p>The software we present, <monospace>sbgm</monospace>, is designed
  to be used by researchers in machine learning and the natural sciences
  for fitting diffusion models with custom architectures for their
  research. These models can be fit easily with multi-accelerator
  training and inference routines within the code (with demonstration
  examples provided). Typical use cases for these kinds of generative
  models are emulator approaches
  (<xref alt="Spurio Mancini et al., 2022" rid="ref-emulating" ref-type="bibr">Spurio
  Mancini et al., 2022</xref>), simulation-based inference
  (<xref alt="Cranmer et al., 2020" rid="ref-sbi" ref-type="bibr">Cranmer
  et al., 2020</xref>), field-level inference
  (<xref alt="Andrews et al., 2023" rid="ref-field_level_inference" ref-type="bibr">Andrews
  et al., 2023</xref>) and general inverse problems
  (<xref alt="Feng et al., 2023" rid="ref-Feng2023" ref-type="bibr">Feng
  et al., 2023</xref>;
  <xref alt="Feng &amp; Bouman, 2024" rid="ref-Feng2024" ref-type="bibr">Feng
  &amp; Bouman, 2024</xref>;
  <xref alt="Remy et al., 2023" rid="ref-Remy" ref-type="bibr">Remy et
  al., 2023</xref>;
  <xref alt="Song et al., 2022" rid="ref-inverse_problem_medical" ref-type="bibr">Song
  et al., 2022</xref>) (e.g.Â image inpainting
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>) and denoising
  (<xref alt="Chung et al., 2022" rid="ref-blinddiffusion" ref-type="bibr">Chung
  et al., 2022</xref>;
  <xref alt="Daras et al., 2024" rid="ref-ambientdiffusion" ref-type="bibr">Daras
  et al., 2024</xref>)). This code allows for seamless integration of
  diffusion models to these applications by providing data-generating
  models with easy conditioning of the data on any modality
  (e.g.Â images, audio or model parameters). Furthermore, the
  implementation in <monospace>equinox</monospace>
  (<xref alt="Kidger &amp; Garcia, 2021" rid="ref-equinox" ref-type="bibr">Kidger
  &amp; Garcia, 2021</xref>) guarantees safe integration of
  <monospace>sbgm</monospace> with any other sampling libraries
  (e.g.Â BlackJAX
  (<xref alt="Cabezas et al., 2024" rid="ref-blackjax" ref-type="bibr">Cabezas
  et al., 2024</xref>)) or <monospace>jax</monospace>
  (<xref alt="Bradbury et al., 2018" rid="ref-jax" ref-type="bibr">Bradbury
  et al., 2018</xref>) based codes.</p>
  <fig>
    <caption><p>A diagram showing how to map data to a noise
    distribution (the prior) with an SDE, and reverse this SDE for
    generative modeling. One can also reverse the associated probability
    flow ODE, which yields a deterministic reverse process. Both the
    reverse-time SDE and probability flow ODE can be obtained by
    estimating the
    score.<styled-content id="figU003Asde_ode"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="sde_ode.png" />
  </fig>
</sec>
<sec id="diffusion">
  <title>Diffusion</title>
  <p>Diffusion in the context of generative modeling describes the
  process of adding small amounts of noise sequentially to samples of
  data <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ğ±</mml:mi></mml:math></alternatives></inline-formula>
  (<xref alt="Sohl-Dickstein et al., 2015" rid="ref-diffusion" ref-type="bibr">Sohl-Dickstein
  et al., 2015</xref>). A generative model for the data arises from
  training a neural network to reverse this process by subtracting the
  noise added to the data.</p>
  <p>Score-based diffusion models
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>) model a forward diffusion process
  with Stochastic Differential Equations (SDEs) of the form</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \text{d}\boldsymbol{x}_t = f(\boldsymbol{x}_t, t)\text{d}t + g(t)\text{d}\boldsymbol{w}_t,
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ°</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[f(\boldsymbol{x}_t, t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is a vector-valued function called the drift coefficient,
  <inline-formula><alternatives>
  <tex-math><![CDATA[g(t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the diffusion coefficient and <inline-formula><alternatives>
  <tex-math><![CDATA[\text{d}\boldsymbol{w}_t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ°</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
  is a sample of noise <inline-formula><alternatives>
  <tex-math><![CDATA[\text{d}\boldsymbol{w}_t\sim \mathcal{G}[\text{d}\boldsymbol{w}_t|\mathbf{0}, \mathbf{I}]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ°</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>âˆ¼</mml:mo><mml:mi>ğ’¢</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ°</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mn>ğŸ</mml:mn><mml:mo>,</mml:mo><mml:mi>ğˆ</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
  This equation describes the infinitely many samples of noise along the
  diffusion time <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
  that perturb the data. The diffusion path, defined by the SDE, begins
  at <inline-formula><alternatives>
  <tex-math><![CDATA[t=0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  and ends at <inline-formula><alternatives>
  <tex-math><![CDATA[t=T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  where the resulting distribution is then a multivariate Gaussian with
  mean zero and covariance <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{I}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ğˆ</mml:mi></mml:math></alternatives></inline-formula>.
  The code implements various SDEs known in the diffusion model
  literature.</p>
  <p>The reverse of the SDE, mapping from multivariate Gaussian samples
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}_T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  to samples of data <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>,
  is of the form</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \text{d}\boldsymbol{x}_t = [f(\boldsymbol{x}_t, t) - g^2(t)\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)]\text{d}t + g(t)\text{d}\boldsymbol{w}_t,
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ°</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where the score function <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is substituted with a neural network <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{s}_{\theta}(\boldsymbol{x}(t), t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>ğ¬</mml:mi><mml:mi>Î¸</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ğ±</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  for the sampling process. The network is fit by score-matching
  (<xref alt="HyvÃ¤rinen, 2005" rid="ref-score_matching" ref-type="bibr">HyvÃ¤rinen,
  2005</xref>;
  <xref alt="Vincent, 2011" rid="ref-score_matching2" ref-type="bibr">Vincent,
  2011</xref>) across the time span <inline-formula><alternatives>
  <tex-math><![CDATA[[0, T]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.
  This network predicts the noise added to the image at time
  <inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
  with the forward diffusion process, in accordance with the SDE, and
  removes it. With a data-dimensional sample of Gaussian noise from the
  prior <inline-formula><alternatives>
  <tex-math><![CDATA[p_T(\boldsymbol{x}_T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  (see Figure
  <xref alt="[fig:sde_ode]" rid="figU003Asde_ode">[fig:sde_ode]</xref>)
  one can reverse the diffusion process to generate data.</p>
  <p>The reverse SDE may be solved with Euler-Maruyama sampling
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>) (or other annealed Langevin
  sampling methods) which is featured in the code.</p>
</sec>
<sec id="likelihood-calculations-with-diffusion-models">
  <title>Likelihood calculations with diffusion models</title>
  <p>Many of the applications of generative models depend on being able
  to calculate the likelihood of data. Song, Sohl-Dickstein, et al.
  (<xref alt="2021" rid="ref-sde" ref-type="bibr">2021</xref>) show that
  any SDE may be converted into an ordinary differential equation (ODE)
  without changing the distributions <inline-formula><alternatives>
  <tex-math><![CDATA[p_t(\boldsymbol{x}_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  defined by the SDE, from which the noise is sampled from in the
  diffusion process (denoted <inline-formula><alternatives>
  <tex-math><![CDATA[p_t(\boldsymbol{x}_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  and shown in grey in Figure
  <xref alt="[fig:sde_ode]" rid="figU003Asde_ode">[fig:sde_ode]</xref>).
  This ODE is known as the probability flow ODE
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>;
  <xref alt="Song, Durkan, et al., 2021" rid="ref-sde_ml" ref-type="bibr">Song,
  Durkan, et al., 2021</xref>) and is written</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
      \text{d}\boldsymbol{x}_t = [f(\boldsymbol{x}_t, t) - g^2(t)\nabla_{\boldsymbol{x}_t}\log p_t(\boldsymbol{x}_t)]\text{d}t = f'(\boldsymbol{x}_t, t)\text{d}t.
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>â€²</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mtext mathvariant="normal">d</mml:mtext><mml:mi>t</mml:mi><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>This ODE can be solved with an initial-value problem to sample new
  data or estimate its density. Starting with a data point
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}_0 \sim p(\boldsymbol{x})=p_0(\boldsymbol{x}_0)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>âˆ¼</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ğ±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  this point is mapped along the probability flow ODE path (see the
  right-hand side of Figure
  <xref alt="[fig:sde_ode]" rid="figU003Asde_ode">[fig:sde_ode]</xref>)
  shows a mapping to a sample from the multivariate Gaussian prior
  <inline-formula><alternatives>
  <tex-math><![CDATA[x_T \sim p_T(\boldsymbol{x}_T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>âˆ¼</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
  This inherits the formalism of continuous normalising flows
  (<xref alt="Chen et al., 2019" rid="ref-neuralodes" ref-type="bibr">Chen
  et al., 2019</xref>;
  <xref alt="Grathwohl et al., 2018" rid="ref-ffjord" ref-type="bibr">Grathwohl
  et al., 2018</xref>) without the expensive ODE simulations used to
  train these models - allowing for a likelihood estimate based on
  diffusion models
  (<xref alt="Song, Durkan, et al., 2021" rid="ref-sde_ml" ref-type="bibr">Song,
  Durkan, et al., 2021</xref>). The initial value problem provides a
  solution <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}_T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  and the change in probability along the path
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Delta=\log p_0(\boldsymbol{x}_0) - \log p_T(\boldsymbol{x}_T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Î”</mml:mi><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[p_T(\boldsymbol{x}_T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is a simple multivariate Gaussian distribution. Various ODE solvers of
  different orders are available (for a user to balance speed and
  accuracy of sampling) which are provided by
  <monospace>diffrax</monospace>
  (<xref alt="Kidger, 2022" rid="ref-kidger" ref-type="bibr">Kidger,
  2022</xref>).</p>
  <fig>
    <caption><p>A diagram showing a log-likelihood calculation over the
    support of a Gaussian mixture model with eight components. Data is
    drawn (shown in red) from this mixture to train the diffusion model
    that gives the likelihood (defined by the diffusion model) in gray.
    The log-likelihood is calculated using the ODE and a trained
    diffusion model.
    <styled-content id="figU003A8gauss"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="8gauss.png" />
  </fig>
  <p>The likelihood estimate under a score-based diffusion model is
  estimated by solving the change-of-variables equation for continuous
  normalising flows</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \frac{\partial}{\partial t} \log p_t(\boldsymbol{x}_t) = - \nabla_{\boldsymbol{x}_t} \cdot f(\boldsymbol{x}_t, t),
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mfrac><mml:mi>âˆ‚</mml:mi><mml:mrow><mml:mi>âˆ‚</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>log</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>âˆ’</mml:mi><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>â‹…</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>which gives the log-likelihood of a single datapoint
  <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{x}_0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></alternatives></inline-formula>
  as</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  \log p(\boldsymbol{x}_0) = \log p(\boldsymbol{x}_T) - \int_{t=0}^{t=T}\text{d}t \; \nabla_{\boldsymbol{x}_t}\cdot f(\boldsymbol{x}_t, t).
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>log</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>log</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>âˆ’</mml:mo><mml:msubsup><mml:mo>âˆ«</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:mtext mathvariant="normal">d</mml:mtext><mml:mi>t</mml:mi><mml:mspace width="0.278em"></mml:mspace><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>â‹…</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>The code implements these calculations also for the Hutchinson
  trace estimation method
  (<xref alt="Grathwohl et al., 2018" rid="ref-ffjord" ref-type="bibr">Grathwohl
  et al., 2018</xref>;
  <xref alt="Hutchinson, 1990" rid="ref-Hutchinson" ref-type="bibr">Hutchinson,
  1990</xref>) which reduces the computational expense of the density
  estimate. This is because the divergence term
  <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla_{\boldsymbol{x}_t}\cdot f(\boldsymbol{x}_t, t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>âˆ‡</mml:mi><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:msub><mml:mo>â‹…</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>ğ±</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is materialised via a cheaper vector-Jacobian product (i.e.Â a
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(\text{dim}(\boldsymbol{x}))]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ğ’ª</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mtext mathvariant="normal">dim</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ğ±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  operation versus a <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(\text{dim}(\boldsymbol{x})^2)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ğ’ª</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mtext mathvariant="normal">dim</mml:mtext><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ğ±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  operation for the full Jacobian). Figure
  <xref alt="[fig:8gauss]" rid="figU003A8gauss">[fig:8gauss]</xref>
  shows an example of a data-likelihood calculation using a trained
  diffusion model, where the density estimate from the ODE is calculated
  after training with the associated SDE of the forward diffusion
  process.</p>
</sec>
<sec id="implementations-and-future-work">
  <title>Implementations and future work</title>
  <p>Diffusion models are defined in <monospace>sbgm</monospace> via a
  score-network model <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{s}_{\theta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>ğ¬</mml:mi><mml:mi>Î¸</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  and an SDE. All the available SDEs (variance exploding (VE), variance
  preserving (VP) and sub-variance preserving (SubVP)
  (<xref alt="Song, Sohl-Dickstein, et al., 2021" rid="ref-sde" ref-type="bibr">Song,
  Sohl-Dickstein, et al., 2021</xref>)) in the literature of score-based
  diffusion models are available. We provide implementations for UNet
  (<xref alt="Ronneberger et al., 2015" rid="ref-unet" ref-type="bibr">Ronneberger
  et al., 2015</xref>), Diffusion Transformers
  (<xref alt="Peebles &amp; Xie, 2023" rid="ref-dit" ref-type="bibr">Peebles
  &amp; Xie, 2023</xref>), MLP-Mixer
  (<xref alt="Tolstikhin et al., 2021" rid="ref-mixer" ref-type="bibr">Tolstikhin
  et al., 2021</xref>) and Residual Network
  (<xref alt="He et al., 2015" rid="ref-resnet" ref-type="bibr">He et
  al., 2015</xref>) models which are state-of-the-art for diffusion
  tasks. It is possible to fit score-based diffusion models to a
  conditional distribution <inline-formula><alternatives>
  <tex-math><![CDATA[p(\boldsymbol{x}|\boldsymbol{\pi}, \boldsymbol{y})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ğ±</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>ğ›‘</mml:mi><mml:mo>,</mml:mo><mml:mi>ğ²</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  where in typical inverse problems <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ğ²</mml:mi></mml:math></alternatives></inline-formula>
  would be an image and <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{\pi}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ğ›‘</mml:mi></mml:math></alternatives></inline-formula>
  a set of parameters in a physical model for the data
  (<xref alt="Batzolis et al., 2021" rid="ref-conditional_diffusion" ref-type="bibr">Batzolis
  et al., 2021</xref>) (e.g.Â to solve inverse problems). The code is
  compatible with any model written in the
  <monospace>equinox</monospace>
  (<xref alt="Kidger &amp; Garcia, 2021" rid="ref-equinox" ref-type="bibr">Kidger
  &amp; Garcia, 2021</xref>) framework. We recently extended the code to
  provide transformer-based diffusion models
  (<xref alt="Peebles &amp; Xie, 2023" rid="ref-dit" ref-type="bibr">Peebles
  &amp; Xie, 2023</xref>) and plan to extend to latent diffusion models
  (<xref alt="Rombach et al., 2022" rid="ref-ldms" ref-type="bibr">Rombach
  et al., 2022</xref>) and flow matching
  (<xref alt="Lipman et al., 2023" rid="ref-lipman2023flowmatchinggenerativemodeling" ref-type="bibr">Lipman
  et al., 2023</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the developers of the packages <monospace>jax</monospace>
  (<xref alt="Bradbury et al., 2018" rid="ref-jax" ref-type="bibr">Bradbury
  et al., 2018</xref>), <monospace>optax</monospace>
  (<xref alt="DeepMind et al., 2020" rid="ref-optax" ref-type="bibr">DeepMind
  et al., 2020</xref>), <monospace>equinox</monospace>
  (<xref alt="Kidger &amp; Garcia, 2021" rid="ref-equinox" ref-type="bibr">Kidger
  &amp; Garcia, 2021</xref>) and <monospace>diffrax</monospace>
  (<xref alt="Kidger, 2022" rid="ref-kidger" ref-type="bibr">Kidger,
  2022</xref>) for their work and for making their code available to the
  community.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-diffusion">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Sohl-Dickstein</surname><given-names>Jascha</given-names></name>
        <name><surname>Weiss</surname><given-names>Eric A.</given-names></name>
        <name><surname>Maheswaranathan</surname><given-names>Niru</given-names></name>
        <name><surname>Ganguli</surname><given-names>Surya</given-names></name>
      </person-group>
      <article-title>Deep unsupervised learning using nonequilibrium thermodynamics</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/abs/1503.03585</uri>
    </element-citation>
  </ref>
  <ref id="ref-ddpm">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ho</surname><given-names>Jonathan</given-names></name>
        <name><surname>Jain</surname><given-names>Ajay</given-names></name>
        <name><surname>Abbeel</surname><given-names>Pieter</given-names></name>
      </person-group>
      <article-title>Denoising diffusion probabilistic models</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2006.11239</uri>
    </element-citation>
  </ref>
  <ref id="ref-sbi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
        <name><surname>Brehmer</surname><given-names>Johann</given-names></name>
        <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
      </person-group>
      <article-title>The frontier of simulation-based inference</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <year iso-8601-date="2020">2020</year>
      <volume>117</volume>
      <issue>48</issue>
      <uri>https://www.pnas.org/doi/abs/10.1073/pnas.1912789117</uri>
      <pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id>
      <fpage>30055</fpage>
      <lpage>30062</lpage>
    </element-citation>
  </ref>
  <ref id="ref-field_level_inference">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Andrews</surname><given-names>Adam</given-names></name>
        <name><surname>Jasche</surname><given-names>Jens</given-names></name>
        <name><surname>Lavaux</surname><given-names>Guilhem</given-names></name>
        <name><surname>Schmidt</surname><given-names>Fabian</given-names></name>
      </person-group>
      <article-title>Bayesian field-level inference of primordial non-gaussianity using next-generation galaxy surveys</article-title>
      <source>Monthly Notices of the Royal Astronomical Society</source>
      <publisher-name>Oxford University Press (OUP)</publisher-name>
      <year iso-8601-date="2023-02">2023</year><month>02</month>
      <volume>520</volume>
      <issue>4</issue>
      <issn>1365-2966</issn>
      <uri>http://dx.doi.org/10.1093/mnras/stad432</uri>
      <pub-id pub-id-type="doi">10.1093/mnras/stad432</pub-id>
      <fpage>5746</fpage>
      <lpage>5763</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Feng2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Feng</surname><given-names>Berthy T.</given-names></name>
        <name><surname>Smith</surname><given-names>Jamie</given-names></name>
        <name><surname>Rubinstein</surname><given-names>Michael</given-names></name>
        <name><surname>Chang</surname><given-names>Huiwen</given-names></name>
        <name><surname>Bouman</surname><given-names>Katherine L.</given-names></name>
        <name><surname>Freeman</surname><given-names>William T.</given-names></name>
      </person-group>
      <article-title>Score-based diffusion models as principled priors for inverse imaging</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2304.11751</uri>
    </element-citation>
  </ref>
  <ref id="ref-Feng2024">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Feng</surname><given-names>Berthy T.</given-names></name>
        <name><surname>Bouman</surname><given-names>Katherine L.</given-names></name>
      </person-group>
      <article-title>Variational Bayesian Imaging with an Efficient Surrogate Score-based Prior</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2309.01949</uri>
    </element-citation>
  </ref>
  <ref id="ref-inverse_problem_medical">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Song</surname><given-names>Yang</given-names></name>
        <name><surname>Shen</surname><given-names>Liyue</given-names></name>
        <name><surname>Xing</surname><given-names>Lei</given-names></name>
        <name><surname>Ermon</surname><given-names>Stefano</given-names></name>
      </person-group>
      <article-title>Solving inverse problems in medical imaging with score-based generative models</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2111.08005</uri>
    </element-citation>
  </ref>
  <ref id="ref-conditional_diffusion">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Batzolis</surname><given-names>Georgios</given-names></name>
        <name><surname>Stanczuk</surname><given-names>Jan</given-names></name>
        <name><surname>SchÃ¶nlieb</surname><given-names>Carola-Bibiane</given-names></name>
        <name><surname>Etmann</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>Conditional image generation with score-based diffusion models</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2111.13606</uri>
    </element-citation>
  </ref>
  <ref id="ref-kidger">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kidger</surname><given-names>Patrick</given-names></name>
      </person-group>
      <article-title>On neural differential equations</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2202.02435</uri>
    </element-citation>
  </ref>
  <ref id="ref-sde">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Song</surname><given-names>Yang</given-names></name>
        <name><surname>Sohl-Dickstein</surname><given-names>Jascha</given-names></name>
        <name><surname>Kingma</surname><given-names>Diederik P.</given-names></name>
        <name><surname>Kumar</surname><given-names>Abhishek</given-names></name>
        <name><surname>Ermon</surname><given-names>Stefano</given-names></name>
        <name><surname>Poole</surname><given-names>Ben</given-names></name>
      </person-group>
      <article-title>Score-based generative modeling through stochastic differential equations</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2011.13456</uri>
    </element-citation>
  </ref>
  <ref id="ref-sde_ml">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Song</surname><given-names>Yang</given-names></name>
        <name><surname>Durkan</surname><given-names>Conor</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
        <name><surname>Ermon</surname><given-names>Stefano</given-names></name>
      </person-group>
      <article-title>Maximum likelihood training of score-based diffusion models</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2101.09258</uri>
    </element-citation>
  </ref>
  <ref id="ref-ffjord">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Grathwohl</surname><given-names>Will</given-names></name>
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Bettencourt</surname><given-names>Jesse</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>FFJORD: Free-form continuous dynamics for scalable reversible generative models</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://arxiv.org/abs/1810.01367</uri>
    </element-citation>
  </ref>
  <ref id="ref-neuralodes">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Rubanova</surname><given-names>Yulia</given-names></name>
        <name><surname>Bettencourt</surname><given-names>Jesse</given-names></name>
        <name><surname>Duvenaud</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Neural ordinary differential equations</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1806.07366</uri>
    </element-citation>
  </ref>
  <ref id="ref-blinddiffusion">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chung</surname><given-names>Hyungjin</given-names></name>
        <name><surname>Kim</surname><given-names>Jeongsol</given-names></name>
        <name><surname>Kim</surname><given-names>Sehui</given-names></name>
        <name><surname>Ye</surname><given-names>Jong Chul</given-names></name>
      </person-group>
      <article-title>Parallel diffusion models of operator and image for blind inverse problems</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2211.10656</uri>
    </element-citation>
  </ref>
  <ref id="ref-ambientdiffusion">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Daras</surname><given-names>Giannis</given-names></name>
        <name><surname>Dimakis</surname><given-names>Alexandros G.</given-names></name>
        <name><surname>Daskalakis</surname><given-names>Constantinos</given-names></name>
      </person-group>
      <article-title>Consistent diffusion meets tweedie: Training exact ambient diffusion models with noisy data</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2404.10177</uri>
    </element-citation>
  </ref>
  <ref id="ref-emulating">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Spurio Mancini</surname><given-names>Alessio</given-names></name>
        <name><surname>Piras</surname><given-names>Davide</given-names></name>
        <name><surname>Alsing</surname><given-names>Justin</given-names></name>
        <name><surname>Joachimi</surname><given-names>Benjamin</given-names></name>
        <name><surname>Hobson</surname><given-names>Michael P</given-names></name>
      </person-group>
      <article-title>CosmoPower: Emulating cosmological power spectra for accelerated Bayesian inference from next-generation surveys</article-title>
      <source>Monthly Notices of the Royal Astronomical Society</source>
      <publisher-name>Oxford University Press (OUP)</publisher-name>
      <year iso-8601-date="2022-01">2022</year><month>01</month>
      <volume>511</volume>
      <issue>2</issue>
      <issn>1365-2966</issn>
      <uri>http://dx.doi.org/10.1093/mnras/stac064</uri>
      <pub-id pub-id-type="doi">10.1093/mnras/stac064</pub-id>
      <fpage>1771</fpage>
      <lpage>1788</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/jax-ml/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-equinox">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kidger</surname><given-names>Patrick</given-names></name>
        <name><surname>Garcia</surname><given-names>Cristian</given-names></name>
      </person-group>
      <article-title>Equinox: Neural networks in JAX via callable PyTrees and filtered transformations</article-title>
      <source>Differentiable Programming workshop at Neural Information Processing Systems 2021</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-optax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>DeepMind</surname></name>
        <name><surname>Babuschkin</surname><given-names>Igor</given-names></name>
        <name><surname>Baumli</surname><given-names>Kate</given-names></name>
        <name><surname>Bell</surname><given-names>Alison</given-names></name>
        <name><surname>Bhupatiraju</surname><given-names>Surya</given-names></name>
        <name><surname>Bruce</surname><given-names>Jake</given-names></name>
        <name><surname>Buchlovsky</surname><given-names>Peter</given-names></name>
        <name><surname>Budden</surname><given-names>David</given-names></name>
        <name><surname>Cai</surname><given-names>Trevor</given-names></name>
        <name><surname>Clark</surname><given-names>Aidan</given-names></name>
        <name><surname>Danihelka</surname><given-names>Ivo</given-names></name>
        <name><surname>Dedieu</surname><given-names>Antoine</given-names></name>
        <name><surname>Fantacci</surname><given-names>Claudio</given-names></name>
        <name><surname>Godwin</surname><given-names>Jonathan</given-names></name>
        <name><surname>Jones</surname><given-names>Chris</given-names></name>
        <name><surname>Hemsley</surname><given-names>Ross</given-names></name>
        <name><surname>Hennigan</surname><given-names>Tom</given-names></name>
        <name><surname>Hessel</surname><given-names>Matteo</given-names></name>
        <name><surname>Hou</surname><given-names>Shaobo</given-names></name>
        <name><surname>Kapturowski</surname><given-names>Steven</given-names></name>
        <name><surname>Keck</surname><given-names>Thomas</given-names></name>
        <name><surname>Kemaev</surname><given-names>Iurii</given-names></name>
        <name><surname>King</surname><given-names>Michael</given-names></name>
        <name><surname>Kunesch</surname><given-names>Markus</given-names></name>
        <name><surname>Martens</surname><given-names>Lena</given-names></name>
        <name><surname>Merzic</surname><given-names>Hamza</given-names></name>
        <name><surname>Mikulik</surname><given-names>Vladimir</given-names></name>
        <name><surname>Norman</surname><given-names>Tamara</given-names></name>
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Quan</surname><given-names>John</given-names></name>
        <name><surname>Ring</surname><given-names>Roman</given-names></name>
        <name><surname>Ruiz</surname><given-names>Francisco</given-names></name>
        <name><surname>Sanchez</surname><given-names>Alvaro</given-names></name>
        <name><surname>Sartran</surname><given-names>Laurent</given-names></name>
        <name><surname>Schneider</surname><given-names>Rosalia</given-names></name>
        <name><surname>Sezener</surname><given-names>Eren</given-names></name>
        <name><surname>Spencer</surname><given-names>Stephen</given-names></name>
        <name><surname>Srinivasan</surname><given-names>Srivatsan</given-names></name>
        <name><surname>StanojeviÄ‡</surname><given-names>MiloÅ¡</given-names></name>
        <name><surname>Stokowiec</surname><given-names>Wojciech</given-names></name>
        <name><surname>Wang</surname><given-names>Luyu</given-names></name>
        <name><surname>Zhou</surname><given-names>Guangyao</given-names></name>
        <name><surname>Viola</surname><given-names>Fabio</given-names></name>
      </person-group>
      <article-title>The DeepMind JAX Ecosystem</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>http://github.com/google-deepmind</uri>
    </element-citation>
  </ref>
  <ref id="ref-resnet">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>Kaiming</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Ren</surname><given-names>Shaoqing</given-names></name>
        <name><surname>Sun</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Deep residual learning for image recognition</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/abs/1512.03385</uri>
    </element-citation>
  </ref>
  <ref id="ref-unet">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ronneberger</surname><given-names>Olaf</given-names></name>
        <name><surname>Fischer</surname><given-names>Philipp</given-names></name>
        <name><surname>Brox</surname><given-names>Thomas</given-names></name>
      </person-group>
      <article-title>U-net: Convolutional networks for biomedical image segmentation</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/abs/1505.04597</uri>
    </element-citation>
  </ref>
  <ref id="ref-ldms">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Rombach</surname><given-names>Robin</given-names></name>
        <name><surname>Blattmann</surname><given-names>Andreas</given-names></name>
        <name><surname>Lorenz</surname><given-names>Dominik</given-names></name>
        <name><surname>Esser</surname><given-names>Patrick</given-names></name>
        <name><surname>Ommer</surname><given-names>BjÃ¶rn</given-names></name>
      </person-group>
      <article-title>High-resolution image synthesis with latent diffusion models</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2112.10752</uri>
    </element-citation>
  </ref>
  <ref id="ref-gans">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Goodfellow</surname><given-names>Ian J.</given-names></name>
        <name><surname>Pouget-Abadie</surname><given-names>Jean</given-names></name>
        <name><surname>Mirza</surname><given-names>Mehdi</given-names></name>
        <name><surname>Xu</surname><given-names>Bing</given-names></name>
        <name><surname>Warde-Farley</surname><given-names>David</given-names></name>
        <name><surname>Ozair</surname><given-names>Sherjil</given-names></name>
        <name><surname>Courville</surname><given-names>Aaron</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>Generative adversarial networks</article-title>
      <year iso-8601-date="2014">2014</year>
      <uri>https://arxiv.org/abs/1406.2661</uri>
    </element-citation>
  </ref>
  <ref id="ref-vaes">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P</given-names></name>
        <name><surname>Welling</surname><given-names>Max</given-names></name>
      </person-group>
      <article-title>Auto-Encoding Variational Bayes</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/1312.6114</uri>
    </element-citation>
  </ref>
  <ref id="ref-flows">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/1912.02762</uri>
    </element-citation>
  </ref>
  <ref id="ref-blackjax">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Cabezas</surname><given-names>Alberto</given-names></name>
        <name><surname>Corenflos</surname><given-names>Adrien</given-names></name>
        <name><surname>Lao</surname><given-names>Junpeng</given-names></name>
        <name><surname>Louf</surname><given-names>RÃ©mi</given-names></name>
      </person-group>
      <article-title>BlackJAX: Composable Bayesian inference in JAX</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2402.10797</uri>
    </element-citation>
  </ref>
  <ref id="ref-Remy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Remy</surname><given-names>B.</given-names></name>
        <name><surname>Lanusse</surname><given-names>F.</given-names></name>
        <name><surname>Jeffrey</surname><given-names>N.</given-names></name>
        <name><surname>Liu</surname><given-names>J.</given-names></name>
        <name><surname>Starck</surname><given-names>J.-L.</given-names></name>
        <name><surname>Osato</surname><given-names>K.</given-names></name>
        <name><surname>Schrabback</surname><given-names>T.</given-names></name>
      </person-group>
      <article-title>Probabilistic mass-mapping with neural score estimation</article-title>
      <source>Astronomy &amp;amp; Astrophysics</source>
      <publisher-name>EDP Sciences</publisher-name>
      <year iso-8601-date="2023-03">2023</year><month>03</month>
      <volume>672</volume>
      <issn>1432-0746</issn>
      <uri>http://dx.doi.org/10.1051/0004-6361/202243054</uri>
      <pub-id pub-id-type="doi">10.1051/0004-6361/202243054</pub-id>
      <fpage>A51</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vdms">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P.</given-names></name>
        <name><surname>Salimans</surname><given-names>Tim</given-names></name>
        <name><surname>Poole</surname><given-names>Ben</given-names></name>
        <name><surname>Ho</surname><given-names>Jonathan</given-names></name>
      </person-group>
      <article-title>Variational diffusion models</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2107.00630</uri>
    </element-citation>
  </ref>
  <ref id="ref-mixer">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tolstikhin</surname><given-names>Ilya</given-names></name>
        <name><surname>Houlsby</surname><given-names>Neil</given-names></name>
        <name><surname>Kolesnikov</surname><given-names>Alexander</given-names></name>
        <name><surname>Beyer</surname><given-names>Lucas</given-names></name>
        <name><surname>Zhai</surname><given-names>Xiaohua</given-names></name>
        <name><surname>Unterthiner</surname><given-names>Thomas</given-names></name>
        <name><surname>Yung</surname><given-names>Jessica</given-names></name>
        <name><surname>Steiner</surname><given-names>Andreas</given-names></name>
        <name><surname>Keysers</surname><given-names>Daniel</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Lucic</surname><given-names>Mario</given-names></name>
        <name><surname>Dosovitskiy</surname><given-names>Alexey</given-names></name>
      </person-group>
      <article-title>MLP-mixer: An all-MLP architecture for vision</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2105.01601</uri>
    </element-citation>
  </ref>
  <ref id="ref-score_matching">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>HyvÃ¤rinen</surname><given-names>Aapo</given-names></name>
      </person-group>
      <article-title>Estimation of non-normalized statistical models by score matching</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2005">2005</year>
      <volume>6</volume>
      <issue>24</issue>
      <uri>http://jmlr.org/papers/v6/hyvarinen05a.html</uri>
      <fpage>695</fpage>
      <lpage>709</lpage>
    </element-citation>
  </ref>
  <ref id="ref-score_matching2">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vincent</surname><given-names>Pascal</given-names></name>
      </person-group>
      <article-title>A connection between score matching and denoising autoencoders</article-title>
      <source>Neural Computation</source>
      <year iso-8601-date="2011-07">2011</year><month>07</month>
      <volume>23</volume>
      <issue>7</issue>
      <issn>0899-7667</issn>
      <uri>https://doi.org/10.1162/NECO\_a\_00142</uri>
      <pub-id pub-id-type="doi">10.1162/NECO_a_00142</pub-id>
      <fpage>1661</fpage>
      <lpage>1674</lpage>
    </element-citation>
  </ref>
  <ref id="ref-azula">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Rozet</surname><given-names>FranÃ§ois</given-names></name>
      </person-group>
      <article-title>Azula: Diffusion models in PyTorch</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/probabilists/azula</uri>
    </element-citation>
  </ref>
  <ref id="ref-dit">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Peebles</surname><given-names>William</given-names></name>
        <name><surname>Xie</surname><given-names>Saining</given-names></name>
      </person-group>
      <article-title>Scalable diffusion models with transformers</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2212.09748</uri>
    </element-citation>
  </ref>
  <ref id="ref-Hutchinson">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hutchinson</surname><given-names>M. F.</given-names></name>
      </person-group>
      <article-title>A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines</article-title>
      <source>Communications in Statistics - Simulation and Computation</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1990">1990</year>
      <volume>19</volume>
      <issue>2</issue>
      <uri> 
              https://doi.org/10.1080/03610919008812866
          </uri>
      <pub-id pub-id-type="doi">10.1080/03610919008812866</pub-id>
      <fpage>433</fpage>
      <lpage>450</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lipman2023flowmatchinggenerativemodeling">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Lipman</surname><given-names>Yaron</given-names></name>
        <name><surname>Chen</surname><given-names>Ricky T. Q.</given-names></name>
        <name><surname>Ben-Hamu</surname><given-names>Heli</given-names></name>
        <name><surname>Nickel</surname><given-names>Maximilian</given-names></name>
        <name><surname>Le</surname><given-names>Matt</given-names></name>
      </person-group>
      <article-title>Flow matching for generative modeling</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2210.02747</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
