<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4705</article-id>
<article-id pub-id-type="doi">10.21105/joss.04705</article-id>
<title-group>
<article-title>aorsf: An R package for supervised learning using the
oblique random survival forest</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7399-2299</contrib-id>
<name>
<surname>Jaeger</surname>
<given-names>Byron C.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Welden</surname>
<given-names>Sawyer</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Lenoir</surname>
<given-names>Kristin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pajewski</surname>
<given-names>Nicholas M</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Wake Forest University School of Medicine</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-05-11">
<day>11</day>
<month>5</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>77</issue>
<fpage>4705</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>machine learning</kwd>
<kwd>supervised learning</kwd>
<kwd>survival</kwd>
<kwd>random forest</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Risk prediction is a type of supervised learning where the goal is
  to predict the probability that a person will experience an event
  within a specific amount of time. This kind of prediction may be
  useful in clinical settings, where identifying patients who are at
  high risk for experiencing an adverse health outcome can help guide
  strategies for prevention and treatment. The oblique random survival
  forest (RSF) is a supervised learning technique that has obtained high
  prediction accuracy in general benchmarks for risk prediction
  (<xref alt="Jaeger et al., 2019" rid="ref-jaeger2019oblique" ref-type="bibr">Jaeger
  et al., 2019</xref>). However, computational overhead and a lack of
  tools for interpretation make it difficult to use the oblique RSF in
  applied settings.</p>
  <p><monospace>aorsf</monospace> is an R package with fast algorithms
  to fit and interpret oblique RSFs, allowing users to obtain an
  accurate risk prediction model without losing efficiency or
  interpretability. <monospace>aorsf</monospace> supports exploration
  and customization of oblique RSFs, allowing users to select the
  fitting procedure for an oblique RSF or supply their own procedure
  (see examples provided in documentation for
  <monospace>orsf_control_custom()</monospace>). Whereas existing
  software for oblique RSFs does not support estimation of variable
  importance (VI), <monospace>aorsf</monospace> provides multiple
  techniques to estimate VI with the added flexibilty of allowing users
  to supply their own functions where applicable (see documentation for
  <monospace>orsf_vi()</monospace>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The purpose of <monospace>aorsf</monospace> is to allow oblique
  RSFs to be fit and interpreted efficiently. The target audience
  includes both <bold>analysts</bold> aiming to develop an accurate risk
  prediction model (e.g., see Segar et al.
  (<xref alt="2021" rid="ref-segar2021development" ref-type="bibr">2021</xref>))
  and <bold>researchers</bold> who want to conduct experiments comparing
  different techniques for fitting oblique RSFs (e.g., see Katuwal,
  Suganthan, &amp; Zhang
  (<xref alt="2020" rid="ref-katuwal2020heterogeneous" ref-type="bibr">2020</xref>)).</p>
  <sec id="related-software">
    <title>Related software</title>
    <p>The <monospace>obliqueRF</monospace> and
    <monospace>RLT</monospace> R packages support oblique random forests
    (RFs) for classification and regression, but not survival. The
    <monospace>ranger</monospace>,
    <monospace>randomForestSRC</monospace>, and
    <monospace>party</monospace> packages support axis-based RSFs (see
    Background section) but not oblique RSFs. The
    <monospace>obliqueRSF</monospace> R package fits oblique RSFs, but
    has high computational overhead, provides a limited set of tools to
    interpret oblique RSFs, and does not enable customization of
    routines used to fit oblique RSFs.</p>
  </sec>
</sec>
<sec id="background">
  <title>Background</title>
  <p>Random forests (RFs) are large sets of de-correlated decision trees
  (<xref alt="Breiman, 2001" rid="ref-breiman2001random" ref-type="bibr">Breiman,
  2001</xref>). Trees in the RF may be axis-based or oblique. Axis-based
  trees split data using a single predictor, creating decision
  boundaries that are perpendicular or parallel to the axes of the
  predictor space. Oblique trees split data using a linear combination
  of predictors, creating decision boundaries that are neither parallel
  nor perpendicular to the axes of their contributing predictors. The
  increased flexibility of their decision boundaries allows oblique
  trees to produce more controlled partitions of a predictor space
  compared to their axis-based counterparts (<bold>Figure 1</bold>).
  However, the added flexibility has a computational cost, as the number
  of potential oblique decision boundaries to search through may be much
  larger than the number of axis-based boundaries.</p>
  <fig id="figU003Afig-tree">
    <caption><p> Decision trees with axis-based splitting (left) and
    oblique splitting (right). Cases are orange squares; controls are
    purple circles. Both trees partition the predictor space defined by
    variables X1 and X2, but the oblique splits do a better job of
    separating the two classes.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/tree_axis_v_oblique.png" xlink:title="" />
  </fig>
  <sec id="censoring">
    <title>Censoring</title>
    <p>A common challenge in risk prediction is censoring. The term
    ‘censor’ indicates partial observation of something. Censoring
    occurs often in medical studies that track the elapsed time from a
    baseline visit to the occurrence of an event. For example, censoring
    can occur if study coordinators lose contact with a participant or
    the study concludes before the participant experiences the
    event.</p>
    <p>Decision trees can engage with censored outcomes by recursively
    partitioning data using standard descriptive tests (e.g., the
    log-rank test) and applying estimation techniques for censored
    outcomes to generate predicted values in leaf nodes, such as the
    survival curve or the cumulative hazard function
    (<xref alt="Ishwaran, Kogalur, Blackstone, &amp; Lauer, 2008" rid="ref-ishwaran2008random" ref-type="bibr">Ishwaran,
    Kogalur, Blackstone, &amp; Lauer, 2008</xref>).</p>
  </sec>
</sec>
<sec id="newton-raphson-scoring">
  <title>Newton Raphson scoring</title>
  <p>The efficiency of <monospace>aorsf</monospace> is primarily
  attributed to its use of Newton Raphson scoring to identify linear
  combinations of predictor variables. <monospace>aorsf</monospace> uses
  the same approach as the <monospace>survival</monospace> package to
  complete this estimation procedure efficiently. Full details on the
  steps involved have been made available by Therneau
  (<xref alt="2022" rid="ref-therneau_survival_2022" ref-type="bibr">2022</xref>).
  Briefly, a vector of estimated regression coefficients,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover></mml:math></alternatives></inline-formula>,
  is updated in each step of the procedure based on its first
  derivative, <inline-formula><alternatives>
  <tex-math><![CDATA[U(\hat{\beta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and second derivative, <inline-formula><alternatives>
  <tex-math><![CDATA[H(\hat{\beta})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[ \hat{\beta}^{k+1} =  \hat{\beta}^{k} + U(\hat{\beta} = \hat{\beta}^{k})\, H^{-1}(\hat{\beta} = \hat{\beta}^{k})]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mrow><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mi>U</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="0.167em"></mml:mspace><mml:msup><mml:mi>H</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msup><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
  While it is standard practice in statistical modeling to iteratively
  update <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover></mml:math></alternatives></inline-formula>
  until a convergence threshold is met, the default approach in
  <monospace>aorsf</monospace> only completes one iteration (see
  <monospace>orsf_control_fast()</monospace>). The rationale for this
  approach is based on two points. First, while completing more
  iterations reduces bias in the regression coefficients, general
  benchmarking experiments have found it does not improve the
  discrimination or Brier score of the oblique RSF
  (<xref alt="Jaeger et al., 2022" rid="ref-aorsf_arxiv" ref-type="bibr">Jaeger
  et al., 2022</xref>). Second, computing <inline-formula><alternatives>
  <tex-math><![CDATA[U]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>U</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[H]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>H</mml:mi></mml:math></alternatives></inline-formula>
  requires computation and exponentiation of the vector
  <inline-formula><alternatives>
  <tex-math><![CDATA[X\hat{\beta}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>X</mml:mi><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[X]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
  is the matrix of predictor values, but these steps can be skipped on
  the first iteration if an initial value of
  <inline-formula><alternatives>
  <tex-math><![CDATA[\hat{\beta} = 0]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:mi>β</mml:mi><mml:mo accent="true">̂</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  is chosen, allowing for a reduction in required computation.</p>
</sec>
<sec id="variable-importance">
  <title>Variable importance</title>
  <p>Estimation of VI is a common technique used for interpretation of
  RFs. Permutation, a standard approach to estimate VI, measures how
  much a RF’s prediction accuracy decreases after randomly permuting
  values of a given variable. If the variable is important,
  de-stabilizing decisions based on the variable should reduce the RF’s
  prediction accuracy.</p>
  <p><monospace>aorsf</monospace> includes multiple functions to
  estimate VI, including permutation VI and a faster approach to
  estimate VI using analysis of variance
  (<xref alt="Menze, Kelm, Splitthoff, Koethe, &amp; Hamprecht, 2011" rid="ref-menze2011oblique" ref-type="bibr">Menze,
  Kelm, Splitthoff, Koethe, &amp; Hamprecht, 2011</xref>). In addition,
  <monospace>aorsf</monospace> includes a novel technique for VI that is
  designed for compatibility with oblique decision trees: ‘negation
  importance.’ Similar to permutation, negation VI measures the
  reduction in an oblique RFs prediction accuracy after de-stabilizing
  decisions based on a variable. However, instead of permuting values of
  a variable, negation VI flips the sign of all coefficients linked to a
  variable (i.e., it negates them). As the magnitude of a coefficient
  increases, so does the probability that negating it will change the
  oblique RF’s predictions.</p>
</sec>
<sec id="benchmarking">
  <title>Benchmarking</title>
  <p>General benchmarks of prediction accuracy, computational
  efficiency, and estimation of VI using <monospace>aorsf</monospace>
  and several other software packages are available
  (<xref alt="Jaeger et al., 2022" rid="ref-aorsf_arxiv" ref-type="bibr">Jaeger
  et al., 2022</xref>). In these benchmarks,
  <monospace>aorsf</monospace> has matched or exceeded the prediction
  accuracy of <monospace>obliqueRSF</monospace> while running hundreds
  of times faster. Additionally, simulation studies have found that
  negation VI improves the oblique RSF’s ability to discriminate between
  signal and noise predictors compared to existing VI techniques.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The development of this software was supported by the Center for
  Biomedical Informatics at Wake Forest University School of Medicine
  and the National Center for Advancing Translational Sciences (NCATS),
  National Institutes of Health (NIH), through Grant Award Number
  UL1TR001420. Dr. Pajewski was additionally supported by grant award
  P30 AG021332 from the NIH. The content is solely the responsibility of
  the authors and does not necessarily represent the official views of
  the NIH.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-ishwaran2008random">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ishwaran</surname><given-names>Hemant</given-names></name>
        <name><surname>Kogalur</surname><given-names>Udaya B</given-names></name>
        <name><surname>Blackstone</surname><given-names>Eugene H</given-names></name>
        <name><surname>Lauer</surname><given-names>Michael S</given-names></name>
      </person-group>
      <article-title>Random survival forests</article-title>
      <source>The Annals of Applied Statistics</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>2</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1214/08-AOAS169</pub-id>
      <fpage>841</fpage>
      <lpage>860</lpage>
    </element-citation>
  </ref>
  <ref id="ref-therneau_survival_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Therneau</surname><given-names>Terry</given-names></name>
      </person-group>
      <article-title>Survival package source code documentation</article-title>
      <year iso-8601-date="2022-04">2022</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-05-12">2022</year><month>05</month><day>12</day></date-in-citation>
      <uri>https://github.com/therneau/survival/blob/5440691d44abea537b08aeb60153a31654d66a9b/noweb</uri>
    </element-citation>
  </ref>
  <ref id="ref-aorsf_arxiv">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Jaeger</surname><given-names>Byron</given-names></name>
        <name><surname>Welden</surname><given-names>Sawyer</given-names></name>
        <name><surname>Lenoir</surname><given-names>Kristin</given-names></name>
        <name><surname>Speiser</surname><given-names>Jaime L.</given-names></name>
        <name><surname>Segar</surname><given-names>Matthew W.</given-names></name>
        <name><surname>Pandey</surname><given-names>Ambarish</given-names></name>
        <name><surname>Pajewski</surname><given-names>Nicholas M.</given-names></name>
      </person-group>
      <article-title>Accelerated and interpretable oblique random survival forests</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2208.01129</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2208.01129</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-breiman2001random">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Breiman</surname><given-names>Leo</given-names></name>
      </person-group>
      <article-title>Random forests</article-title>
      <source>Machine Learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2001">2001</year>
      <volume>45</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      <fpage>5</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-menze2011oblique">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Menze</surname><given-names>Bjoern H</given-names></name>
        <name><surname>Kelm</surname><given-names>B Michael</given-names></name>
        <name><surname>Splitthoff</surname><given-names>Daniel N</given-names></name>
        <name><surname>Koethe</surname><given-names>Ullrich</given-names></name>
        <name><surname>Hamprecht</surname><given-names>Fred A</given-names></name>
      </person-group>
      <article-title>On oblique random forests</article-title>
      <source>Joint european conference on machine learning and knowledge discovery in databases</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <fpage>453</fpage>
      <lpage>469</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jaeger2019oblique">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jaeger</surname><given-names>Byron</given-names></name>
        <name><surname>Long</surname><given-names>D Leann</given-names></name>
        <name><surname>Long</surname><given-names>Dustin M</given-names></name>
        <name><surname>Sims</surname><given-names>Mario</given-names></name>
        <name><surname>Szychowski</surname><given-names>Jeff M</given-names></name>
        <name><surname>Min</surname><given-names>Yuan-I</given-names></name>
        <name><surname>Mcclure</surname><given-names>Leslie A</given-names></name>
        <name><surname>Howard</surname><given-names>George</given-names></name>
        <name><surname>Simon</surname><given-names>Noah</given-names></name>
      </person-group>
      <article-title>Oblique random survival forests</article-title>
      <source>The Annals of Applied Statistics</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>13</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1214/19-AOAS1261</pub-id>
      <fpage>1847</fpage>
      <lpage>1883</lpage>
    </element-citation>
  </ref>
  <ref id="ref-segar2021development">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Segar</surname><given-names>Matthew W</given-names></name>
        <name><surname>Jaeger</surname><given-names>Byron C</given-names></name>
        <name><surname>Patel</surname><given-names>Kershaw V</given-names></name>
        <name><surname>Nambi</surname><given-names>Vijay</given-names></name>
        <name><surname>Ndumele</surname><given-names>Chiadi E</given-names></name>
        <name><surname>Correa</surname><given-names>Adolfo</given-names></name>
        <name><surname>Butler</surname><given-names>Javed</given-names></name>
        <name><surname>Chandra</surname><given-names>Alvin</given-names></name>
        <name><surname>Ayers</surname><given-names>Colby</given-names></name>
        <name><surname>Rao</surname><given-names>Shreya</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Development and validation of machine learning–based race-specific models to predict 10-year risk of heart failure: A multicohort analysis</article-title>
      <source>Circulation</source>
      <publisher-name>Am Heart Assoc</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>143</volume>
      <issue>24</issue>
      <pub-id pub-id-type="doi">10.1161/circulationaha.120.053134</pub-id>
      <fpage>2370</fpage>
      <lpage>2383</lpage>
    </element-citation>
  </ref>
  <ref id="ref-katuwal2020heterogeneous">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Katuwal</surname><given-names>Rakesh</given-names></name>
        <name><surname>Suganthan</surname><given-names>Ponnuthurai Nagaratnam</given-names></name>
        <name><surname>Zhang</surname><given-names>Le</given-names></name>
      </person-group>
      <article-title>Heterogeneous oblique random forest</article-title>
      <source>Pattern Recognition</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>99</volume>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2019.107078</pub-id>
      <fpage>107078</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
