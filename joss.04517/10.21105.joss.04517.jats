<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4517</article-id>
<article-id pub-id-type="doi">10.21105/joss.04517</article-id>
<title-group>
<article-title>Efficiently Learning Relative Similarity Embeddings with
Crowdsourcing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4275-3452</contrib-id>
<name>
<surname>Sievert</surname>
<given-names>Scott</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Nowak</surname>
<given-names>Robert</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6304-755X</contrib-id>
<name>
<surname>Rogers</surname>
<given-names>Timothy</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Wisconsin‚ÄìMadison</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-04-09">
<day>9</day>
<month>4</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>84</issue>
<fpage>4517</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>crowdsourcing</kwd>
<kwd>active machine learning</kwd>
<kwd>relatively similarity</kwd>
<kwd>adaptive sampling</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Social scientists often investigate human reasoning by collecting
  relative similarity judgements with crowdsourcing services. However,
  this often requires too many human responses to be practical for large
  experiments. To address this problem, we introduce software called
  Salmon, which makes intelligent choices on query selection (aka active
  machine learning or adaptive sampling) while collecting relative
  similarity judgments from crowdsourcing participants. Salmon is usable
  by experimentalists because it requires little to no programming
  experience and only requires an Amazon AWS account for launching
  (though a local install is available). Extensive simulations and
  experiments suggest that Salmon requires 2 to 3 times fewer response
  than random sampling.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Relative similarity judgments take the form ‚Äúis item
  <inline-formula><alternatives>
  <tex-math><![CDATA[a]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>a</mml:mi></mml:math></alternatives></inline-formula>
  or <inline-formula><alternatives>
  <tex-math><![CDATA[b]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
  more similar to item <inline-formula><alternatives>
  <tex-math><![CDATA[h]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>?‚Äù
  These queries work well with human working memory limitations, and
  have been used successfully to characterize human perceived similarity
  between faces
  (<xref alt="Sankaranarayanan et al., 2016" rid="ref-faceverification" ref-type="bibr">Sankaranarayanan
  et al., 2016</xref>), vehicles
  (<xref alt="Kuma et al., 2019" rid="ref-vehicles" ref-type="bibr">Kuma
  et al., 2019</xref>) and shoes
  (<xref alt="Heim et al., 2015b" rid="ref-tackl" ref-type="bibr">Heim
  et al., 2015b</xref>).</p>
  <p>Typically, experimentalists require an inordinate number of human
  responses (about 10,000) to produce an accurate embedding when making
  a similarity map in <inline-formula><alternatives>
  <tex-math><![CDATA[d=2]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  dimensions of <inline-formula><alternatives>
  <tex-math><![CDATA[n = 50]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  chemistry molecules
  (<xref alt="Mason et al., 2019" rid="ref-chem" ref-type="bibr">Mason
  et al., 2019</xref>). The number of human responses required will
  scale like <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(nd\log n)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo>log</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  which means that asking about <inline-formula><alternatives>
  <tex-math><![CDATA[n=100]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  molecules for <inline-formula><alternatives>
  <tex-math><![CDATA[d=3]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  dimensions will likely require about 35,000 responses.</p>
  <p>Many ‚Äúactive machine learning‚Äù methods have been proposed to reduce
  the number of queries required
  (<xref alt="Tamuz et al., 2011" rid="ref-ckl" ref-type="bibr">Tamuz et
  al., 2011</xref>;
  <xref alt="Van Der Maaten &amp; Weinberger, 2012" rid="ref-ste" ref-type="bibr">Van
  Der Maaten &amp; Weinberger, 2012</xref>). These show gains, at least
  offline when computation is not a limitation. However, the online
  deployment of these algorithms has posed more challenges
  (<xref alt="Jamieson et al., 2015" rid="ref-next" ref-type="bibr">Jamieson
  et al., 2015</xref>).</p>
</sec>
<sec id="related-work">
  <title>Related work</title>
  <p>Systems to deploy active machine learning (ML) algorithms to
  crowdsourcing audiences include SMART
  (<xref alt="Chew et al., 2019" rid="ref-smart" ref-type="bibr">Chew et
  al., 2019</xref>), NEXT
  (<xref alt="Jamieson et al., 2015" rid="ref-next" ref-type="bibr">Jamieson
  et al., 2015</xref>) and Microsoft‚Äôs Multiworld Testing Decision
  Service
  (<xref alt="Agarwal et al., 2016" rid="ref-agarwal2016multiworld" ref-type="bibr">Agarwal
  et al., 2016</xref>). The most relevant related work, NEXT is capable
  of serving triplet queries to crowdsourcing participants
  (<xref alt="Jamieson et al., 2015" rid="ref-next" ref-type="bibr">Jamieson
  et al., 2015</xref>). In this work the authors concluded that ‚Äúthere
  is no evidence for gains from adaptive sampling.‚Äù However, other work
  has found gains from adaptive sampling when computation is not a
  priority
  (<xref alt="Heim et al., 2015b" rid="ref-tackl" ref-type="bibr">Heim
  et al., 2015b</xref>).</p>
  <p>Several active algorithms for triplet embedding have been developed
  (<xref alt="Tamuz et al., 2011" rid="ref-ckl" ref-type="bibr">Tamuz et
  al., 2011</xref>;
  <xref alt="Van Der Maaten &amp; Weinberger, 2012" rid="ref-ste" ref-type="bibr">Van
  Der Maaten &amp; Weinberger, 2012</xref>). These algorithms require
  searching queries and fitting the responses to the underlying noise
  model. With a naive computation, scoring a single query requires
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(nd)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  floating point operations (FLOPs), and the embedding typically
  requires significant computation
  (<xref alt="Ma et al., 2021" rid="ref-ma2019fast" ref-type="bibr">Ma
  et al., 2021</xref>;
  <xref alt="Vankadara et al., 2019" rid="ref-soe" ref-type="bibr">Vankadara
  et al., 2019</xref>), though some work has been done to reduce the
  amount of computation
  (<xref alt="Heim et al., 2015a" rid="ref-erkle" ref-type="bibr">Heim
  et al., 2015a</xref>).</p>
</sec>
<sec id="design-goals">
  <title>Design goals</title>
  <p>Salmon‚Äôs main design goals are below:</p>
  <list list-type="order">
    <list-item>
      <p>Generate accurate relative similarity embeddings.</p>
    </list-item>
    <list-item>
      <p>Require fewer responses than random sampling to generate an
      embedding.</p>
    </list-item>
    <list-item>
      <p>Allow experimentalists to easily achieve both items above.</p>
    </list-item>
  </list>
  <p>One method to achieve goal (2) above is to use an active machine
  learning (ML) sampling algorithm. This task requires considering how
  to create a responsive query page with a service to run active ML
  algorithms. The result is a frontend server that
  <italic>serves</italic> queries and <italic>receives</italic> answers,
  and a backend server that <italic>searches</italic> queries and
  <italic>processes</italic> answers ‚Äì notably, not the same data flow
  that NEXT has
  (<xref alt="Jamieson et al., 2015" rid="ref-next" ref-type="bibr">Jamieson
  et al., 2015</xref>), though it is common in other systems
  (<xref alt="Agarwal et al., 2016" rid="ref-agarwal2016multiworld" ref-type="bibr">Agarwal
  et al., 2016</xref>;
  <xref alt="Chew et al., 2019" rid="ref-smart" ref-type="bibr">Chew et
  al., 2019</xref>).</p>
  <p>To verify goal (2), extensive crowdsourcing experiments and
  simulations have been run, and have compared with the most relevant
  work
  (<xref alt="Jamieson et al., 2015" rid="ref-next" ref-type="bibr">Jamieson
  et al., 2015</xref>). In this, Salmon‚Äôs architecture required
  modification of the query search algorithm to circumvent some
  experimental design issues. With these modifications, we have observed
  active ML algorithm gains in extensive experiments and simulations. To
  the best of the author‚Äôs knowledge, this is a novel achievement in the
  crowdsourcing context.</p>
  <p>Goal (1) is aided by the fact that Salmon integrates a popular deep
  learning framework, PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>). This allows for easy customization of the
  underlying optimization method during both online and offline
  computation, including by the experimentalist managing Salmon if so
  desired.</p>
  <p>Goal (3) is enabled by a relatively simple launch through Amazon
  AWS using Amazon Machine Images
  (AMIs).<xref ref-type="fn" rid="fn1">1</xref> The AMI for
  Salmon<xref ref-type="fn" rid="fn2">2</xref> pulls the latest release
  of Salmon from GitHub and then launches Salmon. After some other tasks
  (e.g., opening ports, etc), Salmon is ready be launched. Salmon
  requires fairly minimal computational resources; all the experiments
  and simulation were performed with <monospace>t3.xlarge</monospace>
  Amazon EC2 instance, which has 4 cores, 16GB of memory and costs about
  $3.98 per day.</p>
  <p>After launch, Salmon can start an experiment with stimuli
  consisting of text, images, video or HTML strings. It provides a
  mechanism to monitor an ongoing experiment, which includes the
  following information:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Basic experiment statistics:</bold> number of unique
      users, launch date, etc.</p>
    </list-item>
    <list-item>
      <p><bold>Server performance:</bold> processing time for different
      endpoints, rate responses received, etc.</p>
    </list-item>
    <list-item>
      <p><bold>Client timings,</bold> including response and new query
      latency.</p>
    </list-item>
    <list-item>
      <p><bold>Embedding visualization</bold> and a list of targets in
      the embedding.</p>
    </list-item>
  </list>
  <p>In addition, Salmon provides links to download the responses and
  configuration. Salmon also supports experiment persistence through
  downloading and uploading experiments. The embedding that Salmon
  generates can be downloaded, at least if active samplers are used.
  Regardless of the sampler used, Salmon can be used to generate the
  embeddings offline from the downloaded responses.</p>
</sec>
<sec id="uses">
  <title>Uses</title>
  <p>Salmon has been used by several groups, including psychologists at
  the University of Wisconsin‚ÄìMadison and the Louisiana State
  University.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This work has been supported by the SMART Scholarship Program.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-erkle">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Heim</surname><given-names>Eric</given-names></name>
        <name><surname>Berger</surname><given-names>Matthew</given-names></name>
        <name><surname>Seversky</surname><given-names>Lee M</given-names></name>
        <name><surname>Hauskrecht</surname><given-names>Milos</given-names></name>
      </person-group>
      <article-title>Efficient online relative comparison kernel learning</article-title>
      <source>Proceedings of the 2015 SIAM international conference on data mining</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.1137/1.9781611974010.31</pub-id>
      <fpage>271</fpage>
      <lpage>279</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ste">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Van Der Maaten</surname><given-names>Laurens</given-names></name>
        <name><surname>Weinberger</surname><given-names>Kilian</given-names></name>
      </person-group>
      <article-title>Stochastic triplet embedding</article-title>
      <source>2012 IEEE international workshop on machine learning for signal processing</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <uri>http://www.cs.cornell.edu/~kilian/papers/stochastictriplet.pdf</uri>
      <pub-id pub-id-type="doi">10.1109/MLSP.2012.6349720</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ckl">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tamuz</surname><given-names>Omer</given-names></name>
        <name><surname>Liu</surname><given-names>Ce</given-names></name>
        <name><surname>Belongie</surname><given-names>Serge</given-names></name>
        <name><surname>Shamir</surname><given-names>Ohad</given-names></name>
        <name><surname>Kalai</surname><given-names>Adam Tauman</given-names></name>
      </person-group>
      <article-title>Adaptively learning the crowd kernel</article-title>
      <source>Proceedings of the 28th international conference on international conference on machine learning</source>
      <publisher-name>Omnipress</publisher-name>
      <publisher-loc>Madison, WI, USA</publisher-loc>
      <year iso-8601-date="2011">2011</year>
      <isbn>9781450306195</isbn>
      <uri>http://www.icml-2011.org/papers/395_icmlpaper.pdf</uri>
      <fpage>673</fpage>
      <lpage>680</lpage>
    </element-citation>
  </ref>
  <ref id="ref-next">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Jamieson</surname><given-names>Kevin G</given-names></name>
        <name><surname>Jain</surname><given-names>Lalit</given-names></name>
        <name><surname>Fernandez</surname><given-names>Chris</given-names></name>
        <name><surname>Glattard</surname><given-names>Nicholas J.</given-names></name>
        <name><surname>Nowak</surname><given-names>Rob</given-names></name>
      </person-group>
      <article-title>NEXT: A System for Real-World Development, Evaluation, and Application of Active Learning</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Cortes</surname><given-names>C.</given-names></name>
        <name><surname>Lawrence</surname><given-names>N.</given-names></name>
        <name><surname>Lee</surname><given-names>D.</given-names></name>
        <name><surname>Sugiyama</surname><given-names>M.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <uri>https://proceedings.neurips.cc/paper/2015/file/89ae0fe22c47d374bc9350ef99e01685-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tackl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Heim</surname><given-names>Eric</given-names></name>
        <name><surname>Berger</surname><given-names>Matthew</given-names></name>
        <name><surname>Seversky</surname><given-names>Lee</given-names></name>
        <name><surname>Hauskrecht</surname><given-names>Milos</given-names></name>
      </person-group>
      <article-title>Active perceptual similarity modeling with auxiliary information</article-title>
      <source>arXiv preprint arXiv:1511.02254</source>
      <year iso-8601-date="2015">2015</year>
      <uri>https://arxiv.org/pdf/1511.02254.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-smart">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chew</surname><given-names>Rob</given-names></name>
        <name><surname>Wenger</surname><given-names>Michael</given-names></name>
        <name><surname>Kery</surname><given-names>Caroline</given-names></name>
        <name><surname>Nance</surname><given-names>Jason</given-names></name>
        <name><surname>Richards</surname><given-names>Keith</given-names></name>
        <name><surname>Hadley</surname><given-names>Emily</given-names></name>
        <name><surname>Baumgartner</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>SMART: An open source data labeling platform for supervised learning.</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2019">2019</year>
      <volume>20</volume>
      <issue>82</issue>
      <uri>http://jmlr.org/papers/v20/18-859.html</uri>
      <fpage>1</fpage>
      <lpage>5</lpage>
    </element-citation>
  </ref>
  <ref id="ref-faceverification">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sankaranarayanan</surname><given-names>Swami</given-names></name>
        <name><surname>Alavi</surname><given-names>Azadeh</given-names></name>
        <name><surname>Castillo</surname><given-names>Carlos D</given-names></name>
        <name><surname>Chellappa</surname><given-names>Rama</given-names></name>
      </person-group>
      <article-title>Triplet probabilistic embedding for face verification and clustering</article-title>
      <source>2016 IEEE 8th international conference on biometrics theory, applications and systems (BTAS)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/BTAS.2016.7791205</pub-id>
      <fpage>1</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
  <ref id="ref-vehicles">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kuma</surname><given-names>Ratnesh</given-names></name>
        <name><surname>Weill</surname><given-names>Edwin</given-names></name>
        <name><surname>Aghdasi</surname><given-names>Farzin</given-names></name>
        <name><surname>Sriram</surname><given-names>Parthasarathy</given-names></name>
      </person-group>
      <article-title>Vehicle re-identification: An efficient baseline using triplet embedding</article-title>
      <source>2019 international joint conference on neural networks (IJCNN)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1109/IJCNN.2019.8852059</pub-id>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chem">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mason</surname><given-names>Blake</given-names></name>
        <name><surname>Rau</surname><given-names>Martina A</given-names></name>
        <name><surname>Nowak</surname><given-names>Robert</given-names></name>
      </person-group>
      <article-title>Cognitive task analysis for implicit knowledge about visual representations with similarity learning methods</article-title>
      <source>Cognitive science</source>
      <year iso-8601-date="2019">2019</year>
      <volume>43</volume>
      <pub-id pub-id-type="doi">10.1111/cogs.12744</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-agarwal2016multiworld">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Agarwal</surname><given-names>Alekh</given-names></name>
        <name><surname>Bird</surname><given-names>Sarah</given-names></name>
        <name><surname>Cozowicz</surname><given-names>Markus</given-names></name>
        <name><surname>Hoang</surname><given-names>Luong</given-names></name>
        <name><surname>Langford</surname><given-names>John</given-names></name>
        <name><surname>Lee</surname><given-names>Stephen</given-names></name>
        <name><surname>Li</surname><given-names>Jiaji</given-names></name>
        <name><surname>Melamed</surname><given-names>Dan</given-names></name>
        <name><surname>Oshri</surname><given-names>Gal</given-names></name>
        <name><surname>Ribas</surname><given-names>Oswaldo</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Making contextual decisions with low technical debt</article-title>
      <source>arXiv preprint arXiv:1606.03966</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/pdf/1606.03966.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-ma2019fast">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ma</surname><given-names>Ke</given-names></name>
        <name><surname>Zeng</surname><given-names>Jinshan</given-names></name>
        <name><surname>Xiong</surname><given-names>Jiechao</given-names></name>
        <name><surname>Xu</surname><given-names>Qianqian</given-names></name>
        <name><surname>Cao</surname><given-names>Xiaochun</given-names></name>
        <name><surname>Liu</surname><given-names>Wei</given-names></name>
        <name><surname>Yao</surname><given-names>Yuan</given-names></name>
      </person-group>
      <article-title>Fast stochastic ordinal embedding with variance reduction and adaptive step size</article-title>
      <source>IEEE Transactions on Knowledge and Data Engineering</source>
      <year iso-8601-date="2021">2021</year>
      <volume>33</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1109/TKDE.2019.2956700</pub-id>
      <fpage>2467</fpage>
      <lpage>2478</lpage>
    </element-citation>
  </ref>
  <ref id="ref-soe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vankadara</surname><given-names>Leena Chennuru</given-names></name>
        <name><surname>Haghiri</surname><given-names>Siavash</given-names></name>
        <name><surname>Lohaus</surname><given-names>Michael</given-names></name>
        <name><surname>Wahab</surname><given-names>Faiz Ul</given-names></name>
        <name><surname>Luxburg</surname><given-names>Ulrike von</given-names></name>
      </person-group>
      <article-title>Insights into ordinal embedding algorithms: A systematic evaluation</article-title>
      <source>arXiv preprint arXiv:1912.01666</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1912.01666</uri>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlch√©-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <uri>https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>A local install is available, and only requires
    Docker. Collection of crowdsourced responses will require running a
    web server or collecting in-person responses (though a local install
    may be useful for development).</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Details are at
    <ext-link ext-link-type="uri" xlink:href="https://docs.stsievert.com/salmon/installation">https://docs.stsievert.com/salmon/installation</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
