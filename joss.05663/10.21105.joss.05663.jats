<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5663</article-id>
<article-id pub-id-type="doi">10.21105/joss.05663</article-id>
<title-group>
<article-title>samgeo: A Python package for segmenting geospatial data
with the Segment Anything Model (SAM)</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5437-4073</contrib-id>
<name>
<surname>Wu</surname>
<given-names>Qiusheng</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0258-536X</contrib-id>
<name>
<surname>Osco</surname>
<given-names>Lucas Prado</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Geography &amp; Sustainability, University of
Tennessee, Knoxville, TN 37996, United States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Faculty of Engineering and Architecture and Urbanism,
University of Western São Paulo, Rod. Raposo Tavares, km 572 - Limoeiro,
Pres. Prudente 19067-175, SP, Brazil</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-05-18">
<day>18</day>
<month>5</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>89</issue>
<fpage>5663</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>geospatial</kwd>
<kwd>segment anything</kwd>
<kwd>deep learning</kwd>
<kwd>satellite</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Segment-Geospatial (samgeo) is an open-source Python package
  designed to simplify the process of segmenting geospatial data with
  the Segment Anything Model
  (<xref alt="Kirillov et al., 2023" rid="ref-Kirillov2023" ref-type="bibr">Kirillov
  et al., 2023</xref>). The package leverages popular Python libraries,
  such as leafmap
  (<xref alt="Wu, 2021" rid="ref-Wu2021" ref-type="bibr">Wu,
  2021</xref>), ipywidgets
  (<xref alt="Grout et al., 2021" rid="ref-Grout2021" ref-type="bibr">Grout
  et al., 2021</xref>), rasterio
  (<xref alt="Gillies et al., 2013" rid="ref-Gillies2013" ref-type="bibr">Gillies
  et al., 2013</xref>), geopandas
  (<xref alt="Jordahl et al., 2021" rid="ref-Jordahl2021" ref-type="bibr">Jordahl
  et al., 2021</xref>), and segment-anything-py
  (<xref alt="Wu, 2023" rid="ref-Wu2023" ref-type="bibr">Wu,
  2023</xref>), to provide a straightforward interface for users to
  segment remote sensing imagery and export the results in various
  formats, including vector and raster data. The segmentation can be run
  automatically, interactively in a graphical user interface (GUI), or
  by text prompts built upon Grounding DINO
  (<xref alt="Liu et al., 2023" rid="ref-liu2023" ref-type="bibr">Liu et
  al., 2023</xref>). However, it’s worth noting that the text prompt
  approach has its limitations, which may require parameter fine-tuning.
  Additionally, the package provides functionality for downloading
  remote sensing imagery and visualizing segmentation results
  interactively in a Jupyter environment. Segment-Geospatial aims to
  fill the gap in the Python ecosystem by providing a user-friendly,
  efficient, and flexible geospatial segmentation tool without the need
  for training deep learning models.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Image segmentation is a critical task in geospatial analysis as it
  enables the identification and extraction of relevant features from
  satellite or aerial imagery. By segmenting an image into meaningful
  regions, it becomes possible to extract information about the spatial
  distribution and characteristics of various objects and land cover
  types. This information can then be used to support decision-making in
  a wide range of fields, from agriculture and forestry to environmental
  monitoring and national security.</p>
  <p>Traditionally, image segmentation has been performed using manual
  or semi-automatic methods, which are time-consuming and
  labor-intensive. In recent years, deep learning models have been
  developed to automate the segmentation process. However, these models
  generally require large amounts of training data and are
  computationally expensive, making them impractical for many
  applications. The Segment Anything Model (SAM)
  (<xref alt="Kirillov et al., 2023" rid="ref-Kirillov2023" ref-type="bibr">Kirillov
  et al., 2023</xref>) recently released by Meta AI is a promptable
  segmentation system with zero-shot generalization to unfamiliar
  objects and images, without the need for additional training. The
  model was trained on 11 million images with over 1 billion masks.
  Users can segment any object in any image using existing model
  checkpoints without the need for additional training. Since its
  release in April 2023, there has been a plethora of applications of
  SAM in various fields
  (<xref alt="Ji et al., 2023" rid="ref-Ji2023" ref-type="bibr">Ji et
  al., 2023</xref>;
  <xref alt="Zhang et al., 2023" rid="ref-Zhang2023" ref-type="bibr">Zhang
  et al., 2023</xref>), such as medical imaging
  (<xref alt="Ma &amp; Wang, 2023" rid="ref-Ma2023" ref-type="bibr">Ma
  &amp; Wang, 2023</xref>). Currently, there are few Python packages
  available on PyPI and conda-forge for segmenting images in the
  geospatial domain with SAM. Therefore, we developed the
  Segment-Geospatial Python package to fill this gap and provide a
  low-code and no-code solution for segmenting geospatial data with
  SAM.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The segment-geospatial package draws its inspiration from
  segment-anything-eo repository
  (<xref alt="Hancharenka, 2023" rid="ref-Hancharenka2023" ref-type="bibr">Hancharenka,
  2023</xref>). We thank the author Aliaksandr Hancharenka for his
  excellent work. We also thank the authors of the Segment Anything
  Model
  (<xref alt="Kirillov et al., 2023" rid="ref-Kirillov2023" ref-type="bibr">Kirillov
  et al., 2023</xref>) for making the model available to the public.</p>
  <p>This project is based upon work partially supported by the National
  Aeronautics and Space Administration (NASA) under Grant
  No. 80NSSC22K1742 issued through the
  <ext-link ext-link-type="uri" xlink:href="https://bit.ly/3RVBRcQ">Open
  Source Tools, Frameworks, and Libraries 2020 Program</ext-link>. This
  project is also supported by Amazon Web Services
  (<ext-link ext-link-type="uri" xlink:href="https://aws.amazon.com/">AWS</ext-link>).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Jordahl2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Jordahl</surname><given-names>Kelsey</given-names></name>
        <name><surname>Van den Bossche</surname><given-names>Joris</given-names></name>
        <name><surname>Fleischmann</surname><given-names>Martin</given-names></name>
        <name><surname>McBride</surname><given-names>James</given-names></name>
        <name><surname>Wasserman</surname><given-names>Jacob</given-names></name>
        <name><surname>Gerard</surname><given-names>Jeffrey</given-names></name>
      </person-group>
      <article-title>geopandas/geopandas: v0.9.0</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://zenodo.org/record/4569086</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.4569086</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Grout2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Grout</surname><given-names>Jason</given-names></name>
        <name><surname>Frederic</surname><given-names>Jonathan</given-names></name>
        <name><surname>Corlay</surname><given-names>Sylvain</given-names></name>
        <name><surname>Bugnion</surname><given-names>Pascal</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>ipywidgets: Interactive HTML Widgets</article-title>
      <publisher-name>Github</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/jupyter-widgets/ipywidgets</uri>
    </element-citation>
  </ref>
  <ref id="ref-Kirillov2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kirillov</surname><given-names>Alexander</given-names></name>
        <name><surname>Mintun</surname><given-names>Eric</given-names></name>
        <name><surname>Ravi</surname><given-names>Nikhila</given-names></name>
        <name><surname>Mao</surname><given-names>Hanzi</given-names></name>
        <name><surname>Rolland</surname><given-names>Chloe</given-names></name>
        <name><surname>Gustafson</surname><given-names>Laura</given-names></name>
        <name><surname>Xiao</surname><given-names>Tete</given-names></name>
        <name><surname>Whitehead</surname><given-names>Spencer</given-names></name>
        <name><surname>Berg</surname><given-names>Alexander C</given-names></name>
        <name><surname>Lo</surname><given-names>Wan-Yen</given-names></name>
        <name><surname>Dollár</surname><given-names>Piotr</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
      </person-group>
      <article-title>Segment Anything</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>http://arxiv.org/abs/2304.02643</uri>
    </element-citation>
  </ref>
  <ref id="ref-Wu2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Qiusheng</given-names></name>
      </person-group>
      <article-title>Leafmap: A Python package for interactive mapping and geospatial analysis with minimal coding in a Jupyter environment</article-title>
      <source>Journal of open source software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>63</issue>
      <issn>2475-9066</issn>
      <uri>https://joss.theoj.org/papers/10.21105/joss.03414</uri>
      <pub-id pub-id-type="doi">10.21105/joss.03414</pub-id>
      <fpage>3414</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Gillies2013">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gillies</surname><given-names>Sean</given-names></name>
        <name><surname>Ward</surname><given-names>B</given-names></name>
        <name><surname>Petersen</surname><given-names>AS</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Rasterio: Geospatial raster i/o for python programmers</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2013">2013</year>
      <uri>https://github.com/rasterio/rasterio</uri>
    </element-citation>
  </ref>
  <ref id="ref-Wu2023">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Qiusheng</given-names></name>
      </person-group>
      <article-title>Segment-anything-py: An unofficial python package for meta AI’s segment anything model</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023-05">2023</year><month>05</month>
      <uri>https://doi.org/10.5281/zenodo.7948229</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.7948229</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Zhang2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Chunhui</given-names></name>
        <name><surname>Liu</surname><given-names>Li</given-names></name>
        <name><surname>Cui</surname><given-names>Yawen</given-names></name>
        <name><surname>Huang</surname><given-names>Guanjie</given-names></name>
        <name><surname>Lin</surname><given-names>Weilin</given-names></name>
        <name><surname>Yang</surname><given-names>Yiqian</given-names></name>
        <name><surname>Hu</surname><given-names>Yuehong</given-names></name>
      </person-group>
      <article-title>A comprehensive survey on segment anything model for vision and beyond</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://doi.org/10.48550/arXiv.2305.08196</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2305.08196</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Ma2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ma</surname><given-names>Jun</given-names></name>
        <name><surname>Wang</surname><given-names>Bo</given-names></name>
      </person-group>
      <article-title>Segment anything in medical images</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://doi.org/10.48550/arXiv.2304.12306</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2304.12306</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Hancharenka2023">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Hancharenka</surname><given-names>Aliaksandr</given-names></name>
      </person-group>
      <article-title>Segment-anything-eo: Earth observation tools for meta AI segment anything</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/aliaksandr960/segment-anything-eo</uri>
    </element-citation>
  </ref>
  <ref id="ref-Ji2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ji</surname><given-names>Wei</given-names></name>
        <name><surname>Li</surname><given-names>Jingjing</given-names></name>
        <name><surname>Bi</surname><given-names>Qi</given-names></name>
        <name><surname>Li</surname><given-names>Wenbo</given-names></name>
        <name><surname>Cheng</surname><given-names>Li</given-names></name>
      </person-group>
      <article-title>Segment anything is not always perfect: An investigation of SAM on different real-world applications</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://doi.org/10.48550/arXiv.2304.05750</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2304.05750</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-liu2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Shilong</given-names></name>
        <name><surname>Zeng</surname><given-names>Zhaoyang</given-names></name>
        <name><surname>Ren</surname><given-names>Tianhe</given-names></name>
        <name><surname>Li</surname><given-names>Feng</given-names></name>
        <name><surname>Zhang</surname><given-names>Hao</given-names></name>
        <name><surname>Yang</surname><given-names>Jie</given-names></name>
        <name><surname>Li</surname><given-names>Chunyuan</given-names></name>
        <name><surname>Yang</surname><given-names>Jianwei</given-names></name>
        <name><surname>Su</surname><given-names>Hang</given-names></name>
        <name><surname>Zhu</surname><given-names>Jun</given-names></name>
        <name><surname>Zhang</surname><given-names>Lei</given-names></name>
      </person-group>
      <article-title>Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2303.05499</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
