<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7602</article-id>
<article-id pub-id-type="doi">10.21105/joss.07602</article-id>
<title-group>
<article-title>FTorch: a library for coupling PyTorch models to
Fortran</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5001-4812</contrib-id>
<name>
<surname>Atkinson</surname>
<given-names>Jack</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Elafrou</surname>
<given-names>Athena</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-2015-9478</contrib-id>
<name>
<surname>Kasoar</surname>
<given-names>Elliott</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3646-091X</contrib-id>
<name>
<surname>Wallwork</surname>
<given-names>Joseph G.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1740-9550</contrib-id>
<name>
<surname>Meltzer</surname>
<given-names>Thomas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7754-504X</contrib-id>
<name>
<surname>Clifford</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7058-7842</contrib-id>
<name>
<surname>Orchard</surname>
<given-names>Dominic</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6863-2184</contrib-id>
<name>
<surname>Edsall</surname>
<given-names>Chris</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Institute of Computing for Climate Science, University of
Cambridge, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>NVIDIA, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Scientific Computing Department, Science and Technology
Facilities Council, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>University of Kent, UK</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-03-25">
<day>25</day>
<month>3</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>107</issue>
<fpage>7602</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Fortran</kwd>
<kwd>Python</kwd>
<kwd>PyTorch</kwd>
<kwd>machine learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In the last decade, machine learning (ML) and deep learning (DL)
  techniques have revolutionised many fields within science, industry,
  and beyond. Researchers across domains are increasingly seeking to
  combine ML with numerical modelling to advance research. This
  typically brings about the challenge of <italic>programming language
  interoperation</italic>. PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>) is a popular framework for designing and training
  ML/DL models whilst Fortran remains a language of choice for many
  high-performance computing (HPC) scientific models. The
  <monospace>FTorch</monospace> library provides an easy-to-use,
  performant, cross-platform method for coupling the two, allowing users
  to call PyTorch models from Fortran.</p>
  <p><monospace>FTorch</monospace> is open-source, open-development, and
  well-documented with minimal dependencies. A central tenet of its
  design, in contrast to other approaches, is that FTorch removes
  dependence on the Python runtime (and virtual environments). By
  building on the <monospace>LibTorch</monospace> backend (written in
  C++ and accessible via an API) it allows users to run ML models on
  both CPU and GPU architectures without needing to port code to
  device-specific languages.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The explosion of ML/DL has brought several promising opportunities
  to deploy these techniques in scientific research. There are notable
  applications in the physical sciences
  (<xref alt="Carleo et al., 2019" rid="ref-carleo2019machine" ref-type="bibr">Carleo
  et al., 2019</xref>), climate science
  (<xref alt="Kashinath et al., 2021" rid="ref-kashinath2021physics" ref-type="bibr">Kashinath
  et al., 2021</xref>), and materials science
  (<xref alt="Bishara et al., 2023" rid="ref-bishara2023state" ref-type="bibr">Bishara
  et al., 2023</xref>). Common applications include the emulation of
  computationally intensive processes and the development of data-driven
  components. Such deployments of ML can achieve improved computational
  and/or predictive performance, compared to traditional numerical
  techniques. A common example from the geosciences is ML
  parameterisation of subgrid processes — a major source of uncertainty
  in many models (e.g.
  <xref alt="Bony et al., 2015" rid="ref-bony2015clouds" ref-type="bibr">Bony
  et al., 2015</xref>;
  <xref alt="Rasp et al., 2018" rid="ref-rasp2018deep" ref-type="bibr">Rasp
  et al., 2018</xref>).</p>
  <p>Fortran is widely used for scientific codes due to its performance,
  stability, array-oriented design, and native support for shared and
  distributed memory, amongst other features
  (<xref alt="Kedward et al., 2022" rid="ref-kedward2022state" ref-type="bibr">Kedward
  et al., 2022</xref>). Many ML frameworks, on the other hand, are
  accessed using Python. The commonly-used PyTorch framework allows
  users to design and deploy ML models with many advanced features.</p>
  <p>Ideally, users would develop and validate ML models in the PyTorch
  environment before deploying them into a scientific model. This
  deployment should require minimal additional code, and guarantee
  identical results as obtained with the PyTorch interface — something
  not guaranteed if re-implementing by hand in Fortran. Ideally one
  would call out, from Fortran, to an ML model saved from PyTorch, with
  the results returned directly to the scientific code.</p>
  <p><monospace>FTorch</monospace> bridges this gap, reducing the burden
  on researchers seeking to incorporate ML into their numerical models.
  It provides an intuitive and user-friendly interface from Fortran to
  ML models developed using PyTorch. It removes the need for detailed
  knowledge about language interoperation and the need to develop
  bespoke coupling code, instead providing a Fortran interface designed
  to be familiar to both PyTorch and Fortran users.</p>
  <p>Having no dependence on the Python runtime,
  <monospace>FTorch</monospace> avoids the incurred overhead of object
  representations in Python’s runtime and is appealing for HPC systems
  where the management of Python environments can be challenging.</p>
</sec>
<sec id="software-description">
  <title>Software description</title>
  <p><monospace>FTorch</monospace> is a Fortran wrapper to the
  <monospace>LibTorch</monospace> C++ framework using the
  <monospace>iso_c_binding</monospace> module, intrinsic to Fortran
  since the 2003 standard This enables shared memory use (where
  possible) to maximise efficiency by reducing data-transfer during
  coupling<xref ref-type="fn" rid="fn1">1</xref> and avoids any use of
  Python at runtime. PyTorch types are represented through derived types
  in <monospace>FTorch</monospace>, with Tensors supported across a
  range of data types and ranks using the <monospace>fypp</monospace>
  preprocessor
  (<xref alt="Aradi, 2024" rid="ref-fypp" ref-type="bibr">Aradi,
  2024</xref>).</p>
  <p>We utilise the existing support in <monospace>LibTorch</monospace>
  for GPU acceleration without additional device-specific code.
  <monospace>torch_tensor</monospace>s are targeted to a device through
  a <monospace>device_type</monospace> enum, currently supporting CPU,
  CUDA, XPU, and MPS. Multiple GPUs may be targeted through the optional
  <monospace>device_index</monospace> argument.</p>
  <p>Typically, users train a model in PyTorch and save it as
  TorchScript, a strongly-typed subset of Python. Once saved, the
  Torchscript model can be loaded from Fortran using
  <monospace>FTorch</monospace> and run via the
  <monospace>LibTorch</monospace> backend. The library comes with a
  utility script (<monospace>pt2ts.py</monospace>) to assist with the
  process of saving models as well as a comprehensive set of examples
  guiding users through complete Python to Fortran workflows. A focus on
  user experience underpins the development and is a key aspect behind
  the adoption of <monospace>FTorch</monospace> by various scientific
  communities.</p>
  <p>Full details, including user guide, API documentation, slides and
  videos, and links to projects is available at
  <ext-link ext-link-type="uri" xlink:href="https://cambridge-iccs.github.io/FTorch">https://cambridge-iccs.github.io/FTorch</ext-link>.</p>
</sec>
<sec id="comparison-to-other-approaches">
  <title>Comparison to other approaches</title>
  <list list-type="bullet">
    <list-item>
      <p><bold>Replicating a net in Fortran</bold>
      That is, taking a model developed in PyTorch and reimplementing it
      using only Fortran, loading saved weights from file. This is
      likely to require considerable development effort, re-writing code
      that already exists and missing opportunities to use the diverse
      and highly-optimised features of Torch. Re-implementation can be a
      source of error, requiring additional testing to ensure
      correctness.
      If the overall goal is to incorporate ML into Fortran, rather than
      using PyTorch specifically, then another approach is to leverage a
      Fortran-based ML framework such as
      <ext-link ext-link-type="uri" xlink:href="https://github.com/modern-fortran/neural-fortran">neural-fortran</ext-link>
      (<xref alt="Curcic, 2019" rid="ref-curcic2019parallel" ref-type="bibr">Curcic,
      2019</xref>). Whilst it does not allow interaction with PyTorch,
      neural-fortran provides many neural network components for
      building nets directly in Fortran. However, the set of features is
      not as rich as PyTorch and GPU offloading is not currently
      supported.
      The
      <ext-link ext-link-type="uri" xlink:href="https://berkeleylab.github.io/fiats">Fiats</ext-link>
      (`Functional Inference And Training for Surrogates’) library
      (<xref alt="Rouson &amp; Rasmussen, 2024" rid="ref-fiats" ref-type="bibr">Rouson
      &amp; Rasmussen, 2024</xref>) is another approach for developing,
      training, and deploying ML models directly in Fortran, with
      experimental GPU support at present.</p>
    </list-item>
    <list-item>
      <p><bold>Forpy</bold>
      (<xref alt="Rabel, 2020" rid="ref-forpy" ref-type="bibr">Rabel,
      2020</xref>)
      Forpy is a Fortran module that provides access to Python data
      structures (including <monospace>numpy</monospace> arrays) for
      interoperability. Whilst this provides wider access to general
      Python features it has a challenging interface with more
      boilerplate. It also requires access to the Python runtime from
      Fortran.</p>
    </list-item>
    <list-item>
      <p><bold>TorchFort</bold>
      (<xref alt="NVIDIA, 2024" rid="ref-torchfort" ref-type="bibr">NVIDIA,
      2024</xref>)
      Since we began <monospace>FTorch</monospace> NVIDIA has released
      <monospace>TorchFort</monospace>. This has a similar approach to
      <monospace>FTorch</monospace>, avoiding Python to link against the
      <monospace>LibTorch</monospace> backend. It has a focus on
      enabling GPU deployment on NVIDIA hardware.</p>
    </list-item>
    <list-item>
      <p><bold>fortran-tf-lib</bold>
      (<xref alt="Cambridge-ICCS, 2023" rid="ref-fortran-tf-lib" ref-type="bibr">Cambridge-ICCS,
      2023</xref>)
      Whilst <monospace>FTorch</monospace> allows coupling of PyTorch
      models to Fortran, some use TensorFlow
      (<xref alt="Abadi et al., 2015" rid="ref-Abadi_TensorFlow_Large-scale_machine_2015" ref-type="bibr">Abadi
      et al., 2015</xref>) to develop and train models. These can be
      coupled to Fortran in a similar manner to
      <monospace>FTorch</monospace> by using
      <monospace>fortran-tf-lib</monospace>. Whilst it provides an
      alternative solution, the library is less rich in features and
      software sustainability than <monospace>FTorch</monospace>.</p>
    </list-item>
    <list-item>
      <p><bold>SmartSim</bold>
      (<xref alt="Partee et al., 2022" rid="ref-partee2022using" ref-type="bibr">Partee
      et al., 2022</xref>)
      SmartSim is a workflow library developed by HPE and built upon
      Redis API. It provides a framework for launching ML and HPC
      workloads transferring data between the two via a database. This
      is a versatile approach that can work with a variety of languages
      and ML frameworks. However, it has a significant learning curve,
      incurs data-transfer overheads, and requires managing tasks from
      Python.</p>
    </list-item>
  </list>
</sec>
<sec id="examples-of-use">
  <title>Examples of Use</title>
  <p><monospace>FTorch</monospace> is actively used in scientific
  research:</p>
  <list list-type="bullet">
    <list-item>
      <p>in the
      <ext-link ext-link-type="uri" xlink:href="https://datawaveproject.github.io/">DataWave
      project</ext-link> exploring gravity wave drag in atmospheric
      models to:</p>
      <list list-type="bullet">
        <list-item>
          <p>couple an emulator into the MiMA model
          (<xref alt="DataWave, 2023" rid="ref-MiMAML" ref-type="bibr">DataWave,
          2023</xref>) demonstrating variability of models trained
          offline when coupled back to a host
          (<xref alt="Mansfield &amp; Sheshadri, 2024" rid="ref-mansfield2024uncertainty" ref-type="bibr">Mansfield
          &amp; Sheshadri, 2024</xref>).</p>
        </list-item>
        <list-item>
          <p>couple emulators and new data-driven parameterisations to
          the Community Atmosphere Model (CAM).</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>to couple a U-Net based model of multi-scale convection into
      <ext-link ext-link-type="uri" xlink:href="https://www.icon-model.org/">ICON</ext-link>
      (<xref alt="DKRZ, 2025" rid="ref-ICON" ref-type="bibr">DKRZ,
      2025</xref>) and demonstrate via Shapley values that non-causal
      learnt relations are more stable when running online
      (<xref alt="Heuer et al., 2024" rid="ref-heuer2024interpretable" ref-type="bibr">Heuer
      et al., 2024</xref>).</p>
    </list-item>
    <list-item>
      <p>As part of
      <ext-link ext-link-type="uri" xlink:href="https://www.cesm.ucar.edu/">CESM</ext-link>
      (the Community Earth System Model)
      (<xref alt="NCAR, 2025" rid="ref-CESM" ref-type="bibr">NCAR,
      2025</xref>) working to provide a general approach for researchers
      to couple ML models to the various components of the model
      suite.</p>
    </list-item>
  </list>
</sec>
<sec id="future-development">
  <title>Future development</title>
  <p>Recent work in scientific domains suggests that online training is
  likely important for long-term stability of hybrid models
  (<xref alt="Brenowitz et al., 2020" rid="ref-brenowitz2020machine" ref-type="bibr">Brenowitz
  et al., 2020</xref>). We therefore plan to extend FTorch to expose
  PyTorch’s autograd functionality to support this.</p>
  <p>We welcome feature requests and are open to discussion and
  collaboration.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This project is supported by Schmidt Sciences, LLC. We also thank
  the Institute of Computing for Climate Science for their support.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Abadi_TensorFlow_Large-scale_machine_2015">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Mané</surname><given-names>Dan</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow, Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015-11">2015</year><month>11</month>
      <pub-id pub-id-type="doi">10.5281/zenodo.4724125</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bishara2023state">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bishara</surname><given-names>Dana</given-names></name>
        <name><surname>Xie</surname><given-names>Yuxi</given-names></name>
        <name><surname>Liu</surname><given-names>Wing Kam</given-names></name>
        <name><surname>Li</surname><given-names>Shaofan</given-names></name>
      </person-group>
      <article-title>A state-of-the-art review on machine learning-based multiscale modeling, simulation, homogenization and design of materials</article-title>
      <source>Archives of computational methods in engineering</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>30</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1007/s11831-022-09795-8</pub-id>
      <fpage>191</fpage>
      <lpage>222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fiats">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Rouson</surname><given-names>Damien</given-names></name>
        <name><surname>Rasmussen</surname><given-names>Katherine</given-names></name>
      </person-group>
      <article-title>Fiats: Functional inference and training for surrogates</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/BerkeleyLab/fiats</uri>
    </element-citation>
  </ref>
  <ref id="ref-fortran-tf-lib">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Cambridge-ICCS</surname></name>
      </person-group>
      <article-title>Fortran-tf-lib</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/Cambridge-ICCS/fortran-tf-lib</uri>
    </element-citation>
  </ref>
  <ref id="ref-fypp">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Aradi</surname><given-names>Bálint</given-names></name>
      </person-group>
      <article-title>Fypp</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://fypp.readthedocs.io</uri>
    </element-citation>
  </ref>
  <ref id="ref-kashinath2021physics">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kashinath</surname><given-names>Karthik</given-names></name>
        <name><surname>Mustafa</surname><given-names>M</given-names></name>
        <name><surname>Albert</surname><given-names>Adrian</given-names></name>
        <name><surname>Wu</surname><given-names>JL</given-names></name>
        <name><surname>Jiang</surname><given-names>C</given-names></name>
        <name><surname>Esmaeilzadeh</surname><given-names>Soheil</given-names></name>
        <name><surname>Azizzadenesheli</surname><given-names>Kamyar</given-names></name>
        <name><surname>Wang</surname><given-names>R</given-names></name>
        <name><surname>Chattopadhyay</surname><given-names>A</given-names></name>
        <name><surname>Singh</surname><given-names>A</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Physics-informed machine learning: Case studies for weather and climate modelling</article-title>
      <source>Philosophical Transactions of the Royal Society A</source>
      <publisher-name>The Royal Society Publishing</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>379</volume>
      <issue>2194</issue>
      <pub-id pub-id-type="doi">10.1098/rsta.2020.0093</pub-id>
      <fpage>20200093</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kedward2022state">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kedward</surname><given-names>Laurence J</given-names></name>
        <name><surname>Aradi</surname><given-names>Bálint</given-names></name>
        <name><surname>Čertı́k</surname><given-names>Ondřej</given-names></name>
        <name><surname>Curcic</surname><given-names>Milan</given-names></name>
        <name><surname>Ehlert</surname><given-names>Sebastian</given-names></name>
        <name><surname>Engel</surname><given-names>Philipp</given-names></name>
        <name><surname>Goswami</surname><given-names>Rohit</given-names></name>
        <name><surname>Hirsch</surname><given-names>Michael</given-names></name>
        <name><surname>Lozada-Blanco</surname><given-names>Asdrubal</given-names></name>
        <name><surname>Magnin</surname><given-names>Vincent</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>The state of Fortran</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>24</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2022.3159862</pub-id>
      <fpage>63</fpage>
      <lpage>72</lpage>
    </element-citation>
  </ref>
  <ref id="ref-carleo2019machine">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Carleo</surname><given-names>Giuseppe</given-names></name>
        <name><surname>Cirac</surname><given-names>Ignacio</given-names></name>
        <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
        <name><surname>Daudet</surname><given-names>Laurent</given-names></name>
        <name><surname>Schuld</surname><given-names>Maria</given-names></name>
        <name><surname>Tishby</surname><given-names>Naftali</given-names></name>
        <name><surname>Vogt-Maranto</surname><given-names>Leslie</given-names></name>
        <name><surname>Zdeborová</surname><given-names>Lenka</given-names></name>
      </person-group>
      <article-title>Machine learning and the physical sciences</article-title>
      <source>Reviews of Modern Physics</source>
      <publisher-name>APS</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>91</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1103/RevModPhys.91.045002</pub-id>
      <fpage>045002</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-MiMAML">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <string-name>DataWave</string-name>
      </person-group>
      <article-title>MiMA machine learning</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/DataWaveProject/MiMA-machine-learning</uri>
    </element-citation>
  </ref>
  <ref id="ref-mansfield2024uncertainty">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mansfield</surname><given-names>Laura A</given-names></name>
        <name><surname>Sheshadri</surname><given-names>Aditi</given-names></name>
      </person-group>
      <article-title>Uncertainty quantification of a machine learning subgrid-scale parameterization for atmospheric gravity waves</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>16</volume>
      <issue>7</issue>
      <pub-id pub-id-type="doi">10.1029/2024MS004292</pub-id>
      <fpage>e2024MS004292</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-rasp2018deep">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rasp</surname><given-names>Stephan</given-names></name>
        <name><surname>Pritchard</surname><given-names>Michael S</given-names></name>
        <name><surname>Gentine</surname><given-names>Pierre</given-names></name>
      </person-group>
      <article-title>Deep learning to represent subgrid processes in climate models</article-title>
      <source>Proceedings of the national academy of sciences</source>
      <publisher-name>National Academy of Sciences</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>115</volume>
      <issue>39</issue>
      <pub-id pub-id-type="doi">10.1073/pnas.1810286115</pub-id>
      <fpage>9684</fpage>
      <lpage>9689</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bony2015clouds">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bony</surname><given-names>Sandrine</given-names></name>
        <name><surname>Stevens</surname><given-names>Bjorn</given-names></name>
        <name><surname>Frierson</surname><given-names>Dargan MW</given-names></name>
        <name><surname>Jakob</surname><given-names>Christian</given-names></name>
        <name><surname>Kageyama</surname><given-names>Masa</given-names></name>
        <name><surname>Pincus</surname><given-names>Robert</given-names></name>
        <name><surname>Shepherd</surname><given-names>Theodore G</given-names></name>
        <name><surname>Sherwood</surname><given-names>Steven C</given-names></name>
        <name><surname>Siebesma</surname><given-names>A Pier</given-names></name>
        <name><surname>Sobel</surname><given-names>Adam H</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Clouds, circulation and climate sensitivity</article-title>
      <source>Nature Geoscience</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>8</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1038/ngeo2398</pub-id>
      <fpage>261</fpage>
      <lpage>268</lpage>
    </element-citation>
  </ref>
  <ref id="ref-forpy">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>Rabel</surname><given-names>Elias</given-names></name>
      </person-group>
      <article-title>Forpy</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/ylikx/forpy</uri>
    </element-citation>
  </ref>
  <ref id="ref-torchfort">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>NVIDIA</surname></name>
      </person-group>
      <article-title>TorchFort</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://nvidia.github.io/TorchFort/</uri>
    </element-citation>
  </ref>
  <ref id="ref-brenowitz2020machine">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brenowitz</surname><given-names>Noah D</given-names></name>
        <name><surname>Henn</surname><given-names>Brian</given-names></name>
        <name><surname>McGibbon</surname><given-names>Jeremy</given-names></name>
        <name><surname>Clark</surname><given-names>Spencer K</given-names></name>
        <name><surname>Kwa</surname><given-names>Anna</given-names></name>
        <name><surname>Perkins</surname><given-names>W Andre</given-names></name>
        <name><surname>Watt-Meyer</surname><given-names>Oliver</given-names></name>
        <name><surname>Bretherton</surname><given-names>Christopher S</given-names></name>
      </person-group>
      <article-title>Machine learning climate model dynamics: Offline versus online performance</article-title>
      <source>arXiv preprint arXiv:2011.03081</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2011.03081</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-partee2022using">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Partee</surname><given-names>Sam</given-names></name>
        <name><surname>Ellis</surname><given-names>Matthew</given-names></name>
        <name><surname>Rigazzi</surname><given-names>Alessandro</given-names></name>
        <name><surname>Shao</surname><given-names>Andrew E</given-names></name>
        <name><surname>Bachman</surname><given-names>Scott</given-names></name>
        <name><surname>Marques</surname><given-names>Gustavo</given-names></name>
        <name><surname>Robbins</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>Using machine learning at scale in numerical simulations with SmartSim: An application to ocean climate modeling</article-title>
      <source>Journal of Computational Science</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>62</volume>
      <uri>https://www.sciencedirect.com/science/article/pii/S1877750322001065</uri>
      <pub-id pub-id-type="doi">10.1016/j.jocs.2022.101707</pub-id>
      <fpage>101707</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ICON">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>DKRZ</surname></name>
      </person-group>
      <article-title>ICON (icosahedral nonhydrostatic) model</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://www.icon-model.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-CESM">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>NCAR</surname></name>
      </person-group>
      <article-title>CESM, the community earth system model</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://www.cesm.ucar.edu/</uri>
    </element-citation>
  </ref>
  <ref id="ref-heuer2024interpretable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Heuer</surname><given-names>Helge</given-names></name>
        <name><surname>Schwabe</surname><given-names>Mierk</given-names></name>
        <name><surname>Gentine</surname><given-names>Pierre</given-names></name>
        <name><surname>Giorgetta</surname><given-names>Marco A</given-names></name>
        <name><surname>Eyring</surname><given-names>Veronika</given-names></name>
      </person-group>
      <article-title>Interpretable multiscale machine learning-based parameterizations of convection for ICON</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>16</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1029/2024MS004398</pub-id>
      <fpage>e2024MS004398</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-curcic2019parallel">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Curcic</surname><given-names>Milan</given-names></name>
      </person-group>
      <article-title>A parallel Fortran framework for neural networks and deep learning</article-title>
      <source>ACM SIGPLAN fortran forum</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>38</volume>
      <pub-id pub-id-type="doi">10.1145/3323057.3323059</pub-id>
      <fpage>4</fpage>
      <lpage>21</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>i.e. the same data in memory is used by both
    <monospace>LibTorch</monospace> and Fortran without creating a
    copy.</p>
  </fn>
</fn-group>
</back>
</article>
