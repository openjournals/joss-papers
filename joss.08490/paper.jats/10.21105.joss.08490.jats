<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8490</article-id>
<article-id pub-id-type="doi">10.21105/joss.08490</article-id>
<title-group>
<article-title>ManipulaPy: A GPU-Accelerated Python Framework for
Robotic Manipulation, Perception, and Control</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1768-2031</contrib-id>
<name>
<surname>AboElNasr</surname>
<given-names>M. I. M.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Universität Duisburg-Essen</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-05-03">
<day>3</day>
<month>5</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>114</issue>
<fpage>8490</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>robotics</kwd>
<kwd>manipulator</kwd>
<kwd>simulation</kwd>
<kwd>kinematics</kwd>
<kwd>dynamics</kwd>
<kwd>perception</kwd>
<kwd>cuda</kwd>
<kwd>trajectory-planning</kwd>
<kwd>computer-vision</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><bold>ManipulaPy</bold> is an open-source Python toolbox that
  unifies the entire manipulation pipeline—from URDF parsing to
  GPU-accelerated dynamics, vision-based perception, planning and
  control—within a single API. Built on the Product-of-Exponentials
  (PoE) model
  (<xref alt="Lynch &amp; Park, 2017" rid="ref-lynch2017modern" ref-type="bibr">Lynch
  &amp; Park, 2017</xref>), PyBullet
  (<xref alt="Coumans &amp; Bai, 2019" rid="ref-coumans2019" ref-type="bibr">Coumans
  &amp; Bai, 2019</xref>), CuPy
  (<xref alt="Okuta et al., 2017" rid="ref-cupy2021" ref-type="bibr">Okuta
  et al., 2017</xref>) and custom CUDA kernels
  (<xref alt="Liang et al., 2018" rid="ref-liang2018gpu" ref-type="bibr">Liang
  et al., 2018</xref>), the library enables researchers to move from
  robot description to real-time control with up to a 40× speedup over
  CPU implementations. DOF-agnostic GPU trajectory kernels accelerate
  6-DOF and higher manipulators, while specialized inverse-dynamics
  prototypes achieve up to 3600× speedups for batch processing.
  Performance claims are reproducible via benchmarks in the
  repository.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Modern manipulation research requires tight integration of
  geometry, dynamics, perception, planning, and control within a unified
  computational framework. However, existing open-source tools address
  only portions of this pipeline, forcing researchers to write
  substantial integration code:</p>
  <table-wrap>
    <table>
      <colgroup>
        <col width="22%" />
        <col width="32%" />
        <col width="46%" />
      </colgroup>
      <thead>
        <tr>
          <th><bold>Library</bold></th>
          <th><bold>Core Strengths</bold></th>
          <th><bold>Integration Challenges</bold></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>MoveIt
          (<xref alt="Chitta et al., 2012" rid="ref-chitta2012moveit" ref-type="bibr">Chitta
          et al., 2012</xref>)</td>
          <td>Mature sampling-based planners</td>
          <td>Custom ROS nodes for sensor integration, external plugins
          for real-time dynamics, no native GPU acceleration</td>
        </tr>
        <tr>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Pinocchio
          (<xref alt="Carpentier et al., 2025" rid="ref-Pinocchio2025" ref-type="bibr">Carpentier
          et al., 2025</xref>)</td>
          <td>High-performance PoE dynamics (C++)</td>
          <td>CPU-only; perception &amp; planning must be synchronized
          manually</td>
        </tr>
        <tr>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>CuRobo
          (<xref alt="Sundaralingam et al., 2023" rid="ref-sundaralingam2023curobo" ref-type="bibr">Sundaralingam
          et al., 2023</xref>)</td>
          <td>GPU collision checking &amp; trajectory optimization</td>
          <td>Planning-focused; lacks perception pipeline and
          closed-loop control</td>
        </tr>
        <tr>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>Python Robotics Toolbox
          (<xref alt="Corke &amp; Haviland, 2021" rid="ref-corke2021" ref-type="bibr">Corke
          &amp; Haviland, 2021</xref>)</td>
          <td>Educational algorithms, clear APIs</td>
          <td>CPU-only; users build simulation/control/vision components
          separately</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>These integration challenges manifest as sensor-planner gaps,
  dynamics-control mismatches, GPU memory fragmentation, and
  synchronization complexity between components.</p>
  <p><bold>ManipulaPy</bold> eliminates these integration burdens
  through a unified Python API that maintains data consistency across
  the entire manipulation pipeline with GPU acceleration throughout.</p>
  <fig>
    <caption><p>System architecture of ManipulaPy showing the unified
    manipulation pipeline. The framework integrates URDF processing,
    GPU-accelerated kinematics and dynamics, motion planning with
    collision avoidance, multiple control strategies, and PyBullet
    simulation within a single API. Data flows consistently between
    components without manual synchronization, while GPU acceleration
    provides 40× speedup for trajectory generation and real-time
    dynamics computation.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="system_architecture.png" />
  </fig>
</sec>
<sec id="library-architecture">
  <title>Library Architecture</title>
  <p>ManipulaPy implements a unified manipulation pipeline with coherent
  data flow where each component builds upon shared representations:</p>
  <p><bold>Robot Model Processing</bold> converts URDF descriptions into
  PoE representations, extracting screw axes, mass properties, and joint
  constraints through PyBullet integration. This creates fundamental
  <monospace>SerialManipulator</monospace> and
  <monospace>ManipulatorDynamics</monospace> objects used throughout the
  system.</p>
  <p><bold>Kinematics and Dynamics</bold> provide vectorized FK/IK,
  Jacobians, and GPU-accelerated trajectory time-scaling that is
  DOF-agnostic. GPU dynamics kernels are shape-agnostic but simplified
  (per-joint/diagonalized), while fully coupled n-DOF spatial dynamics
  remain on the CPU path for exactness.</p>
  <p><bold>Motion Planning</bold> generates collision-free trajectories
  using GPU-accelerated time-scaling functions, supporting both
  joint-space and Cartesian-space planning with real-time obstacle
  avoidance.</p>
  <p><bold>Control Systems</bold> implement classical (PID, computed
  torque) and modern (adaptive, robust) control algorithms with
  automatic gain tuning, operating on the same dynamic model used in
  planning.</p>
  <p><bold>Simulation Framework</bold> provides PyBullet integration
  with synchronized camera rendering, physics simulation, and control
  execution.</p>
</sec>
<sec id="vision-and-perception-pipeline">
  <title>Vision and Perception Pipeline</title>
  <fig>
    <caption><p>ManipulaPy vision and perception pipeline architecture.
    The five-stage pipeline processes raw sensor data from stereo
    cameras and RGB-D sensors through object detection using YOLO v8,
    transforms 2D detections to 3D world coordinates, applies DBSCAN
    clustering for object segmentation, and maintains multiple obstacle
    representations (point clouds, geometric primitives, SDFs) for robot
    integration at 5-15 Hz refresh rates during trajectory
    execution.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="vision_pipeline.png" />
  </fig>
  <p>ManipulaPy’s perception system converts raw sensor data into
  actionable robot knowledge through a five-stage pipeline:</p>
  <p><bold>Sensor Fusion</bold> handles stereo cameras (RGB+depth via
  OpenCV rectification), RGB-D sensors, and point cloud input with
  temporal alignment across sensor types.</p>
  <p><bold>Object Detection</bold> integrates YOLO v8
  (<xref alt="Jocher et al., 2023" rid="ref-Jocher_Ultralytics_YOLO_2023" ref-type="bibr">Jocher
  et al., 2023</xref>) for real-time 2D bounding box detection and
  supports custom detectors for domain-specific models.</p>
  <p><bold>3D Integration</bold> transforms pixel coordinates to 3D
  world positions using camera intrinsics, performs multi-frame fusion
  to reduce noise, and applies calibrated transforms to register sensor
  data to robot coordinates.</p>
  <p><bold>Spatial Clustering</bold> applies DBSCAN clustering
  (<xref alt="Chu et al., 2021" rid="ref-chu2021boundary" ref-type="bibr">Chu
  et al., 2021</xref>) to group 3D points using ε-neighborhoods for
  object segmentation and generates hierarchical representations.</p>
  <p><bold>Robot Integration</bold> maintains multiple obstacle
  representations simultaneously (geometric primitives, point clouds,
  SDFs) with 5–15 Hz refresh rates during trajectory execution.</p>
  <fig>
    <caption><p>GPU-accelerated trajectory execution demonstration in
    PyBullet simulation. A 6-DOF robotic manipulator executes a complex
    trajectory while avoiding dynamic obstacles in real-time. The
    trajectory planning utilizes GPU acceleration for 40× speedup over
    CPU implementation, enabling 1 kHz control rates with real-time
    collision avoidance through potential field methods integrated with
    CUDA kernels.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="manipulapy_trajectory.png" />
  </fig>
</sec>
<sec id="design-considerations">
  <title>Design Considerations</title>
  <p>ManipulaPy provides tiered functionality that scales gracefully
  from CPU-only to GPU-accelerated operation while maintaining a balance
  between performance, flexibility, and precision.
  This section outlines the computational tiers, vision dependencies,
  and design trade-offs that guided the framework’s architecture.</p>
  <sec id="cpu-only-features">
    <title>CPU-Only Features</title>
    <p>Core robotics modules include URDF processing, forward and
    inverse kinematics, Jacobian analysis, small-scale trajectory
    planning (N &lt; 1000 points), basic control, and simulation setup.
    Performance characteristics include single trajectory generation in
    <bold>~10–50 ms</bold> for 6-DOF robots and real-time control
    frequencies up to <bold>~100 Hz</bold>, primarily limited by
    Python’s Global Interpreter Lock (GIL).</p>
  </sec>
  <sec id="gpu-accelerated-features">
    <title>GPU-Accelerated Features</title>
    <p>High-performance modules support large-scale trajectory planning
    (N &gt; 1000 points) achieving up to <bold>a 40× speedup</bold>,
    batch inverse-dynamics computation, real-time control above <bold>1
    kHz</bold>, workspace analysis via Monte Carlo sampling, and
    GPU-accelerated potential fields.
    Performance typically includes large trajectory generation in
    <bold>~1–5 ms</bold> for 6-DOF manipulators, enabling real-time
    planning and control at <bold>1 kHz</bold> rates.</p>
  </sec>
  <sec id="vision-and-perception-dependencies">
    <title>Vision and Perception Dependencies</title>
    <p>Camera and perception functionality rely on <bold>OpenCV</bold>
    for camera operations, <bold>YOLO models</bold> for object
    detection, and <bold>graphics libraries</bold> for visualization.
    These dependencies support multi-camera setups, object detection,
    and spatial clustering but may require additional system-level
    libraries that are not always available in containerized
    environments.</p>
  </sec>
  <sec id="performance-and-design-trade-offs">
    <title>Performance and Design Trade-offs</title>
    <list list-type="bullet">
      <list-item>
        <p><bold>Performance Constraints</bold>: Consumer GPUs (8 GB)
        limit trajectory planning to approximately <bold>50,000
        points</bold>. GPU acceleration provides tangible benefits
        primarily when N &gt; 1000 due to kernel-launch overhead.
        CPU-only control performance is constrained by Python’s GIL,
        limiting real-time execution to ~100 Hz.
        </p>
      </list-item>
      <list-item>
        <p><bold>Integration Scope</bold>: ManipulaPy operates
        independently of ROS middleware. While this ensures modularity,
        integration with ROS-based systems currently requires manual
        bridging.
        </p>
      </list-item>
      <list-item>
        <p><bold>Algorithmic Focus</bold>: The current implementation
        focuses on potential-field and polynomial-interpolation methods.
        The framework is optimized for <bold>serial kinematic
        chains</bold>, and supporting parallel mechanisms would require
        architectural adaptation.
        </p>
      </list-item>
      <list-item>
        <p><bold>Intended Use</bold>: Designed for <bold>research and
        education</bold>, ManipulaPy is not intended for industrial
        deployment. It does not include safety certifications or formal
        real-time verification mechanisms found in production
        systems.</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="future-development">
  <title>Future Development</title>
  <p>Planned enhancements include native ROS2 integration, advanced
  sampling-based planners, multi-robot support with GPU acceleration,
  direct hardware interfaces, safety monitoring with Control Barrier
  Functions
  (<xref alt="Morton &amp; Pavone, 2025" rid="ref-morton2025oscbf" ref-type="bibr">Morton
  &amp; Pavone, 2025</xref>), and enhanced GPU utilization
  techniques.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Work supported by Universität Duisburg-Essen and inspired by Modern
  Robotics
  (<xref alt="Lynch &amp; Park, 2017" rid="ref-lynch2017modern" ref-type="bibr">Lynch
  &amp; Park, 2017</xref>), PyBullet
  (<xref alt="Coumans &amp; Bai, 2019" rid="ref-coumans2019" ref-type="bibr">Coumans
  &amp; Bai, 2019</xref>), Pinocchio
  (<xref alt="Carpentier et al., 2025" rid="ref-Pinocchio2025" ref-type="bibr">Carpentier
  et al., 2025</xref>), and Ultralytics YOLO
  (<xref alt="Jocher et al., 2023" rid="ref-Jocher_Ultralytics_YOLO_2023" ref-type="bibr">Jocher
  et al., 2023</xref>) projects.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-lynch2017modern">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Lynch</surname><given-names>Kevin M.</given-names></name>
        <name><surname>Park</surname><given-names>Frank C.</given-names></name>
      </person-group>
      <source>Modern robotics: Mechanics, planning, and control</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <uri>http://modernrobotics.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-corke2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Corke</surname><given-names>Peter</given-names></name>
        <name><surname>Haviland</surname><given-names>Jesse</given-names></name>
      </person-group>
      <article-title>Robotics, vision and control: Fundamental algorithms in MATLAB, Python and Julia</article-title>
      <source>Springer Tracts in Advanced Robotics</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>1</volume>
      <uri>https://petercorke.com/books/robotics-vision-control-python-the-practice-of-robotics-vision/</uri>
    </element-citation>
  </ref>
  <ref id="ref-chitta2012moveit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chitta</surname><given-names>Sachin</given-names></name>
        <name><surname>Sucan</surname><given-names>Ioan A.</given-names></name>
        <name><surname>Cousins</surname><given-names>Steve</given-names></name>
      </person-group>
      <article-title>MoveIt!: An introduction</article-title>
      <source>AI Matters</source>
      <publisher-name>ACM</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>3</volume>
      <issue>4</issue>
      <uri>https://moveit.ros.org/</uri>
      <fpage>4</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-coumans2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Coumans</surname><given-names>Erwin</given-names></name>
        <name><surname>Bai</surname><given-names>Yunfei</given-names></name>
      </person-group>
      <article-title>PyBullet: Physics simulation for games, robotics, and machine learning</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>http://pybullet.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-Jocher_Ultralytics_YOLO_2023">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Jocher</surname><given-names>Glenn</given-names></name>
        <name><surname>Qiu</surname><given-names>Jing</given-names></name>
        <name><surname>Chaurasia</surname><given-names>Ayush</given-names></name>
      </person-group>
      <article-title>Ultralytics YOLO</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/ultralytics/ultralytics</uri>
    </element-citation>
  </ref>
  <ref id="ref-liang2018gpu">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Liang</surname><given-names>H.</given-names></name>
        <name><surname>Du</surname><given-names>X.</given-names></name>
        <name><surname>Xiao</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>GPU-based high-performance robot dynamics computation</article-title>
      <source>Proceedings of the IEEE international conference on robotics and automation (ICRA)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <fpage>2396</fpage>
      <lpage>2402</lpage>
    </element-citation>
  </ref>
  <ref id="ref-morton2025oscbf">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Morton</surname><given-names>Daniel</given-names></name>
        <name><surname>Pavone</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>Safe, task-consistent manipulation with operational space control barrier functions</article-title>
      <source>arXiv preprint arXiv:2503.06736</source>
      <year iso-8601-date="2025">2025</year>
    </element-citation>
  </ref>
  <ref id="ref-Pinocchio2025">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Carpentier</surname><given-names>Justin</given-names></name>
        <name><surname>Mansard</surname><given-names>Nicolas</given-names></name>
        <name><surname>Valenza</surname><given-names>Florian</given-names></name>
        <name><surname>Mirabel</surname><given-names>Joseph</given-names></name>
        <name><surname>Saurel</surname><given-names>Guilhem</given-names></name>
        <name><surname>Budhiraja</surname><given-names>Rohan</given-names></name>
      </person-group>
      <article-title>Pinocchio - Efficient and versatile Rigid Body Dynamics algorithms</article-title>
      <year iso-8601-date="2025-05">2025</year><month>05</month>
      <uri>https://github.com/stack-of-tasks/pinocchio</uri>
    </element-citation>
  </ref>
  <ref id="ref-chu2021boundary">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chu</surname><given-names>Yuan</given-names></name>
        <name><surname>Wang</surname><given-names>Li</given-names></name>
        <name><surname>Zhang</surname><given-names>Mei</given-names></name>
      </person-group>
      <article-title>An approach to boundary detection for 3D point clouds based on DBSCAN clustering</article-title>
      <source>Pattern Recognition</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>124</volume>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2021.108431</pub-id>
      <fpage>108431</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-sundaralingam2023curobo">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sundaralingam</surname><given-names>Balakumar</given-names></name>
        <name><surname>Hari</surname><given-names>Siva Kumar Sastry</given-names></name>
        <name><surname>Fishman</surname><given-names>Adam</given-names></name>
        <name><surname>Garrett</surname><given-names>Caelan</given-names></name>
        <name><surname>Van Wyk</surname><given-names>Karl</given-names></name>
        <name><surname>Blukis</surname><given-names>Valts</given-names></name>
        <name><surname>Millane</surname><given-names>Alexander</given-names></name>
        <name><surname>Oleynikova</surname><given-names>Helen</given-names></name>
        <name><surname>Handa</surname><given-names>Ankur</given-names></name>
        <name><surname>Ramos</surname><given-names>Fabio</given-names></name>
        <name><surname>Ratliff</surname><given-names>Nathan</given-names></name>
        <name><surname>Fox</surname><given-names>Dieter</given-names></name>
      </person-group>
      <article-title>CuRobo: Parallelized collision-free robot motion generation</article-title>
      <source>2023 IEEE international conference on robotics and automation (ICRA)</source>
      <year iso-8601-date="2023">2023</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/ICRA48891.2023.10160765</pub-id>
      <fpage>8112</fpage>
      <lpage>8119</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cupy2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Okuta</surname><given-names>Ryosuke</given-names></name>
        <name><surname>Unno</surname><given-names>Yuya</given-names></name>
        <name><surname>Nishino</surname><given-names>Daisuke</given-names></name>
        <name><surname>Hido</surname><given-names>Shohei</given-names></name>
        <name><surname>Loomis</surname><given-names>Crissman</given-names></name>
      </person-group>
      <article-title>CuPy: A NumPy-compatible library for NVIDIA GPU calculations</article-title>
      <source>Proceedings of Workshop on Machine Learning Systems (LearningSys) at NeurIPS</source>
      <year iso-8601-date="2017">2017</year>
      <uri>https://cupy.dev</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
