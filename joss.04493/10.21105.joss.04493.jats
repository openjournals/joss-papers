<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4493</article-id>
<article-id pub-id-type="doi">10.21105/joss.04493</article-id>
<title-group>
<article-title>DIANNA: Deep Insight And Neural Network
Analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9834-1756</contrib-id>
<name>
<surname>Ranguelova</surname>
<given-names>Elena</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5529-5761</contrib-id>
<name>
<surname>Meijer</surname>
<given-names>Christiaan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-8724-8372</contrib-id>
<name>
<surname>Oostrum</surname>
<given-names>Leon</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-1966-8460</contrib-id>
<name>
<surname>Liu</surname>
<given-names>Yang</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6033-960X</contrib-id>
<name>
<surname>Bos</surname>
<given-names>Patrick</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-0823-0121</contrib-id>
<name>
<surname>Crocioni</surname>
<given-names>Giulia</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6022-0046</contrib-id>
<name>
<surname>Laneuville</surname>
<given-names>Matthieu</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-9793-910X</contrib-id>
<name>
<surname>Guevara</surname>
<given-names>Bryan Cardenas</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-2932-3028</contrib-id>
<name>
<surname>Bakhshi</surname>
<given-names>Rena</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-4207-8725</contrib-id>
<name>
<surname>Podareanu</surname>
<given-names>Damian</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Netherlands eScience Center, Amsterdam, the
Netherlands</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>SURF, Amsterdam, the Netherlands</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-03-22">
<day>22</day>
<month>3</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>80</issue>
<fpage>4493</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>explainable AI</kwd>
<kwd>Deep Neural Networks</kwd>
<kwd>ONNX</kwd>
<kwd>benchmark datasets</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The growing demand from science and industry inspired rapid
  advances in Artificial Intelligence (AI). The increased use of AI and
  the reliability and trust required by automated decision making and
  standards of scientific rigour, led to the boom of eXplainable
  Artificial Intelligence (XAI).
  <ext-link ext-link-type="uri" xlink:href="https://dianna.readthedocs.io/en/latest/">DIANNA
  (Deep Insight And Neural Network Analysis)</ext-link> is the first
  Python package of systematically selected XAI methods supporting the
  <ext-link ext-link-type="uri" xlink:href="https://onnx.ai/">Open
  Neural Networks Exchange (ONNX)</ext-link> format. DIANNA is built by
  and designed for all research software engineers and researchers, also
  non-AI experts.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>AI systems have been increasingly used in a wide variety of fields,
  including such data-sensitive areas as healthcare
  (<xref alt="Alshehri &amp; Muhammad, 2021" rid="ref-alshehrifatima" ref-type="bibr">Alshehri
  &amp; Muhammad, 2021</xref>), renewable energy
  (<xref alt="Kuzlu et al., 2020" rid="ref-kuzlumurat" ref-type="bibr">Kuzlu
  et al., 2020</xref>), supply chain
  (<xref alt="Toorajipour et al., 2021" rid="ref-toorajipourreza" ref-type="bibr">Toorajipour
  et al., 2021</xref>) and finance. Automated decision-making and
  scientific research standards require reliability and trust of the AI
  technology (<xref alt="Xu, 2019" rid="ref-xuwei" ref-type="bibr">Xu,
  2019</xref>). Especially in AI-enhanced research, a scientist need to
  be able to trust a high-performant, but opaque AI model used for
  automation of their data processing pipeline. In addition, XAI has the
  potential for helping any scientist to “find new scientific
  discoveries in the analysis of their data”
  (<xref alt="Hey et al., 2020" rid="ref-hey" ref-type="bibr">Hey et
  al., 2020</xref>). Furthermore, tools for supporting repeatable
  science are of high demand
  (<xref alt="Feger, 2020" rid="ref-Feger2020InteractiveTF" ref-type="bibr">Feger,
  2020</xref>).</p>
  <p>DIANNA addresses these needs of researchers in various scientific
  domains who make use of AI models, especially supporting non-AI
  experts. DIANNA provides a Python-based, user-friendly, and uniform
  interface to several XAI methods. To the best of our knowledge, it is
  the only library using Open Neural Network Exchange (ONNX)
  (<xref alt="Bai et al., 2019" rid="ref-onnx" ref-type="bibr">Bai et
  al., 2019</xref>), the open-source, framework-agnostic standard for AI
  models, which supports repeatability of scientific research.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>There are numerous Python XAI libraries, many are listed in the
  Awesome explainable AI
  (<xref alt="Wang &amp; others, 2022" rid="ref-awesomeai" ref-type="bibr">Wang
  &amp; others, 2022</xref>) repository. Popular and widely used
  packages are Pytorch
  (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
  et al., 2019</xref>), LIME
  (<xref alt="Ribeiro et al., 2016" rid="ref-ribeirolime" ref-type="bibr">Ribeiro
  et al., 2016</xref>), Captum
  (<xref alt="Kokhlikyan et al., 2020" rid="ref-kokhlikyan2020captum" ref-type="bibr">Kokhlikyan
  et al., 2020</xref>), Lucid
  (<xref alt="Schubert &amp; contributors, 2021" rid="ref-tflucid" ref-type="bibr">Schubert
  &amp; contributors, 2021</xref>), SHAP
  (<xref alt="Lundberg &amp; Lee, 2017" rid="ref-lundbergshap" ref-type="bibr">Lundberg
  &amp; Lee, 2017</xref>), InterpretML
  (<xref alt="Nori et al., 2019" rid="ref-nori2019interpretml" ref-type="bibr">Nori
  et al., 2019</xref>), PyTorch CNN visualizations
  (<xref alt="Ozbulak, 2019" rid="ref-uozbulak_pytorch_vis_2021" ref-type="bibr">Ozbulak,
  2019</xref>) Pytorch GradCAM
  (<xref alt="Gildenblat &amp; contributors, 2021" rid="ref-jacobgilpytorchcam" ref-type="bibr">Gildenblat
  &amp; contributors, 2021</xref>), Deep Visualization Toolbox
  (<xref alt="Yosinski et al., 2015" rid="ref-yosinski-2015-ICML-DL-understanding-neural-networks" ref-type="bibr">Yosinski
  et al., 2015</xref>), ELI5
  (<xref alt="Korobov et al., 2022" rid="ref-eli5" ref-type="bibr">Korobov
  et al., 2022</xref>). However, these libraries have limitations that
  complicate adoption by scientific communities:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Single XAI method</bold> or <bold>single data
      modality.</bold> While libraries such as SHAP, LIME, Pytorch
      GradCAM. have gained great popularity, their methods are not
      always suitable for the research task and/or data modality. For
      example, GradCAM is applicable only to images. Most importantly,
      each library in that class addresses AI explainability with a
      different method, complicating comparison between methods.</p>
    </list-item>
    <list-item>
      <p><bold>Single Deep Neural Network (DNN)
      format/framework/architecture.</bold> Many XAI libraries support a
      single DNN format: Lucid supports only TensorFlow
      (<xref alt="Abadi et al., 2015" rid="ref-tf" ref-type="bibr">Abadi
      et al., 2015</xref>), Captum - PyTorch
      (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
      et al., 2019</xref>) and iNNvestigate
      (<xref alt="Alber et al., 2019" rid="ref-innvestigatenn" ref-type="bibr">Alber
      et al., 2019</xref>) is aimed at Keras users exclusively. Pytorch
      GradCAM supports a single method for a single format and
      Convolutional Neural Network Visualizations even limits the choice
      to a single DNN type. While this is not an issue for the current
      most popular framework communities, not all mature libraries
      support a spectrum of XAI methods. Most importantly, tools that
      support a single framework are not “future-proof”. For instance,
      Caffe
      (<xref alt="Jia et al., 2014" rid="ref-jia2014caffe" ref-type="bibr">Jia
      et al., 2014</xref>) was the most popular framework in the
      computer vision (CV) community in 2018, but it has since been
      abandoned.</p>
    </list-item>
    <list-item>
      <p><bold>Unclear choice of supported XAI methods.</bold> ELI5
      supports multiple frameworks/formats and XAI methods, but it is
      unclear how the selection of these methods was made. Furthermore,
      the library has not been maintained since 2020, so any methods in
      the rapidly changing XAI field proposed since then are
      missing.</p>
    </list-item>
    <list-item>
      <p><bold>AI expertise is necessary.</bold> The Deep Visualization
      Toolbox requires DNN knowledge and is only used by AI experts
      mostly within the CV community.</p>
    </list-item>
  </list>
  <p>In addition, on more fundamental level, the results of XAI research
  does not help to make the technology understandable and trustworthy
  for non (X)AI experts:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Properties of the output of the explainer.</bold> There
      is no commonly accepted methodology to systematically study XAI
      methods and their output.</p>
    </list-item>
    <list-item>
      <p><bold>Human interpretation intertwined with the one of the
      explainer.</bold> This is a major problem in the current XAI
      literature, and there has been limited research to define what
      constitutes a meaningful explanation in the context of AI systems
      (<xref alt="Lu et al., 2019" rid="ref-joylu" ref-type="bibr">Lu et
      al., 2019</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Lack of suitable (scientific) datasets.</bold> The most
      popular and simplest dataset used as “toy-example” is the MNIST
      dataset of handwritten digits
      (<xref alt="LeCun et al., 2010" rid="ref-mnistdataset" ref-type="bibr">LeCun
      et al., 2010</xref>), composed of 10 classes and with no
      structural variation in the content. Such a dataset is too complex
      for non-AI experts to intuitively understand the XAI output and
      simultaneously too far from scientific research data.</p>
    </list-item>
    <list-item>
      <p><bold>Plethora of current AI model formats.</bold> The amount
      and the speed with which they become obsolete is another important
      show-stopper for reproducibility of AI-enabled science.</p>
    </list-item>
    <list-item>
      <p><bold>Lack of funding.</bold> Some libraries, such as
      iNNvestigate, are becoming obsolete due to the lack of support for
      research projects that sponsored their creation.</p>
    </list-item>
  </list>
</sec>
<sec id="key-features">
  <title>Key Features</title>
  <fig>
    <caption><p>High level architecture of DIANNA</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/628360b3b3e9d7fe53d8aed302537805d5f79aff.png" xlink:title="" />
  </fig>
  <p>DIANNA is an open source XAI Python package with the following key
  characteristics:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Systematically chosen diverse set of XAI methods.</bold>
      We have used a relevant subset of the thorough objective and
      systematic evaluation criteria defined in
      (<xref alt="Sokol &amp; Flach, 2019" rid="ref-peterflatch" ref-type="bibr">Sokol
      &amp; Flach, 2019</xref>). Several complementary and
      model-architecture agnostic state-of-the-art XAI methods have been
      chosen and included in DIANNA
      (<xref alt="Ranguelova &amp; Liu, 2022" rid="ref-ranguelova_how_2022" ref-type="bibr">Ranguelova
      &amp; Liu, 2022</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>Multiple data modalities.</bold> DIANNA supports images
      and text, we will extend the input data modalities to embeddings,
      time-series, tabular data and graphs. This is particularly
      important to scientific researchers, whose data are in domains
      different than the classical examples from CV and natural language
      processing communities.</p>
    </list-item>
    <list-item>
      <p><bold>Open Neural Network Exchange (ONNX) format.</bold> ONNX
      is the de-facto standard format for neural network models. Not
      only is the use of ONNX very beneficial for interoperability,
      enabling reproducible science, but it is also compatible with
      runtimes and libraries designed to maximize performance across
      hardware. To the best of our knowledge, DIANNA is the first and
      only XAI library supporting ONNX.</p>
    </list-item>
    <list-item>
      <p><bold>Simple, intuitive benchmark datasets.</bold> We have
      proposed two new datasets which enable systematic research of the
      properties of the XAI methods’ output and understanding on an
      intuitive level: Simple Geometric Shapes
      (<xref alt="Oostrum et al., 2021" rid="ref-oostrum_leon_2021_5012825" ref-type="bibr">Oostrum
      et al., 2021</xref>) and LeafSnap30
      (<xref alt="Ranguelova et al., 2021" rid="ref-ranguelova_elena_2021_5061353" ref-type="bibr">Ranguelova
      et al., 2021</xref>). The classification of tree species on
      LeafSnap data is a great example of a simple scientific problem
      tackled with both classical CV and a deep learning method, where
      the latter outperforms, but needs explanations. DIANNA also uses
      well-established benchmarks: a simplified MNIST with 2 distinctive
      classes only and the Stanford Sentiment Treebank
      (<xref alt="Socher et al., 2013" rid="ref-socher-etal-2013-recursive" ref-type="bibr">Socher
      et al., 2013</xref>).</p>
    </list-item>
    <list-item>
      <p><bold>User-friendly interface.</bold> DIANNA wraps all XAI
      methods with a common API.</p>
    </list-item>
    <list-item>
      <p><bold>Modular architecture, extensive testing and compliance
      with modern software engineering practices.</bold> It is very easy
      for new XAI methods which do not need to access the ONNX model
      internals to be added to DIANNA. For relevance-propagation type of
      methods, more work is needed within the ONNX standard
      (<xref alt="Levitan, 2020" rid="ref-levitan_onnx_2020" ref-type="bibr">Levitan,
      2020</xref>) and we hope our work will boost the development
      growth of ONNX (scientific) models. We welcome the XAI research
      community to contribute to these developments via DIANNA.</p>
    </list-item>
    <list-item>
      <p><bold>Thorough documentation.</bold> The package includes user
      and developer documentation. It also provides instructions for
      conversion between ONNX and Tensorflow, Pytorch, Keras or
      Scikit-learn.</p>
    </list-item>
  </list>
</sec>
<sec id="used-by">
  <title>Used by</title>
  <p>DIANNA is currently being used in the project “Recognizing
  symbolism in Turkish television drama”
  (<xref alt="Verhaar, 2022" rid="ref-turkishdrama" ref-type="bibr">Verhaar,
  2022</xref>). An important task is the development of an effective
  model for detecting and recognizing symbols in videos. DIANNA is being
  used to increase understanding of AI models in order to explore how to
  improve them.</p>
  <p>DIANNA is also currently being used in the “Visually grounded
  models of spoken language” project, which builds on previous work by
  (<xref alt="Alishahi et al., 2017" rid="ref-alishahiU002B17" ref-type="bibr">Alishahi
  et al., 2017</xref>;
  <xref alt="Chrupala, 2018" rid="ref-chrupala18" ref-type="bibr">Chrupala,
  2018</xref>;
  <xref alt="Chrupała et al., 2017" rid="ref-chrupala17representations" ref-type="bibr">Chrupała
  et al., 2017</xref>,
  <xref alt="2019" rid="ref-chrupalaU002B19" ref-type="bibr">2019</xref>).
  The goal is a multimodal model that projects image and sound data into
  a common embedded space. Within DIANNA, we are developing XAI methods
  to visualize and explain these embedded spaces in their complex
  multimodal network contexts.</p>
  <p>Finally, DIANNA has also been used in the EU-funded
  <ext-link ext-link-type="uri" xlink:href="https://www.examode.eu/">Examode</ext-link>
  medical research project
  (<xref alt="Guevara et al., 2022" rid="ref-bryancardenas" ref-type="bibr">Guevara
  et al., 2022</xref>). It deals with very large datasets and, because
  it aims to support physicians in their decision making, needs
  transparent and trustworthy models.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was supported by the
  <ext-link ext-link-type="uri" xlink:href="https://www.esciencecenter.nl/">Netherlands
  eScience Center</ext-link> and
  <ext-link ext-link-type="uri" xlink:href="https://www.surf.nl/en">SURF</ext-link>.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-levitan_onnx_2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Levitan</surname><given-names>Svetlana</given-names></name>
      </person-group>
      <article-title>Contribute to the open neural network eXchange (ONNX)</article-title>
      <year iso-8601-date="2020-06">2020</year><month>06</month>
      <uri>https://medium.com/codait/contribute-to-the-open-neural-network-exchange-onnx-5cfff6889761</uri>
    </element-citation>
  </ref>
  <ref id="ref-ranguelova_how_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ranguelova</surname><given-names>Elena</given-names></name>
        <name><surname>Liu</surname><given-names>Yang</given-names></name>
      </person-group>
      <article-title>How to find your Artificial Intelligence explainer</article-title>
      <year iso-8601-date="2022-03">2022</year><month>03</month>
      <uri>https://blog.esciencecenter.nl/how-to-find-your-artificial-intelligence-explainer-dbb1ac608009</uri>
    </element-citation>
  </ref>
  <ref id="ref-Feger2020InteractiveTF">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Feger</surname><given-names>Sebastian Stefan</given-names></name>
      </person-group>
      <article-title>Interactive tools for reproducible science - understanding, supporting, and motivating reproducible science practices</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2020">2020</year>
      <volume>abs/2012.02570</volume>
    </element-citation>
  </ref>
  <ref id="ref-hey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hey</surname><given-names>Tony</given-names></name>
        <name><surname>Butler</surname><given-names>Keith</given-names></name>
        <name><surname>Jackson</surname><given-names>Sam</given-names></name>
        <name><surname>Thiyagalingam</surname><given-names>Jeyarajan</given-names></name>
      </person-group>
      <article-title>Machine learning and big scientific data</article-title>
      <source>Philosophical transactions. Series A, Mathematical, physical, and engineering sciences</source>
      <year iso-8601-date="2020">2020</year>
      <volume>378</volume>
      <issue>2166</issue>
      <issn>1364-503X</issn>
      <uri>https://europepmc.org/articles/PMC7015290</uri>
      <pub-id pub-id-type="doi">10.1098/rsta.2019.0054</pub-id>
      <fpage>20190054</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lundbergshap">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lundberg</surname><given-names>Scott M.</given-names></name>
        <name><surname>Lee</surname><given-names>Su-In</given-names></name>
      </person-group>
      <article-title>A unified approach to interpreting model predictions</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2017">2017</year>
      <volume>abs/1705.07874</volume>
      <uri>http://arxiv.org/abs/1705.07874</uri>
    </element-citation>
  </ref>
  <ref id="ref-ribeirolime">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ribeiro</surname><given-names>Marco Túlio</given-names></name>
        <name><surname>Singh</surname><given-names>Sameer</given-names></name>
        <name><surname>Guestrin</surname><given-names>Carlos</given-names></name>
      </person-group>
      <article-title>&quot;Why should I trust you?&quot;: Explaining the predictions of any classifier</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2016">2016</year>
      <volume>abs/1602.04938</volume>
      <uri>http://arxiv.org/abs/1602.04938</uri>
      <pub-id pub-id-type="doi">10.18653/v1/n16-3020</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-xuwei">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Xu</surname><given-names>Wei</given-names></name>
      </person-group>
      <article-title>Toward human-centered AI: A perspective from human-computer interaction</article-title>
      <source>Interactions</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2019-06">2019</year><month>06</month>
      <volume>26</volume>
      <issue>4</issue>
      <issn>1072-5520</issn>
      <uri>https://doi.org/10.1145/3328485</uri>
      <pub-id pub-id-type="doi">10.1145/3328485</pub-id>
      <fpage>42</fpage>
      <lpage>46</lpage>
    </element-citation>
  </ref>
  <ref id="ref-alshehrifatima">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alshehri</surname><given-names>Fatima</given-names></name>
        <name><surname>Muhammad</surname><given-names>Ghulam</given-names></name>
      </person-group>
      <article-title>A comprehensive survey of the internet of things (IoT) and AI-based smart healthcare</article-title>
      <source>IEEE Access</source>
      <year iso-8601-date="2021">2021</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3047960</pub-id>
      <fpage>3660</fpage>
      <lpage>3678</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kuzlumurat">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kuzlu</surname><given-names>Murat</given-names></name>
        <name><surname>Cali</surname><given-names>Umit</given-names></name>
        <name><surname>Sharma</surname><given-names>Vinayak</given-names></name>
        <name><surname>Güler</surname><given-names>Özgür</given-names></name>
      </person-group>
      <article-title>Gaining insight into solar photovoltaic power generation forecasting utilizing explainable artificial intelligence tools</article-title>
      <source>IEEE Access</source>
      <year iso-8601-date="2020">2020</year>
      <volume>8</volume>
      <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3031477</pub-id>
      <fpage>187814</fpage>
      <lpage>187823</lpage>
    </element-citation>
  </ref>
  <ref id="ref-toorajipourreza">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Toorajipour</surname><given-names>Reza</given-names></name>
        <name><surname>Sohrabpour</surname><given-names>Vahid</given-names></name>
        <name><surname>Nazarpour</surname><given-names>Ali</given-names></name>
        <name><surname>Oghazi</surname><given-names>Pejvak</given-names></name>
        <name><surname>Fischl</surname><given-names>Maria</given-names></name>
      </person-group>
      <article-title>Artificial intelligence in supply chain management: A systematic literature review</article-title>
      <source>Journal of Business Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>122</volume>
      <issn>0148-2963</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S014829632030583X</uri>
      <pub-id pub-id-type="doi">10.1016/j.jbusres.2020.09.009</pub-id>
      <fpage>502</fpage>
      <lpage>517</lpage>
    </element-citation>
  </ref>
  <ref id="ref-joylu">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lu</surname><given-names>Joy</given-names></name>
        <name><surname>Lee</surname><given-names>Dokyun</given-names></name>
        <name><surname>Kim</surname><given-names>Tae Wan</given-names></name>
        <name><surname>Danks</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Good explanation for algorithmic transparency</article-title>
      <source>SSRN</source>
      <year iso-8601-date="2019">2019</year>
      <uri>https://ssrn.com/abstract=3503603</uri>
      <pub-id pub-id-type="doi">10.2139/ssrn.3503603</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-peterflatch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sokol</surname><given-names>Kacper</given-names></name>
        <name><surname>Flach</surname><given-names>Peter A.</given-names></name>
      </person-group>
      <article-title>Explainability fact sheets: A framework for systematic assessment of explainable approaches</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2019">2019</year>
      <volume>abs/1912.05100</volume>
      <uri>http://arxiv.org/abs/1912.05100</uri>
    </element-citation>
  </ref>
  <ref id="ref-bryancardenas">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Guevara</surname><given-names>Bryan Cardenas</given-names></name>
        <name><surname>Podareanu</surname><given-names>Damian</given-names></name>
        <name><surname>Laneuville</surname><given-names>Matthieu</given-names></name>
      </person-group>
      <article-title>XAI in practice: Medical case study using DIANNA</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-02">2022</year><month>02</month>
      <uri>https://doi.org/10.5281/zenodo.6303282</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.6303282</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-uozbulak_pytorch_vis_2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ozbulak</surname><given-names>Utku</given-names></name>
      </person-group>
      <article-title>PyTorch CNN visualizations</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/utkuozbulak/pytorch-cnn-visualizations; GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-nori2019interpretml">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nori</surname><given-names>Harsha</given-names></name>
        <name><surname>Jenkins</surname><given-names>Samuel</given-names></name>
        <name><surname>Koch</surname><given-names>Paul</given-names></name>
        <name><surname>Caruana</surname><given-names>Rich</given-names></name>
      </person-group>
      <article-title>InterpretML: A unified framework for machine learning interpretability</article-title>
      <source>arXiv preprint arXiv:1909.09223</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-tflucid">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Schubert</surname><given-names>Ludwig</given-names></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>Lucid</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/tensorflow/lucid</uri>
    </element-citation>
  </ref>
  <ref id="ref-jacobgilpytorchcam">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gildenblat</surname><given-names>Jacob</given-names></name>
        <name><surname>contributors</surname></name>
      </person-group>
      <article-title>PyTorch library for CAM methods</article-title>
      <publisher-name>https://github.com/jacobgil/pytorch-grad-cam; GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-yosinski-2015-ICML-DL-understanding-neural-networks">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Yosinski</surname><given-names>Jason</given-names></name>
        <name><surname>Clune</surname><given-names>Jeff</given-names></name>
        <name><surname>Nguyen</surname><given-names>Anh</given-names></name>
        <name><surname>Fuchs</surname><given-names>Thomas</given-names></name>
        <name><surname>Lipson</surname><given-names>Hod</given-names></name>
      </person-group>
      <article-title>Understanding neural networks through deep visualization</article-title>
      <source>Deep learning workshop, international conference on machine learning (ICML)</source>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-kokhlikyan2020captum">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kokhlikyan</surname><given-names>Narine</given-names></name>
        <name><surname>Miglani</surname><given-names>Vivek</given-names></name>
        <name><surname>Martin</surname><given-names>Miguel</given-names></name>
        <name><surname>Wang</surname><given-names>Edward</given-names></name>
        <name><surname>Alsallakh</surname><given-names>Bilal</given-names></name>
        <name><surname>Reynolds</surname><given-names>Jonathan</given-names></name>
        <name><surname>Melnikov</surname><given-names>Alexander</given-names></name>
        <name><surname>Kliushkina</surname><given-names>Natalia</given-names></name>
        <name><surname>Araya</surname><given-names>Carlos</given-names></name>
        <name><surname>Yan</surname><given-names>Siqi</given-names></name>
        <name><surname>Reblitz-Richardson</surname><given-names>Orion</given-names></name>
      </person-group>
      <article-title>Captum: A unified and generic model interpretability library for PyTorch</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2009.07896</uri>
    </element-citation>
  </ref>
  <ref id="ref-eli5">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Korobov</surname><given-names>Mikhail</given-names></name>
        <name><surname>Lopuhin</surname><given-names>Konstantin</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>ELI5</article-title>
      <publisher-name>https://github.com/TeamHG-Memex/eli5; GitHub</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-onnx">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Lu</surname><given-names>Fang</given-names></name>
        <name><surname>Zhang</surname><given-names>Ke</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>ONNX: Open neural network exchange</article-title>
      <publisher-name>https://github.com/onnx/onnx; GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-awesomeai">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Yongjie</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Awesome-explainable-AI</article-title>
      <publisher-name>https://github.com/wangyongjie-ntu/Awesome-explainable-AI#python-librariessort-in-alphabeta-order; GitHub</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-tf">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Mané</surname><given-names>Dan</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <source>TensorFlow, Large-scale machine learning on heterogeneous systems</source>
      <year iso-8601-date="2015-11">2015</year><month>11</month>
      <pub-id pub-id-type="doi">10.5281/zenodo.4724125</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-innvestigatenn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alber</surname><given-names>Maximilian</given-names></name>
        <name><surname>Lapuschkin</surname><given-names>Sebastian</given-names></name>
        <name><surname>Seegerer</surname><given-names>Philipp</given-names></name>
        <name><surname>Hägele</surname><given-names>Miriam</given-names></name>
        <name><surname>Schütt</surname><given-names>Kristof T.</given-names></name>
        <name><surname>Montavon</surname><given-names>Grégoire</given-names></name>
        <name><surname>Samek</surname><given-names>Wojciech</given-names></name>
        <name><surname>Müller</surname><given-names>Klaus-Robert</given-names></name>
        <name><surname>Dähne</surname><given-names>Sven</given-names></name>
        <name><surname>Kindermans</surname><given-names>Pieter-Jan</given-names></name>
      </person-group>
      <article-title>iNNvestigate neural networks!</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2019">2019</year>
      <volume>20</volume>
      <issue>93</issue>
      <uri>http://jmlr.org/papers/v20/18-540.html</uri>
      <fpage>1</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jia2014caffe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Shelhamer</surname><given-names>Evan</given-names></name>
        <name><surname>Donahue</surname><given-names>Jeff</given-names></name>
        <name><surname>Karayev</surname><given-names>Sergey</given-names></name>
        <name><surname>Long</surname><given-names>Jonathan</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
        <name><surname>Guadarrama</surname><given-names>Sergio</given-names></name>
        <name><surname>Darrell</surname><given-names>Trevor</given-names></name>
      </person-group>
      <article-title>Caffe: Convolutional architecture for fast feature embedding</article-title>
      <source>arXiv preprint arXiv:1408.5093</source>
      <year iso-8601-date="2014">2014</year>
    </element-citation>
  </ref>
  <ref id="ref-mnistdataset">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>LeCun</surname><given-names>Yann</given-names></name>
        <name><surname>Cortes</surname><given-names>Corinna</given-names></name>
        <name><surname>Burges</surname><given-names>Christopher J. C.</given-names></name>
      </person-group>
      <article-title>THE MNIST DATABASE of handwritten digits</article-title>
      <publisher-name>http://yann.lecun.com/exdb/mnist/</publisher-name>
      <year iso-8601-date="2010">2010</year>
    </element-citation>
  </ref>
  <ref id="ref-oostrum_leon_2021_5012825">
    <element-citation publication-type="dataset">
      <person-group person-group-type="author">
        <name><surname>Oostrum</surname><given-names>Leon</given-names></name>
        <name><surname>Liu</surname><given-names>Yang</given-names></name>
        <name><surname>Meijer</surname><given-names>Christiaan</given-names></name>
        <name><surname>Ranguelova</surname><given-names>Elena</given-names></name>
        <name><surname>Bos</surname><given-names>Patrick</given-names></name>
      </person-group>
      <article-title>Simple geometric shapes</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2021-07">2021</year><month>07</month>
      <uri>https://doi.org/10.5281/zenodo.5012825</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.5012825</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ranguelova_elena_2021_5061353">
    <element-citation publication-type="dataset">
      <person-group person-group-type="author">
        <name><surname>Ranguelova</surname><given-names>Elena</given-names></name>
        <name><surname>Meijer</surname><given-names>Christiaan</given-names></name>
        <name><surname>Oostrum</surname><given-names>Leon</given-names></name>
        <name><surname>Liu</surname><given-names>Yang</given-names></name>
        <name><surname>Bos</surname><given-names>Patrick</given-names></name>
      </person-group>
      <article-title>LeafSnap30</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2021-07">2021</year><month>07</month>
      <uri>https://doi.org/10.5281/zenodo.5061353</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.5061353</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-socher-etal-2013-recursive">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Socher</surname><given-names>Richard</given-names></name>
        <name><surname>Perelygin</surname><given-names>Alex</given-names></name>
        <name><surname>Wu</surname><given-names>Jean</given-names></name>
        <name><surname>Chuang</surname><given-names>Jason</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher D.</given-names></name>
        <name><surname>Ng</surname><given-names>Andrew</given-names></name>
        <name><surname>Potts</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>Recursive deep models for semantic compositionality over a sentiment treebank</article-title>
      <source>Proceedings of the 2013 conference on empirical methods in natural language processing</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Seattle, Washington, USA</publisher-loc>
      <year iso-8601-date="2013-10">2013</year><month>10</month>
      <uri>https://aclanthology.org/D13-1170</uri>
      <fpage>1631</fpage>
      <lpage>1642</lpage>
    </element-citation>
  </ref>
  <ref id="ref-turkishdrama">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Verhaar</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Mediating islam in the digital age</article-title>
      <source>Digital Scholarship@Leiden</source>
      <publisher-name>University of Leiden</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://www.digitalscholarshipleiden.nl/articles/mediating-islam-in-the-digital-age</uri>
    </element-citation>
  </ref>
  <ref id="ref-chrupala18">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chrupala</surname><given-names>Grzegorz</given-names></name>
      </person-group>
      <article-title>Symbolic inductive bias for visually grounded learning of spoken language</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2018">2018</year>
      <volume>abs/1812.09244</volume>
      <uri>http://arxiv.org/abs/1812.09244</uri>
      <pub-id pub-id-type="doi">10.18653/v1/p19-1647</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chrupala17representations">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chrupała</surname><given-names>Grzegorz</given-names></name>
        <name><surname>Gelderloos</surname><given-names>Lieke</given-names></name>
        <name><surname>Alishahi</surname><given-names>Afra</given-names></name>
      </person-group>
      <article-title>Representations of language in a model of visually grounded speech signal</article-title>
      <source>Proceedings of the 55th annual meeting of the association for computational linguistics (volume 1: Long papers)</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Vancouver, Canada</publisher-loc>
      <year iso-8601-date="2017-07">2017</year><month>07</month>
      <uri>https://aclanthology.org/P17-1057</uri>
      <pub-id pub-id-type="doi">10.18653/v1/P17-1057</pub-id>
      <fpage>613</fpage>
      <lpage>622</lpage>
    </element-citation>
  </ref>
  <ref id="ref-alishahiU002B17">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alishahi</surname><given-names>Afra</given-names></name>
        <name><surname>Barking</surname><given-names>Marie</given-names></name>
        <name><surname>Chrupala</surname><given-names>Grzegorz</given-names></name>
      </person-group>
      <article-title>Encoding of phonology in a recurrent neural model of grounded speech</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2017">2017</year>
      <volume>abs/1706.03815</volume>
      <uri>http://arxiv.org/abs/1706.03815</uri>
      <pub-id pub-id-type="doi">10.18653/v1/k17-1037</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-chrupalaU002B19">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chrupała</surname><given-names>Grzegorz</given-names></name>
        <name><surname>Gelderloos</surname><given-names>Lieke</given-names></name>
        <name><surname>Kádár</surname></name>
        <name><surname>Alishahi</surname><given-names>Afra</given-names></name>
      </person-group>
      <article-title>On the difficulty of a distributional semantics of spoken language</article-title>
      <source>Proceedings of the society for computation in linguistics</source>
      <year iso-8601-date="2019">2019</year>
      <volume>2</volume>
      <uri>https://blogs.umass.edu/scil/scil-2019/</uri>
      <pub-id pub-id-type="doi">10.7275/extq-7546</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
