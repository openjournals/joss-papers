<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20221215T200149-b786e6e7dcdcd18c2bcd35441742a491e82d71db</doi_batch_id>
    <timestamp>20221215200149</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>12</month>
          <year>2022</year>
        </publication_date>
        <journal_volume>
          <volume>7</volume>
        </journal_volume>
        <issue>80</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>DIANNA: Deep Insight And Neural Network
Analysis</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Elena</given_name>
            <surname>Ranguelova</surname>
            <ORCID>https://orcid.org/0000-0002-9834-1756</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Christiaan</given_name>
            <surname>Meijer</surname>
            <ORCID>https://orcid.org/0000-0002-5529-5761</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Leon</given_name>
            <surname>Oostrum</surname>
            <ORCID>https://orcid.org/0000-0001-8724-8372</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yang</given_name>
            <surname>Liu</surname>
            <ORCID>https://orcid.org/0000-0002-1966-8460</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Patrick</given_name>
            <surname>Bos</surname>
            <ORCID>https://orcid.org/0000-0002-6033-960X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Giulia</given_name>
            <surname>Crocioni</surname>
            <ORCID>https://orcid.org/0000-0002-0823-0121</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Matthieu</given_name>
            <surname>Laneuville</surname>
            <ORCID>https://orcid.org/0000-0001-6022-0046</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Bryan Cardenas</given_name>
            <surname>Guevara</surname>
            <ORCID>https://orcid.org/0000-0001-9793-910X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Rena</given_name>
            <surname>Bakhshi</surname>
            <ORCID>https://orcid.org/0000-0002-2932-3028</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Damian</given_name>
            <surname>Podareanu</surname>
            <ORCID>https://orcid.org/0000-0002-4207-8725</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>12</month>
          <day>15</day>
          <year>2022</year>
        </publication_date>
        <pages>
          <first_page>4493</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.04493</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7387004</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/4493</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.04493</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.04493</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.04493.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="levitan_onnx_2020">
            <article_title>Contribute to the open neural network
eXchange (ONNX)</article_title>
            <author>Levitan</author>
            <cYear>2020</cYear>
            <unstructured_citation>Levitan, S. (2020). Contribute to the
open neural network eXchange (ONNX) [Medium].
https://medium.com/codait/contribute-to-the-open-neural-network-exchange-onnx-5cfff6889761</unstructured_citation>
          </citation>
          <citation key="ranguelova_how_2022">
            <article_title>How to find your Artificial Intelligence
explainer</article_title>
            <author>Ranguelova</author>
            <cYear>2022</cYear>
            <unstructured_citation>Ranguelova, E., &amp; Liu, Y. (2022).
How to find your Artificial Intelligence explainer [Medium].
https://blog.esciencecenter.nl/how-to-find-your-artificial-intelligence-explainer-dbb1ac608009</unstructured_citation>
          </citation>
          <citation key="Feger2020InteractiveTF">
            <article_title>Interactive tools for reproducible science -
understanding, supporting, and motivating reproducible science
practices</article_title>
            <author>Feger</author>
            <journal_title>ArXiv</journal_title>
            <volume>abs/2012.02570</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Feger, S. S. (2020). Interactive
tools for reproducible science - understanding, supporting, and
motivating reproducible science practices. ArXiv,
abs/2012.02570.</unstructured_citation>
          </citation>
          <citation key="hey">
            <article_title>Machine learning and big scientific
data</article_title>
            <author>Hey</author>
            <journal_title>Philosophical transactions. Series A,
Mathematical, physical, and engineering sciences</journal_title>
            <issue>2166</issue>
            <volume>378</volume>
            <doi>10.1098/rsta.2019.0054</doi>
            <issn>1364-503X</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Hey, T., Butler, K., Jackson, S.,
&amp; Thiyagalingam, J. (2020). Machine learning and big scientific
data. Philosophical Transactions. Series A, Mathematical, Physical, and
Engineering Sciences, 378(2166), 20190054.
https://doi.org/10.1098/rsta.2019.0054</unstructured_citation>
          </citation>
          <citation key="lundbergshap">
            <article_title>A unified approach to interpreting model
predictions</article_title>
            <author>Lundberg</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1705.07874</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Lundberg, S. M., &amp; Lee, S.-I.
(2017). A unified approach to interpreting model predictions. CoRR,
abs/1705.07874. http://arxiv.org/abs/1705.07874</unstructured_citation>
          </citation>
          <citation key="ribeirolime">
            <article_title>"Why should I trust you?": Explaining the
predictions of any classifier</article_title>
            <author>Ribeiro</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1602.04938</volume>
            <doi>10.18653/v1/n16-3020</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Ribeiro, M. T., Singh, S., &amp;
Guestrin, C. (2016). "Why should I trust you?": Explaining the
predictions of any classifier. CoRR, abs/1602.04938.
https://doi.org/10.18653/v1/n16-3020</unstructured_citation>
          </citation>
          <citation key="xuwei">
            <article_title>Toward human-centered AI: A perspective from
human-computer interaction</article_title>
            <author>Xu</author>
            <journal_title>Interactions</journal_title>
            <issue>4</issue>
            <volume>26</volume>
            <doi>10.1145/3328485</doi>
            <issn>1072-5520</issn>
            <cYear>2019</cYear>
            <unstructured_citation>Xu, W. (2019). Toward human-centered
AI: A perspective from human-computer interaction. Interactions, 26(4),
42–46. https://doi.org/10.1145/3328485</unstructured_citation>
          </citation>
          <citation key="alshehrifatima">
            <article_title>A comprehensive survey of the internet of
things (IoT) and AI-based smart healthcare</article_title>
            <author>Alshehri</author>
            <journal_title>IEEE Access</journal_title>
            <volume>9</volume>
            <doi>10.1109/ACCESS.2020.3047960</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Alshehri, F., &amp; Muhammad, G.
(2021). A comprehensive survey of the internet of things (IoT) and
AI-based smart healthcare. IEEE Access, 9, 3660–3678.
https://doi.org/10.1109/ACCESS.2020.3047960</unstructured_citation>
          </citation>
          <citation key="kuzlumurat">
            <article_title>Gaining insight into solar photovoltaic power
generation forecasting utilizing explainable artificial intelligence
tools</article_title>
            <author>Kuzlu</author>
            <journal_title>IEEE Access</journal_title>
            <volume>8</volume>
            <doi>10.1109/ACCESS.2020.3031477</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Kuzlu, M., Cali, U., Sharma, V.,
&amp; Güler, Ö. (2020). Gaining insight into solar photovoltaic power
generation forecasting utilizing explainable artificial intelligence
tools. IEEE Access, 8, 187814–187823.
https://doi.org/10.1109/ACCESS.2020.3031477</unstructured_citation>
          </citation>
          <citation key="toorajipourreza">
            <article_title>Artificial intelligence in supply chain
management: A systematic literature review</article_title>
            <author>Toorajipour</author>
            <journal_title>Journal of Business Research</journal_title>
            <volume>122</volume>
            <doi>10.1016/j.jbusres.2020.09.009</doi>
            <issn>0148-2963</issn>
            <cYear>2021</cYear>
            <unstructured_citation>Toorajipour, R., Sohrabpour, V.,
Nazarpour, A., Oghazi, P., &amp; Fischl, M. (2021). Artificial
intelligence in supply chain management: A systematic literature review.
Journal of Business Research, 122, 502–517.
https://doi.org/10.1016/j.jbusres.2020.09.009</unstructured_citation>
          </citation>
          <citation key="joylu">
            <article_title>Good explanation for algorithmic
transparency</article_title>
            <author>Lu</author>
            <journal_title>SSRN</journal_title>
            <doi>10.2139/ssrn.3503603</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Lu, J., Lee, D., Kim, T. W., &amp;
Danks, D. (2019). Good explanation for algorithmic transparency. SSRN.
https://doi.org/10.2139/ssrn.3503603</unstructured_citation>
          </citation>
          <citation key="peterflatch">
            <article_title>Explainability fact sheets: A framework for
systematic assessment of explainable approaches</article_title>
            <author>Sokol</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1912.05100</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Sokol, K., &amp; Flach, P. A. (2019).
Explainability fact sheets: A framework for systematic assessment of
explainable approaches. CoRR, abs/1912.05100.
http://arxiv.org/abs/1912.05100</unstructured_citation>
          </citation>
          <citation key="bryancardenas">
            <article_title>XAI in practice: Medical case study using
DIANNA</article_title>
            <author>Guevara</author>
            <doi>10.5281/zenodo.6303282</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Guevara, B. C., Podareanu, D., &amp;
Laneuville, M. (2022). XAI in practice: Medical case study using DIANNA.
Zenodo. https://doi.org/10.5281/zenodo.6303282</unstructured_citation>
          </citation>
          <citation key="uozbulak_pytorch_vis_2021">
            <article_title>PyTorch CNN visualizations</article_title>
            <author>Ozbulak</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Ozbulak, U. (2019). PyTorch CNN
visualizations. In GitHub repository.
https://github.com/utkuozbulak/pytorch-cnn-visualizations;
GitHub.</unstructured_citation>
          </citation>
          <citation key="nori2019interpretml">
            <article_title>InterpretML: A unified framework for machine
learning interpretability</article_title>
            <author>Nori</author>
            <journal_title>arXiv preprint
arXiv:1909.09223</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Nori, H., Jenkins, S., Koch, P.,
&amp; Caruana, R. (2019). InterpretML: A unified framework for machine
learning interpretability. arXiv Preprint
arXiv:1909.09223.</unstructured_citation>
          </citation>
          <citation key="tflucid">
            <article_title>Lucid</article_title>
            <author>Schubert</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2021</cYear>
            <unstructured_citation>Schubert, L., &amp; contributors.
(2021). Lucid. In GitHub repository. GitHub.
https://github.com/tensorflow/lucid</unstructured_citation>
          </citation>
          <citation key="jacobgilpytorchcam">
            <article_title>PyTorch library for CAM
methods</article_title>
            <author>Gildenblat</author>
            <cYear>2021</cYear>
            <unstructured_citation>Gildenblat, J., &amp; contributors.
(2021). PyTorch library for CAM methods.
https://github.com/jacobgil/pytorch-grad-cam;
GitHub.</unstructured_citation>
          </citation>
          <citation key="yosinski-2015-ICML-DL-understanding-neural-networks">
            <article_title>Understanding neural networks through deep
visualization</article_title>
            <author>Yosinski</author>
            <journal_title>Deep learning workshop, international
conference on machine learning (ICML)</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Yosinski, J., Clune, J., Nguyen, A.,
Fuchs, T., &amp; Lipson, H. (2015). Understanding neural networks
through deep visualization. Deep Learning Workshop, International
Conference on Machine Learning (ICML).</unstructured_citation>
          </citation>
          <citation key="kokhlikyan2020captum">
            <article_title>Captum: A unified and generic model
interpretability library for PyTorch</article_title>
            <author>Kokhlikyan</author>
            <cYear>2020</cYear>
            <unstructured_citation>Kokhlikyan, N., Miglani, V., Martin,
M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A., Kliushkina, N.,
Araya, C., Yan, S., &amp; Reblitz-Richardson, O. (2020). Captum: A
unified and generic model interpretability library for PyTorch.
https://arxiv.org/abs/2009.07896</unstructured_citation>
          </citation>
          <citation key="eli5">
            <article_title>ELI5</article_title>
            <author>Korobov</author>
            <cYear>2022</cYear>
            <unstructured_citation>Korobov, M., Lopuhin, K., &amp;
others. (2022). ELI5. https://github.com/TeamHG-Memex/eli5;
GitHub.</unstructured_citation>
          </citation>
          <citation key="onnx">
            <article_title>ONNX: Open neural network
exchange</article_title>
            <author>Bai</author>
            <cYear>2019</cYear>
            <unstructured_citation>Bai, J., Lu, F., Zhang, K., &amp;
others. (2019). ONNX: Open neural network exchange.
https://github.com/onnx/onnx; GitHub.</unstructured_citation>
          </citation>
          <citation key="awesomeai">
            <article_title>Awesome-explainable-AI</article_title>
            <author>Wang</author>
            <cYear>2022</cYear>
            <unstructured_citation>Wang, Y., &amp; others. (2022).
Awesome-explainable-AI.
https://github.com/wangyongjie-ntu/Awesome-explainable-AI#python-librariessort-in-alphabeta-order;
GitHub.</unstructured_citation>
          </citation>
          <citation key="tf">
            <volume_title>TensorFlow, Large-scale machine learning on
heterogeneous systems</volume_title>
            <author>Abadi</author>
            <doi>10.5281/zenodo.4724125</doi>
            <cYear>2015</cYear>
            <unstructured_citation>Abadi, M., Agarwal, A., Barham, P.,
Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J.,
Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard,
M., Jozefowicz, R., Jia, Y., Kaiser, L., Kudlur, M., … Zheng, X. (2015).
TensorFlow, Large-scale machine learning on heterogeneous systems.
https://doi.org/10.5281/zenodo.4724125</unstructured_citation>
          </citation>
          <citation key="pytorch">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlché-Buc, E.
Fox, &amp; R. Garnett (Eds.), Advances in neural information processing
systems 32 (pp. 8024–8035). Curran Associates, Inc.
http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
          <citation key="innvestigatenn">
            <article_title>iNNvestigate neural networks!</article_title>
            <author>Alber</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <issue>93</issue>
            <volume>20</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Alber, M., Lapuschkin, S., Seegerer,
P., Hägele, M., Schütt, K. T., Montavon, G., Samek, W., Müller, K.-R.,
Dähne, S., &amp; Kindermans, P.-J. (2019). iNNvestigate neural networks!
Journal of Machine Learning Research, 20(93), 1–8.
http://jmlr.org/papers/v20/18-540.html</unstructured_citation>
          </citation>
          <citation key="jia2014caffe">
            <article_title>Caffe: Convolutional architecture for fast
feature embedding</article_title>
            <author>Jia</author>
            <journal_title>arXiv preprint
arXiv:1408.5093</journal_title>
            <cYear>2014</cYear>
            <unstructured_citation>Jia, Y., Shelhamer, E., Donahue, J.,
Karayev, S., Long, J., Girshick, R., Guadarrama, S., &amp; Darrell, T.
(2014). Caffe: Convolutional architecture for fast feature embedding.
arXiv Preprint arXiv:1408.5093.</unstructured_citation>
          </citation>
          <citation key="mnistdataset">
            <article_title>THE MNIST DATABASE of handwritten
digits</article_title>
            <author>LeCun</author>
            <cYear>2010</cYear>
            <unstructured_citation>LeCun, Y., Cortes, C., &amp; Burges,
C. J. C. (2010). THE MNIST DATABASE of handwritten digits.
http://yann.lecun.com/exdb/mnist/.</unstructured_citation>
          </citation>
          <citation key="oostrum_leon_2021_5012825">
            <article_title>Simple geometric shapes</article_title>
            <author>Oostrum</author>
            <doi>10.5281/zenodo.5012825</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Oostrum, L., Liu, Y., Meijer, C.,
Ranguelova, E., &amp; Bos, P. (2021). Simple geometric shapes (Version
1.0) [Data set]. Zenodo.
https://doi.org/10.5281/zenodo.5012825</unstructured_citation>
          </citation>
          <citation key="ranguelova_elena_2021_5061353">
            <article_title>LeafSnap30</article_title>
            <author>Ranguelova</author>
            <doi>10.5281/zenodo.5061353</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Ranguelova, E., Meijer, C., Oostrum,
L., Liu, Y., &amp; Bos, P. (2021). LeafSnap30 (Version v.1.0) [Data
set]. Zenodo.
https://doi.org/10.5281/zenodo.5061353</unstructured_citation>
          </citation>
          <citation key="socher-etal-2013-recursive">
            <article_title>Recursive deep models for semantic
compositionality over a sentiment treebank</article_title>
            <author>Socher</author>
            <journal_title>Proceedings of the 2013 conference on
empirical methods in natural language processing</journal_title>
            <cYear>2013</cYear>
            <unstructured_citation>Socher, R., Perelygin, A., Wu, J.,
Chuang, J., Manning, C. D., Ng, A., &amp; Potts, C. (2013). Recursive
deep models for semantic compositionality over a sentiment treebank.
Proceedings of the 2013 Conference on Empirical Methods in Natural
Language Processing, 1631–1642.
https://aclanthology.org/D13-1170</unstructured_citation>
          </citation>
          <citation key="turkishdrama">
            <article_title>Mediating islam in the digital
age</article_title>
            <author>Verhaar</author>
            <journal_title>Digital Scholarship@Leiden</journal_title>
            <cYear>2022</cYear>
            <unstructured_citation>Verhaar, P. (2022). Mediating islam
in the digital age. In Digital Scholarship@Leiden. University of Leiden.
https://www.digitalscholarshipleiden.nl/articles/mediating-islam-in-the-digital-age</unstructured_citation>
          </citation>
          <citation key="chrupala18">
            <article_title>Symbolic inductive bias for visually grounded
learning of spoken language</article_title>
            <author>Chrupala</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1812.09244</volume>
            <doi>10.18653/v1/p19-1647</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Chrupala, G. (2018). Symbolic
inductive bias for visually grounded learning of spoken language. CoRR,
abs/1812.09244.
https://doi.org/10.18653/v1/p19-1647</unstructured_citation>
          </citation>
          <citation key="chrupala17representations">
            <article_title>Representations of language in a model of
visually grounded speech signal</article_title>
            <author>Chrupała</author>
            <journal_title>Proceedings of the 55th annual meeting of the
association for computational linguistics (volume 1: Long
papers)</journal_title>
            <doi>10.18653/v1/P17-1057</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Chrupała, G., Gelderloos, L., &amp;
Alishahi, A. (2017). Representations of language in a model of visually
grounded speech signal. Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers),
613–622. https://doi.org/10.18653/v1/P17-1057</unstructured_citation>
          </citation>
          <citation key="alishahi+17">
            <article_title>Encoding of phonology in a recurrent neural
model of grounded speech</article_title>
            <author>Alishahi</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1706.03815</volume>
            <doi>10.18653/v1/k17-1037</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Alishahi, A., Barking, M., &amp;
Chrupala, G. (2017). Encoding of phonology in a recurrent neural model
of grounded speech. CoRR, abs/1706.03815.
https://doi.org/10.18653/v1/k17-1037</unstructured_citation>
          </citation>
          <citation key="chrupala+19">
            <article_title>On the difficulty of a distributional
semantics of spoken language</article_title>
            <author>Chrupała</author>
            <journal_title>Proceedings of the society for computation in
linguistics</journal_title>
            <volume>2</volume>
            <doi>10.7275/extq-7546</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Chrupała, G., Gelderloos, L., Kádár,
Ákos, &amp; Alishahi, A. (2019). On the difficulty of a distributional
semantics of spoken language. Proceedings of the Society for Computation
in Linguistics, 2.
https://doi.org/10.7275/extq-7546</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
