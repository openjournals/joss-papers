<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20240707131324-c92af010e27457d86d0680706b83e42bf4c76964</doi_batch_id>
    <timestamp>20240707131324</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>07</month>
          <year>2024</year>
        </publication_date>
        <journal_volume>
          <volume>9</volume>
        </journal_volume>
        <issue>99</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Diart: A Python Library for Real-Time Speaker
Diarization</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Juan Manuel</given_name>
            <surname>Coria</surname>
            <ORCID>https://orcid.org/0000-0002-5035-147X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Hervé</given_name>
            <surname>Bredin</surname>
            <ORCID>https://orcid.org/0000-0002-3739-925X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Sahar</given_name>
            <surname>Ghannay</surname>
            <ORCID>https://orcid.org/0000-0002-7531-2522</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Sophie</given_name>
            <surname>Rosset</surname>
            <ORCID>https://orcid.org/0000-0002-6865-4989</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Khaled</given_name>
            <surname>Zaouk</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Ingo</given_name>
            <surname>Fruend</surname>
            <ORCID>https://orcid.org/0000-0003-4594-1763</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Bertrand</given_name>
            <surname>Higy</surname>
            <ORCID>https://orcid.org/0000-0002-8198-8676</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Amit</given_name>
            <surname>Kesari</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yagna</given_name>
            <surname>Thakkar</surname>
          </person_name>
        </contributors>
        <publication_date>
          <month>07</month>
          <day>07</day>
          <year>2024</year>
        </publication_date>
        <pages>
          <first_page>5266</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05266</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.12510278</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5266</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05266</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05266</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05266.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="pyannote.audio">
            <article_title>pyannote.audio: neural building blocks for
speaker diarization</article_title>
            <author>Bredin</author>
            <journal_title>ICASSP 2020, IEEE international conference on
acoustics, speech, and signal processing</journal_title>
            <doi>10.1109/ICASSP40776.2020.9052974</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Bredin, H., Yin, R., Coria, J. M.,
Gelly, G., Korshunov, P., Lavechin, M., Fustes, D., Titeux, H., Bouaziz,
W., &amp; Gill, M.-P. (2020). pyannote.audio: neural building blocks for
speaker diarization. ICASSP 2020, IEEE International Conference on
Acoustics, Speech, and Signal Processing.
https://doi.org/10.1109/ICASSP40776.2020.9052974</unstructured_citation>
          </citation>
          <citation key="optuna">
            <article_title>Optuna: A Next-generation Hyperparameter
Optimization Framework</article_title>
            <author>Akiba</author>
            <journal_title>Proceedings of the 25rd ACM SIGKDD
international conference on knowledge discovery and data
mining</journal_title>
            <doi>10.1145/3292500.3330701</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Akiba, T., Sano, S., Yanase, T.,
Ohta, T., &amp; Koyama, M. (2019). Optuna: A Next-generation
Hyperparameter Optimization Framework. Proceedings of the 25rd ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining.
https://doi.org/10.1145/3292500.3330701</unstructured_citation>
          </citation>
          <citation key="Coria:2021">
            <article_title>Overlap-Aware Low-Latency Online Speaker
Diarization Based on End-to-End Local Segmentation</article_title>
            <author>Coria</author>
            <journal_title>2021 IEEE automatic speech recognition and
understanding workshop (ASRU)</journal_title>
            <doi>10.1109/ASRU51503.2021.9688044</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Coria, J. M., Bredin, H., Ghannay,
S., &amp; Rosset, S. (2021). Overlap-Aware Low-Latency Online Speaker
Diarization Based on End-to-End Local Segmentation. 2021 IEEE Automatic
Speech Recognition and Understanding Workshop (ASRU), 1139–1146.
https://doi.org/10.1109/ASRU51503.2021.9688044</unstructured_citation>
          </citation>
          <citation key="Ego4D:2022">
            <article_title>Ego4D: Around the World in 3,000 Hours of
Egocentric Video</article_title>
            <author>Grauman</author>
            <journal_title>Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition</journal_title>
            <doi>10.1109/CVPR52688.2022.01842</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Grauman, K., Westbury, A., Byrne, E.,
Chavis, Z., Furnari, A., Girdhar, R., Hamburger, J., Jiang, H., Liu, M.,
Liu, X., &amp; others. (2022). Ego4D: Around the World in 3,000 Hours of
Egocentric Video. Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, 18995–19012.
https://doi.org/10.1109/CVPR52688.2022.01842</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
