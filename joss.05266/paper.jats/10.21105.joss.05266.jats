<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5266</article-id>
<article-id pub-id-type="doi">10.21105/joss.05266</article-id>
<title-group>
<article-title>Diart: A Python Library for Real-Time Speaker
Diarization</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5035-147X</contrib-id>
<name>
<surname>Coria</surname>
<given-names>Juan Manuel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3739-925X</contrib-id>
<name>
<surname>Bredin</surname>
<given-names>Hervé</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7531-2522</contrib-id>
<name>
<surname>Ghannay</surname>
<given-names>Sahar</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6865-4989</contrib-id>
<name>
<surname>Rosset</surname>
<given-names>Sophie</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Zaouk</surname>
<given-names>Khaled</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4594-1763</contrib-id>
<name>
<surname>Fruend</surname>
<given-names>Ingo</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8198-8676</contrib-id>
<name>
<surname>Higy</surname>
<given-names>Bertrand</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kesari</surname>
<given-names>Amit</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Thakkar</surname>
<given-names>Yagna</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Université Paris-Saclay CNRS, LISN, Orsay,
France</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>IRIT, Université de Toulouse, CNRS, Toulouse,
France</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Ava, France</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Verbally GmbH, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Indian Institute of Technology, Tirupati,
India</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Tridhya Intuit Pvt Ltd, Gujarat, India</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-09-23">
<day>23</day>
<month>9</month>
<year>2022</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>5266</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>machine learning</kwd>
<kwd>artificial intelligence</kwd>
<kwd>speaker diarization</kwd>
<kwd>speaker embedding</kwd>
<kwd>speaker clustering</kwd>
<kwd>real time</kwd>
<kwd>streaming</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The term “speaker diarization” denotes the problem of determining
  “who speaks when” in a recorded conversation. Among other reasons, it
  has attracted the attention of the speech research community because
  of its ability to improve transcription performance, readability and
  exploitability. Speaker diarization in real-time holds the potential
  to accelerate and cement the adoption of this technology in our
  everyday lives. However, although “offline” systems today achieve
  outstanding performance in pre-recorded conversations, additional
  problems of “online” real-time diarization, like limited context and
  low latency, require flexible and efficient solutions enabling both
  research and production-ready applications. We introduce a Python
  package called <monospace>Diart</monospace> to address real-time
  speaker diarization in an efficient and flexible way.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>Diart</monospace> is a Python library for real-time
  speaker diarization. It leverages data structures and pre-trained
  models available in <monospace>pyannote.audio</monospace>
  (<xref alt="Bredin et al., 2020" rid="ref-pyannote.audio" ref-type="bibr">Bredin
  et al., 2020</xref>) to implement production-ready real-time inference
  on a variety of audio streams like local and remote audio/video files,
  microphones, and even WebSockets. Moreover,
  <monospace>Diart</monospace> was designed to facilitate research by
  providing fast batched inference and hyper-parameter tuning thanks to
  and in full compatibility with <monospace>Optuna</monospace>
  (<xref alt="Akiba et al., 2019" rid="ref-optuna" ref-type="bibr">Akiba
  et al., 2019</xref>).</p>
  <p><monospace>Diart</monospace> was designed with an object-oriented
  API fully capable of extension and customization. Streaming is powered
  internally by ReactiveX extensions, but available “blocks” allow users
  to mix and match different operations with any streaming library they
  choose. A prototyping tool with a CLI is also provided to quickly
  evaluate, profile, visualize and optimize custom systems.</p>
  <p><monospace>Diart</monospace> is based on previous research on
  low-latency online speaker diarization
  (<xref alt="Coria et al., 2021" rid="ref-CoriaU003A2021" ref-type="bibr">Coria
  et al., 2021</xref>) and allows to reproduce its results. It has also
  participated in the recent Ego4D Audio-only Diarization Challenge
  (<xref alt="Grauman et al., 2022" rid="ref-Ego4DU003A2022" ref-type="bibr">Grauman
  et al., 2022</xref>), outperforming the offline baseline by a large
  margin. We hope <monospace>Diart</monospace>’s flexibility, efficiency
  and customization will allow for exciting new research and
  applications in online speaker diarization.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work has been funded by Université Paris-Saclay under PhD
  contract number 2019-089.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-pyannote.audio">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bredin</surname><given-names>Hervé</given-names></name>
        <name><surname>Yin</surname><given-names>Ruiqing</given-names></name>
        <name><surname>Coria</surname><given-names>Juan Manuel</given-names></name>
        <name><surname>Gelly</surname><given-names>Gregory</given-names></name>
        <name><surname>Korshunov</surname><given-names>Pavel</given-names></name>
        <name><surname>Lavechin</surname><given-names>Marvin</given-names></name>
        <name><surname>Fustes</surname><given-names>Diego</given-names></name>
        <name><surname>Titeux</surname><given-names>Hadrien</given-names></name>
        <name><surname>Bouaziz</surname><given-names>Wassim</given-names></name>
        <name><surname>Gill</surname><given-names>Marie-Philippe</given-names></name>
      </person-group>
      <article-title>pyannote.audio: neural building blocks for speaker diarization</article-title>
      <source>ICASSP 2020, IEEE international conference on acoustics, speech, and signal processing</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1109/ICASSP40776.2020.9052974</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-optuna">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Akiba</surname><given-names>Takuya</given-names></name>
        <name><surname>Sano</surname><given-names>Shotaro</given-names></name>
        <name><surname>Yanase</surname><given-names>Toshihiko</given-names></name>
        <name><surname>Ohta</surname><given-names>Takeru</given-names></name>
        <name><surname>Koyama</surname><given-names>Masanori</given-names></name>
      </person-group>
      <article-title>Optuna: A Next-generation Hyperparameter Optimization Framework</article-title>
      <source>Proceedings of the 25rd ACM SIGKDD international conference on knowledge discovery and data mining</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-CoriaU003A2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Coria</surname><given-names>Juan M.</given-names></name>
        <name><surname>Bredin</surname><given-names>Hervé</given-names></name>
        <name><surname>Ghannay</surname><given-names>Sahar</given-names></name>
        <name><surname>Rosset</surname><given-names>Sophie</given-names></name>
      </person-group>
      <article-title>Overlap-Aware Low-Latency Online Speaker Diarization Based on End-to-End Local Segmentation</article-title>
      <source>2021 IEEE automatic speech recognition and understanding workshop (ASRU)</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1109/ASRU51503.2021.9688044</pub-id>
      <fpage>1139</fpage>
      <lpage>1146</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Ego4DU003A2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Grauman</surname><given-names>Kristen</given-names></name>
        <name><surname>Westbury</surname><given-names>Andrew</given-names></name>
        <name><surname>Byrne</surname><given-names>Eugene</given-names></name>
        <name><surname>Chavis</surname><given-names>Zachary</given-names></name>
        <name><surname>Furnari</surname><given-names>Antonino</given-names></name>
        <name><surname>Girdhar</surname><given-names>Rohit</given-names></name>
        <name><surname>Hamburger</surname><given-names>Jackson</given-names></name>
        <name><surname>Jiang</surname><given-names>Hao</given-names></name>
        <name><surname>Liu</surname><given-names>Miao</given-names></name>
        <name><surname>Liu</surname><given-names>Xingyu</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Ego4D: Around the World in 3,000 Hours of Egocentric Video</article-title>
      <source>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1109/CVPR52688.2022.01842</pub-id>
      <fpage>18995</fpage>
      <lpage>19012</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
