<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5132</article-id>
<article-id pub-id-type="doi">10.21105/joss.05132</article-id>
<title-group>
<article-title>Speakerbox: Few-Shot Learning for Speaker Identification
with Transformers</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2564-0373</contrib-id>
<name>
<surname>Brown</surname>
<given-names>Eva Maxfield</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9664-3662</contrib-id>
<name>
<surname>Huynh</surname>
<given-names>To</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6008-3763</contrib-id>
<name>
<surname>Weber</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Washington Information School, University of
Washington, Seattle</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Washington, Seattle</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-03-13">
<day>13</day>
<month>3</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>83</issue>
<fpage>5132</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>speaker identification</kwd>
<kwd>audio classification</kwd>
<kwd>machine learning</kwd>
<kwd>transformers</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Automated speaker identification is a modeling challenge for
  research when large-scale corpora, such as audio recordings or
  transcripts, are relied upon for evidence (e.g. Journalism,
  Qualitative Research, Law, etc.). To address current difficulties in
  training speaker identification models, we propose Speakerbox: a
  method for few-shot fine-tuning of an audio transformer. Specifically,
  Speakerbox makes multi-recording, multi-speaker identification model
  fine-tuning as simple as possible while still fitting an accurate,
  useful model for application. Speakerbox works by ensuring data are
  safely stratified by speaker id and held-out by recording id prior to
  fine-tuning of a pretrained speaker identification Transformer on a
  small number of audio examples. We show that with less than an hour of
  audio-recorded input, Speakerbox can fine-tune a multi-speaker
  identification model for use in assisting researchers in audio and
  transcript annotation.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Speaker-annotated transcripts from audio recordings are an
  increasingly important applied research problem in natural language
  processing. For example, speaker-annotated audio and transcript data
  has previously been used to create comprehensive analyses of
  conversation dynamics
  (<xref alt="Jacobi &amp; Schweers, 2017" rid="ref-jacobi_justice_2017" ref-type="bibr">Jacobi
  &amp; Schweers, 2017</xref>;
  <xref alt="Maltzman &amp; Sigelman, 1996" rid="ref-maltzman_politics_1996" ref-type="bibr">Maltzman
  &amp; Sigelman, 1996</xref>;
  <xref alt="Miller &amp; Sutherland, 2022" rid="ref-miller_effect_2022" ref-type="bibr">Miller
  &amp; Sutherland, 2022</xref>;
  <xref alt="Morris, 2001" rid="ref-morris_reexamining_2001" ref-type="bibr">Morris,
  2001</xref>;
  <xref alt="Osborn &amp; Mendez, 2010" rid="ref-osborn_speaking_2010" ref-type="bibr">Osborn
  &amp; Mendez, 2010</xref>;
  <xref alt="Slapin &amp; Kirkland, 2020" rid="ref-slapin_sound_2020" ref-type="bibr">Slapin
  &amp; Kirkland, 2020</xref>). However, multi-speaker audio
  classification models (for the purpose of speaker identification) can
  be cumbersome and expensive to train and unwieldy to apply. Speaker
  diarization is, “the unsupervised identification of each speaker
  within an audio stream and the intervals during which each speaker is
  active”
  (<xref alt="Anguera et al., 2012" rid="ref-SpeakerDia2012" ref-type="bibr">Anguera
  et al., 2012</xref>). Diarization is a useful method in certain
  applications of large-scale automated analysis and there are free
  tools available to perform these tasks (e.g.
  <xref alt="Bredin et al., 2020" rid="ref-Bredin2020" ref-type="bibr">Bredin
  et al., 2020</xref>;
  <xref alt="Bredin &amp; Laurent, 2021" rid="ref-Bredin2021" ref-type="bibr">Bredin
  &amp; Laurent, 2021</xref>). However, due to it’s unsupervised nature,
  diarization may inconsistently label speakers across different audio
  recordings. As such, diarization may not adequately meet the needs for
  research purposes depending upon the identification of speakers
  (e.g. Journalism, Qualitative Research, Law, etc.).</p>
  <p>Speakerbox is built with the goal of making multi-recording,
  multi-speaker identification model training as simple as possible
  while still attempting to help train an accurate, useful model for
  application (e.g. a set of recordings where each recording has some
  subset of a set of speakers).</p>
  <p>To this end Speakerbox provides functionality to:</p>
  <list list-type="order">
    <list-item>
      <p>Create or import annotation sets;</p>
    </list-item>
    <list-item>
      <p>Prepare an audio dataset into stratified and held-out, train,
      test, and validation subsets;</p>
    </list-item>
    <list-item>
      <p>Train and evaluate a fine-tuned speaker identification model;
      and,</p>
    </list-item>
    <list-item>
      <p>Apply a speaker identification model to an audio file.</p>
    </list-item>
  </list>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>While there is continuous research in new methods and model
  architectures for few-shot speaker identification models
  (<xref alt="Kumar et al., 2020" rid="ref-kumar2020-few-shot" ref-type="bibr">Kumar
  et al., 2020</xref>;
  <xref alt="Li et al., 2022" rid="ref-li2022-few-shot" ref-type="bibr">Li
  et al., 2022</xref>;
  <xref alt="Wolters et al., 2020" rid="ref-Wolters2020ASO" ref-type="bibr">Wolters
  et al., 2020</xref>), there exists little work in creating an
  open-source, easy-to-use library for their training and
  evaluation.</p>
  <p>For more general speech processing and filtering,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/astorfi/speechpy">SpeechPy</ext-link>
  is an open-source solution for “speech processing and feature
  extraction … [by providing the] most frequent used speech features
  including MFCCs and filterbank energies”
  (<xref alt="Torfi, 2018" rid="ref-Torfi2018" ref-type="bibr">Torfi,
  2018</xref>).</p>
  <p>Open-source libraries more closely related to diarization and
  speaker identification include
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pyannote/pyannote-audio">Pyannote.Audio</ext-link>
  and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface/transformers">Transformers</ext-link>.
  <ext-link ext-link-type="uri" xlink:href="https://github.com/pyannote/pyannote-audio">Pyannote.Audio</ext-link>
  “provides a set of trainable end-to-end neural building blocks that
  can be combined and jointly optimized to build speaker diarization
  pipelines”
  (<xref alt="Bredin et al., 2020" rid="ref-Bredin2020" ref-type="bibr">Bredin
  et al., 2020</xref>;
  <xref alt="Bredin &amp; Laurent, 2021" rid="ref-Bredin2021" ref-type="bibr">Bredin
  &amp; Laurent, 2021</xref>). While
  <ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface/transformers">Transformers</ext-link>
  is a library which “provides thousands of pretrained models to perform
  tasks on different modalities such as text, vision, and audio”
  (<xref alt="Wolf et al., 2020" rid="ref-wolf-etal-2020-transformers" ref-type="bibr">Wolf
  et al., 2020</xref>). Speakerbox makes use of both
  <monospace>pyannote.audio</monospace> and
  <monospace>transformers</monospace>.</p>
  <p>Speaker diarization (provided by
  <monospace>pyannote.audio</monospace>) is the
  <italic>unsupervised</italic> process of splitting audio into segments
  grouped by speaker identity. Speakerbox in comparison, provides
  functionality to train a model for speaker indentification, the
  <italic>supervised</italic> process of splitting audio into segments
  and identifying them as a known speaker.</p>
  <p>The Transformer architecture was originally introduced by Vaswani,
  et al. for use in sequence-to-sequence machine learning tasks such as
  machine translation
  (<xref alt="Vaswani et al., 2017" rid="ref-vaswani2017attention" ref-type="bibr">Vaswani
  et al., 2017</xref>). However, there are now many researchers creating
  or fine-tuning Transformer-based models to suit there own needs. The
  <monospace>transformers</monospace> library provided by HuggingFace is
  one such library that provides an API for downloading, using, and/or
  fine-tuning Transformer-based models. In our case, Speakerbox utilizes
  the <monospace>transformers</monospace> library to fine-tune a speaker
  identification model made available by the HuggingFace platform.</p>
  <p>There are also paid solutions for quickly annotating and training a
  custom speaker diarization and audio classification model such as
  <ext-link ext-link-type="uri" xlink:href="https://prodi.gy/features/audio-video">ExplosionAI’s
  Prodigy Platform</ext-link>.</p>
  <p>Finally,
  <ext-link ext-link-type="uri" xlink:href="https://github.com/huggingface/setfit">SetFit</ext-link>
  (“an efficient and prompt-free framework for few-shot fine-tuning of
  Sentence Transformers”) provides similar functionality as Speakerbox
  but for text-based data
  (<xref alt="Tunstall et al., 2022" rid="ref-setfit" ref-type="bibr">Tunstall
  et al., 2022</xref>).</p>
  <p>Speakerbox brings many of these tools together in order to be a
  solution for speaker identification as an easy-to-use and open-source
  library for the annotation of audio data and the fine-tuning of a
  speaker identification model.</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p>Speakerbox attempts to simplify transformer fine-tuning by making
  the following processes easier:</p>
  <list list-type="bullet">
    <list-item>
      <p>Creation of a dataset via diarization;</p>
    </list-item>
    <list-item>
      <p>Import of a dataset from existing annotation platforms;</p>
    </list-item>
    <list-item>
      <p>Preparation of train, test, and validation subsets protected
      from speaker and recording data leakage;</p>
    </list-item>
    <list-item>
      <p>Fine-tuning and evaluation of a transformer; and,</p>
    </list-item>
    <list-item>
      <p>Application of a trained model across an audio file</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Typical workflow to prepare a speaker identification
    dataset and fine-tune a new model using tools provided from the
    Speakerbox library. The user starts with a collection of audio files
    that include portions speech from the speakers they want to train a
    model to identify. The
    <monospace>diarize_and_split_audio</monospace> function will create
    a new directory with the same name as the audio file, diarize the
    audio file, and finally, sort the audio portions produced from
    diarization into sub-directories within this new directory. The user
    should then manually rename each of the produced sub-directories to
    the correct speaker identifier (i.e. the speaker’s name or a unique
    id) and additionally remove any incorrectly diarized or mislabeled
    portions of audio. Finally, the user can prepare training,
    evaluation, and testing datasets (via the
    <monospace>expand_labeled_diarized_audio_dir_to_dataset</monospace>
    and <monospace>preprocess_dataset</monospace> functions) and
    fine-tune a new speaker identification model (via the
    <monospace>train</monospace> function).</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/937162ea06e6fd2dbac7ec7c6a04ac8debcb928b.png" />
  </fig>
  <p>The goal of this library is to simplify these proceses while
  maintaining fidelity in the training and utility of a speaker
  identification model for application.</p>
  <sec id="dataset-generation-and-import">
    <title>Dataset Generation and Import</title>
    <sec id="diarization">
      <title>Diarization</title>
      <p>Diarization is the unsupervised process of splitting audio into
      segments grouped by speaker identity. The output of a diarization
      model is usually a random ID (e.g. “speaker_0”, “speaker_1”, etc.)
      where there is no guarantee that “speaker_0” from a first audio
      file, is the same “speaker_0” from a second audio file. Because of
      this unsupervised nature, diarization cannot be used as a single
      solution for multi-speaker, multi-recording speaker
      identification. It can however be used for quickly generating
      large amounts of training examples which can be validated and
      labeled for use in a later speaker identification training
      set.</p>
      <p>We make use of diarization as one method for preparing a
      speaker identification training set by using a model provided by
      <monospace>pyannote.audio</monospace> to diarize an audio file and
      place the unlabeled portions of audio into directories on the
      user’s file system. A user can then listen to a few or all of the
      samples of audio in each directory, remove any samples that were
      mis-classified, and finally rename each of the directories with a
      true and consistent speaker identifier (e.g. a name, database ID,
      etc.).</p>
    </sec>
    <sec id="using-gecko-annotations">
      <title>Using Gecko Annotations</title>
      <p>If a fully supervised method for dataset generation is
      preferred, or to improve model accuracy and improve coverage of
      edge cases, users of Speakerbox may use
      <ext-link ext-link-type="uri" xlink:href="https://github.com/gong-io/gecko">Gecko</ext-link>:
      a free web application for manual segmentation of audio files by
      speaker as well as annotation of the linguistic content of a
      conversation
      (<xref alt="Golan Levy, 2019" rid="ref-Gecko2019" ref-type="bibr">Golan
      Levy, 2019</xref>). Speakerbox can make use of Gecko annotations
      as a method for training set creation by providing functions to
      split and prepare audio files using the annotations stored in a
      Gecko created JSON file.</p>
    </sec>
  </sec>
  <sec id="preparation-for-model-training-and-evaluation">
    <title>Preparation for Model Training and Evaluation</title>
    <p>To ensure that a Speakerbox model is learning the features of
    each speaker’s voice (and not the features of the microphone or the
    specific words and phrases of each recording) we create dataset
    training, test, and evaluation subsets based off of a recording
    holdout and speaker stratification pattern. Each train, test, and
    evaluation subset must contain unique recording IDs to reduce the
    chance of learning the features of specific microphones or recording
    contexts, and each produced subset must contain recordings of every
    speaker available from the whole dataset. For example, if there are
    nine unique speakers in the complete dataset, then each train, test,
    and evaluation subset is required to have examples of all nine
    speakers.</p>
    <p>If these recording holdout and speaker stratification conditions
    are not met, Speakerbox iteratively retries this random sampling
    process again. If the sampling function cannot find a valid dataset
    configuration given the sampling iterations, we inform the user of
    this failure and prompt them to add more examples to the
    dataset.</p>
  </sec>
  <sec id="model-fine-tuning">
    <title>Model Fine-Tuning</title>
    <p>The Speakerbox training process consists of fine-tuning a
    pre-trained speaker identification model
    (<xref alt="Yang et al., 2021" rid="ref-yang2021superb" ref-type="bibr">Yang
    et al., 2021</xref>) provided by Huggingface’s Transformers library
    (<xref alt="Wolf et al., 2020" rid="ref-wolf-etal-2020-transformers" ref-type="bibr">Wolf
    et al., 2020</xref>). The default model for fine-tuning
    (<ext-link ext-link-type="uri" xlink:href="https://huggingface.co/superb/wav2vec2-base-superb-sid">superb/wav2vec2-base-superb-sid</ext-link>)
    was pre-trained on the VoxCeleb1 dataset
    (<xref alt="Nagrani et al., 2017" rid="ref-Nagrani17" ref-type="bibr">Nagrani
    et al., 2017</xref>).</p>
    <p>As Speakerbox is a framework for fine-tuning a pre-trained
    speaker identification transformer, we will not cover the original
    evaluation of the model but instead, provide details about what we
    believe is a more typical use case for a fine-tuned speaker
    identification model.</p>
    <p>Our example dataset contains 9 unique speakers across 10 unique
    recordings and each recording has some or all of the 9 unique
    speakers for a total of 1 hour of audio. For our use case in
    computational social science, this dataset represents a typical
    audio (and transcript) dataset created from government meetings,
    1-on-1 interviews, group interviews, etc.</p>
    <p>We further created random samples of this dataset with 15 minutes
    and 30 minutes of audio (each then split between the train, test,
    and evaluation subsets). The results reported in Table 1 are the
    mean accuracy, precision, and recall of five iterations of model
    training and evaluation using the differently sized datasets as
    inputs to our <monospace>train</monospace> and
    <monospace>eval_model</monospace> functions.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="16%" />
          <col width="17%" />
          <col width="18%" />
          <col width="15%" />
          <col width="34%" />
        </colgroup>
        <thead>
          <tr>
            <th align="left">dataset_size</th>
            <th align="right">mean_accuracy</th>
            <th align="right">mean_precision</th>
            <th align="right">mean_recall</th>
            <th align="right">mean_training_duration_seconds</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">15-minutes</td>
            <td align="right">0.874 ± 0.029</td>
            <td align="right">0.881 ± 0.037</td>
            <td align="right">0.874 ± 0.029</td>
            <td align="right">101 ± 1</td>
          </tr>
          <tr>
            <td align="left">30-minutes</td>
            <td align="right">0.929 ± 0.006</td>
            <td align="right">0.94 ± 0.007</td>
            <td align="right">0.929 ± 0.006</td>
            <td align="right">186 ± 3</td>
          </tr>
          <tr>
            <td align="left">60-minutes</td>
            <td align="right">0.937 ± 0.02</td>
            <td align="right">0.94 ± 0.017</td>
            <td align="right">0.937 ± 0.02</td>
            <td align="right">453 ± 7</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>All results reported are the average of five model training and
    evaluation trials for each of the different dataset sizes. All
    models were fine-tuned using an NVIDIA GTX 1070 TI.</p>
    <p>We provide a method to reproduce these models by
    <ext-link ext-link-type="uri" xlink:href="https://drive.google.com/file/d/1snDuv45cYCYxCae19Dz4tsQauLrA425w/view?usp=sharing">downloading
    the example dataset</ext-link> unzipping its contents and then
    running:</p>
    <code language="python">from speakerbox.examples import train_and_eval_all_example_models

# Returns a pandas DataFrame
results = train_and_eval_all_example_models(
  &quot;/your/path/to/the/unzipped/example-data/&quot;,
)</code>
  </sec>
</sec>
<sec id="usage-in-existing-research">
  <title>Usage in Existing Research</title>
  <p>We are utilizing Speakerbox-trained models to annotate municipal
  council meeting transcripts provided by the Council Data Project
  (<xref alt="Brown et al., 2021" rid="ref-Brown2021" ref-type="bibr">Brown
  et al., 2021</xref>). In our initial research, we first annotated ~10
  hours of audio using the Gecko platform in ~12 hours of time, we then
  used our diarization and labeling method to annotate an additional ~21
  hours of audio in ~6 hours of time. In total, the dataset was
  annotated and compiled in less than ~18 hours and contained ~31 hours
  of audio from meetings of the Seattle City Council
  (<xref alt="Eva &amp; Weber, 2022" rid="ref-eva2022councils" ref-type="bibr">Eva
  &amp; Weber, 2022</xref>). The model trained from the annotated
  dataset with the best precision and recall achieved 0.977 and 0.976
  respectively. We additionally have used this model to annotate ~200
  audio-aligned transcripts of Seattle City Council meetings and are now
  conducting analysis of speaker behaviors and group dynamics in such
  meetings.</p>
</sec>
<sec id="future-work">
  <title>Future Work</title>
  <p>The few-shot approach we have described in Speakerbox can be
  extended, and made more accessible in future work. This could
  include:</p>
  <list list-type="order">
    <list-item>
      <p><bold>The creation of a GUI for dataset preparation, training,
      and application:</bold> GUIs developed with Python have become
      much more accessible in recent years with tools such as
      <ext-link ext-link-type="uri" xlink:href="https://github.com/napari/magicgui">MagicGUI</ext-link>
      being developed. A GUI would likely help non-computational
      scientists more easily train their own fine-tuned Speakerbox
      models.</p>
    </list-item>
    <list-item>
      <p><bold>The creation of a template repository to assist in the
      “productization” of Speakerbox models:</bold> The construction of
      a “template” or “cookiecutter”
      (<xref alt="Greenfeld et al., 2015" rid="ref-cookiecutter" ref-type="bibr">Greenfeld
      et al., 2015</xref>) repository may also be useful. A template
      repository may provide a standard configuration for the storage
      and versioning of the training data and the management of model
      training and evaluation via continuous integration. We plan to
      create and embed a similar system within Council Data Project
      (CDP) infrastructures to enable the transparent training of
      speaker identification models specific to each municipal council.
      In our case, we plan to have a continuous integration system
      attempt to train a new model when new training data is added to
      the infrastructure repository. If the trained model reaches an
      accuracy threshold, we plan to store this model on our remote
      storage system. This model would then be used to annotate
      transcripts with their speakers during the processing of each
      municipal meeting.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We wish to thank the University of Washington Information School
  for support, and a grant from New America Ventures for the Puget Sound
  Public Interest Technology Clinic. We wish to thank all the past and
  present contributors of the Council Data Project.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-jacobi_justice_2017">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Jacobi</surname><given-names>Tonja</given-names></name>
        <name><surname>Schweers</surname><given-names>Dylan</given-names></name>
      </person-group>
      <article-title>Justice, Interrupted: The Effect of Gender, Ideology and Seniority at Supreme Court Oral Arguments</article-title>
      <publisher-loc>Rochester, NY</publisher-loc>
      <year iso-8601-date="2017-10">2017</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <uri>https://papers.ssrn.com/abstract=2933016</uri>
    </element-citation>
  </ref>
  <ref id="ref-morris_reexamining_2001">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Morris</surname><given-names>Jonathan S.</given-names></name>
      </person-group>
      <article-title>Reexamining the Politics of Talk: Partisan Rhetoric in the 104th House</article-title>
      <source>Legislative Studies Quarterly</source>
      <year iso-8601-date="2001">2001</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <volume>26</volume>
      <issue>1</issue>
      <issn>0362-9805</issn>
      <uri>https://www.jstor.org/stable/440405</uri>
      <pub-id pub-id-type="doi">10.2307/440405</pub-id>
      <fpage>101</fpage>
      <lpage>121</lpage>
    </element-citation>
  </ref>
  <ref id="ref-osborn_speaking_2010">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Osborn</surname><given-names>Tracy</given-names></name>
        <name><surname>Mendez</surname><given-names>Jeanette Morehouse</given-names></name>
      </person-group>
      <article-title>Speaking as Women: Women and Floor Speeches in the Senate</article-title>
      <source>Journal of Women, Politics &amp; Policy</source>
      <year iso-8601-date="2010-03">2010</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <volume>31</volume>
      <issue>1</issue>
      <issn>1554-477X</issn>
      <uri>https://doi.org/10.1080/15544770903501384</uri>
      <pub-id pub-id-type="doi">10.1080/15544770903501384</pub-id>
      <fpage>1</fpage>
      <lpage>21</lpage>
    </element-citation>
  </ref>
  <ref id="ref-miller_effect_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Miller</surname><given-names>Michael G.</given-names></name>
        <name><surname>Sutherland</surname><given-names>Joseph L.</given-names></name>
      </person-group>
      <article-title>The Effect of Gender on Interruptions at Congressional Hearings</article-title>
      <source>American Political Science Review</source>
      <year iso-8601-date="2022-05">2022</year><month>05</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <issn>0003-0554</issn>
      <uri>https://www.cambridge.org/core/journals/american-political-science-review/article/effect-of-gender-on-interruptions-at-congressional-hearings/DDD33F28A1C8ED9C162E1793D8126243</uri>
      <pub-id pub-id-type="doi">10.1017/S0003055422000260</pub-id>
      <fpage>1</fpage>
      <lpage>19</lpage>
    </element-citation>
  </ref>
  <ref id="ref-maltzman_politics_1996">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Maltzman</surname><given-names>Forrest</given-names></name>
        <name><surname>Sigelman</surname><given-names>Lee</given-names></name>
      </person-group>
      <article-title>The Politics of Talk: Unconstrained Floor Time in the U.S. House of Representatives</article-title>
      <source>The Journal of Politics</source>
      <year iso-8601-date="1996">1996</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-21">2022</year><month>07</month><day>21</day></date-in-citation>
      <volume>58</volume>
      <issue>3</issue>
      <issn>0022-3816</issn>
      <uri>https://www.jstor.org/stable/2960448</uri>
      <pub-id pub-id-type="doi">10.2307/2960448</pub-id>
      <fpage>819</fpage>
      <lpage>830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-slapin_sound_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Slapin</surname><given-names>Jonathan B.</given-names></name>
        <name><surname>Kirkland</surname><given-names>Justin H.</given-names></name>
      </person-group>
      <article-title>The Sound of Rebellion: Voting Dissent and Legislative Speech in the UK House of Commons</article-title>
      <source>Legislative Studies Quarterly</source>
      <year iso-8601-date="2020">2020</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-25">2022</year><month>07</month><day>25</day></date-in-citation>
      <volume>45</volume>
      <issue>2</issue>
      <issn>1939-9162</issn>
      <uri>http://onlinelibrary.wiley.com/doi/abs/10.1111/lsq.12251</uri>
      <pub-id pub-id-type="doi">10.1111/lsq.12251</pub-id>
      <fpage>153</fpage>
      <lpage>176</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Brown2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Eva Maxfield</given-names></name>
        <name><surname>Huynh</surname><given-names>To</given-names></name>
        <name><surname>Na</surname><given-names>Isaac</given-names></name>
        <name><surname>Ledbetter</surname><given-names>Brian</given-names></name>
        <name><surname>Ticehurst</surname><given-names>Hawk</given-names></name>
        <name><surname>Liu</surname><given-names>Sarah</given-names></name>
        <name><surname>Gilles</surname><given-names>Emily</given-names></name>
        <name><surname>Greene</surname><given-names>Katlyn M. f.</given-names></name>
        <name><surname>Cho</surname><given-names>Sung</given-names></name>
        <name><surname>Ragoler</surname><given-names>Shak</given-names></name>
        <name><surname>Weber</surname><given-names>Nicholas</given-names></name>
      </person-group>
      <article-title>Council Data Project: Software for Municipal Data Collection, Analysis, and Publication</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>68</issue>
      <uri>https://doi.org/10.21105/joss.03904</uri>
      <pub-id pub-id-type="doi">10.21105/joss.03904</pub-id>
      <fpage>3904</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-eva2022councils">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Eva</surname><given-names>Maxfield Brown</given-names></name>
        <name><surname>Weber</surname><given-names>Nicholas</given-names></name>
      </person-group>
      <article-title>Councils in action: Automating the curation of municipal governance data for research</article-title>
      <source>Proceedings of the Association for Information Science and Technology</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>59</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1002/pra2.601</pub-id>
      <fpage>23</fpage>
      <lpage>31</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cookiecutter">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Greenfeld</surname><given-names>Audrey Roy</given-names></name>
        <name><surname>Greenfeld</surname><given-names>Daniel Roy</given-names></name>
        <name><surname>Pierzina</surname><given-names>Raphael</given-names></name>
      </person-group>
      <article-title>cookiecutter</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <uri>https://github.com/cookiecutter/cookiecutter</uri>
    </element-citation>
  </ref>
  <ref id="ref-SpeakerDia2012">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Anguera</surname><given-names>Xavier</given-names></name>
        <name><surname>Bozonnet</surname><given-names>Simon</given-names></name>
        <name><surname>Evans</surname><given-names>Nicholas</given-names></name>
        <name><surname>Fredouille</surname><given-names>Corinne</given-names></name>
        <name><surname>Friedland</surname><given-names>Gerald</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
      </person-group>
      <article-title>Speaker diarization: A review of recent research</article-title>
      <source>IEEE Transactions on Audio, Speech, and Language Processing</source>
      <year iso-8601-date="2012">2012</year>
      <volume>20</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/TASL.2011.2125954</pub-id>
      <fpage>356</fpage>
      <lpage>370</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bredin2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bredin</surname><given-names>Hervé</given-names></name>
        <name><surname>Yin</surname><given-names>Ruiqing</given-names></name>
        <name><surname>Coria</surname><given-names>Juan Manuel</given-names></name>
        <name><surname>Gelly</surname><given-names>Gregory</given-names></name>
        <name><surname>Korshunov</surname><given-names>Pavel</given-names></name>
        <name><surname>Lavechin</surname><given-names>Marvin</given-names></name>
        <name><surname>Fustes</surname><given-names>Diego</given-names></name>
        <name><surname>Titeux</surname><given-names>Hadrien</given-names></name>
        <name><surname>Bouaziz</surname><given-names>Wassim</given-names></name>
        <name><surname>Gill</surname><given-names>Marie-Philippe</given-names></name>
      </person-group>
      <article-title>pyannote.audio: neural building blocks for speaker diarization</article-title>
      <source>ICASSP 2020, IEEE international conference on acoustics, speech, and signal processing</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-Bredin2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bredin</surname><given-names>Hervé</given-names></name>
        <name><surname>Laurent</surname><given-names>Antoine</given-names></name>
      </person-group>
      <article-title>End-to-end speaker segmentation for overlap-aware resegmentation</article-title>
      <source>Proc. Interspeech 2021</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.21437/interspeech.2021-560</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Gecko2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Golan Levy</surname><given-names>Ido Amir</given-names><suffix>Raquel Sitman</suffix></name>
      </person-group>
      <article-title>GECKO - a tool for effective annotation of human conversations</article-title>
      <source>20th annual conference of the international speech communication association, interspeech 2019</source>
      <publisher-loc>Herzliya, Israel</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/gong-io/gecko/blob/master/docs/gecko_interspeech_2019_paper.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-yang2021superb">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yang</surname><given-names>Shu-wen</given-names></name>
        <name><surname>Chi</surname><given-names>Po-Han</given-names></name>
        <name><surname>Chuang</surname><given-names>Yung-Sung</given-names></name>
        <name><surname>Lai</surname><given-names>Cheng-I Jeff</given-names></name>
        <name><surname>Lakhotia</surname><given-names>Kushal</given-names></name>
        <name><surname>Lin</surname><given-names>Yist Y</given-names></name>
        <name><surname>Liu</surname><given-names>Andy T</given-names></name>
        <name><surname>Shi</surname><given-names>Jiatong</given-names></name>
        <name><surname>Chang</surname><given-names>Xuankai</given-names></name>
        <name><surname>Lin</surname><given-names>Guan-Ting</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>SUPERB: Speech processing universal PERformance benchmark</article-title>
      <source>arXiv preprint arXiv:2105.01051</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.21437/interspeech.2021-1775</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wolf-etal-2020-transformers">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>Rémi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>Davison</surname><given-names>Joe</given-names></name>
        <name><surname>Shleifer</surname><given-names>Sam</given-names></name>
        <name><surname>Platen</surname><given-names>Patrick von</given-names></name>
        <name><surname>Ma</surname><given-names>Clara</given-names></name>
        <name><surname>Jernite</surname><given-names>Yacine</given-names></name>
        <name><surname>Plu</surname><given-names>Julien</given-names></name>
        <name><surname>Xu</surname><given-names>Canwen</given-names></name>
        <name><surname>Scao</surname><given-names>Teven Le</given-names></name>
        <name><surname>Gugger</surname><given-names>Sylvain</given-names></name>
        <name><surname>Drame</surname><given-names>Mariama</given-names></name>
        <name><surname>Lhoest</surname><given-names>Quentin</given-names></name>
        <name><surname>Rush</surname><given-names>Alexander M.</given-names></name>
      </person-group>
      <article-title>Transformers: State-of-the-art natural language processing</article-title>
      <source>Proceedings of the 2020 conference on empirical methods in natural language processing: System demonstrations</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <uri>https://www.aclweb.org/anthology/2020.emnlp-demos.6</uri>
      <fpage>38</fpage>
      <lpage>45</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Nagrani17">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nagrani</surname><given-names>A.</given-names></name>
        <name><surname>Chung</surname><given-names>J. S.</given-names></name>
        <name><surname>Zisserman</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>VoxCeleb: A large-scale speaker identification dataset</article-title>
      <source>INTERSPEECH</source>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.21437/interspeech.2017-950</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Wolters2020ASO">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wolters</surname><given-names>Piper</given-names></name>
        <name><surname>Careaga</surname><given-names>Chris</given-names></name>
        <name><surname>Hutchinson</surname><given-names>Brian</given-names></name>
        <name><surname>Phillips</surname><given-names>Lauren A.</given-names></name>
      </person-group>
      <article-title>A study of few-shot audio classification</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2020">2020</year>
      <volume>abs/2012.01573</volume>
    </element-citation>
  </ref>
  <ref id="ref-li2022-few-shot">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Yanxiong</given-names></name>
        <name><surname>Wang</surname><given-names>Wucheng</given-names></name>
        <name><surname>Chen</surname><given-names>Hao</given-names></name>
        <name><surname>Cao</surname><given-names>Wenchang</given-names></name>
        <name><surname>Li</surname><given-names>Wei</given-names></name>
        <name><surname>He</surname><given-names>Qianhua</given-names></name>
      </person-group>
      <article-title>Few-shot speaker identification using depthwise separable convolutional network with channel attention</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2204.11180</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2204.11180</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kumar2020-few-shot">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kumar</surname><given-names>Neeraj</given-names></name>
        <name><surname>Goel</surname><given-names>Srishti</given-names></name>
        <name><surname>Narang</surname><given-names>Ankur</given-names></name>
        <name><surname>Lall</surname><given-names>Brejesh</given-names></name>
      </person-group>
      <article-title>Few shot adaptive normalization driven multi-speaker speech synthesis</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2012.07252</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2012.07252</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-setfit">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tunstall</surname><given-names>Lewis</given-names></name>
        <name><surname>Reimers</surname><given-names>Nils</given-names></name>
        <name><surname>Jo</surname><given-names>Unso Eun Seo</given-names></name>
        <name><surname>Bates</surname><given-names>Luke</given-names></name>
        <name><surname>Korat</surname><given-names>Daniel</given-names></name>
        <name><surname>Wasserblat</surname><given-names>Moshe</given-names></name>
        <name><surname>Pereg</surname><given-names>Oren</given-names></name>
      </person-group>
      <article-title>Efficient few-shot learning without prompts</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2209.11055</uri>
      <pub-id pub-id-type="doi">10.48550/ARXIV.2209.11055</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Torfi2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Torfi</surname><given-names>Amirsina</given-names></name>
      </person-group>
      <article-title>SpeechPy - a library for speech processing and recognition</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>3</volume>
      <issue>27</issue>
      <uri>https://doi.org/10.21105/joss.00749</uri>
      <pub-id pub-id-type="doi">10.21105/joss.00749</pub-id>
      <fpage>749</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vaswani2017attention">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N</given-names></name>
        <name><surname>Kaiser</surname><given-names>Łukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention is all you need</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2017">2017</year>
      <volume>30</volume>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
