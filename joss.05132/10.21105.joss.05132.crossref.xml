<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230320T212821-85497e13dc1bc5db4c999f40dd0764b3ce3d9563</doi_batch_id>
    <timestamp>20230320212821</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>03</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>83</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Speakerbox: Few-Shot Learning for Speaker
Identification with Transformers</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Eva Maxfield</given_name>
            <surname>Brown</surname>
            <ORCID>https://orcid.org/0000-0003-2564-0373</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>To</given_name>
            <surname>Huynh</surname>
            <ORCID>https://orcid.org/0000-0002-9664-3662</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Nicholas</given_name>
            <surname>Weber</surname>
            <ORCID>https://orcid.org/0000-0002-6008-3763</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>03</month>
          <day>20</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5132</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05132</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7729994</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5132</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05132</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05132</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05132.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="jacobi_justice_2017">
            <article_title>Justice, Interrupted: The Effect of Gender,
Ideology and Seniority at Supreme Court Oral Arguments</article_title>
            <author>Jacobi</author>
            <cYear>2017</cYear>
            <unstructured_citation>Jacobi, T., &amp; Schweers, D.
(2017). Justice, Interrupted: The Effect of Gender, Ideology and
Seniority at Supreme Court Oral Arguments [{SSRN} {Scholarly} {Paper}].
https://papers.ssrn.com/abstract=2933016</unstructured_citation>
          </citation>
          <citation key="morris_reexamining_2001">
            <article_title>Reexamining the Politics of Talk: Partisan
Rhetoric in the 104th House</article_title>
            <author>Morris</author>
            <journal_title>Legislative Studies Quarterly</journal_title>
            <issue>1</issue>
            <volume>26</volume>
            <doi>10.2307/440405</doi>
            <issn>0362-9805</issn>
            <cYear>2001</cYear>
            <unstructured_citation>Morris, J. S. (2001). Reexamining the
Politics of Talk: Partisan Rhetoric in the 104th House. Legislative
Studies Quarterly, 26(1), 101–121.
https://doi.org/10.2307/440405</unstructured_citation>
          </citation>
          <citation key="osborn_speaking_2010">
            <article_title>Speaking as Women: Women and Floor Speeches
in the Senate</article_title>
            <author>Osborn</author>
            <journal_title>Journal of Women, Politics &amp;
Policy</journal_title>
            <issue>1</issue>
            <volume>31</volume>
            <doi>10.1080/15544770903501384</doi>
            <issn>1554-477X</issn>
            <cYear>2010</cYear>
            <unstructured_citation>Osborn, T., &amp; Mendez, J. M.
(2010). Speaking as Women: Women and Floor Speeches in the Senate.
Journal of Women, Politics &amp; Policy, 31(1), 1–21.
https://doi.org/10.1080/15544770903501384</unstructured_citation>
          </citation>
          <citation key="miller_effect_2022">
            <article_title>The Effect of Gender on Interruptions at
Congressional Hearings</article_title>
            <author>Miller</author>
            <journal_title>American Political Science
Review</journal_title>
            <doi>10.1017/S0003055422000260</doi>
            <issn>0003-0554</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Miller, M. G., &amp; Sutherland, J.
L. (2022). The Effect of Gender on Interruptions at Congressional
Hearings. American Political Science Review, 1–19.
https://doi.org/10.1017/S0003055422000260</unstructured_citation>
          </citation>
          <citation key="maltzman_politics_1996">
            <article_title>The Politics of Talk: Unconstrained Floor
Time in the U.S. House of Representatives</article_title>
            <author>Maltzman</author>
            <journal_title>The Journal of Politics</journal_title>
            <issue>3</issue>
            <volume>58</volume>
            <doi>10.2307/2960448</doi>
            <issn>0022-3816</issn>
            <cYear>1996</cYear>
            <unstructured_citation>Maltzman, F., &amp; Sigelman, L.
(1996). The Politics of Talk: Unconstrained Floor Time in the U.S. House
of Representatives. The Journal of Politics, 58(3), 819–830.
https://doi.org/10.2307/2960448</unstructured_citation>
          </citation>
          <citation key="slapin_sound_2020">
            <article_title>The Sound of Rebellion: Voting Dissent and
Legislative Speech in the UK House of Commons</article_title>
            <author>Slapin</author>
            <journal_title>Legislative Studies Quarterly</journal_title>
            <issue>2</issue>
            <volume>45</volume>
            <doi>10.1111/lsq.12251</doi>
            <issn>1939-9162</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Slapin, J. B., &amp; Kirkland, J. H.
(2020). The Sound of Rebellion: Voting Dissent and Legislative Speech in
the UK House of Commons. Legislative Studies Quarterly, 45(2), 153–176.
https://doi.org/10.1111/lsq.12251</unstructured_citation>
          </citation>
          <citation key="Brown2021">
            <article_title>Council Data Project: Software for Municipal
Data Collection, Analysis, and Publication</article_title>
            <author>Brown</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>68</issue>
            <volume>6</volume>
            <doi>10.21105/joss.03904</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Brown, E. M., Huynh, T., Na, I.,
Ledbetter, B., Ticehurst, H., Liu, S., Gilles, E., Greene, K. M. f.,
Cho, S., Ragoler, S., &amp; Weber, N. (2021). Council Data Project:
Software for Municipal Data Collection, Analysis, and Publication.
Journal of Open Source Software, 6(68), 3904.
https://doi.org/10.21105/joss.03904</unstructured_citation>
          </citation>
          <citation key="eva2022councils">
            <article_title>Councils in action: Automating the curation
of municipal governance data for research</article_title>
            <author>Eva</author>
            <journal_title>Proceedings of the Association for
Information Science and Technology</journal_title>
            <issue>1</issue>
            <volume>59</volume>
            <doi>10.1002/pra2.601</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Eva, M. B., &amp; Weber, N. (2022).
Councils in action: Automating the curation of municipal governance data
for research. Proceedings of the Association for Information Science and
Technology, 59(1), 23–31.
https://doi.org/10.1002/pra2.601</unstructured_citation>
          </citation>
          <citation key="cookiecutter">
            <article_title>cookiecutter</article_title>
            <author>Greenfeld</author>
            <journal_title>GitHub repository</journal_title>
            <cYear>2015</cYear>
            <unstructured_citation>Greenfeld, A. R., Greenfeld, D. R.,
&amp; Pierzina, R. (2015). cookiecutter. In GitHub repository. GitHub.
https://github.com/cookiecutter/cookiecutter</unstructured_citation>
          </citation>
          <citation key="SpeakerDia2012">
            <article_title>Speaker diarization: A review of recent
research</article_title>
            <author>Anguera</author>
            <journal_title>IEEE Transactions on Audio, Speech, and
Language Processing</journal_title>
            <issue>2</issue>
            <volume>20</volume>
            <doi>10.1109/TASL.2011.2125954</doi>
            <cYear>2012</cYear>
            <unstructured_citation>Anguera, X., Bozonnet, S., Evans, N.,
Fredouille, C., Friedland, G., &amp; Vinyals, O. (2012). Speaker
diarization: A review of recent research. IEEE Transactions on Audio,
Speech, and Language Processing, 20(2), 356–370.
https://doi.org/10.1109/TASL.2011.2125954</unstructured_citation>
          </citation>
          <citation key="Bredin2020">
            <article_title>pyannote.audio: neural building blocks for
speaker diarization</article_title>
            <author>Bredin</author>
            <journal_title>ICASSP 2020, IEEE international conference on
acoustics, speech, and signal processing</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Bredin, H., Yin, R., Coria, J. M.,
Gelly, G., Korshunov, P., Lavechin, M., Fustes, D., Titeux, H., Bouaziz,
W., &amp; Gill, M.-P. (2020). pyannote.audio: neural building blocks for
speaker diarization. ICASSP 2020, IEEE International Conference on
Acoustics, Speech, and Signal Processing.</unstructured_citation>
          </citation>
          <citation key="Bredin2021">
            <article_title>End-to-end speaker segmentation for
overlap-aware resegmentation</article_title>
            <author>Bredin</author>
            <journal_title>Proc. Interspeech 2021</journal_title>
            <doi>10.21437/interspeech.2021-560</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Bredin, H., &amp; Laurent, A. (2021).
End-to-end speaker segmentation for overlap-aware resegmentation. Proc.
Interspeech 2021.
https://doi.org/10.21437/interspeech.2021-560</unstructured_citation>
          </citation>
          <citation key="Gecko2019">
            <article_title>GECKO - a tool for effective annotation of
human conversations</article_title>
            <author>Golan Levy</author>
            <journal_title>20th annual conference of the international
speech communication association, interspeech 2019</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Golan Levy, I. A., Raquel Sitman.
(2019). GECKO - a tool for effective annotation of human conversations.
20th Annual Conference of the International Speech Communication
Association, Interspeech 2019.
https://github.com/gong-io/gecko/blob/master/docs/gecko_interspeech_2019_paper.pdf</unstructured_citation>
          </citation>
          <citation key="yang2021superb">
            <article_title>SUPERB: Speech processing universal
PERformance benchmark</article_title>
            <author>Yang</author>
            <journal_title>arXiv preprint
arXiv:2105.01051</journal_title>
            <doi>10.21437/interspeech.2021-1775</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Yang, S., Chi, P.-H., Chuang, Y.-S.,
Lai, C.-I. J., Lakhotia, K., Lin, Y. Y., Liu, A. T., Shi, J., Chang, X.,
Lin, G.-T., &amp; others. (2021). SUPERB: Speech processing universal
PERformance benchmark. arXiv Preprint arXiv:2105.01051.
https://doi.org/10.21437/interspeech.2021-1775</unstructured_citation>
          </citation>
          <citation key="wolf-etal-2020-transformers">
            <article_title>Transformers: State-of-the-art natural
language processing</article_title>
            <author>Wolf</author>
            <journal_title>Proceedings of the 2020 conference on
empirical methods in natural language processing: System
demonstrations</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Wolf, T., Debut, L., Sanh, V.,
Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,
Funtowicz, M., Davison, J., Shleifer, S., Platen, P. von, Ma, C.,
Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger, S., … Rush, A. M.
(2020). Transformers: State-of-the-art natural language processing.
Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations, 38–45.
https://www.aclweb.org/anthology/2020.emnlp-demos.6</unstructured_citation>
          </citation>
          <citation key="Nagrani17">
            <article_title>VoxCeleb: A large-scale speaker
identification dataset</article_title>
            <author>Nagrani</author>
            <journal_title>INTERSPEECH</journal_title>
            <doi>10.21437/interspeech.2017-950</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Nagrani, A., Chung, J. S., &amp;
Zisserman, A. (2017). VoxCeleb: A large-scale speaker identification
dataset. INTERSPEECH.
https://doi.org/10.21437/interspeech.2017-950</unstructured_citation>
          </citation>
          <citation key="Wolters2020ASO">
            <article_title>A study of few-shot audio
classification</article_title>
            <author>Wolters</author>
            <journal_title>ArXiv</journal_title>
            <volume>abs/2012.01573</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Wolters, P., Careaga, C., Hutchinson,
B., &amp; Phillips, L. A. (2020). A study of few-shot audio
classification. ArXiv, abs/2012.01573.</unstructured_citation>
          </citation>
          <citation key="li2022-few-shot">
            <article_title>Few-shot speaker identification using
depthwise separable convolutional network with channel
attention</article_title>
            <author>Li</author>
            <doi>10.48550/ARXIV.2204.11180</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Li, Y., Wang, W., Chen, H., Cao, W.,
Li, W., &amp; He, Q. (2022). Few-shot speaker identification using
depthwise separable convolutional network with channel attention. arXiv.
https://doi.org/10.48550/ARXIV.2204.11180</unstructured_citation>
          </citation>
          <citation key="kumar2020-few-shot">
            <article_title>Few shot adaptive normalization driven
multi-speaker speech synthesis</article_title>
            <author>Kumar</author>
            <doi>10.48550/ARXIV.2012.07252</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Kumar, N., Goel, S., Narang, A.,
&amp; Lall, B. (2020). Few shot adaptive normalization driven
multi-speaker speech synthesis. arXiv.
https://doi.org/10.48550/ARXIV.2012.07252</unstructured_citation>
          </citation>
          <citation key="setfit">
            <article_title>Efficient few-shot learning without
prompts</article_title>
            <author>Tunstall</author>
            <doi>10.48550/ARXIV.2209.11055</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Tunstall, L., Reimers, N., Jo, U. E.
S., Bates, L., Korat, D., Wasserblat, M., &amp; Pereg, O. (2022).
Efficient few-shot learning without prompts. arXiv.
https://doi.org/10.48550/ARXIV.2209.11055</unstructured_citation>
          </citation>
          <citation key="Torfi2018">
            <article_title>SpeechPy - a library for speech processing
and recognition</article_title>
            <author>Torfi</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>27</issue>
            <volume>3</volume>
            <doi>10.21105/joss.00749</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Torfi, A. (2018). SpeechPy - a
library for speech processing and recognition. Journal of Open Source
Software, 3(27), 749.
https://doi.org/10.21105/joss.00749</unstructured_citation>
          </citation>
          <citation key="vaswani2017attention">
            <article_title>Attention is all you need</article_title>
            <author>Vaswani</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>30</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Vaswani, A., Shazeer, N., Parmar, N.,
Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., &amp; Polosukhin, I.
(2017). Attention is all you need. Advances in Neural Information
Processing Systems, 30.</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
