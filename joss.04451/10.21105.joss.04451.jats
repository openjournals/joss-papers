<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4451</article-id>
<article-id pub-id-type="doi">10.21105/joss.04451</article-id>
<title-group>
<article-title><monospace>pygetpapers</monospace>: a Python library for
automated retrieval of scientific literature</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7016-747X</contrib-id>
<name>
<surname>Garg</surname>
<given-names>Ayush</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-8721-7197</contrib-id>
<name>
<surname>Smith-Unna</surname>
<given-names>Richard D</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-3386-3972</contrib-id>
<name>
<surname>Murray-Rust</surname>
<given-names>Peter</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Richmond, Richmond, VA 23173, United
States</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>LabDAO, https://www.labdao.com/</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Yusuf-Hamied Department of Chemistry, University of
Cambridge, Lensfield Road, Cambridge, CB2 1EW, UK</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>7</volume>
<issue>75</issue>
<fpage>4451</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>REST API</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>pygetpapers</monospace> has been developed to allow
  searching of the scientific literature in repositories with a range of
  textual queries and metadata. It downloads content using APIs in an
  automated fashion and is designed to be extensible to the growing
  number of Open Access repositories.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>An increasing amount of research, particularly in medicine and
  applied science, is now based on meta-analysis and systematic review
  of the existing literature
  (<xref alt="“Systematic Reviews,” 2022" rid="ref-systematic_review" ref-type="bibr">“Systematic
  Reviews,” 2022</xref>). In such reviews, scientists frequently
  download thousands of articles and analyze them by Natural Language
  Processing (NLP) through Text and Data Mining (TDM) or Content Mining.
  A common approach is to search bibliographic resources with keywords,
  download the hits, scan them manually, and reject papers that do not
  fit the criteria for the meta-analysis. The typical text-based
  searches on sites are broad, with many false positives and often only
  based on abstracts. We know of cases where systematic reviewers
  downloaded 30,000 articles and eventually used 30. Retrieval is often
  done by crawling/scraping sites, such as journals but is easier and
  faster when these articles are in Open Access repositories such as
  <monospace>arXiv</monospace>, <monospace>EuropePMC</monospace>,
  <monospace>bioRxiv</monospace>, <monospace>medRxiv</monospace>. But
  each repository has its own API and functionality, which makes it hard
  for individuals to (a) access, (b) set flags, and (c) use generic
  queries. In 2015, we reviewed tools for scraping websites and decided
  that none met our needs and so developed
  <monospace>getpapers</monospace>
  (<xref alt="Smith-Unna, 2021" rid="ref-getpapers" ref-type="bibr">Smith-Unna,
  2021</xref>), with the key advance of integrating a query submission
  with bulk fulltext-download of all the hits.</p>
  <sec id="pygetpapers">
    <title>pygetpapers</title>
    <p><monospace>getpapers</monospace> was written in NodeJS and has
    now been completely rewritten in Python3
    (<monospace>pygetpapers</monospace>) for easier distribution and
    integration. Typical use of <monospace>getpapers</monospace> is
    shown in a recent paper
    (<xref alt="Wind et al., 2021" rid="ref-getpapers_use" ref-type="bibr">Wind
    et al., 2021</xref>) where the authors “analyzed key term frequency
    within 20,000 representatives [Antimicrobial Resistance]
    articles”.</p>
    <p>An important aspect is to provide a simple cross-platform
    approach for scientists who may find tools like
    <monospace>curl</monospace> too complex and want a one-line command
    to combine the search, download, and analysis into a single: “please
    give me the results”. We’ve tested this on many interns who learn
    <monospace>pygetpapers</monospace> in minutes. It was also easy to
    wrap it into a <monospace>tkinter</monospace> graphical user
    interface (GUI)
    (<xref alt="Lundh, 1999" rid="ref-tkinter" ref-type="bibr">Lundh,
    1999</xref>). The architecture of the results is simple and natural,
    based on full-text files in the normal filesystem. The result of
    <monospace>pygetpapers</monospace> is interfaced using a “main” or
    “controller” JSON file (for eg. eupmc_results.json), which allows
    corpus to be reused/added to. This allows maximum flexibility of
    re-use and some projects have large amounts of derived data in these
    directories.</p>
    <preformat>pygetpapers -q &quot;METHOD: invasive plant species&quot; -k 10 -o &quot;invasive_plant&quot; 
-c --makehtml -x</preformat>
    <p>OUTPUT:</p>
    <preformat>INFO: Final query is METHOD: invasive plant species
INFO: Total Hits are 17910
0it [00:00, ?it/s]WARNING: Keywords not found for paper 1
WARNING: Keywords not found for paper 4
1it [00:00, 164.87it/s]
INFO: Saving XML files to C:\invasive_plant\*\fulltext.xml
100%|███████████████████████████████████| 10/10 [00:21&lt;00:00,  2.11s/it]</preformat>
    <p>Example query of <monospace>pygetpapers</monospace></p>
    <p>The number of repositories is rapidly expanding, driven by the
    rise in preprint use. These repositories and aggregation sites such
    as <monospace>EuropePMC</monospace>, <monospace>HAL</monospace>,
    <monospace>SciELO</monospace> use their own dialect of query syntax
    and API access. A major aspect of <monospace>pygetpapers</monospace>
    is to make it easy to add new repository downloaders, often by
    people who have little coding experience.
    <monospace>pygetpapers</monospace> is built on a modular system and
    repository-specific code can be swapped in as needed. By configuring
    repositories in a configuration file, users can easily configure
    support for new repositories.</p>
    <preformat>[europe_pmc]
query_url=https://www.ebi.ac.uk/europepmc/webservices/rest/searchPOST

citationurl=https://www.ebi.ac.uk/europepmc/webservices/rest
/{source}/{pmcid}/citations?page=1&amp;pageSize=1000&amp;format=xml

referencesurl=https://www.ebi.ac.uk/europepmc/webservices/rest
/{source}/{pmcid}/references?page=1&amp;pageSize=1000&amp;format=xml

xmlurl=https://www.ebi.ac.uk/europepmc/webservices/rest/{pmcid}/fullTextXML

suppurl=https://www.ebi.ac.uk/europepmc/webservices/rest
/{pmcid}/supplementaryFiles

zipurl= http://europepmc.org/ftp/suppl/OA/{key}/{pmcid}.zip

date_query=SUPPORTED
term=SUPPORTED
update=SUPPORTED
restart=SUPPORTED
class_name=EuropePmc
library_name= europe_pmc
features_not_supported = [&quot;filter&quot;,]</preformat>
    <p>Example configuration for a repository (europePMC)</p>
    <p>Many <bold>searches</bold> are simple keywords or phrases.
    However, these often fail to include synonyms and phrases and
    authors spend time creating complex error-prone boolean queries. We
    have developed a dictionary-based approach to automate much of the
    creation of complex queries.</p>
    <p>The <bold>downloaded material</bold> is inherently complex (See
    <xref alt="Data" rid="data">Data</xref>). Some of this has been
    systematized, especially in biosciences, and the NIH (US National
    Institutes of Health) led to the JATS/NISO standard to create highly
    structured documents.</p>
    <p>Frequently users want to search <bold>incrementally</bold>, e.g.,
    downloading part and resuming later (especially with poor
    connectivity where work is often lost). Also,
    <monospace>pygetpapers</monospace> allows regular updates, e.g.,
    weekly searches of preprint servers.</p>
    <p><monospace>pygetpapers</monospace> takes the approach of
    downloading once and re-analyzing later on local filestore. This
    saves repeated querying where connections are poor or where there is
    suspicion that publishers may surveil users. Moreover, publishers
    rarely provide more than full-text Boolean searches, whereas local
    tools can analyze sections and non-textual material.</p>
    <p>We do not know of other tools which have the same functionality.
    <monospace>curl</monospace>
    (<xref alt="Hostetter et al., 1997" rid="ref-curl" ref-type="bibr">Hostetter
    et al., 1997</xref>) requires detailed knowledge of the download
    protocol. <monospace>VosViewer</monospace>
    (<xref alt="J. &amp; L, 2010" rid="ref-VOSviewer" ref-type="bibr">J.
    &amp; L, 2010</xref>) is mainly aimed at bibliography/citations.</p>
  </sec>
</sec>
<sec id="overview-of-the-architecture">
  <title>Overview of the architecture</title>
  <sec id="data">
    <title>Data</title>
    <sec id="raw-data">
      <title>raw data</title>
      <p>The download may be repository-dependent but usually
      contains:</p>
      <list list-type="bullet">
        <list-item>
          <p>download metadata. (query, date, errors, etc.)</p>
        </list-item>
        <list-item>
          <p>journal/article metadata. We use JATS-NISO
          (<xref alt="Standardized Markup for Journal Articles, 2021" rid="ref-JATS" ref-type="bibr"><italic>Standardized
          Markup for Journal Articles</italic>, 2021</xref>) which is
          widely used by publishers and repository owners, especially in
          bioscience and medicine. There are over 200 tags.</p>
        </list-item>
        <list-item>
          <p>fulltext. This can be</p>
          <list list-type="bullet">
            <list-item>
              <p>XML (fulltext and metadata)</p>
            </list-item>
            <list-item>
              <p>images (these may not always be available)</p>
            </list-item>
            <list-item>
              <p>tables (these are often separate)</p>
            </list-item>
            <list-item>
              <p>PDF - usually includes the whole material but not
              machine-sectioned</p>
            </list-item>
            <list-item>
              <p>HTML . often avaliable on websites</p>
            </list-item>
          </list>
        </list-item>
        <list-item>
          <p>supplemental data. This is very variable, often as PDF but
          also raw data files and sometimes zipped. It is not
          systematically arranged but <monospace>pygetpapers</monospace>
          allows for some heuristics.</p>
        </list-item>
        <list-item>
          <p>figures. This is not supported by some repositories, and
          others may require custom code.</p>
        </list-item>
      </list>
      <fig>
        <caption><p>Fig.1 Architecture of
        <monospace>pygetpapers</monospace></p></caption>
        <graphic mimetype="image" mime-subtype="png" xlink:href="media/0ea70e34c119b29bf1bf072c5c89fa01938fb560.png" xlink:title="" />
      </fig>
      <p>Fig.1 Architecture of <monospace>pygetpapers</monospace></p>
      <p>For this reason we create a directory structure with a root
      (<ext-link ext-link-type="uri" xlink:href="https://github.com/ContentMine/CTree/blob/master/CProject.md"><monospace>CProject</monospace></ext-link>)
      and a
      (<ext-link ext-link-type="uri" xlink:href="https://github.com/ContentMine/CTree/blob/master/CTree.md"><monospace>CTree</monospace></ext-link>)
      subdirectory for each downloaded article or document.
      <monospace>pygetpapers</monospace> will routinely populate this
      with 1-5 files or subdirectories (see above). At present
      <monospace>pygetpapers</monospace> always creates a *_result.json
      file (possibly empty) and this can be used as a marker for
      identifying <monospace>CTrees</monospace>. This means that a
      <ext-link ext-link-type="uri" xlink:href="https://github.com/ContentMine/CTree/blob/master/CProject.md"><monospace>CProject</monospace></ext-link>
      contains subdirectories which may be
      <ext-link ext-link-type="uri" xlink:href="https://github.com/ContentMine/CTree/blob/master/CTree.md"><monospace>CTree</monospace></ext-link>s
      or not, distinguished by this marker.</p>
    </sec>
    <sec id="derived-data">
      <title>derived data</title>
      <p>In addition to the downloaded data (already quite variable),
      users often wish to create new derived data and this directory
      structure is designed so that tools can add an arbitrary amount of
      new data, normally in sub-directory trees. For example we have
      sibling projects that add data to the
      <ext-link ext-link-type="uri" xlink:href="https://github.com/ContentMine/CTree/blob/master/CTree.md"><monospace>CTree</monospace></ext-link>:</p>
      <list list-type="bullet">
        <list-item>
          <p><monospace>docanalysis</monospace> (text analysis including
          <monospace>NLTK</monospace> and
          <ext-link ext-link-type="uri" xlink:href="https://github.com/explosion/spaCy"><monospace>spaCy/sciSpaCy</monospace></ext-link></p>
        </list-item>
        <list-item>
          <p><monospace>pyamiimage</monospace>
          <ext-link ext-link-type="uri" xlink:href="https://github.com/petermr/pyamiimage">image
          processing and analysis of figures</ext-link></p>
        </list-item>
      </list>
      <preformat>C:.
│   eupmc_results.json
│
├───PMC8157994
│       eupmc_result.json
│       fulltext.xml
│
├───PMC8180188
│       eupmc_result.json
│       fulltext.xml
│</preformat>
      <p>and with examples of derived data</p>
      <preformat>├───PMC8198815
│       eupmc_result.json
│       fulltext.xml
|.      bag_of_words.txt
|.      figure/
|.          raw.jpg
|.          skeleton.png
│
├───10.9999_123456 # CTree due to fooRxiv_result.json
│       fooRxiv_result.json
│       fulltext.xml
|.      bag_of_words.txt
|.      search/
|.          results/
|.              terpenes.csv
│
|.   univ_bar_thesis_studies_on_lantana/ # CTree dues to thesis_12345_results.json
|.      thesis_12345_results.json
|       fulltext.pdf
|.      figures/
|.          figure/
|               Fig1/
|.       
|____summary/ # not CTree as no child *_results.json
|.       bag_of_words.txt
|.       figures/
|            &lt;aggregated and filtered figures&gt;
  </preformat>
      <p>Typical download directory</p>
      <p>Several types of download have been combined in this CProject
      and some CTrees have derived data.</p>
    </sec>
  </sec>
  <sec id="code">
    <title>Code</title>
    <sec id="download-protocol">
      <title>Download protocol</title>
      <p>Most repository APIs provide a cursor-based approach to
      querying:</p>
      <list list-type="order">
        <list-item>
          <p>A query is sent and the repository creates a list of M hits
          (pointers to documents), sets a cursor start, and returns this
          information to the <monospace>pygetpapers</monospace>
          client.</p>
        </list-item>
        <list-item>
          <p>The client requests a chunk of size N &lt;= M (normally
          25-1000) and the repository replies with N pointers to
          documents.</p>
        </list-item>
        <list-item>
          <p>The server response is pages of hits (metadata) as XML ,
          normally &lt;= 1000 hits per page (1 sec).</p>
        </list-item>
        <list-item>
          <p><monospace>pygetpapers</monospace> - incremental aggregates
          XML metadata as python dict in memory.</p>
        </list-item>
        <list-item>
          <p>If cursor indicates next page,
          <monospace>pygetpapers</monospace> submits a query for next
          page, otherwise it terminates the data collection and
          processes the python dict.</p>
        </list-item>
        <list-item>
          <p>If user has requested supplemental data (e.g., references,
          citations, fulltext) then the
          <monospace>pygetpapers</monospace> iterates through the python
          dict and uses the identifier, usually in the form of DOI, to
          query and download supplemental data seperately.</p>
        </list-item>
        <list-item>
          <p>When the search is finished,
          <monospace>pygetpapers</monospace> writes the metadata to
          CProject (Top level project directory) as JSON (total, and
          creates CTrees (per-article directories) with individual
          metadata).</p>
        </list-item>
        <list-item>
          <p>It also recovers from crashes and restarts if needed).</p>
        </list-item>
      </list>
      <p>The control module <monospace>pygetpapers.py</monospace> reads
      the commandline and</p>
      <list list-type="bullet">
        <list-item>
          <p>Selects the repository-specific downloader</p>
        </list-item>
        <list-item>
          <p>Creates a query from user input and/or terms from
          dictionaries</p>
        </list-item>
        <list-item>
          <p>Adds options and constraints</p>
        </list-item>
        <list-item>
          <p>Downloads according to the protocol above, including
          recording progress in a metadata file</p>
        </list-item>
      </list>
    </sec>
  </sec>
</sec>
<sec id="generic-downloading-concerns">
  <title>Generic downloading concerns</title>
  <list list-type="bullet">
    <list-item>
      <p>Download speeds. Excessively rapid or voluminous downloads can
      overload servers and are sometimes hostile (DOS). We have
      discussed this with major sites (<monospace>EuropePMC</monospace>,
      <monospace>biorXiv</monospace>, <monospace>Crossref</monospace>,
      etc. and therefore choose to download sequentially instead of
      sending parallel requests in
      <monospace>pygetpapers</monospace>.</p>
    </list-item>
    <list-item>
      <p>Authentication (alerting repo to downloader header).
      <monospace>pygetpapers</monospace> supports anonymous,
      non-authenticated, access but includes a header (e.g., for
      <monospace>Crossref</monospace>)</p>
    </list-item>
  </list>
</sec>
<sec id="design">
  <title>Design</title>
  <p>The tool has been designed for ease of implementation, installation
  (including platform independence) and future extension. It also
  abstracts some of the variation in query languages and APIs (where
  there do not appear to be standards). For example for “date”,
  <monospace>EuropePMC</monospace> uses
  <monospace>FIRST_PDATE[DD-MM-YYYY to DD-MM-YY]</monospace> (This is
  the format in which you provide a date constraint to a query for
  <monospace>EuropePMC</monospace>) but <monospace>bioRxiv</monospace>
  uses <monospace>DD-MM-YYYY/DD-MM-YY</monospace>.
  <monospace>pygetpapers</monospace> provides
  <monospace>DATE</monospace> as an abstraction. It also uses a
  commandline, which makes it easy either to wrap the use in system
  calls, or layer a GUI on top.</p>
  <p>Some repositories only support metadata while others include text
  and some even provide links to data downloads; again pygetpapers
  supports this range. Because there are hundreds of repositories
  (including preprints) the design includes a modular approach. And
  because some repositories emit variable amounts of information we can
  customise the outputs.</p>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <p><monospace>getpapers</monospace> was implemented in
  <monospace>NodeJS</monospace>, which allows multithreading and
  therefore potentially download rates of several XML documents per
  second on a fast line. Installing <monospace>NodeJS</monospace> was a
  problem on some systems (especially Windows) and was not well suited
  for integration with scientific libraries (mainly coded in Java and
  Python). We, therefore, decided to rewrite in Python, keeping only the
  command line and output structure, and have found very easy
  integration with other tools, including GUIs.
  <monospace>pygetpapers</monospace> can be run both as a command-line
  tool and a module, which makes it versatile.</p>
  <sec id="core">
    <title>core</title>
    <p>The core mainly consists of:</p>
    <list list-type="bullet">
      <list-item>
        <p><monospace>pygetpapers.py</monospace> (query-builder and
        runner). This includes query abstractions such as dates and
        Boolean queries for terms</p>
      </list-item>
      <list-item>
        <p><monospace>download_tools.py</monospace> (generic code for
        query/download (REST))</p>
      </list-item>
    </list>
  </sec>
  <sec id="repository-interfaces">
    <title>repository interfaces</title>
    <p>We have tried to minimise the amount of repository-specific code,
    choosing to use declarative configuration files. To add a new
    repository you will need to:</p>
    <list list-type="bullet">
      <list-item>
        <p>create a configuration file (Fig. 2)</p>
      </list-item>
      <list-item>
        <p>subclass the repo from
        <monospace>repository_interface.py</monospace></p>
      </list-item>
      <list-item>
        <p>add any repository specific code to add features or disable
        others</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="interface-with-other-tools">
  <title>Interface with other tools</title>
  <p>Downloading is naturally modular, rather slow, and we interface by
  writing all output to the filesystem. This means that a wide range of
  tools (Unix, Windows, Java, Python, etc.) can analyze and transform
  it. The target documents are usually static so downloads only need to
  be done once. Among our own downstream tools are</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>pyami</monospace>
      (<xref alt="Murray-Rust, 2021" rid="ref-pyami" ref-type="bibr">Murray-Rust,
      2021</xref>) - sectioning the document</p>
    </list-item>
    <list-item>
      <p><monospace>docanalysis</monospace>
      (<xref alt="Ayush Garg, 2021" rid="ref-docanalysis" ref-type="bibr">Ayush
      Garg, 2021</xref>) - textual analysis and Natural Language
      Processing</p>
    </list-item>
    <list-item>
      <p><monospace>pyamiimage</monospace>
      (<xref alt="Peter Murray-Rust, 2021" rid="ref-pyamiimage" ref-type="bibr">Peter
      Murray-Rust, 2021</xref>) - analysis of the content of images in
      downloaded documents</p>
    </list-item>
    <list-item>
      <p>third party text analysis of PDF using
      GROBID(<xref alt="GROBID, 2008--2021" rid="ref-GROBID" ref-type="bibr"><italic>GROBID</italic>,
      2008--2021</xref>) and
      PDFBox(<xref alt="Apache PDFBox® - a Java PDF Library, n.d." rid="ref-PDFBox" ref-type="bibr"><italic>Apache
      PDFBox® - a Java PDF Library</italic>, n.d.</xref>).</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from Shweata N. Hegde in helping write
  the documentation. We also acknowledge Matthew Evans’ support to help
  improve the quality of the code, and the repository.</p>
</sec>
<sec id="contribution-statement">
  <title>Contribution statement</title>
  <p>Ayush Garg: Development of the Tool, Architecture. Richard D.
  Smith-Unna: Base framework, work on <monospace>getpapers</monospace>
  (predecessor to <monospace>pygetpapers</monospace>). Peter
  Murray-Rust: Supervision, framework, writing manuscript.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-systematic_review">
    <element-citation publication-type="article-journal">
      <article-title>Systematic reviews</article-title>
      <source>BioMed Central</source>
      <year iso-8601-date="2022-02-12">2022</year><month>02</month><day>12</day>
    </element-citation>
  </ref>
  <ref id="ref-getpapers_use">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wind</surname><given-names>L. L.</given-names></name>
        <name><surname>Briganti</surname><given-names>J. S.</given-names></name>
        <name><surname>Brown</surname><given-names>A. M.</given-names></name>
      </person-group>
      <article-title>Finding what is inaccessible: Antimicrobial resistance language use among the one health domains. antibiotics</article-title>
      <publisher-loc>Basel, Switzerland</publisher-loc>
      <year iso-8601-date="2021-04-10">2021</year><month>04</month><day>10</day>
      <pub-id pub-id-type="doi">10.3390/antibiotics10040385</pub-id>
      <pub-id pub-id-type="pmid">33916878</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-curl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hostetter</surname><given-names>M.</given-names></name>
        <name><surname>Kranz</surname><given-names>D. A.</given-names></name>
        <name><surname>Seed</surname><given-names>C.</given-names></name>
        <name><surname>Terman</surname><given-names>C.</given-names></name>
        <name><surname>Ward</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>Curl: A gentle slope language for the web</article-title>
      <source>World Wide Web Journal</source>
      <year iso-8601-date="1997">1997</year>
      <volume>2</volume>
      <issue>2</issue>
      <fpage>121</fpage>
      <lpage>134</lpage>
    </element-citation>
  </ref>
  <ref id="ref-VOSviewer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>J.</surname><given-names>Eck N.</given-names></name>
        <name><surname>L</surname><given-names>Waltman</given-names></name>
      </person-group>
      <article-title>Software survey: VOSviewer, a computer program for bibliometric mapping’</article-title>
      <source>Scientometrics</source>
      <year iso-8601-date="2010">2010</year>
      <volume>84</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1007/s11192-009-0146-3</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-JATS">
    <element-citation>
      <article-title>Standardized markup for journal articles: Journal article tag suite (JATS) | NISO website</article-title>
      <year iso-8601-date="2021-07-07">2021</year><month>07</month><day>07</day>
      <uri>www.niso.org/standards-committees/jats .</uri>
    </element-citation>
  </ref>
  <ref id="ref-pyami">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Murray-Rust</surname><given-names>Peter</given-names></name>
      </person-group>
      <source>Pyami - semantic reader of the scientific literature</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/petermr/pyami/</uri>
    </element-citation>
  </ref>
  <ref id="ref-docanalysis">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ayush Garg</surname><given-names>Shweata N. Hedge</given-names></name>
      </person-group>
      <source>Docanalysis - unsupervised entity extraction</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/petermr/docanalysis</uri>
    </element-citation>
  </ref>
  <ref id="ref-pyamiimage">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Peter Murray-Rust</surname><given-names>V. S. M. Roopa</given-names><suffix>Anubhab Chakraborty</suffix></name>
      </person-group>
      <source>Pyamiimage - tools to extract semantic information from scientific diagrams</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/petermr/pyamiimage</uri>
    </element-citation>
  </ref>
  <ref id="ref-getpapers">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Smith-Unna</surname><given-names>Rik</given-names></name>
      </person-group>
      <source>Getpapers - tools to extract semantic information from scientific diagrams</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/ContentMine/getpapers</uri>
    </element-citation>
  </ref>
  <ref id="ref-tkinter">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lundh</surname><given-names>Fredrik</given-names></name>
      </person-group>
      <article-title>An introduction to tkinter</article-title>
      <source>URL: www. pythonware. com/library/tkinter/introduction/index. htm</source>
      <year iso-8601-date="1999">1999</year>
    </element-citation>
  </ref>
  <ref id="ref-GROBID">
    <element-citation>
      <article-title>GROBID</article-title>
      <publisher-name>GitHub</publisher-name>
      <uri>https://github.com/kermitt2/grobid</uri>
    </element-citation>
  </ref>
  <ref id="ref-PDFBox">
    <element-citation>
      <article-title>Apache PDFBox® - a java PDF library</article-title>
      <uri>https://pdfbox.apache.org/</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
