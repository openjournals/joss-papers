<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7217</article-id>
<article-id pub-id-type="doi">10.21105/joss.07217</article-id>
<title-group>
<article-title>Salt: Multimodal Multitask Machine Learning for High
Energy Physics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9752-9204</contrib-id>
<name>
<surname>Barr</surname>
<given-names>Jackson</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7543-3471</contrib-id>
<name>
<surname>Biswas</surname>
<given-names>Diptaparna</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1530-0519</contrib-id>
<name>
<surname>Draguet</surname>
<given-names>Maxence</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4475-6734</contrib-id>
<name>
<surname>Gadow</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5417-2081</contrib-id>
<name>
<surname>Haines</surname>
<given-names>Emil</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4907-9499</contrib-id>
<name>
<surname>Karkout</surname>
<given-names>Osama</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-0070-5900</contrib-id>
<name>
<surname>Kobylianskii</surname>
<given-names>Dmitrii</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-6726-9851</contrib-id>
<name>
<surname>Lai</surname>
<given-names>Wei Sheng</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1406-1413</contrib-id>
<name>
<surname>Leigh</surname>
<given-names>Matthew</given-names>
</name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6527-0253</contrib-id>
<name>
<surname>Luongo</surname>
<given-names>Nicholas</given-names>
</name>
<xref ref-type="aff" rid="aff-10"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4784-6340</contrib-id>
<name>
<surname>Oleksiyuk</surname>
<given-names>Ivan</given-names>
</name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5966-0332</contrib-id>
<name>
<surname>Pond</surname>
<given-names>Nikita</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7092-3893</contrib-id>
<name>
<surname>Rettie</surname>
<given-names>Sébastien</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0393-666X</contrib-id>
<name>
<surname>Vaitkus</surname>
<given-names>Andrius</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7969-0301</contrib-id>
<name>
<surname>Stroud</surname>
<given-names>Samuel Van</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5588-0020</contrib-id>
<name>
<surname>Wagner</surname>
<given-names>Johannes</given-names>
</name>
<xref ref-type="aff" rid="aff-9"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University College London, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Siegen</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University of Oxford, United Kingdom</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>European Laboratory for Particle Physics CERN,
Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Nikhef</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>Department of Particle Physics and Astrophysics, Weizmann
Institute of Science, Israel</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Université de Genève, Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>Technical University of Munich, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-9">
<institution-wrap>
<institution>University of California, Berkeley</institution>
</institution-wrap>
</aff>
<aff id="aff-10">
<institution-wrap>
<institution>Argonne National Laboratory</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>10</volume>
<issue>112</issue>
<fpage>7217</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>high energy physics</kwd>
<kwd>machine learning</kwd>
<kwd>jet physics</kwd>
<kwd>flavour tagging</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>High energy physics studies the fundamental particles and forces
  that constitute the universe, often through experiments conducted in
  large particle accelerators such as the Large Hadron Collider (LHC)
  (<xref alt="Evans &amp; Bryant, 2008" rid="ref-EvansU003A2008" ref-type="bibr">Evans
  &amp; Bryant, 2008</xref>). <monospace>Salt</monospace> is a Python
  application developed for the high energy physics community that
  streamlines the training and deployment of advanced machine learning
  (ML) models, making them more accessible and promoting shared best
  practices. <monospace>Salt</monospace> features a generic multimodal,
  multitask model skeleton which, coupled with a strong emphasis on
  modularity, configurability, and ease of use, can be used to tackle a
  wide variety of high energy physics ML applications.</p>
  <p>Some key features of <monospace>Salt</monospace> are listed
  below:</p>
  <list list-type="bullet">
    <list-item>
      <p>Based on established frameworks: <monospace>Salt</monospace> is
      built upon PyTorch
      (<xref alt="Paszke et al., 2019" rid="ref-pytorch" ref-type="bibr">Paszke
      et al., 2019</xref>) and Lightning
      (<xref alt="Falcon &amp; The PyTorch Lightning team, 2019" rid="ref-lightning" ref-type="bibr">Falcon
      &amp; The PyTorch Lightning team, 2019</xref>) for maximum
      performance and scalability with minimal boilerplate code.</p>
    </list-item>
    <list-item>
      <p>Multimodal, multitask models: <monospace>Salt</monospace>
      models support multimodal inputs and can be configured to perform
      various tasks such as classification, regression, segmentation,
      and edge classification tasks. Any combination of these can be
      used to flexibly define models for multitask learning
      problems.</p>
    </list-item>
    <list-item>
      <p>Customisable and extensible: <monospace>Salt</monospace>
      supports full customisation of training and model configuration
      through YAML config files. Its modular design allows for the easy
      integration of custom dataloaders, layers, and models.</p>
    </list-item>
    <list-item>
      <p>Train at scale: <monospace>Salt</monospace> can handle large
      volumes of data with efficient HDF5
      (<xref alt="The HDF Group, 1997" rid="ref-hdf5U003A2023" ref-type="bibr">The
      HDF Group, 1997</xref>) dataloaders. It also includes multi-GPU
      support from Lightning, enabling distributed training.</p>
    </list-item>
    <list-item>
      <p>Deployment ready: <monospace>Salt</monospace> facilitates ONNX
      (<xref alt="Bai et al., 2019" rid="ref-onnx" ref-type="bibr">Bai
      et al., 2019</xref>) serialisation for integrating models into C++
      based software environments.</p>
    </list-item>
  </list>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In high energy physics research the reliance on ML for data
  analysis and object classification is growing
  (<xref alt="Cagnotta et al., 2022" rid="ref-CagnottaU003A2022" ref-type="bibr">Cagnotta
  et al., 2022</xref>;
  <xref alt="Guest et al., 2018" rid="ref-GuestU003A2018" ref-type="bibr">Guest
  et al., 2018</xref>). <monospace>Salt</monospace> meets this growing
  need by providing a versatile, performant, and user-friendly tool for
  developing advanced ML models. <monospace>Salt</monospace> was
  originally developed to train state of the art flavour tagging models
  at the ATLAS experiment
  (<xref alt="ATLAS Collaboration, 2008" rid="ref-ATLASU003A2008" ref-type="bibr">ATLAS
  Collaboration, 2008</xref>) at the LHC. Flavour tagging, the
  identification of jets from bottom and charm quarks, plays a crucial
  role in analysing ATLAS collision data. This process is key for
  precision Standard Model measurements, particularly in the
  characterisation of the Higgs boson, and for investigating new
  phenomena. The unique characteristics of hadrons containing bottom and
  charm quarks – such as their long lifetimes, high mass, and high decay
  multiplicity – create distinct signatures in particle detectors that
  can be effectively exploited by ML algorithms. The presence of hadrons
  containing bottom and charm quarks can be inferred via the
  identification of approximately 3-5 reconstructed charged particle
  trajectories from the weak decay of the heavy flavour hadron amidst
  several more tracks from the primary proton-proton interaction
  vertex.</p>
  <p>While initially developed for flavour tagging,
  <monospace>Salt</monospace> has evolved into a flexible tool that can
  be used for a wide range of tasks, from object and event
  classification, regression of object properties, to object
  reconstruction (via edge classification or input segmentation),
  demonstrating its broad applicability across various data analysis
  challenges in high energy physics.</p>
</sec>
<sec id="model-architecture">
  <title>Model Architecture</title>
  <p>Salt is designed to be fully modular, but ships with a flexible
  model architecture that can be configured for a variety of use cases.
  This architecture facilitates the training of multimodal and multitask
  models as depicted in
  <xref alt="[fig:salt-arch]" rid="figU003Asalt-arch">[fig:salt-arch]</xref>,
  and is designed to take advantage of multiple input modalities. In the
  context of jet classification, these input modalities might include
  global features of the jet and varying numbers of jet constituents
  such as charged particle trajectories, calorimeter energy depositions,
  reconstructed leptons, or inner detector spacepoints. The architecture
  is described briefly below. First, each input type (e.g., tracks,
  calorimeter deposits) is independently projected into a common
  embedding space of fixed dimension using separate initialisation
  networks. These initialisation networks can optionally concatenate
  global features with constituent features and apply positional
  encoding to certain features, for example azimuthal angle. Once
  embedded, the different types of constituents are considered to be in
  the same semantic space and are processed together by a transformer
  encoder that allows them to interact through stacked multi-head
  attention layers. The encoder maintains the same embedding dimension
  throughout its layers and can optionally update edge features if they
  are present. The encoder then outputs to a set of task-specific
  modules, each tailored to a specific learning objective. The
  initialisation networks, transformer encoder, and task-specific
  networks are trained together. This approach allows the model to
  leverage all the available detector information, leading to improved
  performance. Concrete examples of this architecture are in use at
  ATLAS
  (<xref alt="Collaboration, 2025" rid="ref-GN2" ref-type="bibr">Collaboration,
  2025</xref>;
  <xref alt="Graph Neural Network Jet Flavour Tagging with the ATLAS Detector, 2022" rid="ref-GN1" ref-type="bibr"><italic><named-content content-type="nocase">Graph
  Neural Network Jet Flavour Tagging with the ATLAS
  Detector</named-content></italic>, 2022</xref>;
  <xref alt="Transformer Neural Networks for Identifying Boosted Higgs Bosons decaying into b\bar{b} and c\bar{c} in ATLAS, 2023" rid="ref-GN2X" ref-type="bibr"><italic><named-content content-type="nocase">Transformer
  Neural Networks for Identifying Boosted Higgs Bosons decaying into
  <inline-formula><alternatives>
  <tex-math><![CDATA[b\bar{b}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>b</mml:mi><mml:mover><mml:mi>b</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[c\bar{c}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>c</mml:mi><mml:mover><mml:mi>c</mml:mi><mml:mo accent="true">‾</mml:mo></mml:mover></mml:mrow></mml:math></alternatives></inline-formula>
  in ATLAS</named-content></italic>, 2023</xref>).</p>
  <fig>
    <caption><p>This diagram illustrates the flow of information within
    a generic model trained using <monospace>Salt</monospace>. In this
    example, global object features are provided alongside two types of
    constituents, “Type A” and “Type B”, which represent different input
    modalities such as charged particle trajectories or calorimeter
    energy depositions. The model is configured with three training
    objectives, each of which may relate to the global object or one of
    the constituent modalities. Concatenation is denoted by
    <inline-formula><alternatives>
    <tex-math><![CDATA[\oplus]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>⊕</mml:mi></mml:math></alternatives></inline-formula>.<styled-content id="figU003Asalt-arch"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="salt-arch.png" />
  </fig>
</sec>
<sec id="related-work">
  <title>Related work</title>
  <p><monospace>Umami</monospace>
  (<xref alt="Barr &amp; others, 2024" rid="ref-umami" ref-type="bibr">Barr
  &amp; others, 2024</xref>) is a related software package in use at
  ATLAS. While <monospace>Salt</monospace> relies on similar
  preprocessing techniques as those provided by
  <monospace>Umami</monospace>, it provides several additional features
  which make it a more powerful and flexible tool for creating advanced
  ML models. Namely, <monospace>Salt</monospace> provides support for
  multimodal and multitask learning, optimised Transformer encoders
  (<xref alt="Vaswani et al., 2017" rid="ref-2017arXiv170603762V" ref-type="bibr">Vaswani
  et al., 2017</xref>), and distributed model training.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The development of <monospace>Salt</monospace> is part of the
  offline software research and development programme of the ATLAS
  Collaboration, and we thank the collaboration for its support and
  cooperation. This work is funded in part by the UK’s Science and
  Technology Facilities Council via University College London’s Centre
  for Doctoral Training in Data Intensive Science, and the Royal
  Society.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-EvansU003A2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Evans</surname><given-names>Lyndon</given-names></name>
        <name><surname>Bryant</surname><given-names>Philip</given-names></name>
      </person-group>
      <article-title>LHC Machine</article-title>
      <source>JINST</source>
      <year iso-8601-date="2008">2008</year>
      <volume>3</volume>
      <pub-id pub-id-type="doi">10.1088/1748-0221/3/08/S08001</pub-id>
      <fpage>S08001</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ATLASU003A2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>ATLAS Collaboration</string-name>
      </person-group>
      <article-title>The ATLAS Experiment at the CERN Large Hadron Collider</article-title>
      <source>JINST</source>
      <year iso-8601-date="2008">2008</year>
      <volume>3</volume>
      <pub-id pub-id-type="doi">10.1088/1748-0221/3/08/S08003</pub-id>
      <fpage>S08003</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hdf5U003A2023">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <string-name>The HDF Group</string-name>
      </person-group>
      <article-title>Hierarchical Data Format, version 5</article-title>
      <year iso-8601-date="1997">1997</year>
    </element-citation>
  </ref>
  <ref id="ref-GuestU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Guest</surname><given-names>Dan</given-names></name>
        <name><surname>Cranmer</surname><given-names>Kyle</given-names></name>
        <name><surname>Whiteson</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>Deep learning and its application to LHC physics</article-title>
      <source>Annual Review of Nuclear and Particle Science</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2018-10">2018</year><month>10</month>
      <volume>68</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1146/annurev-nucl-101917-021019</uri>
      <pub-id pub-id-type="doi">10.1146/annurev-nucl-101917-021019</pub-id>
      <fpage>161</fpage>
      <lpage>181</lpage>
    </element-citation>
  </ref>
  <ref id="ref-CagnottaU003A2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cagnotta</surname><given-names>Antimo</given-names></name>
        <name><surname>Carnevali</surname><given-names>Francesco</given-names></name>
        <name><surname>Iorio</surname><given-names>Agostino De</given-names></name>
      </person-group>
      <article-title>Machine learning applications for jet tagging in the CMS experiment</article-title>
      <source>Applied Sciences</source>
      <publisher-name>MDPI AG</publisher-name>
      <year iso-8601-date="2022-10">2022</year><month>10</month>
      <volume>12</volume>
      <issue>20</issue>
      <uri>https://doi.org/10.3390/app122010574</uri>
      <pub-id pub-id-type="doi">10.3390/app122010574</pub-id>
      <fpage>10574</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-pytorch">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lightning">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Falcon</surname><given-names>William</given-names></name>
        <string-name>The PyTorch Lightning team</string-name>
      </person-group>
      <article-title>PyTorch Lightning</article-title>
      <year iso-8601-date="2019-03">2019</year><month>03</month>
      <uri>https://github.com/Lightning-AI/lightning</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.3828935</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-GN1">
    <element-citation publication-type="report">
      <article-title>Graph Neural Network Jet Flavour Tagging with the ATLAS Detector</article-title>
      <publisher-name>CERN</publisher-name>
      <publisher-loc>Geneva</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <uri>https://cds.cern.ch/record/2811135</uri>
    </element-citation>
  </ref>
  <ref id="ref-GN2X">
    <element-citation publication-type="report">
      <article-title>Transformer Neural Networks for Identifying Boosted Higgs Bosons decaying into b\bar{b} and c\bar{c} in ATLAS</article-title>
      <publisher-name>CERN</publisher-name>
      <publisher-loc>Geneva</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <uri>https://cds.cern.ch/record/2866601</uri>
    </element-citation>
  </ref>
  <ref id="ref-onnx">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Lu</surname><given-names>Fang</given-names></name>
        <name><surname>Zhang</surname><given-names>Ke</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>ONNX: Open neural network exchange</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/onnx/onnx; GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-umami">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Barr</surname><given-names>Jackson</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Umami: A python toolkit for jet flavour tagging</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/umami-hep/umami-preprocessing; GitHub</publisher-name>
      <year iso-8601-date="2024">2024</year>
    </element-citation>
  </ref>
  <ref id="ref-2017arXiv170603762V">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vaswani</surname><given-names>Ashish</given-names></name>
        <name><surname>Shazeer</surname><given-names>Noam</given-names></name>
        <name><surname>Parmar</surname><given-names>Niki</given-names></name>
        <name><surname>Uszkoreit</surname><given-names>Jakob</given-names></name>
        <name><surname>Jones</surname><given-names>Llion</given-names></name>
        <name><surname>Gomez</surname><given-names>Aidan N.</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Polosukhin</surname><given-names>Illia</given-names></name>
      </person-group>
      <article-title>Attention Is All You Need</article-title>
      <source>arXiv e-prints</source>
      <year iso-8601-date="2017-06">2017</year><month>06</month>
      <uri>https://arxiv.org/abs/1706.03762</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id>
      <fpage>arXiv:1706.03762</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-GN2">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Collaboration</surname><given-names>ATLAS</given-names></name>
      </person-group>
      <article-title>Transforming jet flavour tagging at ATLAS</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://arxiv.org/abs/2505.19689</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
