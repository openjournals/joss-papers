<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4628</article-id>
<article-id pub-id-type="doi">10.21105/joss.04628</article-id>
<title-group>
<article-title>DBMS-Benchmarker: Benchmark and Evaluate DBMS in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0002-3359-2386</contrib-id>
<name>
<surname>Erdelt</surname>
<given-names>Patrick K.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jestel</surname>
<given-names>Jascha</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Berliner Hochschule für Technik (BHT)</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-10-16">
<day>16</day>
<month>10</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>79</issue>
<fpage>4628</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>DBMS</kwd>
<kwd>JDBC</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>DBMS-Benchmarker is a Python-based application-level blackbox
  benchmark tool for Database Management Systems (DBMS). It is intended
  for reproducible measurement and easy evaluation of the performance
  the user receives, even in complex benchmark situations. It connects
  to a given list of DBMS (via JDBC) and runs a given list of (SQL)
  benchmark queries. Queries can be parametrized and randomized. Results
  and evaluations are available via a Python interface and can be
  inspected with standard Python tools like pandas DataFrames. An
  interactive visual dashboard assists in multi-dimensional analysis of
  the results.</p>
  <p>This module has been tested with Clickhouse, Exasol, Citus Data
  (Hyperscale), IBM DB2, MariaDB, MariaDB Columnstore, MemSQL
  (SingleStore), MonetDB, MySQL, OmniSci (HEAVY.AI), Oracle DB,
  PostgreSQL, SQL Server, SAP HANA, TimescaleDB, and Vertica.</p>
  <p>See the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Beuth-Erdelt/DBMS-Benchmarker">homepage</ext-link>
  and the
  <ext-link ext-link-type="uri" xlink:href="https://dbmsbenchmarker.readthedocs.io/en/latest/Docs.html">documentation</ext-link>
  for more details.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Performance benchmarking of database management systems (DBMS) is
  an active research area and has a broad audience. It is used
  “<italic>by DBMS developers to evaluate their work and to find out
  which algorithm works best in which situation. Benchmarks are used by
  (potential) customers to evaluate what system or hardware to buy or
  rent. Benchmarks are used by administrators to find bottlenecks and
  adjust configurations. Benchmarks are used by users to compare
  semantically equivalent queries and to find the best formulation
  alternative</italic>”, Erdelt
  (<xref alt="2021" rid="ref-10.1007U002F978-3-030-84924-5_6" ref-type="bibr">2021</xref>).
  Approaches and their special implementations are also examined in
  benchmarks in academia. There is a large variety of DBMS types and
  products. For example, solid IT GmbH
  (<xref alt="2022" rid="ref-DBEngines" ref-type="bibr">2022</xref>)
  ranks 350 DBMS (150 Relational) and Carnegie Mellon Database Group
  (<xref alt="2022" rid="ref-DBDBIO" ref-type="bibr">2022</xref>) lists
  850 DBMS (280 Relational). We focus on Relational DBMS (RDBMS) in the
  following. Their types can be divided into, for example, row-wise,
  column-wise, in-memory, distributed, and GPU-enhanced. All of these
  products have unique characteristics, special use cases, advantages
  and disadvantages, and their own justification. In order to be able to
  verify and ensure performance measurements, we want to be able to
  create and repeat benchmarking scenarios. Repetition and thorough
  evaluation are crucial, in particular in the age of cloud-based
  systems with their diversity of hardware configurations
  (<xref alt="Kersten et al., 2018" rid="ref-DBLPconfsigmodKerstenKZ18" ref-type="bibr">Kersten
  et al., 2018</xref>;
  <xref alt="Kounev et al., 2020" rid="ref-KounevLK20" ref-type="bibr">Kounev
  et al., 2020</xref>;
  <xref alt="Raasveldt et al., 2018" rid="ref-Raasveldt2018FBC32099503209955" ref-type="bibr">Raasveldt
  et al., 2018</xref>).</p>
  <p>Thus there is widespread need for a tool to support the repetition
  and reproducibility of benchmarking situations, and that is capable of
  connecting to all these systems.</p>
  <p>When we collect a lot of data during benchmarking processes, we
  also need a tool that will help with the statistical, visual, and
  interactive analysis of the results. The authors advocate using Python
  as a common Data Science language, since “<italic>it is a mature
  language programming, easy for the newbies, and can be used as a
  specific platform for data scientists, thanks to its large ecosystem
  of scientific libraries and its high and vibrant community</italic>”,
  Igual &amp; Seguí
  (<xref alt="2017" rid="ref-seriesU002FutcsU002FIgualS17" ref-type="bibr">2017</xref>).
  This helps in implementing the tool within a pipeline, for example to
  make use of closed-loop benchmarking situations
  (<xref alt="He et al., 2019" rid="ref-10114533389063338912" ref-type="bibr">He
  et al., 2019</xref>), or to closely inspect parts of queries
  (<xref alt="Kersten et al., 2018" rid="ref-DBLPconfsigmodKerstenKZ18" ref-type="bibr">Kersten
  et al., 2018</xref>). It also allows the use of common and
  sophisticated tools to inspect and evaluate the results. To name a
  few: pandas
  (<xref alt="McKinney, 2010" rid="ref-mckinney-proc-scipy-2010" ref-type="bibr">McKinney,
  2010</xref>;
  <xref alt="The pandas development team, 2020" rid="ref-reback2020pandas" ref-type="bibr">The
  pandas development team, 2020</xref>) for statistical evaluation of
  tabular data, scipy
  (<xref alt="Virtanen et al., 2020" rid="ref-2020SciPy-NMeth" ref-type="bibr">Virtanen
  et al., 2020</xref>) for scientific investigation of data, IPython and
  Jupyter notebooks
  (<xref alt="Kluyver et al., 2016" rid="ref-Kluyver2016jupyter" ref-type="bibr">Kluyver
  et al., 2016</xref>) for interactive analysis and display of results,
  Matplotlib
  (<xref alt="Hunter, 2007" rid="ref-HunterU003A2007" ref-type="bibr">Hunter,
  2007</xref>) and Seaborn
  (<xref alt="Waskom, 2021" rid="ref-Waskom2021" ref-type="bibr">Waskom,
  2021</xref>) for visual analysis, or even machine learning tools.
  Moreover, Python is currently the most popular computer language
  (<xref alt="PYPL, 2022" rid="ref-PYPL" ref-type="bibr">PYPL,
  2022</xref>;
  <xref alt="TIOBE, 2022" rid="ref-TIOBE" ref-type="bibr">TIOBE,
  2022</xref>).</p>
  <p>To our knowledge, there is no other such tool, c.f. also the
  studies in Seybold &amp; Domaschka
  (<xref alt="2017" rid="ref-10.1007U002F978-3-319-67162-8_12" ref-type="bibr">2017</xref>)
  and Brent &amp; Fekete
  (<xref alt="2019" rid="ref-10.1007U002F978-3-030-12079-5_4" ref-type="bibr">2019</xref>).
  There are other tools like Apache JMeter (Java), HammerDB (Tcl),
  Sysbench (LuaJIT), OLTPBench (Java), and BenchBase (Java) that provide
  very nice features. However they do not fit these needs, since they
  are not Python-based. Moreover some are limited in supported DBMS, in
  supporting repetition and (statistical) evaluation, or do not support
  randomized queries. The design decisions of this tool have been
  described in more detail in Erdelt
  (<xref alt="2021" rid="ref-10.1007U002F978-3-030-84924-5_6" ref-type="bibr">2021</xref>).
  DBMS-Benchmarker has been used as to support receiving scientific
  results about benchmarking DBMS performance in Cloud environments as
  in Erdelt
  (<xref alt="2021" rid="ref-10.1007U002F978-3-030-84924-5_6" ref-type="bibr">2021</xref>)
  and Erdelt
  (<xref alt="2022" rid="ref-10.1007U002F978-3-030-94437-7_6" ref-type="bibr">2022</xref>).</p>
  <sec id="summary-of-solution">
    <title>Summary of Solution</title>
    <p>DBMS-Benchmarker is Python3-based and helps to <bold>benchmark
    DBMS</bold>. It</p>
    <list list-type="bullet">
      <list-item>
        <p>connects to all DBMS having a JDBC interface</p>
      </list-item>
      <list-item>
        <p>requires <italic>only</italic> JDBC - no vendor specific
        supplements are used</p>
      </list-item>
      <list-item>
        <p>benchmarks arbitrary SQL queries</p>
      </list-item>
      <list-item>
        <p>supports planning of complex test scenarios</p>
      </list-item>
      <list-item>
        <p>allows easy repetition of benchmarks in varying settings</p>
      </list-item>
      <list-item>
        <p>allows randomized queries to avoid caching side effects</p>
      </list-item>
      <list-item>
        <p>investigates a number of timing aspects</p>
      </list-item>
      <list-item>
        <p>investigates a number of other aspects - received result
        sets, precision, number of clients</p>
      </list-item>
      <list-item>
        <p>collects hardware metrics from a Prometheus server
        (<xref alt="Rabenstein &amp; Volz, 2015" rid="ref-208870" ref-type="bibr">Rabenstein
        &amp; Volz, 2015</xref>)</p>
      </list-item>
    </list>
    <p>DBMS-Benchmarker helps to <bold>evaluate results</bold> - by
    providing</p>
    <list list-type="bullet">
      <list-item>
        <p>metrics that can be analyzed by aggregation in
        multi-dimensions</p>
      </list-item>
      <list-item>
        <p>predefined evaluations like statistics</p>
      </list-item>
      <list-item>
        <p>in standard Python data structures</p>
      </list-item>
      <list-item>
        <p>in Jupyter notebooks - see
        <ext-link ext-link-type="uri" xlink:href="https://beuth-erdelt.github.io/DBMS-Benchmarker/Evaluation-Demo.html">rendered
        example</ext-link></p>
      </list-item>
      <list-item>
        <p>in an interactive dashboard</p>
      </list-item>
    </list>
    <p>Some features are inspired by
    <ext-link ext-link-type="uri" xlink:href="http://www.tpc.org/tpch/">TPC-H</ext-link>
    and
    <ext-link ext-link-type="uri" xlink:href="http://www.tpc.org/tpcds/">TPC-DS</ext-link>
    - Decision Support Benchmarks, which are provided in part as
    predefined configs.</p>
  </sec>
</sec>
<sec id="a-basic-example">
  <title>A Basic Example</title>
  <p>The following simple use case runs the query
  <monospace>SELECT COUNT(*) FROM test</monospace> 10 times against one
  local (existing) MySQL installation.</p>
  <p>Run <monospace>pip install dbmsbenchmarker</monospace> for
  installation. Make sure Java is set up correctly. We assume here we
  have downloaded the required JDBC driver, e.g.,
  <monospace>mysql-connector-java-8.0.13.jar</monospace>.</p>
  <sec id="configuration">
    <title>Configuration</title>
    <sec id="dbms-configuration-file-e.g.-in-.configconnections.config">
      <title>DBMS configuration file, e.g. in
      <monospace>./config/connections.config</monospace></title>
      <preformat>[
  {
    'name': &quot;MySQL&quot;,
    'active': True,
    'JDBC': {
      'driver': &quot;com.mysql.cj.jdbc.Driver&quot;,
      'url': &quot;jdbc:mysql://localhost:3306/database&quot;,
      'auth': [&quot;username&quot;, &quot;password&quot;],
      'jar': &quot;mysql-connector-java-8.0.13.jar&quot;
    }
  }
]</preformat>
    </sec>
    <sec id="queries-configuration-file-e.g.-in-.configqueries.config">
      <title>Queries configuration file, e.g. in
      <monospace>./config/queries.config</monospace></title>
      <preformat>{
  'name': 'Some simple queries',
  'connectionmanagement': {
        'timeout': 5 # in seconds
    },
  'queries':
  [
    {
      'title': &quot;Count all rows in test&quot;,
      'query': &quot;SELECT COUNT(*) FROM test&quot;,
      'numRun': 10
    }
  ]
}</preformat>
    </sec>
  </sec>
  <sec id="perform-benchmark-and-evaluate-results">
    <title>Perform Benchmark and Evaluate Results</title>
    <p>Run the CLI command:
    <monospace>dbmsbenchmarker run -e yes -b -f ./config</monospace></p>
    <p>After benchmarking has completed we will see a message like
    <monospace>Experiment &lt;code&gt; has been finished</monospace>.
    The script has created a result folder in the current directory
    containing the results. <monospace>&lt;code&gt;</monospace> is the
    name of the folder.</p>
    <p>Run the CLI command: <monospace>dbmsdashboard</monospace></p>
    <p>This will start the evaluation dashboard at
    <monospace>localhost:8050</monospace>. Visit the address in a
    browser and select the experiment
    <monospace>&lt;code&gt;</monospace>. Alternatively you may use
    Python’s pandas.</p>
  </sec>
</sec>
<sec id="description">
  <title>Description</title>
  <sec id="experiment">
    <title>Experiment</title>
    <p>An <bold>experiment</bold> is organized in
    <italic>queries</italic>. A <bold>query</bold> is a statement that
    is understood by a Database Management System (DBMS).</p>
  </sec>
  <sec id="single-query">
    <title>Single Query</title>
    <p>A <bold>benchmark of a query</bold> consists of these steps:</p>
    <fig>
      <caption><p>measured times of query processing
      parts.<styled-content id="figU003AConcept-Query"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/docs/Concept-Query.png" xlink:title="" />
    </fig>
    <list list-type="order">
      <list-item>
        <p>Establish a <bold>connection</bold> between client and server
        This uses <monospace>jaydebeapi.connect()</monospace> (and also
        creates a cursor - time not measured)</p>
      </list-item>
      <list-item>
        <p>Send the query from client to server and</p>
      </list-item>
      <list-item>
        <p><bold>Execute</bold> the query on server
        These two steps use <monospace>execute()</monospace> on a cursor
        of the JDBC connection</p>
      </list-item>
      <list-item>
        <p><bold>Transfer</bold> the result back to client
        This uses <monospace>fetchall()</monospace> on a cursor of the
        JDBC connection</p>
      </list-item>
      <list-item>
        <p>Close the connection
        This uses <monospace>close()</monospace> on the cursor and the
        connection</p>
      </list-item>
    </list>
    <p>The times needed for the connection (1), execution (2 and 3), and
    transfer (4) steps are measured on the client side. A unit of
    connect, send, execute, and transfer of a single query is called a
    <bold>run</bold>. Connection time will be zero if an existing
    connection is reused. A sequence of units of sending, executing, and
    transmitting between establishing and discarding a connection is
    called a <bold>session</bold>. This is the same as a run, if we
    always reconnect prior to sending a query, but if we choose to reuse
    a connection this will cover multiple runs.</p>
    <p>A basic parameter of a query is the <bold>number of runs</bold>.
    To configure sessions it is also possible to adjust</p>
    <list list-type="bullet">
      <list-item>
        <p>the <bold>number of runs per connection</bold> (session
        length) and</p>
      </list-item>
      <list-item>
        <p>the <bold>number of parallel connections</bold> (to simulate
        several simultanious clients)</p>
      </list-item>
      <list-item>
        <p>a <bold>timeout</bold> (maximum lifespan of a connection)</p>
      </list-item>
      <list-item>
        <p>a <bold>delay</bold> for throttling (waiting time before each
        connection or execution)</p>
      </list-item>
    </list>
    <p>for the same query. Parallel clients are simulated using the
    <monospace>pool.apply_async()</monospace> method of a
    <monospace>Pool</monospace> object of the module
    <monospace>multiprocessing</monospace>. Runs and their benchmark
    times are ordered by numbering. Moreover we can
    <bold>randomize</bold> a query, such that each run will look
    slightly different. This means we exchange a part of the query for a
    random value.</p>
  </sec>
  <sec id="basic-metrics">
    <title>Basic Metrics</title>
    <p>We have several <bold>timers</bold> to collect timing information
    in milliseconds and per run, corresponding to the parts of query
    processing: <bold>timerConnection</bold>,
    <bold>timerExecution</bold>, and <bold>timerTransfer</bold>. The
    tool also computes <bold>timerRun</bold> (the sum of
    <italic>timerConnection</italic>, <italic>timerExecution</italic>,
    and <italic>timerTransfer</italic>) and
    <bold>timerSession</bold>.</p>
    <p>We also measure and store the <bold>total time</bold> of the
    benchmark of the query, since for parallel execution this differs
    from the <bold>sum of times</bold> based on
    <italic>timerRun</italic>. Total time means measurement starts
    before the first benchmark run and it stops after the last benchmark
    run has finished. Thus total time also includes some overhead (for
    spawning a pool of subprocesses, computing the size of result sets,
    and joining results of subprocesses.) We also compute
    <bold>latency</bold> (measured time) and <bold>throughput</bold>
    (number of parallel clients per measured time) for each query and
    DBMS. Additionally error messages and timestamps of the begin and
    end of benchmarking a query are stored.</p>
  </sec>
  <sec id="comparison">
    <title>Comparison</title>
    <p>We can specify a dict of DBMS. Each query will be sent to every
    DBMS in the same number of runs. This also respects randomization,
    i.e., every DBMS receives exactly the same versions of the query in
    the same order. We assume all DBMS will give us the same result
    sets. Without randomization, each run should yield the same result
    set. The tool can automatically check these assumptions by
    <bold>comparison</bold> of sorted result tables (small data sets) or
    their hash value or size (bigger data sets). In order to do so,
    result sets (or their hash value or size) are stored as lists of
    lists and additionally can be saved as csv files or pickled pandas
    DataFrames.</p>
  </sec>
  <sec id="monitoring-hardware-metrics">
    <title>Monitoring Hardware Metrics</title>
    <p>To make hardware metrics available, we must provide the API URL
    of a Prometheus Server. The tool collects metrics from the
    Prometheus server with a step size of 1 second. We may define the
    metrics in terms of Prometheus’s <bold>promql</bold>. Metrics can be
    defined per connection.</p>
  </sec>
  <sec id="results">
    <title>Results</title>
    <p>As a result, we obtain measured times in milliseconds for the
    query processing parts: connection, execution, and data
    transfer.</p>
    <fig>
      <caption><p>evaluation cubes for time and hardware
      metrics.<styled-content id="figU003AEvaluation-Cubes"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/docs/Evaluation-Cubes.png" xlink:title="" />
    </fig>
    <p>These are described in three dimensions: number of run, of query,
    and of configuration. The configuration dimension can consist of
    various nominal attributes like DBMS, selected processor, assigned
    cluster node, number of clients, and execution order. We also can
    have various hardware metrics like CPU and GPU utilization, CPU
    throttling, memory caching, and working set. These are also
    described in three dimensions: Second of query execution time,
    number of query, and of configuration.</p>
  </sec>
</sec>
<sec id="evaluation">
  <title>Evaluation</title>
  <sec id="python---pandas">
    <title>Python - Pandas</title>
    <p>The cubes of measurements can be sliced or diced, rolled-up, or
    drilled-down into the various dimensions and several aggregation
    functions for evaluation of the metrics can be applied: first, last,
    minimum, maximum, arithmetic, and geometric mean, range, and
    interquartile range, standard deviation, median, some quantiles,
    coefficient of variation, and quartile coefficient of dispersion.
    This helps in univariate analysis of center and dispersion of the
    metrics to evaluate measures and stability.</p>
    <p>The package includes tools to convert the three-dimensional
    results into pandas DataFrames, like covering errors and warnings
    that have occured, and timing and hardware metrics that have been
    collected or derived. For example the latency of execution,
    aggregated in the query dimension by computing the mean value, can
    be obtained as:</p>
    <preformat>df = evaluate.get_aggregated_query_statistics(
    type='latency', name='execution', query_aggregate='Mean')</preformat>
    <fig>
      <caption><p>example DataFrame: latency of execution times
      aggregated.<styled-content id="figU003Atable"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/docs/latency-table-example.png" xlink:title="" />
    </fig>
  </sec>
  <sec id="gui---dashboard">
    <title>GUI - Dashboard</title>
    <p>The package includes a dashboard that helps in interactive
    evaluation of experiment results. It shows predefined plots of
    various types, which can be customized and filtered by DBMS
    configuration and query.</p>
    <fig>
      <caption><p>screenshot of
      dashboard.<styled-content id="figU003Adashboard"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/docs/dashboard.png" xlink:title="" />
    </fig>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge contributions from Andre Bubbel to include TPC-DS
  queries.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-10.1007U002F978-3-030-84924-5_6">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Erdelt</surname><given-names>Patrick K.</given-names></name>
      </person-group>
      <article-title>A framework for supporting repetition and evaluation in the process of cloud-based DBMS performance benchmarking</article-title>
      <source>Performance evaluation and benchmarking</source>
      <person-group person-group-type="editor">
        <name><surname>Nambiar</surname><given-names>Raghunath</given-names></name>
        <name><surname>Poess</surname><given-names>Meikel</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <isbn>978-3-030-84924-5</isbn>
      <uri>https://link.springer.com/chapter/10.1007/978-3-030-84924-5_6</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-84924-5_6</pub-id>
      <fpage>75</fpage>
      <lpage>92</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1007U002F978-3-030-94437-7_6">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Erdelt</surname><given-names>Patrick K.</given-names></name>
      </person-group>
      <article-title>Orchestrating DBMS benchmarking in the cloud with kubernetes</article-title>
      <source>Performance evaluation and benchmarking</source>
      <person-group person-group-type="editor">
        <name><surname>Nambiar</surname><given-names>Raghunath</given-names></name>
        <name><surname>Poess</surname><given-names>Meikel</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <isbn>978-3-030-94437-7</isbn>
      <uri>https://link.springer.com/chapter/10.1007/978-3-030-94437-7_6</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-94437-7_6</pub-id>
      <fpage>81</fpage>
      <lpage>97</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1007U002F978-3-319-67162-8_12">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Seybold</surname><given-names>Daniel</given-names></name>
        <name><surname>Domaschka</surname><given-names>Jörg</given-names></name>
      </person-group>
      <article-title>Is distributed database evaluation cloud-ready?</article-title>
      <source>New trends in databases and information systems</source>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2017">2017</year>
      <isbn>978-3-319-67162-8</isbn>
      <uri>https://link.springer.com/chapter/10.1007/978-3-319-67162-8_12</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-319-67162-8_12</pub-id>
      <fpage>100</fpage>
      <lpage>108</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1007U002F978-3-030-12079-5_4">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Brent</surname><given-names>Lexi</given-names></name>
        <name><surname>Fekete</surname><given-names>Alan</given-names></name>
      </person-group>
      <article-title>A versatile framework for painless benchmarking of database management systems</article-title>
      <source>Databases theory and applications</source>
      <person-group person-group-type="editor">
        <name><surname>Chang</surname><given-names>Lijun</given-names></name>
        <name><surname>Gan</surname><given-names>Junhao</given-names></name>
        <name><surname>Cao</surname><given-names>Xin</given-names></name>
      </person-group>
      <publisher-name>Springer International Publishing</publisher-name>
      <publisher-loc>Cham</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <isbn>978-3-030-12079-5</isbn>
      <uri>https://link.springer.com/chapter/10.1007/978-3-030-12079-5_4</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-12079-5_4</pub-id>
      <fpage>45</fpage>
      <lpage>56</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Raasveldt2018FBC32099503209955">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Raasveldt</surname><given-names>Mark</given-names></name>
        <name><surname>Holanda</surname><given-names>Pedro</given-names></name>
        <name><surname>Gubner</surname><given-names>Tim</given-names></name>
        <name><surname>Mühleisen</surname><given-names>Hannes</given-names></name>
      </person-group>
      <article-title>Fair benchmarking considered difficult: Common pitfalls in database performance testing</article-title>
      <source>Proceedings of the workshop on testing database systems</source>
      <publisher-name>ACM</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <isbn>978-1-4503-5826-2</isbn>
      <uri>http://doi.acm.org/10.1145/3209950.3209955</uri>
      <pub-id pub-id-type="doi">10.1145/3209950.3209955</pub-id>
      <fpage>2:1</fpage>
      <lpage>2:6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DBLPconfsigmodKerstenKZ18">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kersten</surname><given-names>Martin L.</given-names></name>
        <name><surname>Koutsourakis</surname><given-names>Panagiotis</given-names></name>
        <name><surname>Zhang</surname><given-names>Ying</given-names></name>
      </person-group>
      <article-title>Finding the pitfalls in query performance</article-title>
      <source>Proceedings of the 7th International Workshop on Testing Database Systems, DBTest@SIGMOD 2018,</source>
      <person-group person-group-type="editor">
        <name><surname>Böhm</surname><given-names>Alexander</given-names></name>
        <name><surname>Rabl</surname><given-names>Tilmann</given-names></name>
      </person-group>
      <publisher-name>ACM</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://doi.org/10.1145/3209950.3209951</uri>
      <pub-id pub-id-type="doi">10.1145/3209950.3209951</pub-id>
      <fpage>3:1</fpage>
      <lpage>3:6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-reback2020pandas">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>The pandas development team</string-name>
      </person-group>
      <source>Pandas-dev/pandas: pandas</source>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2020-02">2020</year><month>02</month>
      <uri>https://doi.org/10.5281/zenodo.3509134</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.3509134</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mckinney-proc-scipy-2010">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>McKinney</surname><given-names>Wes</given-names></name>
      </person-group>
      <article-title>Data Structures for Statistical Computing in Python</article-title>
      <source>Proceedings of the 9th Python in Science Conference</source>
      <person-group person-group-type="editor">
        <name><surname>Walt</surname></name>
        <name><surname>Millman</surname></name>
      </person-group>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id>
      <fpage>56 </fpage>
      <lpage> 61</lpage>
    </element-citation>
  </ref>
  <ref id="ref-208870">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Rabenstein</surname><given-names>Bjorn</given-names></name>
        <name><surname>Volz</surname><given-names>Julius</given-names></name>
      </person-group>
      <article-title>Prometheus: A next-generation monitoring system (talk)</article-title>
      <publisher-name>USENIX Association</publisher-name>
      <publisher-loc>Dublin</publisher-loc>
      <year iso-8601-date="2015-05">2015</year><month>05</month>
    </element-citation>
  </ref>
  <ref id="ref-Kluyver2016jupyter">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kluyver</surname><given-names>Thomas</given-names></name>
        <name><surname>Ragan-Kelley</surname><given-names>Benjamin</given-names></name>
        <name><surname>Pérez</surname><given-names>Fernando</given-names></name>
        <name><surname>Granger</surname><given-names>Brian</given-names></name>
        <name><surname>Bussonnier</surname><given-names>Matthias</given-names></name>
        <name><surname>Frederic</surname><given-names>Jonathan</given-names></name>
        <name><surname>Kelley</surname><given-names>Kyle</given-names></name>
        <name><surname>Hamrick</surname><given-names>Jessica</given-names></name>
        <name><surname>Grout</surname><given-names>Jason</given-names></name>
        <name><surname>Corlay</surname><given-names>Sylvain</given-names></name>
        <name><surname>Ivanov</surname><given-names>Paul</given-names></name>
        <name><surname>Avila</surname><given-names>Damián</given-names></name>
        <name><surname>Abdalla</surname><given-names>Safia</given-names></name>
        <name><surname>Willing</surname><given-names>Carol</given-names></name>
      </person-group>
      <article-title>Jupyter notebooks – a publishing format for reproducible computational workflows</article-title>
      <person-group person-group-type="editor">
        <name><surname>Loizides</surname><given-names>F.</given-names></name>
        <name><surname>Schmidt</surname><given-names>B.</given-names></name>
      </person-group>
      <publisher-name>IOS Press</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.3233/978-1-61499-649-1-87</pub-id>
      <fpage>87 </fpage>
      <lpage> 90</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HunterU003A2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hunter</surname><given-names>J. D.</given-names></name>
      </person-group>
      <article-title>Matplotlib: A 2D graphics environment</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE COMPUTER SOC</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>9</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      <fpage>90</fpage>
      <lpage>95</lpage>
    </element-citation>
  </ref>
  <ref id="ref-2020SciPy-NMeth">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>van der Walt</surname><given-names>Stéfan J.</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Wilson</surname><given-names>Joshua</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Mayorov</surname><given-names>Nikolay</given-names></name>
        <name><surname>Nelson</surname><given-names>Andrew R. J.</given-names></name>
        <name><surname>Jones</surname><given-names>Eric</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Larson</surname><given-names>Eric</given-names></name>
        <name><surname>Carey</surname><given-names>C J</given-names></name>
        <name><surname>Polat</surname><given-names>İlhan</given-names></name>
        <name><surname>Feng</surname><given-names>Yu</given-names></name>
        <name><surname>Moore</surname><given-names>Eric W.</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Laxalde</surname><given-names>Denis</given-names></name>
        <name><surname>Perktold</surname><given-names>Josef</given-names></name>
        <name><surname>Cimrman</surname><given-names>Robert</given-names></name>
        <name><surname>Henriksen</surname><given-names>Ian</given-names></name>
        <name><surname>Quintero</surname><given-names>E. A.</given-names></name>
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Archibald</surname><given-names>Anne M.</given-names></name>
        <name><surname>Ribeiro</surname><given-names>Antônio H.</given-names></name>
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>van Mulbregt</surname><given-names>Paul</given-names></name>
        <string-name>SciPy 1.0 Contributors</string-name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>
      <source>Nature Methods</source>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KounevLK20">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Kounev</surname><given-names>Samuel</given-names></name>
        <name><surname>Lange</surname><given-names>Klaus-Dieter</given-names></name>
        <name><surname>Kistowski</surname><given-names>Jóakim von</given-names></name>
      </person-group>
      <source>Systems benchmarking - for scientists and engineers</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <isbn>978-3-030-41704-8</isbn>
      <uri>https://doi.org/10.1007/978-3-030-41705-5</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-41705-5</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-seriesU002FutcsU002FIgualS17">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Igual</surname><given-names>Laura</given-names></name>
        <name><surname>Seguí</surname><given-names>Santi</given-names></name>
      </person-group>
      <source>Introduction to data science - a Python approach to concepts, techniques and applications</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <isbn>978-3-319-50017-1</isbn>
      <pub-id pub-id-type="doi">10.1007/978-3-319-50017-1</pub-id>
      <fpage>1</fpage>
      <lpage>215</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Waskom2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Waskom</surname><given-names>Michael L.</given-names></name>
      </person-group>
      <article-title>Seaborn: Statistical data visualization</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>6</volume>
      <issue>60</issue>
      <uri>https://doi.org/10.21105/joss.03021</uri>
      <pub-id pub-id-type="doi">10.21105/joss.03021</pub-id>
      <fpage>3021</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-TIOBE">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>TIOBE</surname></name>
      </person-group>
      <article-title>TIOBE Index - TIOBE</article-title>
      <source>TIOBE</source>
      <year iso-8601-date="2022-06">2022</year><month>06</month>
      <uri>https://www.tiobe.com/tiobe-index</uri>
    </element-citation>
  </ref>
  <ref id="ref-PYPL">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>PYPL</surname></name>
      </person-group>
      <article-title>PYPL PopularitY of Programming Language index</article-title>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <uri>https://pypl.github.io/PYPL.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-10114533389063338912">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>Sen</given-names></name>
        <name><surname>Manns</surname><given-names>Glenna</given-names></name>
        <name><surname>Saunders</surname><given-names>John</given-names></name>
        <name><surname>Wang</surname><given-names>Wei</given-names></name>
        <name><surname>Pollock</surname><given-names>Lori</given-names></name>
        <name><surname>Soffa</surname><given-names>Mary Lou</given-names></name>
      </person-group>
      <article-title>A statistics-based performance testing methodology for cloud applications</article-title>
      <source>Proceedings of the 2019 27th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2019">2019</year>
      <isbn>9781450355728</isbn>
      <uri>https://doi.org/10.1145/3338906.3338912</uri>
      <pub-id pub-id-type="doi">10.1145/3338906.3338912</pub-id>
      <fpage>188</fpage>
      <lpage>199</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DBDBIO">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Carnegie Mellon Database Group</string-name>
      </person-group>
      <article-title>Database of Databases</article-title>
      <source>Database of Databases</source>
      <year iso-8601-date="2022-08">2022</year><month>08</month>
      <uri>https://dbdb.io</uri>
    </element-citation>
  </ref>
  <ref id="ref-DBEngines">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>solid IT GmbH</string-name>
      </person-group>
      <article-title>DB-Engines Ranking</article-title>
      <source>DB-Engines</source>
      <year iso-8601-date="2022-08">2022</year><month>08</month>
      <uri>https://db-engines.com/en/ranking</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
