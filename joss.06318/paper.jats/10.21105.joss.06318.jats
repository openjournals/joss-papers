<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6318</article-id>
<article-id pub-id-type="doi">10.21105/joss.06318</article-id>
<title-group>
<article-title>BibDedupe: An Open-Source Python Library for
Bibliographic Record Deduplication</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3926-7717</contrib-id>
<name>
<surname>Wagner</surname>
<given-names>Gerit</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Otto-Friedrich Universität Bamberg</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-01-22">
<day>22</day>
<month>1</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>97</issue>
<fpage>6318</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bibliographic Records</kwd>
<kwd>Deduplication</kwd>
<kwd>Data Preprocessing</kwd>
<kwd>Blocking</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>BibDedupe is a Python library developed for bibliographic record
  deduplication in meta-analysis and research synthesis. It is
  constructed with a focus on four requirements: (1) <bold>Zero false
  positives</bold>: The primary objective is to prevent incorrectly
  merging distinct entries. This focus on zero false positives is
  crucial to ensure trustworthiness and prevent biased conclusions in
  the analysis. (2) <bold>Reproducibility</bold>: BibDedupe implements
  fixed rules to produce consistent results, in line with the scientific
  standard of reproducibility. (3) <bold>Efficiency</bold>: The library
  is also tuned for low false-negative rates and rapid processing, to
  ensure scalability of the duplicate identification process. (4)
  <bold>Continuous evaluation and improvement</bold>: It is continuously
  evaluated on over 160,000 records from 10 datasets to ensure its
  effectiveness, especially in follow-up refinements. Unlike
  general-purpose deduplication tools, BibDedupe is specifically
  designed for the unique requirements of bibliographic data in
  meta-analysis and research synthesis. In this context, BibDedupe aims
  to provide a Python library that improves the effectiveness and
  efficiency of duplicate identification, potentially benefitting review
  papers across scientific disciplines.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Handling duplicates is a critical step in meta-analysis and
  research synthesis
  (<xref alt="Harrer et al., 2021" rid="ref-HarrerEtAl2021" ref-type="bibr">Harrer
  et al., 2021</xref>), given that errors in this step can directly
  affect conclusions
  (<xref alt="Wood, 2008" rid="ref-Wood2008" ref-type="bibr">Wood,
  2008</xref>). Prior research has invested considerable efforts to
  evaluate duplicate identification software for bibliographic data
  (<xref alt="Binette &amp; Steorts, 2022" rid="ref-BinetteSteorts2022" ref-type="bibr">Binette
  &amp; Steorts, 2022</xref>;
  <xref alt="Bramer et al., 2016" rid="ref-BramerEtAl2016" ref-type="bibr">Bramer
  et al., 2016</xref>;
  <xref alt="Koumarelas et al., 2020" rid="ref-KoumarelasEtAl2020" ref-type="bibr">Koumarelas
  et al., 2020</xref>;
  <xref alt="Rathbone et al., 2015" rid="ref-RathboneEtAl2015" ref-type="bibr">Rathbone
  et al., 2015</xref>). While methodologists have repeatedly cautioned
  against the risk of treating identical studies independently when they
  are published in different papers
  (<xref alt="Fairfield et al., 2017" rid="ref-FairfieldEtAl2017" ref-type="bibr">Fairfield
  et al., 2017</xref>;
  <xref alt="Senn, 2009" rid="ref-Senn2009" ref-type="bibr">Senn,
  2009</xref>), the risk of erroneously classifying papers as duplicates
  has arguably received less attention. However, once removed from the
  process, it is rarely possible to recover false positives, or to
  quantify and correct their effect on meta-analytic results. As such,
  preventing false positives is of critical
  importance<xref ref-type="fn" rid="fn1">1</xref>, while false
  negatives can be detected and merged in the subsequent screening and
  analysis steps
  (<xref alt="McLoughlin, 2022" rid="ref-McLoughlin2022" ref-type="bibr">McLoughlin,
  2022</xref>). </p>
  <p>Proprietary software for duplicate identification often suffers
  from shortcomings related to the four requirements. Tools like Endnote
  or Covidence require compromises related to false positives, have
  limited transparency of black-box algorithms, or lack peer-review and
  external validation. Moreover, the use of proprietary software incurs
  costs, and restricts the combination of research tools, because data
  is hard to access and programmatic interfaces are not offered.</p>
  <p>General purpose deduplication libraries often lack the specificity
  needed for bibliographic data, requiring skills and excessive amounts
  of effort to develop and evaluate algorithms. For example, libraries
  such as the <italic>Python Record Linkage Toolkit</italic>
  (<xref alt="De Bruin, 2019" rid="ref-DeBruin2019" ref-type="bibr">De
  Bruin, 2019</xref>) and <italic>dedupe (io)</italic>
  (<xref alt="Gregg &amp; Eder, 2022" rid="ref-GreggEder2022" ref-type="bibr">Gregg
  &amp; Eder, 2022</xref>) provide an arsenal of similarity measures,
  blocking rules, and utility functions. As such, they provide a
  valuable basis to support the design of domain-specific duplicate
  identification tools, but they are rarely used directly by researchers
  conducting a meta-analysis
  (<xref alt="Nguyen et al., 2022" rid="ref-NguyenEtAl2022" ref-type="bibr">Nguyen
  et al., 2022</xref>). When developing a custom deduplication
  algorithm, its effectiveness can only be evaluated by creating an
  independently deduplicated dataset. More severely, developing an
  accurate algorithm require in-depth knowledge of publication practices
  and errors typically introduced by academic databases, or other
  systems handling bibliographic metadata. Experience shows that minor
  changes potentially have significant effects on overall performance.
  Finally, machine-learning libraries, such as <italic>dedupe
  (io)</italic>, involve the learning of blocking rules and similarity
  functions from each dataset, and based on user input. Such manual
  processing steps reduce efficiency and limit reproducibility.</p>
  <p>Open-source research software for duplicate identification is
  scarce, and to-date, peer-reviewed software is non-existent in this
  area. In the Python ecosystem, the only library I found is
  <italic>ASReview Datatools</italic>, provided by the team behind the
  <italic>ASReview</italic> screening tool
  (<xref alt="Van De Schoot et al., 2021" rid="ref-VanDeSchootetAl2021" ref-type="bibr">Van
  De Schoot et al., 2021</xref>). My evaluations show that this library
  introduces a considerable number of false positives, and cannot be
  used for meta-analyses. R users or Python users willing to switch the
  ecosystem, may use ASySD
  (<xref alt="Hair et al., 2023" rid="ref-Hair2023" ref-type="bibr">Hair
  et al., 2023</xref>), a recently published R package with a Shiny web
  interface. The code of this package resembles BibDedupe, but it does
  not achieve zero-false-positives, uses a relatively small test dataset
  from medicine (n=1845) in the unit tests, and was not evaluated in the
  peer review process.</p>
  <p>In conclusion, researchers are not served well by proprietary
  tools, or general purpose deduplication libraries. Effective and
  peer-reviewed libraries are urgently needed for meta-analyses and
  research synthesis to facilitate researchers’ trust and adoption of
  open-source libraries in the area of literature reviews.</p>
</sec>
<sec id="example-usage">
  <title>Example usage</title>
  <code language="python">import pandas as pd
from bib_dedupe.bib_dedupe import merge

# Load your bibliographic dataset into a pandas DataFrame
records_df = pd.read_csv(&quot;records.csv&quot;)
# Get the merged_df
merged_df = merge(records_df)
  </code>
  <p>For advanced use cases, it is also possible to complete and
  customize each step individually</p>
  <code language="python"> from bib_dedupe.bib_dedupe import prep, block, match, merge
 from bib_dedupe.bib_dedupe import export_maybe, import_maybe

# Preprocess records
records_df = prep(records_df)
# Block records
blocked_df = block(records_df)
# Identify matches
matched_df = match(blocked_df)
# Export and import maybe cases
export_maybe(matched_df, records_df)
matches = import_maybe(matched_df)
# Merge
merged_df = merge(records_df, matches=matches)</code>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <p>I define duplicates as potentially differing bibliographic
  representations of the same real-world record (cf.
  <xref alt="Rathbone et al., 2015" rid="ref-RathboneEtAl2015" ref-type="bibr">Rathbone
  et al., 2015</xref>). This conceptual definition is operationalized as
  follows. The following are considered <bold>duplicates</bold>:</p>
  <list list-type="bullet">
    <list-item>
      <p>Papers referring to the same record (per definition)</p>
    </list-item>
    <list-item>
      <p>Paper versions, including the author’s original, submitted,
      accepted, proof, and corrected versions
      (<xref alt="NISO/ALPSP JAV Working Group, 2008" rid="ref-NISO2008" ref-type="bibr">NISO/ALPSP
      JAV Working Group, 2008</xref>)</p>
    </list-item>
    <list-item>
      <p>Papers that are continuously updated (e.g., versions of
      Cochrane reviews)</p>
    </list-item>
    <list-item>
      <p>Papers with different DOIs if they refer to the same record
      (e.g., redundantly registered DOIs for online and print
      versions)</p>
    </list-item>
  </list>
  <p>The following are considered <bold>non-duplicates</bold>:</p>
  <list list-type="bullet">
    <list-item>
      <p>Papers reporting on the same study if they are published
      separately (e.g., involving different stages of the study such as
      pilots and protocols, or differences in outcomes, interventions,
      or populations)</p>
    </list-item>
    <list-item>
      <p>A conference paper and its extended journal publication</p>
    </list-item>
    <list-item>
      <p>A journal paper and a reprint in another journal</p>
    </list-item>
  </list>
  <p>It is noted that the focus is on duplicates of bibliographic
  <italic>records</italic>. The linking of multiple records reporting
  results from the <italic>same study</italic> is typically done in a
  separate step after full-text retrieval, using information from the
  full-text document, querying dedicated registers, and potentially
  corresponding with the authors (see
  <xref alt="Higgins et al., 2023, sec. 4.6.2" rid="ref-HigginsEtAl2023" ref-type="bibr">Higgins
  et al., 2023, sec. 4.6.2</xref> and 4.6.2).</p>
  <p>These clarifications are necessary for the evaluation dataset, and
  for users to understand what will (not) be considered a duplicate. The
  rationale is that cases of duplicates are rarely or never cited as
  separate items in a reference section, while non-duplicates can in
  principle be cited separately. It is a different issue whether the
  corresponding research and administrative practices are considered
  questionable or ethical (e.g., salami publications, or registering
  multiple DOIs for the same paper).</p>
  <p>To accurately identify and merge duplicates, BibDedupe implements
  the steps of preprocessing, blocking, rule-based matching, and
  merging. As seen in the usage example, each step can be adapted.</p>
  <sec id="preprocessing">
    <title>Preprocessing</title>
    <p>Preprocessing involves an array of standardizations across
    fields, including replacement of special characters. For titles and
    journals, stop words are removed to give more weight to distinctive
    words in the similarity measures. For the author field, name
    particles are removed because they are often handled incorrectly in
    the data creation process. Additional notes and translations are
    removed from the title field. For translated journal names, the
    English version is used as a replacement.</p>
  </sec>
  <sec id="blocking">
    <title>Blocking</title>
    <p>To avoid checking all possible combinations of papers, blocking
    selects the pairs that are likely to be duplicates. This is a common
    technique in deduplication where only records within the same block
    are compared for potential duplication. </p>
    <p>BibDedupe relies on a comprehensive set of blocking rules to
    avoid false negatives in this step. After the set of blocking rules
    is applied, pairs not sharing a minimum number of words in the
    titles are removed, effectively reducing the number of pairs by
    50-95% without losing true pairs. This leads to a more efficient
    matching step.</p>
  </sec>
  <sec id="matching">
    <title>Matching</title>
    <p>The matching function selects duplicates or potential duplicates
    from the list of blocked record pairs. Potential duplicates, also
    known as “maybe cases”, are marked separately for manual
    verification. To achieve accurate and interpretable matching, I
    specified an array of human-readable conditions, which are based on
    pre-calculated and context-specific similarities between fields.</p>
    <p>The conditions and similarity functions account for bibliographic
    errors commonly introduced between duplicates. I summarize the key
    design decisions of BibDedupe, which differ from other approaches
    (notably ASySD):</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Robust author similarities</bold>: The most substantial
        format variation is observed in the author field, requiring
        robust similarity measures. This is particularly challenging for
        non-Western names, which are not supported well by current
        <ext-link ext-link-type="uri" xlink:href="https://tp.libguides.com/c.php?g=920621&amp;p=6640859">citation
        style conventions</ext-link>, or name-parsing software (see
        <ext-link ext-link-type="uri" xlink:href="https://github.com/derek73/python-nameparser/issues/83">nameparser</ext-link>).
        Given that Chinese authors are leading in many research output
        and impact rankings
        (<xref alt="Brainard &amp; Normile, 2022" rid="ref-BrainardNormile2022" ref-type="bibr">Brainard
        &amp; Normile, 2022</xref>), this is a limitation. After testing
        multiple similarity measures, I found that the agreement between
        capital or beginning-of-word letters provided the most robust
        measure of author similarity, suggesting that common similarity
        measures like Jaro-Winkler are less appropriate in this case. I
        briefly illustrate this with an example of non-Western names
        that were erroneously abbreviated:</p>
      </list-item>
    </list>
    <preformat>Author string 1: &quot;Chen J. M.Gong X. Q.Zhong J. G.Chen S. C.Zhang G. Y.&quot;
Author string 2: &quot;Jin-Ming C.Xiao-Qi G.Ji-Gen Z.Si-Cong C.Guo-Yuan Z.&quot;
Jaccard similarity          : 0.18
Cosine similarity           : 0.31
Jaro-Winkler similarity     : 0.64
First-letters similarity    : 1.0</preformat>
    <list list-type="bullet">
      <list-item>
        <p><bold>Sensitive title similarities</bold>: For titles,
        similarity measures must be sensitive to minor differences
        between non-duplicates, as exemplified in so-called
        <italic>salami-publications</italic> or publications consisting
        of multiple parts. In these cases, titles are almost identical,
        and general similarity measures yield values close to 1, i.e.,
        they are not sensitive enough to differences that are
        significant in the context of bibliographic data. BibDedupe
        implements a similarity function that is sensitive to
        differences in numbers (e.g., part 1 vs. part 2), populations
        (e.g., men vs. women, in vivo vs. in vitro, cats vs. rats),
        interventions (e.g., effect of X vs. effect of Y), and outcomes
        (e.g., effect on X vs. effect on Y).</p>
      </list-item>
    </list>
    <list list-type="bullet">
      <list-item>
        <p><bold>Translations of container titles</bold>: Given the
        nested data structure, in which papers are contained in
        journals, proceedings, or other containers, accurate matching is
        required for the field of container titles. To accomplish this,
        BibDedupe uses a list of approx. 1,300 translated journal names
        as replacements in the preprocessing step, effectively
        increasing the average Jaro-Winkler similarity between journals
        and their translated titles from 0.45 to 1.0. This leads to a
        substantial improvement in false negatives.</p>
      </list-item>
      <list-item>
        <p><bold>Handling missing values</bold>: While values author,
        title, and container_title fields are rarely missing, there can
        be missing values in the other fields, such as the volume, DOI,
        or abstract. Similarity measures typically return insufficient
        results when only one value is missing. For instance, when one
        paper contains a DOI and the other does not, the similarity
        would be zero, as it would be the case for different DOIs. I
        distinguish these cases based on a
        <monospace>non_contradictory()</monospace> function, which is
        robust against missing values, and indicates whether non-missing
        values differ between records.</p>
      </list-item>
    </list>
    <p>I note that global IDs (like DOIs) contribute to duplicate
    identification, but neither are identical DOIs considered a
    sufficient condition for a duplicate, nor are distinct DOIs
    considered a sufficient condition for non-duplicates. This is
    confirmed by the data. For the iterative tuning, I designed
    diagnostic utilities to assess which conditions match for selected
    (FP/FN) cases.</p>
  </sec>
  <sec id="merging">
    <title>Merging</title>
    <p>Upon merging a set of records, BibDedupe keeps track of the
    original IDs in the <italic>origin</italic> field. Compared to the
    common approach of deleting n-1 records from the set of duplicates,
    this approach has three distinct advantages: (1)
    <bold>validation</bold>: together with the original dataset, it
    allows users to validate whether duplicate decisions are accurate,
    (2) <bold>undo</bold>: it is possible to restore selected cases
    where erroneous duplicates were merged, and (3)
    <bold>evaluation</bold>: it enables subsequent use of datasets to
    evaluate and tune duplicate detection algorithms.</p>
    <p>The merging function uses heuristics to select the most
    appropriate fields from duplicate records, instead of selecting all
    fields from one record regardless of field-level quality. For
    instance, proper capitalization is preferred when one record has
    author or title fields in all-caps, and DOIs are selected when other
    DOI fields are empty.</p>
  </sec>
  <sec id="evaluation">
    <title>Evaluation</title>
    <p>To evaluate BibDedupe, I collected 10 datasets comprising over
    160,000 records and 34,900 duplicates
    (<xref alt="Hair et al., 2023" rid="ref-Hair2023" ref-type="bibr">Hair
    et al., 2023</xref>;
    <xref alt="Rathbone et al., 2015" rid="ref-RathboneEtAl2015" ref-type="bibr">Rathbone
    et al., 2015</xref>;
    <xref alt="Wagner et al., 2021" rid="ref-WagnerPresterPare2021" ref-type="bibr">Wagner
    et al., 2021</xref>). The results are displayed in Table 1. This is,
    to the best of my knowledge, the only evaluation that is updated
    automatically on a regular basis, and the most comprehensive
    evaluation of bibliographic duplicate detection algorithms to date.
    Complementary evaluation data, including proprietary software and
    tools that do not offer programmatic access, is reported by Hair et
    al.
    (<xref alt="2023" rid="ref-Hair2023" ref-type="bibr">2023</xref>).</p>
    <p>I completed over 3,000 iterations to evaluate and improve
    BibDedupe based on these datasets. The efforts involved tuning the
    preprocessing, blocking, and matching steps, vetting different
    similarity measures, and validating the false positives and
    negatives based on the definition of (non)-duplicates. I carefully
    reviewed the conditions to combine and generalize narrowly defined
    cases. In addition, I implemented unit tests to ensure consistency,
    and understand how changes in the code affect each step. Runtime was
    optimized by implementing and evaluating different approaches to
    parallel processing, such as processing NumPy-arrays vs. splitting
    dataframes horizontally. As a result, the depression dataset with
    approx. 80,000 records is processed in under 10 minutes with 8
    CPUs.</p>
    <boxed-text id="bib_dedupe_asreview_results">
      <table-wrap>
        <caption>
          <p>Comparison of BibDedupe, ASySD, and ASReview</p>
        </caption>
        <table>
          <thead>
            <tr>
              <th align="left"><bold>Package</bold></th>
              <th align="right"><bold>FP</bold></th>
              <th align="right"><bold>TP</bold></th>
              <th align="right"><bold>FN</bold></th>
              <th align="right"><bold>TN</bold></th>
              <th align="center"><bold>Specificity</bold></th>
              <th align="center"><bold>Sensitivity</bold></th>
              <th align="center"><bold>F1</bold></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td align="left">BibDedupe</td>
              <td align="right">0</td>
              <td align="right">35,036</td>
              <td align="right">229</td>
              <td align="right">125,546</td>
              <td align="center">1.0</td>
              <td align="center">0.99</td>
              <td align="center">1.0</td>
            </tr>
            <tr>
              <td align="left">ASySD</td>
              <td align="right">53</td>
              <td align="right">24,464</td>
              <td align="right">641</td>
              <td align="right">55,781</td>
              <td align="center">1.0</td>
              <td align="center">0.97</td>
              <td align="center">0.97</td>
            </tr>
            <tr>
              <td align="left">asreview</td>
              <td align="right">5,617</td>
              <td align="right">29,919</td>
              <td align="right">5,346</td>
              <td align="right">119,929</td>
              <td align="center">0.96</td>
              <td align="center">0.85</td>
              <td align="center">0.85</td>
            </tr>
            <tr>
              <td align="left" colspan="8"><bold>Abbreviations</bold>
              FP: False positives, TP: True positives,</td>
            </tr>
            <tr>
              <td align="left" colspan="8">FN: False negatives, TN: True
              negatives</td>
            </tr>
          </tbody>
        </table>
      </table-wrap>
    </boxed-text>
  </sec>
</sec>
<sec id="ongoing-improvements">
  <title>Ongoing improvements</title>
  <p>BibDedupe provides duplicate identification functionality, which
  performs with zero false positives on a dataset comprising over
  160,000 records. It builds on carefully crafted rules and high-quality
  training data to ensure effectiveness, transparency, and
  reproducibility. The evaluation runs automatically and provides a
  solid foundation for continuous improvements and additions of
  datasets. I intend to incorporate additional datasets and continue
  refining the rules and procedures.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Hair2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hair</surname><given-names>Kaitlyn</given-names></name>
        <name><surname>Bahor</surname><given-names>Zsanett</given-names></name>
        <name><surname>Macleod</surname><given-names>Malcolm</given-names></name>
        <name><surname>Liao</surname><given-names>Jing</given-names></name>
        <name><surname>Sena</surname><given-names>Emily S</given-names></name>
      </person-group>
      <article-title>The automated systematic search deduplicator (ASySD): A rapid, open-source, interoperable tool to remove duplicate citations in biomedical systematic reviews</article-title>
      <source>BMC biology</source>
      <year iso-8601-date="2023">2023</year>
      <volume>21</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1186/s12915-023-01686-z</pub-id>
      <fpage>189</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-DeBruin2019">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>De Bruin</surname><given-names>J</given-names></name>
      </person-group>
      <article-title>Python record linkage toolkit: A toolkit for record linkage and duplicate detection in python</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://doi.org/10.5281/zenodo.3559043</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.3559043</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-BramerEtAl2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bramer</surname><given-names>Wichor M</given-names></name>
        <name><surname>Giustini</surname><given-names>Dean</given-names></name>
        <name><surname>Jonge</surname><given-names>Gerdien B de</given-names></name>
        <name><surname>Holland</surname><given-names>Leslie</given-names></name>
        <name><surname>Bekhuis</surname><given-names>Tanja</given-names></name>
      </person-group>
      <article-title>De-duplication of database search results for systematic reviews in EndNote</article-title>
      <source>Journal of the Medical Library Association</source>
      <year iso-8601-date="2016">2016</year>
      <volume>104</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.3163/1536-5050.104.3.014</pub-id>
      <fpage>240</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wood2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wood</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>Methodology for dealing with duplicate study effects in a meta-analysis</article-title>
      <source>Organizational Research Methods</source>
      <year iso-8601-date="2008">2008</year>
      <volume>11</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1177/1094428106296638</pub-id>
      <fpage>79</fpage>
      <lpage>95</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KoumarelasEtAl2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Koumarelas</surname><given-names>Ioannis</given-names></name>
        <name><surname>Jiang</surname><given-names>Lan</given-names></name>
        <name><surname>Naumann</surname><given-names>Felix</given-names></name>
      </person-group>
      <article-title>Data preparation for duplicate detection</article-title>
      <source>Journal of Data and Information Quality</source>
      <year iso-8601-date="2020">2020</year>
      <volume>12</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1145/3377878</pub-id>
      <fpage>1</fpage>
      <lpage>24</lpage>
    </element-citation>
  </ref>
  <ref id="ref-RathboneEtAl2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rathbone</surname><given-names>John</given-names></name>
        <name><surname>Carter</surname><given-names>Matt</given-names></name>
        <name><surname>Hoffmann</surname><given-names>Tammy</given-names></name>
        <name><surname>Glasziou</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>Better duplicate detection for systematic reviewers: Evaluation of systematic review assistant-deduplication module</article-title>
      <source>Systematic Reviews</source>
      <year iso-8601-date="2015">2015</year>
      <volume>4</volume>
      <pub-id pub-id-type="doi">10.1186/2046-4053-4-6</pub-id>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GreggEder2022">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Gregg</surname><given-names>Forest</given-names></name>
        <name><surname>Eder</surname><given-names>Derek</given-names></name>
      </person-group>
      <article-title>dedupe</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/dedupeio/dedupe</uri>
    </element-citation>
  </ref>
  <ref id="ref-BinetteSteorts2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Binette</surname><given-names>Olivier</given-names></name>
        <name><surname>Steorts</surname><given-names>Rebecca C</given-names></name>
      </person-group>
      <article-title>(Almost) all of entity resolution</article-title>
      <source>Science Advances</source>
      <year iso-8601-date="2022">2022</year>
      <volume>8</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1126/sciadv.abi8021</pub-id>
      <fpage>eabi8021</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-HarrerEtAl2021">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Harrer</surname><given-names>Mathias</given-names></name>
        <name><surname>Cuijpers</surname><given-names>Pim</given-names></name>
        <name><surname>Furukawa</surname><given-names>Toshi</given-names></name>
        <name><surname>Ebert</surname><given-names>David</given-names></name>
      </person-group>
      <source>Doing meta-analysis with r: A hands-on guide</source>
      <publisher-name>Chapman; Hall/CRC</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <isbn>978-0-367-61007-4</isbn>
    </element-citation>
  </ref>
  <ref id="ref-Senn2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Senn</surname><given-names>Stephen J</given-names></name>
      </person-group>
      <article-title>Overstating the evidence–double counting in meta-analysis and related problems</article-title>
      <source>BMC Medical Research Methodology</source>
      <year iso-8601-date="2009">2009</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.1186/1471-2288-9-10</pub-id>
      <fpage>1</fpage>
      <lpage>7</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FairfieldEtAl2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fairfield</surname><given-names>Cameron J</given-names></name>
        <name><surname>Harrison</surname><given-names>Ewen M</given-names></name>
        <name><surname>Wigmore</surname><given-names>Stephen J</given-names></name>
      </person-group>
      <article-title>Duplicate publication bias weakens the validity of meta-analysis of immunosuppression after transplantation</article-title>
      <source>World journal of gastroenterology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>23</volume>
      <issue>39</issue>
      <pub-id pub-id-type="doi">10.3748/wjg.v23.i39.7198</pub-id>
      <fpage>7198</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-McLoughlin2022">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>McLoughlin</surname><given-names>Rachael</given-names></name>
      </person-group>
      <article-title>Improving our deduplication process</article-title>
      <publisher-name>Covidence</publisher-name>
      <year iso-8601-date="2022-03-07">2022</year><month>03</month><day>07</day>
      <uri>https://www.covidence.org/blog/improving-our-deduplication-process/</uri>
    </element-citation>
  </ref>
  <ref id="ref-VanDeSchootetAl2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Van De Schoot</surname><given-names>Rens</given-names></name>
        <name><surname>De Bruin</surname><given-names>Jonathan</given-names></name>
        <name><surname>Schram</surname><given-names>Raoul</given-names></name>
        <name><surname>Zahedi</surname><given-names>Parisa</given-names></name>
        <name><surname>De Boer</surname><given-names>Jan</given-names></name>
        <name><surname>Weijdema</surname><given-names>Felix</given-names></name>
        <name><surname>Kramer</surname><given-names>Bianca</given-names></name>
        <name><surname>Huijts</surname><given-names>Martijn</given-names></name>
        <name><surname>Hoogerwerf</surname><given-names>Maarten</given-names></name>
        <name><surname>Ferdinands</surname><given-names>Gerbrich</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>An open source machine learning framework for efficient and transparent systematic reviews</article-title>
      <source>Nature machine intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>3</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1038/s42256-020-00287-7</pub-id>
      <fpage>125</fpage>
      <lpage>133</lpage>
    </element-citation>
  </ref>
  <ref id="ref-BrainardNormile2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brainard</surname><given-names>J</given-names></name>
        <name><surname>Normile</surname><given-names>D</given-names></name>
      </person-group>
      <article-title>China rises to first place in most cited papers</article-title>
      <source>Science</source>
      <year iso-8601-date="2022">2022</year>
      <volume>377</volume>
      <issue>6608</issue>
      <pub-id pub-id-type="doi">10.1126/science.ade4585</pub-id>
      <fpage>799</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-WagnerPresterPare2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wagner</surname><given-names>Gerit</given-names></name>
        <name><surname>Prester</surname><given-names>Julian</given-names></name>
        <name><surname>Paré</surname><given-names>Guy</given-names></name>
      </person-group>
      <article-title>Exploring the boundaries and processes of digital platforms for knowledge work: A review of information systems research</article-title>
      <source>The Journal of Strategic Information Systems</source>
      <year iso-8601-date="2021">2021</year>
      <volume>30</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1016/j.jsis.2021.101694</pub-id>
      <fpage>101694</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-NISO2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>NISO/ALPSP JAV Working Group</string-name>
      </person-group>
      <article-title>NISO-RP-8-2008, journal article versions (JAV): recommendations</article-title>
      <publisher-name>NISO: National Information Standards Organization</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <pub-id pub-id-type="doi">10.3789/niso-rp-8-2008</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-NguyenEtAl2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nguyen</surname><given-names>Phi-Yen</given-names></name>
        <name><surname>Kanukula</surname><given-names>Raju</given-names></name>
        <name><surname>McKenzie</surname><given-names>Joanne E</given-names></name>
        <name><surname>Alqaidoom</surname><given-names>Zainab</given-names></name>
        <name><surname>Brennan</surname><given-names>Sue E</given-names></name>
        <name><surname>Haddaway</surname><given-names>Neal R</given-names></name>
        <name><surname>Hamilton</surname><given-names>Daniel G</given-names></name>
        <name><surname>Karunananthan</surname><given-names>Sathya</given-names></name>
        <name><surname>McDonald</surname><given-names>Steve</given-names></name>
        <name><surname>Moher</surname><given-names>David</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Changing patterns in reporting and sharing of review data in systematic reviews with meta-analysis of the effects of interventions: Cross sectional meta-research study</article-title>
      <source>bmj</source>
      <year iso-8601-date="2022">2022</year>
      <volume>379</volume>
      <pub-id pub-id-type="doi">10.1136/bmj-2022-072428</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-HigginsEtAl2023">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Higgins</surname><given-names>JPT</given-names></name>
        <name><surname>Thomas</surname><given-names>J</given-names></name>
        <name><surname>Chandler</surname><given-names>J</given-names></name>
        <name><surname>Cumpston</surname><given-names>M</given-names></name>
        <name><surname>Li</surname><given-names>T</given-names></name>
        <name><surname>Page</surname><given-names>MJ</given-names></name>
        <name><surname>Welch</surname><given-names>VA</given-names></name>
      </person-group>
      <source>Cochrane handbook for systematic reviews of interventions version 6.4 (updated august 2023)</source>
      <publisher-name>Cochrane</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>www.training.cochrane.org/handbook</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>When evaluating the performance of classification
    algorithms, it is important to avoid overfitting, i.e., relying on
    rules that perfectly fit known data, but do not generalize to
    unknown data. This means that the objective of <italic>zero false
    positives</italic> should not be achieved by combining many
    idiosyncratic rules, which apply to very few or individual cases.
    Instead, the focus of BibDedupe is on curating and generalizing
    rules, which are not limited to specific papers. For instance,
    identifying the issue of journal translations was a starting point
    to acquire comprehensive lists of journal translations and specify
    pre-processing rules that generalize beyond the cases observed in
    the evaluation dataset.</p>
  </fn>
</fn-group>
</back>
</article>
