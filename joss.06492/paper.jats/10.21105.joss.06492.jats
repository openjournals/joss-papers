<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6492</article-id>
<article-id pub-id-type="doi">10.21105/joss.06492</article-id>
<title-group>
<article-title>ATHENA: A Fortran package for neural
networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9134-9712</contrib-id>
<name>
<surname>Taylor</surname>
<given-names>Ned Thaddeus</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Physics and Astronomy, University of Exeter,
United Kingdom, EX4 4QL</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-02-18">
<day>18</day>
<month>2</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6492</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Fortran</kwd>
<kwd>neural network</kwd>
<kwd>machine learning</kwd>
<kwd>convolution</kwd>
<kwd>3D convolution</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In the landscape of modern Fortran programming, there exists a
  compelling need for neural network libraries tailored to the language.
  Given the extensive set of legacy codes built with Fortran, there is
  an ever-growing necessity to provide new libraries implementing on
  modern data science tools and methodologies. Fortran’s inherent
  compatibility with high-performance computing resources, particularly
  CPUs, positions it as a language of choice for many machine learning
  problems.</p>
  <p>The vast amount of computing capabilities available within current
  supercomputers worldwide would be an invaluable asset to the growing
  demand for machine learning and artificial intelligence. The
  <monospace>ATHENA</monospace> library is developed as a resource to
  bridge this gap; It provides a robust suite of tools designed for
  building, training, and testing fully-connected and convolutional
  feed-forward neural networks.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>ATHENA</monospace> (Adaptive Training for High
  Efficiency Neural Network Applications) is a Fortran-based library
  aimed at providing users with the ability to build, train, and test
  feed-forward neural networks. The library leverages Fortran’s strong
  support of array arithmatics, and its compatibility with parallel and
  high-performance computing resources. Additionally, there exist many
  improvements made available since Fortran 95, specifically in Fortran
  2018
  (<xref alt="Reid, 2018" rid="ref-reid2018new" ref-type="bibr">Reid,
  2018</xref>) (and upcoming ones in Fortran 2023), as well as continued
  development by the Fortran Standards committee. All of this provides a
  clear incentive to develop further libraries and frameworks focused on
  providing machine learning capabilities to the Fortran community.</p>
  <p>While existing Fortran-based libraries, such as neural-fortran
  (<xref alt="Curcic, 2019" rid="ref-curcic2019parallel" ref-type="bibr">Curcic,
  2019</xref>), address many aspects of neural networks,
  <monospace>ATHENA</monospace> provides implementation of some
  well-known features not currently available within other libraries;
  these features include batch normalisation, regularisation layers
  (such as Dropout and DropBlock), and average pooling layers.
  Additionally, the library provides support for 1, 2, and 3D input data
  for most features currently implemented; this includes 1, 2, and 3D
  data for convolutional layers. Finally, more features for
  convolutional techniques are supported in the
  <monospace>ATHENA</monospace> library, including various data padding
  types and stride.</p>
  <p>Notably, discussions with a spectrum of stakeholders have
  significantly influenced the development of
  <monospace>ATHENA</monospace>, placing paramount importance on
  accessibility and usability. This user-centric approach ensures that
  <monospace>ATHENA</monospace> is not just a library but a tool that
  seamlessly integrates with the evolving needs of the neural network
  community. One of the primary intended applications of
  <monospace>ATHENA</monospace> is in materials science, which heavily
  utilises convolutional and graph neural networks for learning based on
  charge densities and atomic structures. Given the unique data
  structure of atomic configurations, specifically their graph-based
  nature, a specialised API must be developed to accommodate these
  needs.</p>
</sec>
<sec id="features">
  <title>Features</title>
  <p>A full list of features available within the
  <monospace>ATHENA</monospace> library, including available layer
  types, optimisers, activation functions, and initialisers, can be
  found on the repository’s wiki.</p>
  <p><monospace>ATHENA</monospace> is developed to handle the following
  network layer types: batch normalisation (1, 2, and 3D,
  <xref alt="Ioffe &amp; Szegedy, 2015" rid="ref-ioffe2015batch" ref-type="bibr">Ioffe
  &amp; Szegedy, 2015</xref>), convolution (1, 2, and 3D), Dropout
  (<xref alt="Srivastava et al., 2014" rid="ref-srivastava2014dropout" ref-type="bibr">Srivastava
  et al., 2014</xref>), DropBlock (2D and 3D,
  <xref alt="Ghiasi et al., 2018" rid="ref-ghiasi2018dropblock" ref-type="bibr">Ghiasi
  et al., 2018</xref>), flatten, fully-connected (dense), pooling (1, 2,
  and 3D; average and maximum).</p>
  <p>The library can handle feed-forward networks with an arbirtray
  number of hidden layers and neurons (or filter sizes). There exist
  several activation functions, including Gaussian, linear, sigmoid,
  ReLU, leaky ReLU, tangent hyberbolic functions, and more. Optimiser
  functions include stochastic gradient decent (SGD), RMSprop, Adam, and
  AdaGrad. Network models can be saved to and loaded from files.</p>
</sec>
<sec id="ongoing-research-projects">
  <title>Ongoing research projects</title>
  <p>The <monospace>ATHENA</monospace> library is being used in ongoing
  materials science research, with a focus on structural and materials
  property prediction.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The author thanks the Leverhulme for funding via Grant
  No. RPG-2021-086. The development of this code has benefitted through
  discussions with and contributions from many members of the
  Hepplestone research group, including Steven Paul Hepplestone, Francis
  Huw Davies, Harry McClean, Shane Davies, Ed Baker, Joe Pitfield, and
  Conor Price. Of particular note, Francis has provided contributions
  towards the development of code in some procedures focused on handling
  variables and files.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-curcic2019parallel">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Curcic</surname><given-names>Milan</given-names></name>
      </person-group>
      <article-title>A parallel Fortran framework for neural networks and deep learning</article-title>
      <source>SIGPLAN Fortran Forum</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2019-03">2019</year><month>03</month>
      <volume>38</volume>
      <issue>1</issue>
      <issn>1061-7264</issn>
      <uri>https://doi.org/10.1145/3323057.3323059</uri>
      <pub-id pub-id-type="doi">10.1145/3323057.3323059</pub-id>
      <fpage>4</fpage>
      <lpage>21</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ghiasi2018dropblock">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ghiasi</surname><given-names>Golnaz</given-names></name>
        <name><surname>Lin</surname><given-names>Tsung-Yi</given-names></name>
        <name><surname>Le</surname><given-names>Quoc V.</given-names></name>
      </person-group>
      <article-title>DropBlock: A regularization method for convolutional networks</article-title>
      <source>Proceedings of the 32nd international conference on neural information processing systems</source>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <publisher-loc>Red Hook, NY, USA</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <uri>https://dl.acm.org/doi/10.5555/3327546.3327732</uri>
      <fpage>10750</fpage>
      <lpage>10760</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ioffe2015batch">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ioffe</surname><given-names>Sergey</given-names></name>
        <name><surname>Szegedy</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title>
      <source>Proceedings of the 32nd international conference on international conference on machine learning - volume 37</source>
      <publisher-name>JMLR.org</publisher-name>
      <publisher-loc>Lille, France</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <uri>https://dl.acm.org/doi/10.5555/3045118.3045167</uri>
      <fpage>448</fpage>
      <lpage>456</lpage>
    </element-citation>
  </ref>
  <ref id="ref-srivastava2014dropout">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Srivastava</surname><given-names>Nitish</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Krizhevsky</surname><given-names>Alex</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name>
      </person-group>
      <article-title>Dropout: A simple way to prevent neural networks from overfitting</article-title>
      <source>J. Mach. Learn. Res.</source>
      <publisher-name>JMLR.org</publisher-name>
      <year iso-8601-date="2014-01">2014</year><month>01</month>
      <volume>15</volume>
      <issue>1</issue>
      <issn>1532-4435</issn>
      <uri>https://dl.acm.org/doi/10.5555/2627435.2670313</uri>
      <fpage>1929</fpage>
      <lpage>1958</lpage>
    </element-citation>
  </ref>
  <ref id="ref-reid2018new">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Reid</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>The new features of Fortran 2018</article-title>
      <source>SIGPLAN Fortran Forum</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2018-04">2018</year><month>04</month>
      <volume>37</volume>
      <issue>1</issue>
      <issn>1061-7264</issn>
      <uri>https://doi.org/10.1145/3206214.3206215</uri>
      <pub-id pub-id-type="doi">10.1145/3206214.3206215</pub-id>
      <fpage>5</fpage>
      <lpage>43</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
