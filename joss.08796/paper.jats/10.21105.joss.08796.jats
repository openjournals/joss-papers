<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8796</article-id>
<article-id pub-id-type="doi">10.21105/joss.08796</article-id>
<title-group>
<article-title>IBBI: A Python package for the detection and
classification of bark and ambrosia beetles</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3409-0473</contrib-id>
<name>
<surname>Marais</surname>
<given-names>Christopher</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-4748-4241</contrib-id>
<name>
<surname>Kuo</surname>
<given-names>Eric</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8706-4618</contrib-id>
<name>
<surname>Hulcr</surname>
<given-names>Jiri</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3680-6834</contrib-id>
<name>
<surname>Dias</surname>
<given-names>Raquel</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>School of Forest, Fisheries, and Geomatics Sciences,
University of Florida, Gainesville, FL, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Microbiology and Cell Science, University of Florida,
Gainesville, FL, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-06-13">
<day>13</day>
<month>6</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>116</issue>
<fpage>8796</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PyTorch</kwd>
<kwd>computer vision</kwd>
<kwd>deep learning</kwd>
<kwd>entomology</kwd>
<kwd>object detection</kwd>
<kwd>object classification</kwd>
<kwd>forest health</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The Intelligent Bark Beetle Identifier (IBBI) is an open-source
  Python package that provides a simple, unified interface for the
  automated detection and classification of bark and ambrosia beetles
  (Coleoptera: Curculionidae: Scolytinae and Platypodinae) from images.
  This package addresses a critical need in forest health and
  biodiversity research by leveraging multiple state-of-the-art computer
  vision models to streamline a traditionally labor-intensive and
  expert-dependent task. The target audience for
  <monospace>ibbi</monospace> includes entomologists, ecologists, forest
  health professionals, and biodiversity researchers who require
  high-throughput identification of beetle specimens from field or lab
  images. By providing programmatic access to a suite of pre-trained
  models for object-detection and species classification,
  <monospace>ibbi</monospace> empowers researchers to accelerate data
  analysis for ecological studies, invasive species monitoring, and
  biodiversity assessments. The core functionality is accessible through
  a single factory function, <monospace>create_model()</monospace>,
  which simplifies model loading and inference, making deep learning
  techniques accessible to users without requiring specialized expertise
  in computer vision.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>The accurate and timely identification of bark and ambrosia beetle
  species is fundamental to forest health monitoring, pest management,
  and quarantine efforts worldwide
  (<xref alt="Hulcr &amp; Stelinski, 2017" rid="ref-hulcr2017ambrosia" ref-type="bibr">Hulcr
  &amp; Stelinski, 2017</xref>;
  <xref alt="Piper et al., 2019" rid="ref-piper2019prospects" ref-type="bibr">Piper
  et al., 2019</xref>;
  <xref alt="Ramsfield et al., 2016" rid="ref-ramsfield2016forest" ref-type="bibr">Ramsfield
  et al., 2016</xref>). Many species are cryptic or morphologically
  similar (i.e., exhibit high homoplasy), making identification a
  significant bottleneck that requires highly specialized taxonomic
  expertise
  (<xref alt="Hulcr et al., 2015" rid="ref-hulcr2015morphology" ref-type="bibr">Hulcr
  et al., 2015</xref>;
  <xref alt="Kirkendall et al., 2015" rid="ref-kirkendall2015" ref-type="bibr">Kirkendall
  et al., 2015</xref>). Traditional morphological identification methods
  are slow, not scalable for large ecological datasets, and can be
  prohibitively expensive, hindering rapid responses to invasive species
  outbreaks and broad-scale biodiversity research.</p>
  <p>While computer vision offers a promising solution, a critical gap
  exists in the available software ecosystem. There is a lack of
  specialized, accessible, and benchmarked tools for bark beetle
  identification. <monospace>ibbi</monospace> fills this gap by
  providing a robust, easy-to-install Python package that: 1. Focuses
  specifically on the taxonomically challenging and economically
  critical bark and ambrosia beetle groups. 2. Offers a range of
  pre-trained, benchmarked models for different tasks (object-detection
  and object-classification, zero-shot object-detection). 3.
  Standardizes model access through a simple, high-level API, lowering
  the barrier to entry for researchers.</p>
  <p>By consolidating these functionalities into a single package,
  <monospace>ibbi</monospace> provides a foundational tool that enables
  reproducible, high-throughput research that was previously
  infeasible.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the Field</title>
  <p>Automated species identification using computer vision has seen
  significant progress, with deep learning models achieving expert-level
  accuracy in various biological domains
  (<xref alt="Høye et al., 2021" rid="ref-hoye2021" ref-type="bibr">Høye
  et al., 2021</xref>;
  <xref alt="Valan et al., 2019" rid="ref-valan2019" ref-type="bibr">Valan
  et al., 2019</xref>). This progress has been largely driven by two
  trends: broad-scale generalist models and bespoke academic models.</p>
  <p>Large-scale projects, often powered by citizen-science datasets
  like iNaturalist, have successfully developed generalist models for
  identifying a wide range of common taxa
  (<xref alt="Horn et al., 2018" rid="ref-horn2018" ref-type="bibr">Horn
  et al., 2018</xref>). Similarly, comprehensive toolkits like
  Pytorch-Wildlife offer powerful, pre-trained models, such as
  MegaDetector, for identifying a broad spectrum of wildlife in camera
  trap images
  (<xref alt="Hernandez et al., 2024" rid="ref-hernandez2024pytorch" ref-type="bibr">Hernandez
  et al., 2024</xref>). However, these generalist models, while powerful
  for common categories, often struggle with the fine-grained
  distinctions required for less common or cryptic species, particularly
  in specialized groups like bark and ambrosia beetles, where
  morphological similarities can lead to misidentification
  (<xref alt="Kirkendall et al., 2015" rid="ref-kirkendall2015" ref-type="bibr">Kirkendall
  et al., 2015</xref>).</p>
  <p>This has led to a growing trend of developing specialized tools for
  specific taxonomic groups of high economic or ecological importance.
  For example, various research initiatives have focused on creating
  models for identifying bees to monitor pollinators or for classifying
  mosquito species to track disease vectors
  (<xref alt="Buschbacher et al., 2020" rid="ref-buschbacher2020image" ref-type="bibr">Buschbacher
  et al., 2020</xref>;
  <xref alt="Goodwin et al., 2021" rid="ref-goodwin2021mosquito" ref-type="bibr">Goodwin
  et al., 2021</xref>). These efforts demonstrate the recognized need
  for domain-specific tools, but the resulting models are rarely
  distributed in an accessible, unified software package.</p>
  <p>On the other end of the spectrum, numerous studies have developed
  bespoke computer vision models for bark beetle classification
  (<xref alt="Marais et al., 2025" rid="ref-marais2025progress" ref-type="bibr">Marais
  et al., 2025</xref>;
  <xref alt="Sun et al., 2024" rid="ref-sun2024intelligent" ref-type="bibr">Sun
  et al., 2024</xref>). While valuable, these models are typically
  created for a specific research question and are not released as
  accessible, reusable, or maintained software. Consequently, they are
  not universally benchmarked, and their performance on different
  datasets is unknown.</p>
  <p><monospace>ibbi</monospace> is novel in that it bridges this gap.
  It is, to our knowledge, the first software package to:</p>
  <list list-type="order">
    <list-item>
      <p>Aggregate multiple deep learning architectures specifically for
      bark and ambrosia beetle identification into a single, cohesive
      framework.</p>
    </list-item>
    <list-item>
      <p>Train these models on one of the largest and most taxonomically
      diverse datasets of bark and ambrosia beetles available.</p>
    </list-item>
    <list-item>
      <p>Provide a standardized method for benchmarking these models,
      allowing for direct performance comparisons.</p>
    </list-item>
    <list-item>
      <p>Distribute the models and inference code as an easy-to-install
      Python package, promoting transparency, reusability, and
      extensibility.</p>
    </list-item>
  </list>
  <p>The first version of <monospace>ibbi</monospace> is made possible
  by powerful open-source libraries that provide state-of-the-art model
  implementations, such as timm
  (<xref alt="Wightman, 2019" rid="ref-rw2021timm" ref-type="bibr">Wightman,
  2019</xref>) and ultralytics
  (<xref alt="Jocher et al., 2020" rid="ref-jocher2020yolov5" ref-type="bibr">Jocher
  et al., 2020</xref>), and frameworks such as Hugging Face for model
  and data sharing
  (<xref alt="Wolf et al., 2019" rid="ref-wolf2019huggingface" ref-type="bibr">Wolf
  et al., 2019</xref>).</p>
  <p>Consequently, <monospace>ibbi</monospace> is established not as a
  substitute for broad frameworks like MegaDetector, but as a
  specialized complement that fills a critical gap in functionality,
  while simultaneously overcoming the accessibility barriers often
  associated with bespoke academic models.</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p>The <monospace>ibbi</monospace> package is designed for ease of use
  and flexibility. Its core functionality revolves around a factory
  function, <monospace>create_model()</monospace>, which handles model
  instantiation and the downloading of pre-trained weights from Hugging
  Face Hub
  (<xref alt="Wolf et al., 2019" rid="ref-wolf2019huggingface" ref-type="bibr">Wolf
  et al., 2019</xref>).</p>
  <sec id="model-curation-and-datasets">
    <title>Model Curation and Datasets</title>
    <p>The models provided in <monospace>ibbi</monospace> are the result
    of a comprehensive data pipeline (Figure 1). The process involved
    collecting images from diverse sources, using a zero-shot detection
    model for initial beetle localization, and employing a
    human-in-the-loop process to verify annotations. The curated data
    was then used to train the object-detection and
    object-classification models. To promote transparency and
    reproducibility, the package provides a helper function to access
    the testing dataset used for evaluation. Furthermore, users can
    programmatically view all available models and their performance
    metrics using the <monospace>ibbi.list_models()</monospace>
    function, allowing for informed model selection.</p>
    <p><inline-graphic mimetype="image" mime-subtype="png" xlink:href="https://media.githubusercontent.com/media/ChristopherMarais/IBBI/main/docs/assets/images/data_flow_ibbi.png">
      <alt-text>Overview of the data collection, curation, and model
      training workflow for the ibbi package.</alt-text>
    </inline-graphic> <italic>Figure 1: Overview of the data collection,
    curation, and model training workflow for the
    <monospace>ibbi</monospace> package.</italic></p>
  </sec>
  <sec id="core-tasks-and-api">
    <title>Core Tasks and API</title>
    <p>The package supports four primary tasks:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Single-class Object-Detection</bold>: Identifies and
        localizes any bark and ambrosia beetles in an image, returning
        bounding box coordinates. This is ideal for processing raw
        images from traps or collection sheets and is based on
        fine-tuned high-performance architectures like YOLO
        (<xref alt="Redmon et al., 2016" rid="ref-redmon2016" ref-type="bibr">Redmon
        et al., 2016</xref>) and detection transformers
        (<xref alt="Lv et al., 2023" rid="ref-lv2023rtdetr" ref-type="bibr">Lv
        et al., 2023</xref>).</p>
      </list-item>
      <list-item>
        <p><bold>Zero-Shot Object-Detection</bold>: Detects arbitrary
        objects in an image based on a user-provided text prompt. This
        powerful feature uses the GroundingDINO model
        (<xref alt="Liu et al., 2023" rid="ref-liu2023grounding" ref-type="bibr">Liu
        et al., 2023</xref>) and allows for flexible analysis beyond
        predefined categories. However, this model has not been
        fine-tuned specifically on bark and ambrosia beetle images.</p>
      </list-item>
      <list-item>
        <p><bold>Multi-class Object-Detection</bold>: Predicts the
        species of a beetle from an image, returning species class
        probabilities. This currently supports 63 species and is also
        based on fine-tuned high-performance architectures like YOLO
        (<xref alt="Redmon et al., 2016" rid="ref-redmon2016" ref-type="bibr">Redmon
        et al., 2016</xref>) and detection transformers
        (<xref alt="Lv et al., 2023" rid="ref-lv2023rtdetr" ref-type="bibr">Lv
        et al., 2023</xref>). However, it is important to note that the
        models are not fine tuned on a dataset as large as the one used
        for single-class object-detection, and therefore may not perform
        as well on images that are not tightly cropped around the
        specimen. This task is the main focus for the future roadmap of
        the package, with plans to expand the number of species
        supported and improve model performance.</p>
      </list-item>
      <list-item>
        <p><bold>Feature Extraction</bold>: Generates deep-learning
        feature embeddings (vectors) from an image. This task is
        available for models of all tasks. These feature embeddings can
        be used for downstream tasks like clustering, similarity
        analysis, or visual search engines.</p>
      </list-item>
    </list>
  </sec>
  <sec id="a-typical-workflow">
    <title>A typical Workflow:</title>
    <code language="python">import ibbi
from PIL import Image

# 1. List available models to see choices and performance metrics
ibbi.list_models()

# 2. Choose and load a pre-trained model by name
# For this example, we'll use a species detection and classification model
model_name = &quot;yolov10x_bb_multi_class_detect_model&quot;
model = ibbi.create_model(model_name, pretrained=True)

# 3. Prepare an image (from path, URL, or PIL/Numpy object)
image = Image.open(&quot;path/to/beetle_image.jpg&quot;)

# 4. Run inference
results = model.predict(image)

# 5. Extract features
features = model.extract_features(image)</code>
  </sec>
  <sec id="inputs-outputs-and-hardware">
    <title>Inputs, Outputs, and Hardware</title>
    <list list-type="bullet">
      <list-item>
        <p><bold>Inputs</bold>: The <monospace>predict</monospace> and
        <monospace>extract_features</monospace> methods accept image
        inputs as a file path, a URL, a <monospace>PIL.Image</monospace>
        object, or a <monospace>numpy.ndarray</monospace>.</p>
      </list-item>
      <list-item>
        <p><bold>Outputs</bold>: The <monospace>predict</monospace>
        method returns a list of custom <monospace>Results</monospace>
        objects, which contain easily accessible attributes such as
        bounding boxes (<monospace>.boxes</monospace>), probability
        scores (<monospace>.probs</monospace>), and class names
        (<monospace>.names</monospace>).</p>
      </list-item>
      <list-item>
        <p><bold>Hardware &amp; Limitations</bold>: While
        <monospace>ibbi</monospace> can run on a standard CPU, a
        CUDA-enabled GPU is highly recommended for faster inference,
        especially for batch processing. The performance of the models
        is dependent on the quality and resolution of the input images.
        Object-classification models achieve the highest accuracy when
        used on images that are well-lit and tightly cropped around the
        specimen.</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="future-development-roadmap">
  <title>Future Development Roadmap</title>
  <p><monospace>ibbi</monospace> is under active development with a
  commitment to long-term support and expansion. The future roadmap is
  centered on two key areas: expanding the core platform and developing
  a novel, specialized identification model.</p>
  <sec id="platform-and-ecosystem-expansion">
    <title>Platform and Ecosystem Expansion</title>
    <list list-type="bullet">
      <list-item>
        <p><bold>Model repository expansion</bold>: We are committed to
        continually expanding the <monospace>ibbi</monospace> model
        repository by fine-tuning additional state-of-the-art
        architectures on our dataset.</p>
      </list-item>
      <list-item>
        <p><bold>Dataset Collection Growth</bold>: The training dataset
        itself will be perpetually augmented with more images and
        species to improve taxonomic coverage.</p>
      </list-item>
      <list-item>
        <p><bold>Enhanced Evaluation Framework</bold>: We will expand
        the evaluation capabilities of the package to serve as a
        standardized benchmarking tool for other models in the field.
        This will includes integrating explainable AI (XAI) techniques
        for better model interpretability and enhancing the ability to
        perform clustering analyses on feature embeddings.</p>
      </list-item>
      <list-item>
        <p><bold>Specialized Identification Model Development</bold>: We
        will develop a new, specialized identification model that is
        optimized for bark beetle identification, moving beyond the
        generalist models currently available. This model will be
        modular and incrementally developed, with new capabilities
        released through package updates.</p>
      </list-item>
    </list>
  </sec>
</sec>
<sec id="licensing-authorship-and-conflicts-of-interest">
  <title>Licensing, Authorship, and Conflicts of Interest</title>
  <sec id="license">
    <title>License</title>
    <p>This software is licensed under the OSI-approved MIT License.</p>
  </sec>
  <sec id="authorship">
    <title>Authorship</title>
    <p>All authors have made substantial contributions to the design of
    the software.</p>
  </sec>
  <sec id="conflicts-of-interest">
    <title>Conflicts of Interest</title>
    <p>The authors declare that they have no conflicts of interest.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank the numerous collaborators, students, and technicians in
  the Hulcr lab at the University of Florida who have contributed to the
  collection, curation, and annotation of the image dataset that made
  this work possible. We also acknowledge the National Science
  Foundation, the USDA Forest Service, and the Florida Forest Service
  for their financial support.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-kirkendall2015">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Kirkendall</surname><given-names>Lawrence R.</given-names></name>
        <name><surname>Biedermann</surname><given-names>Peter H. W.</given-names></name>
        <name><surname>Jordal</surname><given-names>Bjarte H.</given-names></name>
      </person-group>
      <article-title>Evolution and diversity of bark and ambrosia beetles</article-title>
      <source>Bark beetles: Biology and ecology of native and invasive species</source>
      <person-group person-group-type="editor">
        <name><surname>Vega</surname><given-names>Fernando E.</given-names></name>
        <name><surname>Hofstetter</surname><given-names>Richard W.</given-names></name>
      </person-group>
      <publisher-name>Academic Press</publisher-name>
      <publisher-loc>San Diego</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>978-0-12-417156-5</isbn>
      <pub-id pub-id-type="doi">10.1016/B978-0-12-417156-5.00003-4</pub-id>
      <fpage>85</fpage>
      <lpage>156</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hulcr2015morphology">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Hulcr</surname><given-names>Jiri</given-names></name>
        <name><surname>Atkinson</surname><given-names>Thomas H.</given-names></name>
        <name><surname>Cognato</surname><given-names>Anthony I.</given-names></name>
        <name><surname>Jordal</surname><given-names>Bjarte H.</given-names></name>
        <name><surname>McKenna</surname><given-names>Duane D.</given-names></name>
      </person-group>
      <article-title>Morphology, taxonomy, and phylogenetics of bark beetles</article-title>
      <source>Bark beetles: Biology and ecology of native and invasive species</source>
      <person-group person-group-type="editor">
        <name><surname>Vega</surname><given-names>Fernando E.</given-names></name>
        <name><surname>Hofstetter</surname><given-names>Richard W.</given-names></name>
      </person-group>
      <publisher-name>Academic Press</publisher-name>
      <publisher-loc>San Diego</publisher-loc>
      <year iso-8601-date="2015">2015</year>
      <isbn>978-0-12-417156-5</isbn>
      <pub-id pub-id-type="doi">10.1016/B978-0-12-417156-5.00002-2</pub-id>
      <fpage>41</fpage>
      <lpage>84</lpage>
    </element-citation>
  </ref>
  <ref id="ref-valan2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Valan</surname><given-names>Miroslav</given-names></name>
        <name><surname>Makonyi</surname><given-names>Karoly</given-names></name>
        <name><surname>Maki</surname><given-names>Atsuto</given-names></name>
        <name><surname>Vondráček</surname><given-names>Dominik</given-names></name>
        <name><surname>Ronquist</surname><given-names>Fredrik</given-names></name>
      </person-group>
      <article-title>Automated taxonomic identification of insects with expert-level accuracy using effective feature transfer from convolutional networks</article-title>
      <source>Systematic Biology</source>
      <publisher-name>Oxford University Press</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>68</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1093/sysbio/syz014</pub-id>
      <fpage>876</fpage>
      <lpage>895</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hoye2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Høye</surname><given-names>Toke T.</given-names></name>
        <name><surname>Ärje</surname><given-names>Johanna</given-names></name>
        <name><surname>Bjerge</surname><given-names>Kim</given-names></name>
        <name><surname>Hansen</surname><given-names>O. L. P.</given-names></name>
        <name><surname>Iosifidis</surname><given-names>Alexandros</given-names></name>
        <name><surname>Leitch</surname><given-names>Kirk J.</given-names></name>
        <name><surname>Mann</surname><given-names>H. M. R.</given-names></name>
        <name><surname>Meissner</surname><given-names>J. J.</given-names></name>
        <name><surname>Melvad</surname><given-names>Claus</given-names></name>
        <name><surname>Raitoharju</surname><given-names>Jenni</given-names></name>
      </person-group>
      <article-title>Deep learning and computer vision will transform entomology</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <publisher-name>National Academy of Sciences</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>118</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1073/pnas.2002545117</pub-id>
      <fpage>e2002545117</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-horn2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Horn</surname><given-names>Grant Van</given-names></name>
        <name><surname>Aodha</surname><given-names>Oisin Mac</given-names></name>
        <name><surname>Song</surname><given-names>Yang</given-names></name>
        <name><surname>Shepard</surname><given-names>Abe</given-names></name>
        <name><surname>Adam</surname><given-names>Hartwig</given-names></name>
        <name><surname>Perona</surname><given-names>Pietro</given-names></name>
        <name><surname>Belongie</surname><given-names>Serge</given-names></name>
      </person-group>
      <article-title>The iNaturalist species classification and detection dataset</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2018.00914</pub-id>
      <fpage>8769</fpage>
      <lpage>8778</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hulcr2017ambrosia">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hulcr</surname><given-names>Jiri</given-names></name>
        <name><surname>Stelinski</surname><given-names>Lukasz L.</given-names></name>
      </person-group>
      <article-title>The ambrosia symbiosis: From evolutionary ecology to practical management</article-title>
      <source>Annual Review of Entomology</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>62</volume>
      <pub-id pub-id-type="doi">10.1146/annurev-ento-031616-035105</pub-id>
      <fpage>285</fpage>
      <lpage>303</lpage>
    </element-citation>
  </ref>
  <ref id="ref-piper2019prospects">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Piper</surname><given-names>Alexander M.</given-names></name>
        <name><surname>Batovska</surname><given-names>Jana</given-names></name>
        <name><surname>Cogan</surname><given-names>Noel O. I.</given-names></name>
        <name><surname>Cunningham</surname><given-names>John P.</given-names></name>
        <name><surname>Blacket</surname><given-names>Mark J.</given-names></name>
      </person-group>
      <article-title>Prospects and challenges of implementing DNA metabarcoding for high-throughput insect surveillance</article-title>
      <source>GigaScience</source>
      <publisher-name>Oxford University Press</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>8</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1093/gigascience/giz092</pub-id>
      <fpage>giz092</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ramsfield2016forest">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ramsfield</surname><given-names>T. D.</given-names></name>
        <name><surname>Bentz</surname><given-names>B. J.</given-names></name>
        <name><surname>Faccoli</surname><given-names>M.</given-names></name>
        <name><surname>Jactel</surname><given-names>H.</given-names></name>
        <name><surname>Brockerhoff</surname><given-names>E. G.</given-names></name>
      </person-group>
      <article-title>Forest health in a changing world: Effects of globalization and climate change on forest insect and pathogen impacts</article-title>
      <source>Forestry: An International Journal of Forest Research</source>
      <publisher-name>Oxford University Press</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>89</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1093/forestry/cpw018</pub-id>
      <fpage>245</fpage>
      <lpage>252</lpage>
    </element-citation>
  </ref>
  <ref id="ref-redmon2016">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Redmon</surname><given-names>Joseph</given-names></name>
        <name><surname>Divvala</surname><given-names>Santosh</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
        <name><surname>Farhadi</surname><given-names>Ali</given-names></name>
      </person-group>
      <article-title>You only look once: Unified, real-time object detection</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>
      <fpage>779</fpage>
      <lpage>788</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rw2021timm">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Wightman</surname><given-names>Ross</given-names></name>
      </person-group>
      <article-title>PyTorch image models</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/rwightman/pytorch-image-models; GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.4414861</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-jocher2020yolov5">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Jocher</surname><given-names>Glenn</given-names></name>
        <name><surname>Stoken</surname><given-names>August</given-names></name>
        <name><surname>Borovec</surname><given-names>Jirka</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Ultralytics YOLOv5</article-title>
      <publisher-name>https://github.com/ultralytics/yolov5; Zenodo</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.5281/zenodo.3908559</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wolf2019huggingface">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wolf</surname><given-names>Thomas</given-names></name>
        <name><surname>Debut</surname><given-names>Lysandre</given-names></name>
        <name><surname>Sanh</surname><given-names>Victor</given-names></name>
        <name><surname>Chaumond</surname><given-names>Julien</given-names></name>
        <name><surname>Delangue</surname><given-names>Clement</given-names></name>
        <name><surname>Moi</surname><given-names>Anthony</given-names></name>
        <name><surname>Cistac</surname><given-names>Pierric</given-names></name>
        <name><surname>Rault</surname><given-names>Tim</given-names></name>
        <name><surname>Louf</surname><given-names>Rémi</given-names></name>
        <name><surname>Funtowicz</surname><given-names>Morgan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Huggingface’s transformers: State-of-the-art natural language processing</article-title>
      <source>arXiv preprint arXiv:1910.03771</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1910.03771</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-lv2023rtdetr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lv</surname><given-names>Wenyu</given-names></name>
        <name><surname>Gao</surname><given-names>Shang-Hua</given-names></name>
        <name><surname>He</surname><given-names>Cheng</given-names></name>
        <name><surname>Jiao</surname><given-names>Jincheng</given-names></name>
        <name><surname>Wang</surname><given-names>Shuo</given-names></name>
        <name><surname>Liu</surname><given-names>Zheng</given-names></name>
        <name><surname>Wang</surname><given-names>Yuning</given-names></name>
        <name><surname>Wang</surname><given-names>Lin</given-names></name>
        <name><surname>Han</surname><given-names>Kai</given-names></name>
        <name><surname>Chen</surname><given-names>Chen</given-names></name>
        <name><surname>You</surname><given-names>Yang</given-names></name>
        <name><surname>Wang</surname><given-names>Jingdong</given-names></name>
      </person-group>
      <article-title>DETRs with Collaborative Hybrid Assignments for Real-Time Object Detection</article-title>
      <source>arXiv preprint arXiv:2304.08069</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2304.08069</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-liu2023grounding">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Shilong</given-names></name>
        <name><surname>Zeng</surname><given-names>Zhaoyang</given-names></name>
        <name><surname>Ren</surname><given-names>Tianhe</given-names></name>
        <name><surname>Li</surname><given-names>Feng</given-names></name>
        <name><surname>Zhang</surname><given-names>Hao</given-names></name>
        <name><surname>Yang</surname><given-names>Jie</given-names></name>
        <name><surname>Li</surname><given-names>Chunyuan</given-names></name>
        <name><surname>Su</surname><given-names>Jianfeng</given-names></name>
        <name><surname>Zhu</surname><given-names>Jun</given-names></name>
        <name><surname>Zhang</surname><given-names>Lei</given-names></name>
        <name><surname>Su</surname><given-names>Hang</given-names></name>
      </person-group>
      <article-title>Grounding DINO: Marrying DINO with grounded pre-training for open-set object detection</article-title>
      <source>Proceedings of the IEEE/CVF international conference on computer vision</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/ICCV51070.2023.01349</pub-id>
      <fpage>14480</fpage>
      <lpage>14490</lpage>
    </element-citation>
  </ref>
  <ref id="ref-marais2025progress">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marais</surname><given-names>G Christopher</given-names></name>
        <name><surname>Stratton</surname><given-names>Isabelle C</given-names></name>
        <name><surname>Johnson</surname><given-names>Andrew J</given-names></name>
        <name><surname>Hulcr</surname><given-names>Jiri</given-names></name>
      </person-group>
      <article-title>Progress in developing a bark beetle identification tool</article-title>
      <source>PLoS One</source>
      <publisher-name>Public Library of Science San Francisco, CA USA</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <volume>20</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0310716</pub-id>
      <fpage>e0310716</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-sun2024intelligent">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sun</surname><given-names>Li</given-names></name>
        <name><surname>Cai</surname><given-names>Zhenghua</given-names></name>
        <name><surname>Liang</surname><given-names>Kaibo</given-names></name>
        <name><surname>Wang</surname><given-names>Yuzhi</given-names></name>
        <name><surname>Zeng</surname><given-names>Wang</given-names></name>
        <name><surname>Yan</surname><given-names>Xueqian</given-names></name>
      </person-group>
      <article-title>An intelligent system for high-density small target pest identification and infestation level determination based on an improved YOLOv5 model</article-title>
      <source>Expert Systems with Applications</source>
      <year iso-8601-date="2024">2024</year>
      <volume>239</volume>
      <pub-id pub-id-type="doi">10.1016/j.eswa.2023.122190</pub-id>
      <fpage>122190</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hernandez2024pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hernandez</surname><given-names>Andres</given-names></name>
        <name><surname>Miao</surname><given-names>Zhongqi</given-names></name>
        <name><surname>Vargas</surname><given-names>Luisa</given-names></name>
        <name><surname>Beery</surname><given-names>Sara</given-names></name>
        <name><surname>Dodhia</surname><given-names>Rahul</given-names></name>
        <name><surname>Arbelaez</surname><given-names>Pablo</given-names></name>
        <name><surname>Ferres</surname><given-names>Juan M Lavista</given-names></name>
      </person-group>
      <article-title>Pytorch-wildlife: A collaborative deep learning framework for conservation</article-title>
      <source>arXiv preprint arXiv:2405.12930</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2405.12930</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-buschbacher2020image">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Buschbacher</surname><given-names>Keanu</given-names></name>
        <name><surname>Ahrens</surname><given-names>Dirk</given-names></name>
        <name><surname>Espeland</surname><given-names>Marianne</given-names></name>
        <name><surname>Steinhage</surname><given-names>Volker</given-names></name>
      </person-group>
      <article-title>Image-based species identification of wild bees using convolutional neural networks</article-title>
      <source>Ecological Informatics</source>
      <year iso-8601-date="2020">2020</year>
      <volume>55</volume>
      <pub-id pub-id-type="doi">10.1016/j.ecoinf.2019.101017</pub-id>
      <fpage>101017</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodwin2021mosquito">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goodwin</surname><given-names>Autumn</given-names></name>
        <name><surname>Padmanabhan</surname><given-names>Sanket</given-names></name>
        <name><surname>Hira</surname><given-names>Sanchit</given-names></name>
        <name><surname>Glancey</surname><given-names>Margaret</given-names></name>
        <name><surname>Slinowsky</surname><given-names>Monet</given-names></name>
        <name><surname>Immidisetti</surname><given-names>Rakhil</given-names></name>
        <name><surname>Scavo</surname><given-names>Laura</given-names></name>
        <name><surname>Brey</surname><given-names>Jewell</given-names></name>
        <name><surname>Sai Sudhakar</surname><given-names>Bala Murali Manoghar</given-names></name>
        <name><surname>Ford</surname><given-names>Tristan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Mosquito species identification using convolutional neural networks with a multitiered ensemble model for novel species detection</article-title>
      <source>Scientific reports</source>
      <year iso-8601-date="2021">2021</year>
      <volume>11</volume>
      <issue>1</issue>
      <uri>https://www.sciencedirect.com/science/article/pii/S1574954119303280</uri>
      <pub-id pub-id-type="doi">10.1038/s41598-021-92891-9</pub-id>
      <fpage>13656</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-goodwin2021mosquito">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goodwin</surname><given-names>Autumn</given-names></name>
        <name><surname>Padmanabhan</surname><given-names>Sanket</given-names></name>
        <name><surname>Hira</surname><given-names>Sanchit</given-names></name>
        <name><surname>Glancey</surname><given-names>Margaret</given-names></name>
        <name><surname>Slinowsky</surname><given-names>Monet</given-names></name>
        <name><surname>Immidisetti</surname><given-names>Rakhil</given-names></name>
        <name><surname>Scavo</surname><given-names>Laura</given-names></name>
        <name><surname>Brey</surname><given-names>Jewell</given-names></name>
        <name><surname>Sai Sudhakar</surname><given-names>Bala Murali Manoghar</given-names></name>
        <name><surname>Ford</surname><given-names>Tristan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Mosquito species identification using convolutional neural networks with a multitiered ensemble model for novel species detection</article-title>
      <source>Scientific reports</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>11</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-021-92891-9</pub-id>
      <fpage>13656</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
