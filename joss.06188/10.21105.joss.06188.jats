<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6188</article-id>
<article-id pub-id-type="doi">10.21105/joss.06188</article-id>
<title-group>
<article-title>Surjectors: surjection layers for density estimation with
normalizing flows</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Dirmeier</surname>
<given-names>Simon</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Swiss Data Science Center, Zurich,
Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>ETH Zurich, Zurich, Switzerland</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-10-10">
<day>10</day>
<month>10</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>94</issue>
<fpage>6188</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>JAX</kwd>
<kwd>Density estimation</kwd>
<kwd>Normalizing flow</kwd>
<kwd>Machine learning</kwd>
<kwd>Statistics</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Normalizing flows (NFs,
  <xref alt="Papamakarios et al., 2021" rid="ref-papamakarios2021normalizing" ref-type="bibr">Papamakarios
  et al., 2021</xref>) are tractable neural density estimators which
  have in the recent past been applied successfully for, e.g.,
  generative modelling (Kingma &amp; Dhariwal
  (<xref alt="2018" rid="ref-kingma2018glow" ref-type="bibr">2018</xref>),
  Ping et al.
  (<xref alt="2020" rid="ref-ping20wave" ref-type="bibr">2020</xref>)),
  Bayesian inference (Rezende &amp; Mohamed
  (<xref alt="2015" rid="ref-rezende15flow" ref-type="bibr">2015</xref>),
  Hoffman et al.
  (<xref alt="2019" rid="ref-hoffman2019neutra" ref-type="bibr">2019</xref>))
  or simulation-based inference (Papamakarios et al.
  (<xref alt="2019" rid="ref-papamakarios2019sequential" ref-type="bibr">2019</xref>),
  Dirmeier, Albert, et al.
  (<xref alt="2023" rid="ref-dirmeier2023simulation" ref-type="bibr">2023</xref>)).
  <monospace>Surjectors</monospace> is a Python library in particular
  for <italic>surjective</italic>, i.e., dimensionality-reducing
  normalizing flows (SNFs, Klein et al.
  (<xref alt="2021" rid="ref-klein2021funnels" ref-type="bibr">2021</xref>)).
  <monospace>Surjectors</monospace> is based on the libraries JAX, Haiku
  and Distrax (Bradbury et al.
  (<xref alt="2018" rid="ref-jax2018github" ref-type="bibr">2018</xref>),
  Babuschkin et al.
  (<xref alt="2020" rid="ref-deepmind2020jax" ref-type="bibr">2020</xref>))
  and is fully compatible with them. By virtue of being entirely written
  in JAX
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>), <monospace>Surjectors</monospace> naturally
  supports usage on either CPU, GPU or TPU.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Real-world data are often lying in a high-dimensional ambient space
  embedded in a lower-dimensional manifold
  (<xref alt="Fefferman et al., 2016" rid="ref-fefferman2016testing" ref-type="bibr">Fefferman
  et al., 2016</xref>) which can complicate estimation of probability
  densities (Dai &amp; Seljak
  (<xref alt="2021" rid="ref-dai2020sliced" ref-type="bibr">2021</xref>),
  Klein et al.
  (<xref alt="2021" rid="ref-klein2021funnels" ref-type="bibr">2021</xref>),
  Nalisnick et al.
  (<xref alt="2019" rid="ref-nalisnick2018deep" ref-type="bibr">2019</xref>)).
  As a remedy, recently neural density estimators using surjective
  normalizing flows (SNFs) have been proposed which reduce the
  dimensionality of the data while still allowing for exact computation
  of data likelihoods
  (<xref alt="Klein et al., 2021" rid="ref-klein2021funnels" ref-type="bibr">Klein
  et al., 2021</xref>). While several computational libraries exist that
  implement <italic>bijective</italic> normalizing flows, i.e., flows
  that are dimensionality-preserving, currently none exist that
  efficiently implement dimensionality-reducing flows.</p>
  <p><monospace>Surjectors</monospace> is a normalizing flow library
  that implements both bijective and surjective normalizing flows.
  <monospace>Surjectors</monospace> is light-weight, conceptually simple
  to understand if familiar with the JAX ecosystem, and computationally
  efficient due to leveraging the XLA compilation and vectorization from
  JAX. We additionally make use of several well-established packages
  within the JAX ecosystem
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>) and probabilistic deep learning community. For
  composing the conditioning networks that NFs facilitate,
  <monospace>Surjectors</monospace> uses the deep learning library Haiku
  (<xref alt="Hennigan et al., 2020" rid="ref-haiku2020github" ref-type="bibr">Hennigan
  et al., 2020</xref>). For training and optimization, we utilize the
  gradient transformation library Optax
  (<xref alt="Babuschkin et al., 2020" rid="ref-deepmind2020jax" ref-type="bibr">Babuschkin
  et al., 2020</xref>). <monospace>Surjectors</monospace> leverages
  Distrax
  (<xref alt="Babuschkin et al., 2020" rid="ref-deepmind2020jax" ref-type="bibr">Babuschkin
  et al., 2020</xref>) and TensorFlow probability
  (<xref alt="Dillon et al., 2017" rid="ref-dillon2017tensorflow" ref-type="bibr">Dillon
  et al., 2017</xref>) for probability distributions and several base
  bijector implementations.</p>
</sec>
<sec id="adoption">
  <title>Adoption</title>
  <p>Dirmeier, Albert, et al.
  (<xref alt="2023" rid="ref-dirmeier2023simulation" ref-type="bibr">2023</xref>)
  have proposed a novel method for simulation-based inference where they
  make use autoregressive inference surjections for density estimation
  and where they are using <monospace>Surjectors</monospace> for their
  implementations. Dirmeier, Hong, et al.
  (<xref alt="2023" rid="ref-dirmeier2023uncertainty" ref-type="bibr">2023</xref>)
  used <monospace>Surjectors</monospace> for uncertainty quantification
  and out-of-distribution detection in deep neural network models.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-papamakarios2021normalizing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <source>The Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>1</issue>
      <fpage>2617</fpage>
      <lpage>2680</lpage>
    </element-citation>
  </ref>
  <ref id="ref-papamakarios2019sequential">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Sterratt</surname><given-names>David</given-names></name>
        <name><surname>Murray</surname><given-names>Iain</given-names></name>
      </person-group>
      <article-title>Sequential neural likelihood: Fast likelihood-free inference with autoregressive flows</article-title>
      <source>Proceedings of the 22nd international conference on artificial intelligence and statistics</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-dirmeier2023simulation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dirmeier</surname><given-names>Simon</given-names></name>
        <name><surname>Albert</surname><given-names>Carlo</given-names></name>
        <name><surname>Perez-Cruz</surname><given-names>Fernando</given-names></name>
      </person-group>
      <article-title>Simulation-based inference using surjective sequential neural likelihood estimation</article-title>
      <source>arXiv preprint arXiv:2308.01054</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-dirmeier2023uncertainty">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dirmeier</surname><given-names>Simon</given-names></name>
        <name><surname>Hong</surname><given-names>Ye</given-names></name>
        <name><surname>Xin</surname><given-names>Yanan</given-names></name>
        <name><surname>Perez-Cruz</surname><given-names>Fernando</given-names></name>
      </person-group>
      <article-title>Uncertainty quantification and out-of-distribution detection using surjective normalizing flows</article-title>
      <source>arXiv preprint arXiv:2311.00377</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-hoffman2019neutra">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matthew</given-names></name>
        <name><surname>Sountsov</surname><given-names>Pavel</given-names></name>
        <name><surname>Dillon</surname><given-names>Joshua V</given-names></name>
        <name><surname>Langmore</surname><given-names>Ian</given-names></name>
        <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Srinivas</given-names></name>
      </person-group>
      <article-title>Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport</article-title>
      <source>arXiv preprint arXiv:1903.03704</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-kingma2018glow">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Durk P</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
      </person-group>
      <article-title>Glow: Generative flow with invertible 1x1 convolutions</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-ping20wave">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ping</surname><given-names>Wei</given-names></name>
        <name><surname>Peng</surname><given-names>Kainan</given-names></name>
        <name><surname>Zhao</surname><given-names>Kexin</given-names></name>
        <name><surname>Song</surname><given-names>Zhao</given-names></name>
      </person-group>
      <article-title>WaveFlow: A compact flow-based model for raw audio</article-title>
      <source>Proceedings of the 37th international conference on machine learning</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-rezende15flow">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rezende</surname><given-names>Danilo</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
      </person-group>
      <article-title>Variational inference with normalizing flows</article-title>
      <source>Proceedings of the 32nd international conference on machine learning</source>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-klein2021funnels">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Klein</surname><given-names>Samuel</given-names></name>
        <name><surname>Raine</surname><given-names>John A.</given-names></name>
        <name><surname>Pina-Otey</surname><given-names>Sebastian</given-names></name>
        <name><surname>Voloshynovskiy</surname><given-names>Slava</given-names></name>
        <name><surname>Golling</surname><given-names>Tobias</given-names></name>
      </person-group>
      <article-title>Funnels: Exact maximum likelihood with dimensionality reduction</article-title>
      <source>Workshop on bayesian deep learning, advances in neural information processing systems</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-jax2018github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-deepmind2020jax">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Babuschkin</surname><given-names>Igor</given-names></name>
        <name><surname>Baumli</surname><given-names>Kate</given-names></name>
        <name><surname>Bell</surname><given-names>Alison</given-names></name>
        <name><surname>Bhupatiraju</surname><given-names>Surya</given-names></name>
        <name><surname>Bruce</surname><given-names>Jake</given-names></name>
        <name><surname>Buchlovsky</surname><given-names>Peter</given-names></name>
        <name><surname>Budden</surname><given-names>David</given-names></name>
        <name><surname>Cai</surname><given-names>Trevor</given-names></name>
        <name><surname>Clark</surname><given-names>Aidan</given-names></name>
        <name><surname>Danihelka</surname><given-names>Ivo</given-names></name>
        <name><surname>Dedieu</surname><given-names>Antoine</given-names></name>
        <name><surname>Fantacci</surname><given-names>Claudio</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>The DeepMind JAX Ecosystem</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>http://github.com/deepmind</uri>
    </element-citation>
  </ref>
  <ref id="ref-fefferman2016testing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fefferman</surname><given-names>Charles</given-names></name>
        <name><surname>Mitter</surname><given-names>Sanjoy</given-names></name>
        <name><surname>Narayanan</surname><given-names>Hariharan</given-names></name>
      </person-group>
      <article-title>Testing the manifold hypothesis</article-title>
      <source>Journal of the American Mathematical Society</source>
      <year iso-8601-date="2016">2016</year>
      <volume>29</volume>
      <issue>4</issue>
      <fpage>983</fpage>
      <lpage>1049</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dai2020sliced">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dai</surname><given-names>Biwei</given-names></name>
        <name><surname>Seljak</surname><given-names>Uros</given-names></name>
      </person-group>
      <article-title>Sliced iterative normalizing flows</article-title>
      <source>ICML workshop on invertible neural networks, normalizing flows, and explicit likelihood models</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-nalisnick2018deep">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Matsukawa</surname><given-names>Akihiro</given-names></name>
        <name><surname>Teh</surname><given-names>Yee Whye</given-names></name>
        <name><surname>Gorur</surname><given-names>Dilan</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Do deep generative models know what they don’t know?</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-haiku2020github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Hennigan</surname><given-names>Tom</given-names></name>
        <name><surname>Cai</surname><given-names>Trevor</given-names></name>
        <name><surname>Norman</surname><given-names>Tamara</given-names></name>
        <name><surname>Martens</surname><given-names>Lena</given-names></name>
        <name><surname>Babuschkin</surname><given-names>Igor</given-names></name>
      </person-group>
      <article-title>Haiku: Sonnet for JAX</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>http://github.com/deepmind/dm-haiku</uri>
    </element-citation>
  </ref>
  <ref id="ref-dillon2017tensorflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dillon</surname><given-names>Joshua V</given-names></name>
        <name><surname>Langmore</surname><given-names>Ian</given-names></name>
        <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Srinivas</given-names></name>
        <name><surname>Moore</surname><given-names>Dave</given-names></name>
        <name><surname>Patton</surname><given-names>Brian</given-names></name>
        <name><surname>Alemi</surname><given-names>Alex</given-names></name>
        <name><surname>Hoffman</surname><given-names>Matt</given-names></name>
        <name><surname>Saurous</surname><given-names>Rif A</given-names></name>
      </person-group>
      <article-title>Tensorflow distributions</article-title>
      <source>arXiv preprint arXiv:1711.10604</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
