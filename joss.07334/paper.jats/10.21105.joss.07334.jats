<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7334</article-id>
<article-id pub-id-type="doi">10.21105/joss.07334</article-id>
<title-group>
<article-title>DiscreteEntropy.jl: Entropy Estimation of Discrete Random
Variables with Julia</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5368-6769</contrib-id>
<name>
<surname>Kelly</surname>
<given-names>David A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-2733-5283</contrib-id>
<name>
<surname>Torre</surname>
<given-names>Ilaria Pia La</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>King’s College London, UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University College London, UK</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<volume>9</volume>
<issue>103</issue>
<fpage>7334</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>julia</kwd>
<kwd>discrete entropy estimation</kwd>
<kwd>information theory</kwd>
<kwd>mutual information</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>DiscreteEntropy.jl</monospace> is a Julia package to
  facilitate entropy estimation of discrete random variables. The
  entropy of a random variable, <monospace>X</monospace>, is the average
  amount of surprise associated with the different outcomes of that
  variable. When <monospace>X</monospace> is known completely,
  calculating the entropy is easy. It is given by</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[H(X) = - \sum_{x \in X} p(x) \log (p(x))]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>−</mml:mi><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>log</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>However, it is a very hard problem when knowledge of the
  distribution is incomplete. It is well known that the
  <monospace>MaximumLikelihood</monospace> (or Plugin) estimator,
  underestimates the true entropy on average
  (<xref alt="Basharin, 1959" rid="ref-basharin" ref-type="bibr">Basharin,
  1959</xref>). This difficulty has led to a large number of improved
  estimators.
  (<xref alt="Contreras Rodrı́guez et al., 2021" rid="ref-Rodriguez2021EntropyEst" ref-type="bibr">Contreras
  Rodrı́guez et al., 2021</xref>), for example, evaluate 18 different
  estimators, among which are <italic>Grassberger</italic>
  (<xref alt="Grassberger, 2008" rid="ref-grassberger2008entropy" ref-type="bibr">Grassberger,
  2008</xref>), <italic>Chao Shen</italic>
  (<xref alt="Chao &amp; Shen, 2003" rid="ref-chaoshen" ref-type="bibr">Chao
  &amp; Shen, 2003</xref>) , <italic>NSB</italic>
  (<xref alt="Nemenman et al., 2001" rid="ref-nemenman2002entropy" ref-type="bibr">Nemenman
  et al., 2001</xref>), <italic>Zhang</italic>
  (<xref alt="Zhang, 2012" rid="ref-zhang" ref-type="bibr">Zhang,
  2012</xref>) and <italic>James-Stein</italic>
  (<xref alt="Hausser &amp; Strimmer, 2009" rid="ref-hausser2009entropy" ref-type="bibr">Hausser
  &amp; Strimmer, 2009</xref>). These estimators were scattered across 3
  different programming languages and 7 different libraries. Some of
  these estimators are hard to find or poorly maintained. Each
  implementation calculates and reports entropy to a different number of
  significant digits, which can lead to difficulties in comparison.</p>
  <p>If one can estimate entropy more accurately, then one can also
  estimate mutual information more accurately. There are numerous,
  cross-domain, applications for entropy and mutual information, such as
  in telecommunications, machine learning
  (<xref alt="MacKay, 2003" rid="ref-MacKay2003" ref-type="bibr">MacKay,
  2003</xref>) and software engineering
  (<xref alt="Blackwell et al., 2023" rid="ref-blackwell2023hyperfuzzing" ref-type="bibr">Blackwell
  et al., 2023</xref>;
  <xref alt="Böhme et al., 2020" rid="ref-bohmeU003AfseU003A2020" ref-type="bibr">Böhme
  et al., 2020</xref>). <monospace>DiscreteEntropy.jl</monospace> makes
  it easy to apply different estimators to the problem of mutual
  information, cross entropy and Kullback–Leibler
  divergence(<xref alt="Cover &amp; Thomas, 2006" rid="ref-Cover2006" ref-type="bibr">Cover
  &amp; Thomas, 2006</xref>), amongst other measures.</p>
  <p><monospace>DiscreteEntropy.jl</monospace> provides a comprehensive
  collection of popular entropy estimators and utilities for working
  with other Shannon measures. <monospace>DiscreteEntropy.jl</monospace>
  implements a variety of different entropy estimators, which were
  previously scattered over different languages and libraries. Some of
  these scattered implementations are slow, hard to find, or difficult
  to compile. <monospace>DiscreteEntropy.jl</monospace> removes all of
  these problems. <monospace>DiscreteEntropy.jl</monospace> also
  provides functions for estimating cross entropy, KL divergence, mutual
  information and many other Shannon measures.
  <monospace>DiscreteEntropy.jl</monospace> is intended to be easy to
  use, with a flexible but type safe interface.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The <monospace>DiscreteEntropy.jl</monospace> package has native
  Julia implementations of all of the estimators explored in
  (<xref alt="Contreras Rodrı́guez et al., 2021" rid="ref-Rodriguez2021EntropyEst" ref-type="bibr">Contreras
  Rodrı́guez et al., 2021</xref>) and a number of estimators which were
  not considered. <monospace>DiscreteEntropy.jl</monospace> provides a
  unified and consistent interface for those who wish to estimate
  entropy and other Shannon measures for their research, or those who
  want to research entropy estimation directly.</p>
  <p>There is no other open-source software package known to us, in any
  language, with similar features or similar breadth of estimators. The
  R
  entropy(<xref alt="Hausser &amp; Strimmer, 2009" rid="ref-hausser2009entropy" ref-type="bibr">Hausser
  &amp; Strimmer, 2009</xref>) package covers many basic estimators,
  such as the maximum likelihood, Miller-Madow, Chao Shen and many
  bayesian estimators. Code for
  PYM(<xref alt="Pillowlab, 2020" rid="ref-pym" ref-type="bibr">Pillowlab,
  2020</xref>),
  BUB(<xref alt="Paninski" rid="ref-bub" ref-type="bibr">Paninski</xref>)
  and
  Unseen(<xref alt="Valiant &amp; Valiant" rid="ref-unseenimp" ref-type="bibr">Valiant
  &amp; Valiant</xref>) estimators are found only in the Matlab
  implementations by the authors of the original papers. Other
  estimators, such as Zhang and Grassberger, can be found in the R
  Entropart(<xref alt="Marcon &amp; Hérault, 2015" rid="ref-entropart" ref-type="bibr">Marcon
  &amp; Hérault, 2015</xref>) library. Code for the NSB estimator exists
  in multiple different versions, in
  C++(<xref alt="Nemenman" rid="ref-nsb" ref-type="bibr">Nemenman</xref>),
  Matlab(<xref alt="Nemenman" rid="ref-nsb" ref-type="bibr">Nemenman</xref>)
  and
  Python(<xref alt="Marsili, 2021" rid="ref-ndd" ref-type="bibr">Marsili,
  2021</xref>). The estimators in
  <monospace>DiscreteEntropy.jl</monospace> were mostly implemented from
  the original papers, though some (such as BUB and Unseen) are
  idiomatic ports of original Matlab code.</p>
  <p><monospace>DiscreteEntropy.jl</monospace> is a fast, simple to use,
  library that fills a gap between the scattered implementations
  available online. It ensures type safety throughout, even preventing
  confusion between vectors of samples or vectors which represent
  histograms of samples.</p>
</sec>
<sec id="example">
  <title>Example</title>
  <p><monospace>DiscreteEntropy.jl</monospace> allows the user to call
  each estimator directly, or to use a helper function
  <monospace>estimate_h</monospace>. The
  <monospace>estimate_h</monospace> function is the easiest entry to the
  library. This function takes a <monospace>CountData</monospace>
  object, which can be constructed from a vector using either
  <monospace>from_data</monospace>, <monospace>from_counts</monospace>
  or <monospace>from_samples</monospace>. Both
  <monospace>from_data</monospace> and <monospace>estimate_h</monospace>
  are parameterised by types, making them both typesafe and allowing for
  simple autocompletion. All results are in <monospace>nats</monospace>,
  but <monospace>DiscreteEntropy.jl</monospace> provides helper
  functions to convert between units.</p>
  <code language="julia">data = [1,2,3,4,5,4,3,2,1]
count_data = from_data(data, Histogram)

estimate_h(count_data, MaximumLikelihood)
2.078803548653078</code>
  <p>Unsurprisingly, different estimators give different results,
  depending on their underlying assumptions:</p>
  <code language="julia">estimate_h(count_data, ChaoShen)
2.2526294444274044</code>
  <p>These assumptions can have a profound effect on estimations of more
  complex measures, such as mutual information:</p>
  <code language="julia">to_bits(mutual_information(Matrix([1 0; 0 1]), MaximumLikelihood))
1.0

to_bits(mutual_information(Matrix([1 0; 0 1]), ChaoWangJost))
1.7548875021634693</code>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-basharin">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Basharin</surname><given-names>G. P.</given-names></name>
      </person-group>
      <article-title>On a statistical estimate for the entropy of a sequence of independent random variables</article-title>
      <source>Theory of Probability &amp; Its Applications</source>
      <year iso-8601-date="1959">1959</year>
      <volume>4</volume>
      <issue>3</issue>
      <uri>https://doi.org/10.1137/1104033</uri>
      <pub-id pub-id-type="doi">10.1137/1104033</pub-id>
      <fpage>333</fpage>
      <lpage>336</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bohmeU003AfseU003A2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Böhme</surname><given-names>Marcel</given-names></name>
        <name><surname>Manès</surname><given-names>Valentin</given-names></name>
        <name><surname>Cha</surname><given-names>Sang Kil</given-names></name>
      </person-group>
      <article-title>Boosting fuzzer efficiency: An information theoretic perspective</article-title>
      <source>Proceedings of the 14th joint meeting of the european software engineering conference and the ACM SIGSOFT symposium on the foundations of software engineering</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1145/3368089.3409748</pub-id>
      <fpage>970</fpage>
      <lpage>981</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blackwell2023hyperfuzzing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Blackwell</surname><given-names>Daniel</given-names></name>
        <name><surname>Becker</surname><given-names>Ingolf</given-names></name>
        <name><surname>Clark</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Hyperfuzzing: Black-box security hypertesting with a grey-box fuzzer</article-title>
      <source>arXiv preprint arXiv:2308.09081</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-MacKay2003">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>MacKay</surname><given-names>David J. C.</given-names></name>
      </person-group>
      <source>Information theory, inference, and learning algorithms</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <pub-id pub-id-type="doi">10.1109/tit.2004.834752</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Rodriguez2021EntropyEst">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Contreras Rodrı́guez</surname><given-names>Lianet</given-names></name>
        <name><surname>Madarro-Capó</surname><given-names>Evaristo José</given-names></name>
        <name><surname>Legón-Pérez</surname><given-names>Carlos Miguel</given-names></name>
        <name><surname>Rojas</surname><given-names>Omar</given-names></name>
        <name><surname>Sosa-Gómez</surname><given-names>Guillermo</given-names></name>
      </person-group>
      <article-title>Selecting an effective entropy estimator for short sequences of bits and bytes with maximum entropy</article-title>
      <source>Entropy</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>23</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.3390/e23050561</pub-id>
      <fpage>561</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-grassberger2008entropy">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Grassberger</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Entropy estimates from insufficient samplings</article-title>
      <year iso-8601-date="2008">2008</year>
      <uri>https://arxiv.org/abs/physics/0307138</uri>
    </element-citation>
  </ref>
  <ref id="ref-chaoshen">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chao</surname><given-names>Anne</given-names></name>
        <name><surname>Shen</surname><given-names>Tsung-Jen</given-names></name>
      </person-group>
      <article-title>Nonparametric estimation of Shannon’s diversity index when there are unseen species in sample. Environ ecol stat 10: 429-443</article-title>
      <source>Environmental and Ecological Statistics</source>
      <year iso-8601-date="2003-12">2003</year><month>12</month>
      <volume>10</volume>
      <pub-id pub-id-type="doi">10.1023/A:1026096204727</pub-id>
      <fpage>429</fpage>
      <lpage>443</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nemenman2002entropy">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nemenman</surname><given-names>Ilya</given-names></name>
        <name><surname>Shafee</surname><given-names>Fariel</given-names></name>
        <name><surname>Bialek</surname><given-names>William</given-names></name>
      </person-group>
      <article-title>Entropy and inference, revisited</article-title>
      <source>Proceedings of the 14th international conference on neural information processing systems: Natural and synthetic</source>
      <publisher-name>MIT Press</publisher-name>
      <publisher-loc>Cambridge, MA, USA</publisher-loc>
      <year iso-8601-date="2001">2001</year>
      <pub-id pub-id-type="doi">10.7551/mitpress/1120.003.0065</pub-id>
      <fpage>471</fpage>
      <lpage>478</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhang">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Zhiyi</given-names></name>
      </person-group>
      <article-title>Entropy Estimation in Turing’s Perspective</article-title>
      <source>Neural Computation</source>
      <year iso-8601-date="2012-05">2012</year><month>05</month>
      <volume>24</volume>
      <issue>5</issue>
      <issn>0899-7667</issn>
      <uri>https://doi.org/10.1162/NECO\_a\_00266</uri>
      <pub-id pub-id-type="doi">10.1162/NECO_a_00266</pub-id>
      <fpage>1368</fpage>
      <lpage>1389</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hausser2009entropy">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hausser</surname><given-names>Jean</given-names></name>
        <name><surname>Strimmer</surname><given-names>Korbinian</given-names></name>
      </person-group>
      <article-title>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</article-title>
      <year iso-8601-date="2009">2009</year>
      <uri>https://arxiv.org/abs/0811.3579</uri>
    </element-citation>
  </ref>
  <ref id="ref-Cover2006">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Cover</surname><given-names>Thomas M.</given-names></name>
        <name><surname>Thomas</surname><given-names>Joy A.</given-names></name>
      </person-group>
      <source>Elements of information theory 2nd edition (wiley series in telecommunications and signal processing)</source>
      <publisher-name>Hardcover; Wiley-Interscience</publisher-name>
      <year iso-8601-date="2006">2006</year>
      <isbn>0471241954</isbn>
    </element-citation>
  </ref>
  <ref id="ref-hausser2009entropy">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hausser</surname><given-names>Jean</given-names></name>
        <name><surname>Strimmer</surname><given-names>Korbinian</given-names></name>
      </person-group>
      <article-title>Entropy inference and the James-Stein estimator, with application to nonlinear gene association networks</article-title>
      <year iso-8601-date="2009">2009</year>
      <uri>https://arxiv.org/abs/0811.3579</uri>
    </element-citation>
  </ref>
  <ref id="ref-pym">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Pillowlab</surname></name>
      </person-group>
      <article-title>PYM entropy estimator MATLAB reference implementation</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/pillowlab/PYMentropy/</uri>
    </element-citation>
  </ref>
  <ref id="ref-bub">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Paninski</surname><given-names>Liam</given-names></name>
      </person-group>
      <article-title>BUB</article-title>
      <publisher-name>http://www.stat.columbia.edu/~liam/research/code/BUBfunc.m [Accessed 2024-10-22]</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-nsb">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Nemenman</surname><given-names>Ilya</given-names></name>
      </person-group>
      <article-title>NSB Entropy Estimation</article-title>
      <publisher-name>https://sourceforge.net/projects/nsb-entropy/ [Accessed 2024-10-24]</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-unseenimp">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Valiant</surname><given-names>Paul</given-names></name>
        <name><surname>Valiant</surname><given-names>Gregory</given-names></name>
      </person-group>
      <article-title>Unseen</article-title>
      <publisher-name>https://theory.stanford.edu/~valiant/code.html [Accessed 2024-10-24]</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-ndd">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Marsili</surname><given-names>Simone</given-names></name>
      </person-group>
      <article-title>ndd - bayesian entropy estimation from discrete data</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/simomarsili/ndd</uri>
    </element-citation>
  </ref>
  <ref id="ref-entropart">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Marcon</surname><given-names>Eric</given-names></name>
        <name><surname>Hérault</surname><given-names>Bruno</given-names></name>
      </person-group>
      <article-title>entropart: An R package to measure and partition diversity</article-title>
      <source>Journal of Statistical Software</source>
      <year iso-8601-date="2015">2015</year>
      <volume>67</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.18637/jss.v067.i08</pub-id>
      <fpage>1</fpage>
      <lpage>26</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
