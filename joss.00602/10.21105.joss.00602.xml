<?xml version="1.0" encoding="utf-8" ?>
<article>
  <articleinfo>
    <title>Flux: Elegant machine learning with Julia</title>
    <authors>
      <author>
        <name>Mike Innes</name>
        <orcid>0000-0003-0788-0242</orcid>
        <affiliation>
          <orgname>
            1
          </orgname>
        </affiliation>
      </author>
    </authors>
    <tags>
      <tag>deep learning</tag>
      <tag>machine learning</tag>
      <tag>natural language processing</tag>
      <tag>computer vision</tag>
      <tag>reinforcement learning</tag>
      <tag>robotics</tag>
      <tag>automatic differentiation</tag>
      <tag>compiler</tag>
    </tags>
    <date>16 February 2018</date>
    <paper_doi>10.21105/joss.00602</paper_doi>
    <software_repository>https://github.com/FluxML/Flux.jl</software_repository>
    <software_archive>http://dx.doi.org/10.5281/zenodo.1240350</software_archive>
    <paper_url>http://www.theoj.org/joss-papers/joss.00602/10.21105.joss.00602.pdf</paper_url>
  </articleinfo>
  <body>
    <h1 id="summary">Summary</h1>
    <p>Flux is library for machine learning (ML), written using the numerical computing language Julia <span class="citation" data-cites="Julia">(Bezanson et al. 2017)</span>. The package allows models to be written using Julia’s simple mathematical syntax, and applies automatic differentiation (AD) to seamlessly calculate derivatives and train the model. Meanwhile, it makes heavy use of Julia’s language and compiler features to carry out code analysis and make optimisations. For example, Julia’s GPU compilation support <span class="citation" data-cites="besard:2017">(Besard, Foket, and De Sutter 2017)</span> can be used to JIT-compile custom GPU kernels for model layers <span class="citation" data-cites="CuArrays">(Innes and others 2017a)</span>.</p>
    <p>The machine learning community has traditionally been divided between “static” and “dynamic” frameworks that are easy to optimise and easy to use, respectively <span class="citation" data-cites="MLPL">(Innes and others 2017b)</span>. Flux blurs the line between these two approaches, combining a highly intuitive programming model with the compiler techniques needed by ML. This enables research into advanced compiler transforms such as batching <span class="citation" data-cites="Minibatch">(Bradbury 2018)</span> without changing any user code.</p>
    <p>Flux has been used heavily for natural language processing, but can also support state-of-the-art research models in areas like computer vision, reinforcement learning and robotics. Many examples of such models can be found in the model zoo <span class="citation" data-cites="Zoo">(Innes and others 2018)</span>.</p>
    <h1 id="references" class="unnumbered">References</h1>
    <div id="refs" class="references">
    <div id="ref-besard:2017">
    <p>Besard, Tim, Christophe Foket, and Bjorn De Sutter. 2017. “Effective Extensible Programming: Unleashing Julia on GPUs.” <em>arXiv</em> abs/11712.03112. <a href="http://arxiv.org/abs/1712.03112" class="uri">http://arxiv.org/abs/1712.03112</a>.</p>
    </div>
    <div id="ref-Julia">
    <p>Bezanson, Jeff, Alan Edelman, Stefan Karpinski, and Viral B. Shah. 2017. “Julia: A Fresh Approach to Numerical Computing.” <em>SIAM Review</em>. <a href="julialang.org/publications/julia-fresh-approach-BEKS.pdf" class="uri">julialang.org/publications/julia-fresh-approach-BEKS.pdf</a>. <a href="https://doi.org/10.1137/141000671" class="uri">https://doi.org/10.1137/141000671</a>.</p>
    </div>
    <div id="ref-Minibatch">
    <p>Bradbury, James. 2018. “Minibatch.jl.” 2018. <a href="https://github.com/jekbradbury/Minibatch.jl" class="uri">https://github.com/jekbradbury/Minibatch.jl</a>.</p>
    </div>
    <div id="ref-CuArrays">
    <p>Innes, Mike, and others. 2017a. “Generic Gpu Kernels.” 2017. <a href="http://mikeinnes.github.io/2017/08/24/cudanative.html" class="uri">http://mikeinnes.github.io/2017/08/24/cudanative.html</a>.</p>
    </div>
    <div id="ref-MLPL">
    <p>———. 2017b. “On Machine Learning and Programming Languages.” 2017. <a href="https://julialang.org/blog/2017/12/ml&amp;pl" class="uri">https://julialang.org/blog/2017/12/ml&amp;pl</a>.</p>
    </div>
    <div id="ref-Zoo">
    <p>———. 2018. “Flux Model Zoo.” 2018. <a href="https://github.com/FluxML/model-zoo/" class="uri">https://github.com/FluxML/model-zoo/</a>.</p>
    </div>
    </div>
  </body>
</article>
