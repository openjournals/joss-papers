<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7286</article-id>
<article-id pub-id-type="doi">10.21105/joss.07286</article-id>
<title-group>
<article-title>xesn: Echo state networks powered by Xarray and
Dask</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4463-6126</contrib-id>
<name>
<surname>Smith</surname>
<given-names>Timothy A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5223-8307</contrib-id>
<name>
<surname>Penny</surname>
<given-names>Stephen G.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6579-6546</contrib-id>
<name>
<surname>Platt</surname>
<given-names>Jason A.</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6300-5659</contrib-id>
<name>
<surname>Chen</surname>
<given-names>Tse-Chun</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Physical Sciences Laboratory (PSL), National Oceanic and
Atmospheric Administration (NOAA), Boulder, CO, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Sofar Ocean, San Francisco, CA, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Cooperative Institute for Research in Environmental
Sciences (CIRES) at the University of Colorado Boulder, Boulder, CO,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>University of California San Diego (UCSD), La Jolla, CA,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Pacific Northwest National Laboratory, Richland, WA,
USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-11-01">
<day>1</day>
<month>11</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>103</issue>
<fpage>7286</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2024</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>echo state networks</kwd>
<kwd>xarray</kwd>
<kwd>dask</kwd>
<kwd>numpy</kwd>
<kwd>cupy</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Xesn</monospace> is a Python package that allows
  scientists to easily design Echo State Networks (ESNs) for forecasting
  problems. ESNs are a Recurrent Neural Network architecture introduced
  by Jaeger
  (<xref alt="2001" rid="ref-jaeger_echo_2001" ref-type="bibr">2001</xref>)
  that are part of a class of techniques termed Reservoir Computing. One
  defining characteristic of these techniques is that all internal
  weights are determined by a handful of global, scalar parameters,
  thereby avoiding problems during backpropagation and reducing training
  time significantly. Because this architecture is conceptually simple,
  many scientists implement ESNs from scratch, leading to questions
  about computational performance. <monospace>Xesn</monospace> offers a
  straightforward, standard implementation of ESNs that operates
  efficiently on CPU and GPU hardware. The package leverages
  optimization tools to automate the parameter selection process, so
  that scientists can reduce the time finding a good architecture and
  focus on using ESNs for their domain application. Importantly, the
  package flexibly handles forecasting tasks for out-of-core,
  multi-dimensional datasets, eliminating the need to write parallel
  programming code. <monospace>Xesn</monospace> was initially developed
  to handle the problem of forecasting weather dynamics, and so it
  integrates naturally with Python packages that have become familiar to
  weather and climate scientists such as <monospace>Xarray</monospace>
  (<xref alt="Hoyer &amp; Hamman, 2017" rid="ref-hoyer_xarray_2017" ref-type="bibr">Hoyer
  &amp; Hamman, 2017</xref>). However, the software is ultimately
  general enough to be utilized in other domains where ESNs have been
  useful, such as in signal processing
  (<xref alt="Jaeger &amp; Haas, 2004" rid="ref-jaeger_harnessing_2004" ref-type="bibr">Jaeger
  &amp; Haas, 2004</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>ESNs are a conceptually simple Recurrent Neural Network
  architecture, leading many scientists who use ESNs to implement them
  from scratch. While this approach can work well for low dimensional
  problems, the situation quickly becomes more complicated when:</p>
  <list list-type="order">
    <list-item>
      <p>deploying the code on GPUs,</p>
    </list-item>
    <list-item>
      <p>interacting with a parameter optimization algorithm in order to
      tune the model, and</p>
    </list-item>
    <list-item>
      <p>parallelizing the architecture for higher dimensional
      applications.</p>
    </list-item>
  </list>
  <p><monospace>Xesn</monospace> is designed to address all of these
  points. Additionally, while there are some design flexibilities for
  the ESN architectures, the overall interface is streamlined based on
  the parameter and design impact study shown by Platt et al.
  (<xref alt="2022" rid="ref-platt_systematic_2022" ref-type="bibr">2022</xref>).</p>
  <sec id="gpu-deployment">
    <title>GPU Deployment</title>
    <p>At its core, <monospace>xesn</monospace> uses NumPy
    (<xref alt="Harris et al., 2020" rid="ref-harris_array_2020" ref-type="bibr">Harris
    et al., 2020</xref>) and SciPy
    (<xref alt="Virtanen et al., 2020" rid="ref-scipy_2020" ref-type="bibr">Virtanen
    et al., 2020</xref>) to perform array based operations on CPUs. The
    package then harnesses the CPU/GPU agnostic code capabilities
    afforded by CuPy
    (<xref alt="Okuta et al., 2017" rid="ref-cupy_learningsys2017" ref-type="bibr">Okuta
    et al., 2017</xref>) to operate on GPUs.</p>
  </sec>
  <sec id="parameter-optimization">
    <title>Parameter Optimization</title>
    <p>Although ESNs do not employ backpropagation to train internal
    weights, their behavior and performance is highly sensitive to a set
    of 5 scalar parameters. Moreover, the interaction of these
    parameters is often not straightforward, and it is therefore
    advantageous to optimize these parameter values
    (<xref alt="Platt et al., 2022" rid="ref-platt_systematic_2022" ref-type="bibr">Platt
    et al., 2022</xref>). Additionally, Platt et al.
    (<xref alt="2023" rid="ref-platt_constraining_2023" ref-type="bibr">2023</xref>)
    and Smith et al.
    (<xref alt="2023" rid="ref-smith_temporal_2023" ref-type="bibr">2023</xref>)
    showed that adding invariant metrics to the loss function, like the
    leading Lyapunov exponent or the Kinetic Energy spectrum, improved
    generalizability. As a generic implementation of these metrics,
    <monospace>xesn</monospace> offers the capability to constrain the
    system‚Äôs Power Spectral Density during parameter optimization in
    addition to a more traditional mean squared error loss function.</p>
    <p><monospace>Xesn</monospace> enables parameter optimization by
    integrating with the Surrogate Modeling Toolbox
    (<xref alt="Bouhlel et al., 2020" rid="ref-bouhlel_scalable_2020" ref-type="bibr">Bouhlel
    et al., 2020</xref>), which has a Bayesian optimization
    implementation. <monospace>Xesn</monospace> provides a simple
    interface so that the user can specify all of the settings for
    training, parameter optimization, and testing with a single YAML
    file. By doing so, all parts of the experiment are more easily
    reproducible and easier to manage with scheduling systems like SLURM
    on HPC environments or in the cloud.</p>
  </sec>
  <sec id="scaling-to-higher-dimensions">
    <title>Scaling to Higher Dimensions</title>
    <p>It is typical for ESNs to use a hidden layer that is
    <inline-formula><alternatives>
    <tex-math><![CDATA[\mathcal{O}(10-100)]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>10</mml:mn><mml:mo>‚àí</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
    times larger than the input and target space, so forecasting large
    target spaces quickly becomes intractable with a single reservoir.
    To address this, Pathak et al.
    (<xref alt="2018" rid="ref-pathak_model-free_2018" ref-type="bibr">2018</xref>)
    developed a parallelization strategy so that multiple,
    semi-independent reservoirs make predictions of a single, high
    dimensional system. This parallelization was generalized for
    multiple dimensions by Arcomano et al.
    (<xref alt="2020" rid="ref-arcomano_machine_2020" ref-type="bibr">2020</xref>)
    and Smith et al.
    (<xref alt="2023" rid="ref-smith_temporal_2023" ref-type="bibr">2023</xref>),
    the latter of which serves as the basis for
    <monospace>xesn</monospace>.</p>
    <p><monospace>Xesn</monospace> enables prediction for
    multi-dimensional systems by integrating its high level operations
    with <monospace>Xarray</monospace>
    (<xref alt="Hoyer &amp; Hamman, 2017" rid="ref-hoyer_xarray_2017" ref-type="bibr">Hoyer
    &amp; Hamman, 2017</xref>). As with <monospace>Xarray</monospace>,
    users refer to dimensions based on their named axes.
    <monospace>Xesn</monospace> parallelizes the core array based
    operations by using <monospace>Dask</monospace>
    (<xref alt="Dask Development Team, 2016" rid="ref-dask_2016" ref-type="bibr">Dask
    Development Team, 2016</xref>;
    <xref alt="Rocklin, 2015" rid="ref-rocklin_scipy_2015" ref-type="bibr">Rocklin,
    2015</xref>) to map them across available resources, from a laptop
    to a distributed HPC or cloud cluster.</p>
  </sec>
  <sec id="existing-reservoir-computing-software">
    <title>Existing Reservoir Computing Software</title>
    <p>It is important to note that there is already an existing
    software package in Python for Reservoir Computing, called
    <monospace>ReservoirPy</monospace>
    (<xref alt="Trouvain et al., 2020" rid="ref-Trouvain2020" ref-type="bibr">Trouvain
    et al., 2020</xref>). To our knowledge, the purpose of this package
    is distinctly different. The focus of
    <monospace>ReservoirPy</monospace> is to develop a highly generic
    framework for Reservoir Computing, for example, allowing one to
    change the network node type and graph structure underlying the
    reservoir, and allowing for delayed connections. On the other hand,
    <monospace>xesn</monospace> is focused specifically on implementing
    ESN architectures that can scale to multi-dimensional forecasting
    tasks. Additionally, while <monospace>ReservoirPy</monospace>
    enables hyperparameter grid search capabilities via Hyperopt
    (<xref alt="Bergstra et al., 2013" rid="ref-hyperopt" ref-type="bibr">Bergstra
    et al., 2013</xref>), <monospace>xesn</monospace> enables Bayesian
    optimization as noted above.</p>
    <p>Another ESN implementation is that of
    (<xref alt="Arcomano et al., 2020" rid="ref-arcomano_machine_2020" ref-type="bibr">Arcomano
    et al., 2020</xref>,
    <xref alt="2022" rid="ref-arcomano_hybrid_2022" ref-type="bibr">2022</xref>,
    <xref alt="2023" rid="ref-arcomano_hybrid_2023" ref-type="bibr">2023</xref>),
    available at
    (<xref alt="Arcomano, 2023" rid="ref-arcomano_code" ref-type="bibr">Arcomano,
    2023</xref>). The code implements ESNs in Fortran, and focuses on
    using ESNs for hybrid physics-ML modeling.</p>
  </sec>
</sec>
<sec id="computational-performance">
  <title>Computational Performance</title>
  <p>Here we show brief scaling results in order to show how the
  standard (eager)
  <ext-link ext-link-type="uri" xlink:href="https://xesn.readthedocs.io/en/latest/generated/xesn.ESN.html#xesn.ESN"><monospace>xesn.ESN</monospace></ext-link>
  scales with increasing hidden and input dimensions. Additionally, we
  provide some baseline results to serve as guidance when configuring
  <monospace>Dask</monospace> to use the parallelized
  <ext-link ext-link-type="uri" xlink:href="https://xesn.readthedocs.io/en/latest/generated/xesn.LazyESN.html"><monospace>xesn.LazyESN</monospace></ext-link>
  architecture. The scripts used to setup, execute, and visualize these
  scaling tests can be found
  <ext-link ext-link-type="uri" xlink:href="https://github.com/timothyas/xesn/tree/1524713149efa38a0fd52ecdeb32ca5aacb62693/scaling">here</ext-link>.
  For methodological details on these two architectures, please refer to
  <ext-link ext-link-type="uri" xlink:href="https://xesn.readthedocs.io/en/latest/methods.html">the
  methods section of the documentation</ext-link>.</p>
  <sec id="standard-eager-esn-performance">
    <title>Standard (Eager) ESN Performance</title>
    <fig>
      <caption><p>Wall time and peak memory usage for the standard ESN
      architecture for two different system sizes
      (<inline-formula><alternatives>
      <tex-math><![CDATA[N_u]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></alternatives></inline-formula>)
      and a variety of reservoir sizes (<inline-formula><alternatives>
      <tex-math><![CDATA[N_r]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></alternatives></inline-formula>).
      Wall time is captured with Python‚Äôs <monospace>time</monospace>
      module, and peak memory usage is captured with
      <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/memory-profiler/">memory-profiler</ext-link>
      for the CPU runs and with
      <ext-link ext-link-type="uri" xlink:href="https://developer.nvidia.com/nsight-systems">NVIDIA
      Nsight Systems</ext-link> for the GPU runs. Note that the peak
      memory usage for the GPU runs indicates GPU memory usage only,
      since this is a typical bottleneck. The gray and black lines
      indicate the general trend in memory usage during the CPU and GPU
      simulations, respectively. The empirically derived gray and black
      curves are a function of the problem size, and are provided so
      users can estimate how much memory might be required for their
      applications. The constants are as follows:
      <inline-formula><alternatives>
      <tex-math><![CDATA[a=250,000]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>250</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      is ~3 times the total number of samples used,
      <inline-formula><alternatives>
      <tex-math><![CDATA[b=20,000]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      is the batch size, and <inline-formula><alternatives>
      <tex-math><![CDATA[c=8\cdot10^9]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
      is a conversion to GB.
      <styled-content id="figU003Aeager"></styled-content></p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="eager-scaling.pdf" />
    </fig>
    <p>For reference, in
    <xref alt="[fig:eager]" rid="figU003Aeager">[fig:eager]</xref> we
    show the wall time and peak memory usage required to train the
    standard (eager) <monospace>ESN</monospace> architecture as a
    function of the input dimension <inline-formula><alternatives>
    <tex-math><![CDATA[N_u]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>u</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    and reservoir size <inline-formula><alternatives>
    <tex-math><![CDATA[N_r]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.
    We ran the scaling tests in the <monospace>us-central-1c</monospace>
    zone on Google Cloud Platform (GCP), using a single
    <monospace>c2-standard-60</monospace> instance to test the CPU
    (NumPy) implementation and a single
    <monospace>a2-highgpu-8g</monospace> (i.e., with 8 A100 cards)
    instance to test the GPU (CuPy) implementation. The training data
    was generated from the Lorenz96 model
    (<xref alt="Lorenz, 1996" rid="ref-lorenz_predictability_1996" ref-type="bibr">Lorenz,
    1996</xref>) with dimensions <inline-formula><alternatives>
    <tex-math><![CDATA[N_u=\{16,256\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>256</mml:mn><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
    and we generated 80,000 total samples in the training dataset.</p>
    <p>In the CPU tests, wall time scales quadratically with the
    reservoir size, while it is mostly constant on a GPU. For this
    problem, it becomes advantageous to use GPUs once the reservoir size
    is approximately <inline-formula><alternatives>
    <tex-math><![CDATA[N_r=8,000]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    or greater. In both the CPU and GPU tests, memory scales
    quadratically with reservoir size, although the increasing memory
    usage with reservoir size is more dramatic on the CPU than GPU. This
    result serves as a motivation for our parallelized architecture.</p>
  </sec>
  <sec id="parallel-lazy-architecture-strong-scaling-results">
    <title>Parallel (Lazy) Architecture Strong Scaling Results</title>
    <p>In order to evaluate the performance of the parallelized
    architecture, we take the Lorenz96 system with dimension
    <inline-formula><alternatives>
    <tex-math><![CDATA[N_u=256]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    and subdivide the domain into <inline-formula><alternatives>
    <tex-math><![CDATA[N_g = \{2, 4, 8, 16, 32\}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>32</mml:mn><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
    groups. We then fix the problem size such that
    <inline-formula><alternatives>
    <tex-math><![CDATA[N_r*N_g = 16,000]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>*</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
    so that the timing results reflect strong scaling. That is, the
    results show how the code performs with increasing resources on a
    fixed problem size, which in theory correspond to Amdahl‚Äôs Law
    (<xref alt="Amdahl, 1967" rid="ref-amdahl_1967" ref-type="bibr">Amdahl,
    1967</xref>). The training task and resources used are otherwise the
    same as for the standard ESN results shown in
    <xref alt="[fig:eager]" rid="figU003Aeager">[fig:eager]</xref>. We
    then create 3 different <monospace>dask.distributed</monospace>
    Clusters, testing:</p>
    <list list-type="order">
      <list-item>
        <p>Purely threaded mode (CPU only).</p>
      </list-item>
      <list-item>
        <p>The relevant default ‚ÄúLocalCluster‚Äù (i.e., single node)
        configuration for our resources. On the CPU resource, a GCP
        <monospace>c2-standard-60</monospace> instance, the default
        <monospace>dask.distributed.LocalCluster</monospace> has 6
        workers, each with 5 threads. On the GPU resource, a GCP
        <monospace>a2-highgpu-8g</monospace> instance, the default
        <monospace>dask_cuda.LocalCUDACluster</monospace> has 8 workers,
        each with 1 thread.</p>
      </list-item>
      <list-item>
        <p>A <monospace>LocalCluster</monospace> with 1
        <monospace>Dask</monospace> worker per group. On GPUs, this
        assumes 1 GPU per worker and we are able to use a maximum of 8
        workers due to our available resources.</p>
      </list-item>
    </list>
    <fig>
      <caption><p>Strong scaling results, showing speedup as a ratio of
      serial training time to parallel training time as a function of
      number of groups or subdomains of the Lorenz96 system. Serial
      training time is evaluated with <inline-formula><alternatives>
      <tex-math><![CDATA[N_u=256]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>u</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives>
      <tex-math><![CDATA[N_r=16,000]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      with <monospace>xesn.ESN</monospace> from
      <xref alt="[fig:eager]" rid="figU003Aeager">[fig:eager]</xref>,
      and parallel training time uses
      <monospace>xesn.LazyESN</monospace> with the number of groups as
      shown. See text for a description of the different schedulers
      used.
      <styled-content id="figU003Alazy"></styled-content></p></caption>
      <graphic mimetype="application" mime-subtype="pdf" xlink:href="lazy-scaling.pdf" />
    </fig>
    <p><xref alt="[fig:lazy]" rid="figU003Alazy">[fig:lazy]</xref> shows
    the strong scaling results of <monospace>xesn.LazyESN</monospace>
    for each of these cluster configurations, where each point shows the
    ratio of the wall time with the standard (serial) architecture to
    the lazy (parallel) architecture with <inline-formula><alternatives>
    <tex-math><![CDATA[N_g]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    groups. On CPUs, using 1 <monospace>Dask</monospace> worker process
    per ESN group generally scales well, which makes sense because each
    group is trained entirely independently.</p>
    <p>On GPUs, the timing is largely determined by how many workers
    (GPUs) there are relative to the number of groups. When the number
    of workers is less than the number of groups, performance is
    detrimental. However, when there is at least one worker per group,
    the timing is almost the same as for the single worker case, only
    improving performance by 10-20%. While the strong scaling is
    somewhat muted, the invariance of wall time to reservoir size in
    <xref alt="[fig:eager]" rid="figU003Aeager">[fig:eager]</xref> and
    number of groups in
    <xref alt="[fig:lazy]" rid="figU003Alazy">[fig:lazy]</xref> means
    that the distributed GPU implementation is able to tackle larger
    problems at roughly the same computational cost.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>T.A. Smith and S.G. Penny acknowledge support from NOAA Grant
  NA20OAR4600277. S.G. Penny and J.A. Platt acknowledge support from the
  Office of Naval Research (ONR) Grants N00014-19-1-2522 and
  N00014-20-1- 2580. T.A. Smith acknowledges support from the
  Cooperative Institute for Research in Environmental Sciences (CIRES)
  at the University of Colorado Boulder. The authors thank the editor
  Jonny Saunders for comments that significantly improved the
  manuscript, and the reviewers Troy Arcomano and William Nicholas.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-arcomano_machine_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arcomano</surname><given-names>Troy</given-names></name>
        <name><surname>Szunyogh</surname><given-names>Istvan</given-names></name>
        <name><surname>Pathak</surname><given-names>Jaideep</given-names></name>
        <name><surname>Wikner</surname><given-names>Alexander</given-names></name>
        <name><surname>Hunt</surname><given-names>Brian R.</given-names></name>
        <name><surname>Ott</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>A Machine Learning-Based Global Atmospheric Forecast Model</article-title>
      <source>Geophysical Research Letters</source>
      <year iso-8601-date="2020">2020</year>
      <volume>47</volume>
      <issue>9</issue>
      <issn>1944-8007</issn>
      <uri>http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020GL087776</uri>
      <pub-id pub-id-type="doi">10.1029/2020GL087776</pub-id>
      <fpage>e2020GL087776</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-platt_systematic_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Platt</surname><given-names>Jason A.</given-names></name>
        <name><surname>Penny</surname><given-names>Stephen G.</given-names></name>
        <name><surname>Smith</surname><given-names>Timothy A.</given-names></name>
        <name><surname>Chen</surname><given-names>Tse-Chun</given-names></name>
        <name><surname>Abarbanel</surname><given-names>Henry D. I.</given-names></name>
      </person-group>
      <article-title>A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics</article-title>
      <source>Neural Networks</source>
      <year iso-8601-date="2022-09">2022</year><month>09</month>
      <volume>153</volume>
      <issn>0893-6080</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0893608022002404</uri>
      <pub-id pub-id-type="doi">10.1016/j.neunet.2022.06.025</pub-id>
      <fpage>530</fpage>
      <lpage>552</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pathak_model-free_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pathak</surname><given-names>Jaideep</given-names></name>
        <name><surname>Hunt</surname><given-names>Brian</given-names></name>
        <name><surname>Girvan</surname><given-names>Michelle</given-names></name>
        <name><surname>Lu</surname><given-names>Zhixin</given-names></name>
        <name><surname>Ott</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>Model-Free Prediction of Large Spatiotemporally Chaotic Systems from Data: A Reservoir Computing Approach</article-title>
      <source>Physical Review Letters</source>
      <year iso-8601-date="2018-01">2018</year><month>01</month>
      <volume>120</volume>
      <issue>2</issue>
      <uri>https://link.aps.org/doi/10.1103/PhysRevLett.120.024102</uri>
      <pub-id pub-id-type="doi">10.1103/PhysRevLett.120.024102</pub-id>
      <fpage>024102</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-jaeger_echo_2001">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jaeger</surname><given-names>Herbert</given-names></name>
      </person-group>
      <article-title>The &quot;echo state‚Äù approach to analysing and training recurrent neural networks ‚Äì with an Erratum note</article-title>
      <source>Bonn, Germany: German National Research Center for Information Technology GMD Technical Report</source>
      <year iso-8601-date="2001">2001</year>
      <volume>148</volume>
      <issue>34</issue>
      <fpage>13</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-scipy_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>van der Walt</surname><given-names>St√©fan J.</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Wilson</surname><given-names>Joshua</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Mayorov</surname><given-names>Nikolay</given-names></name>
        <name><surname>Nelson</surname><given-names>Andrew R. J.</given-names></name>
        <name><surname>Jones</surname><given-names>Eric</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Larson</surname><given-names>Eric</given-names></name>
        <name><surname>Carey</surname><given-names>C J</given-names></name>
        <name><surname>Polat</surname><given-names>ƒ∞lhan</given-names></name>
        <name><surname>Feng</surname><given-names>Yu</given-names></name>
        <name><surname>Moore</surname><given-names>Eric W.</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Laxalde</surname><given-names>Denis</given-names></name>
        <name><surname>Perktold</surname><given-names>Josef</given-names></name>
        <name><surname>Cimrman</surname><given-names>Robert</given-names></name>
        <name><surname>Henriksen</surname><given-names>Ian</given-names></name>
        <name><surname>Quintero</surname><given-names>E. A.</given-names></name>
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Archibald</surname><given-names>Anne M.</given-names></name>
        <name><surname>Ribeiro</surname><given-names>Ant√¥nio H.</given-names></name>
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>van Mulbregt</surname><given-names>Paul</given-names></name>
        <string-name>SciPy 1.0 Contributors</string-name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python</article-title>
      <source>Nature Methods</source>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lorenz_predictability_1996">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Lorenz</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>Predictability - a problem partly solved</article-title>
      <source>Proceedings of a Seminar Held at ECMWF on Predictability</source>
      <year iso-8601-date="1996">1996</year>
    </element-citation>
  </ref>
  <ref id="ref-dask_2016">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>Dask Development Team</string-name>
      </person-group>
      <source>Dask: Library for dynamic task scheduling</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://dask.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-cupy_learningsys2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Okuta</surname><given-names>Ryosuke</given-names></name>
        <name><surname>Unno</surname><given-names>Yuya</given-names></name>
        <name><surname>Nishino</surname><given-names>Daisuke</given-names></name>
        <name><surname>Hido</surname><given-names>Shohei</given-names></name>
        <name><surname>Loomis</surname><given-names>Crissman</given-names></name>
      </person-group>
      <article-title>CuPy: A NumPy-compatible library for NVIDIA GPU calculations</article-title>
      <source>Proceedings of workshop on machine learning systems (LearningSys) in the thirty-first annual conference on neural information processing systems (NIPS)</source>
      <year iso-8601-date="2017">2017</year>
      <uri>http://learningsys.org/nips17/assets/papers/paper_16.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-bouhlel_scalable_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bouhlel</surname><given-names>Mohamed Amine</given-names></name>
        <name><surname>He</surname><given-names>Sicheng</given-names></name>
        <name><surname>Martins</surname><given-names>Joaquim R. R. A.</given-names></name>
      </person-group>
      <article-title>Scalable gradient‚Äìenhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes</article-title>
      <source>Structural and Multidisciplinary Optimization</source>
      <year iso-8601-date="2020-04">2020</year><month>04</month>
      <volume>61</volume>
      <issue>4</issue>
      <issn>1615-1488</issn>
      <uri>https://doi.org/10.1007/s00158-020-02488-5</uri>
      <pub-id pub-id-type="doi">10.1007/s00158-020-02488-5</pub-id>
      <fpage>1363</fpage>
      <lpage>1376</lpage>
    </element-citation>
  </ref>
  <ref id="ref-platt_constraining_2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Platt</surname><given-names>Jason A.</given-names></name>
        <name><surname>Penny</surname><given-names>Stephen G.</given-names></name>
        <name><surname>Smith</surname><given-names>Timothy A.</given-names></name>
        <name><surname>Chen</surname><given-names>Tse-Chun</given-names></name>
        <name><surname>Abarbanel</surname><given-names>Henry D. I.</given-names></name>
      </person-group>
      <article-title>Constraining Chaos: Enforcing dynamical invariants in the training of recurrent neural networks</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2023-04">2023</year><month>04</month>
      <uri>http://arxiv.org/abs/2304.12865</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2304.12865</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-smith_temporal_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Smith</surname><given-names>Timothy A.</given-names></name>
        <name><surname>Penny</surname><given-names>Stephen G.</given-names></name>
        <name><surname>Platt</surname><given-names>Jason A.</given-names></name>
        <name><surname>Chen</surname><given-names>Tse-Chun</given-names></name>
      </person-group>
      <article-title>Temporal Subsampling Diminishes Small Spatial Scales in Recurrent Neural Network Emulators of Geophysical Turbulence</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2023">2023</year>
      <volume>15</volume>
      <issue>12</issue>
      <issn>1942-2466</issn>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1029/2023MS003792</uri>
      <pub-id pub-id-type="doi">10.1029/2023MS003792</pub-id>
      <fpage>e2023MS003792</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hoyer_xarray_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Hamman</surname><given-names>Joe</given-names></name>
      </person-group>
      <article-title>Xarray: N-D labeled Arrays and Datasets in Python</article-title>
      <source>Journal of Open Research Software</source>
      <year iso-8601-date="2017-04">2017</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-05-06">2024</year><month>05</month><day>06</day></date-in-citation>
      <volume>5</volume>
      <issue>1</issue>
      <issn>2049-9647</issn>
      <uri>https://openresearchsoftware.metajnl.com/articles/10.5334/jors.148</uri>
      <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-jaeger_harnessing_2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jaeger</surname><given-names>Herbert</given-names></name>
        <name><surname>Haas</surname><given-names>Harald</given-names></name>
      </person-group>
      <article-title>Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication</article-title>
      <source>Science</source>
      <year iso-8601-date="2004-04">2004</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-05-06">2024</year><month>05</month><day>06</day></date-in-citation>
      <volume>304</volume>
      <issue>5667</issue>
      <uri>https://www.science.org/doi/10.1126/science.1091277</uri>
      <pub-id pub-id-type="doi">10.1126/science.1091277</pub-id>
      <fpage>78</fpage>
      <lpage>80</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris_array_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>St√©fan J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Kerkwijk</surname><given-names>Marten H. van</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>R√≠o</surname><given-names>Jaime Fern√°ndez del</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>G√©rard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-05-06">2024</year><month>05</month><day>06</day></date-in-citation>
      <volume>585</volume>
      <issue>7825</issue>
      <issn>1476-4687</issn>
      <uri>https://www.nature.com/articles/s41586-020-2649-2</uri>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Trouvain2020">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Trouvain</surname><given-names>Nathan</given-names></name>
        <name><surname>Pedrelli</surname><given-names>Luca</given-names></name>
        <name><surname>Dinh</surname><given-names>Thanh Trung</given-names></name>
        <name><surname>Hinaut</surname><given-names>Xavier</given-names></name>
      </person-group>
      <article-title>ReservoirPy: An efficient and user-friendly library to design echo state networks</article-title>
      <source>Artificial neural networks and machine learning  ICANN 2020</source>
      <publisher-name>Springer International Publishing</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://doi.org/10.1007/978-3-030-61616-8_40</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-61616-8_40</pub-id>
      <fpage>494</fpage>
      <lpage>505</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hyperopt">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bergstra</surname><given-names>James</given-names></name>
        <name><surname>Yamins</surname><given-names>Daniel</given-names></name>
        <name><surname>Cox</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Making a science of model search: Hyperparameter optimization in hundreds of dimensions for vision architectures</article-title>
      <source>Proceedings of the 30th international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>Dasgupta</surname><given-names>Sanjoy</given-names></name>
        <name><surname>McAllester</surname><given-names>David</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <publisher-loc>Atlanta, Georgia, USA</publisher-loc>
      <year iso-8601-date="2013">2013</year>
      <volume>28</volume>
      <uri>https://proceedings.mlr.press/v28/bergstra13.html</uri>
      <fpage>115</fpage>
      <lpage>123</lpage>
    </element-citation>
  </ref>
  <ref id="ref-arcomano_hybrid_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arcomano</surname><given-names>Troy</given-names></name>
        <name><surname>Szunyogh</surname><given-names>Istvan</given-names></name>
        <name><surname>Wikner</surname><given-names>Alexander</given-names></name>
        <name><surname>Pathak</surname><given-names>Jaideep</given-names></name>
        <name><surname>Hunt</surname><given-names>Brian R.</given-names></name>
        <name><surname>Ott</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>A Hybrid Approach to Atmospheric Modeling That Combines Machine Learning With a Physics-Based Numerical Model</article-title>
      <source>Journal of Advances in Modeling Earth Systems</source>
      <year iso-8601-date="2022">2022</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-06-17">2022</year><month>06</month><day>17</day></date-in-citation>
      <volume>14</volume>
      <issue>3</issue>
      <issn>1942-2466</issn>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1029/2021MS002712</uri>
      <pub-id pub-id-type="doi">10.1029/2021MS002712</pub-id>
      <fpage>e2021MS002712</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-arcomano_hybrid_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arcomano</surname><given-names>Troy</given-names></name>
        <name><surname>Szunyogh</surname><given-names>Istvan</given-names></name>
        <name><surname>Wikner</surname><given-names>Alexander</given-names></name>
        <name><surname>Hunt</surname><given-names>Brian R.</given-names></name>
        <name><surname>Ott</surname><given-names>Edward</given-names></name>
      </person-group>
      <article-title>A Hybrid Atmospheric Model Incorporating Machine Learning Can Capture Dynamical Processes Not Captured by Its Physics-Based Component</article-title>
      <source>Geophysical Research Letters</source>
      <year iso-8601-date="2023">2023</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-05-02">2023</year><month>05</month><day>02</day></date-in-citation>
      <volume>50</volume>
      <issue>8</issue>
      <issn>1944-8007</issn>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL102649</uri>
      <pub-id pub-id-type="doi">10.1029/2022GL102649</pub-id>
      <fpage>e2022GL102649</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-arcomano_code">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Arcomano</surname><given-names>Troy</given-names></name>
      </person-group>
      <article-title>Arcomano1234/SPEEDY-ML: V1 - GRL Paper</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023-01">2023</year><month>01</month>
      <uri>https://doi.org/10.5281/zenodo.7508156</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.7508156</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-amdahl_1967">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Amdahl</surname><given-names>Gene M.</given-names></name>
      </person-group>
      <article-title>Validity of the single processor approach to achieving large scale computing capabilities</article-title>
      <source>Proceedings of the april 18-20, 1967, spring joint computer conference</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="1967">1967</year>
      <isbn>9781450378956</isbn>
      <uri>https://doi.org/10.1145/1465482.1465560</uri>
      <pub-id pub-id-type="doi">10.1145/1465482.1465560</pub-id>
      <fpage>483</fpage>
      <lpage>485</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rocklin_scipy_2015">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rocklin</surname></name>
      </person-group>
      <article-title>Dask: Parallel Computation with Blocked algorithms and Task Scheduling</article-title>
      <source>Proceedings of the 14th Python in Science Conference</source>
      <person-group person-group-type="editor">
        <name><surname>Huff</surname></name>
        <name><surname>Bergstra</surname></name>
      </person-group>
      <year iso-8601-date="2015">2015</year>
      <pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-013</pub-id>
      <fpage>126 </fpage>
      <lpage> 132</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
