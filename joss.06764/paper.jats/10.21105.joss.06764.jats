<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6764</article-id>
<article-id pub-id-type="doi">10.21105/joss.06764</article-id>
<title-group>
<article-title>FastVPINNs: An efficient tensor-based Python library for
solving partial differential equations using hp-Variational Physics
Informed Neural Networks</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4969-3242</contrib-id>
<name>
<surname>Anandh</surname>
<given-names>Thivin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-6295-543X</contrib-id>
<name>
<surname>Ghose</surname>
<given-names>Divij</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1858-3972</contrib-id>
<name>
<surname>Ganesan</surname>
<given-names>Sashikumaar</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-2"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Computational and Data Sciences, Indian
Institute of Science, Bangalore, India</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
<corresp id="cor-2">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-05-17">
<day>17</day>
<month>5</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6764</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>physics-informed neural networks</kwd>
<kwd>scientific machine learning</kwd>
<kwd>partial differential equations</kwd>
<kwd>hp-variational physics informed neural networks</kwd>
<kwd>tensorflow</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="introduction">
  <title>Introduction</title>
  <p>Partial differential equations (PDEs) are essential in modeling
  physical phenomena such as heat transfer, electromagnetics and fluid
  dynamics. The current progress in the field of scientific machine
  learning (SciML) has resulted in incorporating deep learning and
  data-driven methods to solve PDEs. Let us consider a two-dimensional
  Poisson equation as an example, defined on the domain
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Ω</mml:mi></mml:math></alternatives></inline-formula>,
  which reads as follows:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\begin{aligned}
      - \nabla^2 u(x) &= f(x), \quad \Omega \in \mathbb{R}^2,  \\
      u(x) &= g(x), \quad x \in \partial \Omega.
  \end{aligned}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>−</mml:mi><mml:msup><mml:mi>∇</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mi>Ω</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>∂</mml:mi><mml:mi>Ω</mml:mi><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
  <p>Here, <inline-formula><alternatives>
  <tex-math><![CDATA[x \in \Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∈</mml:mo><mml:mi>Ω</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is the spatial coordinate, <inline-formula><alternatives>
  <tex-math><![CDATA[u(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the solution of the PDE, <inline-formula><alternatives>
  <tex-math><![CDATA[f(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is a known source term, and <inline-formula><alternatives>
  <tex-math><![CDATA[g(x)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the value of the solution on the domain boundary,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\partial \Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∂</mml:mi><mml:mi>Ω</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <p>A neural network is a parametric function of
  <inline-formula><alternatives>
  <tex-math><![CDATA[x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>x</mml:mi></mml:math></alternatives></inline-formula>,
  denoted as <inline-formula><alternatives>
  <tex-math><![CDATA[u_{\text{NN}}(x; W, b)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.
  In this context, <inline-formula><alternatives>
  <tex-math><![CDATA[W]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>W</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[b]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>b</mml:mi></mml:math></alternatives></inline-formula>
  represent the weights and biases of the network. When the neural
  network consists of <inline-formula><alternatives>
  <tex-math><![CDATA[h]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>
  hidden layers, with the <inline-formula><alternatives>
  <tex-math><![CDATA[i^{\text{th}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>i</mml:mi><mml:mtext mathvariant="normal">th</mml:mtext></mml:msup></mml:math></alternatives></inline-formula>
  layer containing <inline-formula><alternatives>
  <tex-math><![CDATA[n_i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  neurons, the mathematical representation of the function takes the
  following shape:
  <named-content id="eqU003ANN" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[u_{\text{NN}}(x; W, b) = l \circ \mathrm{T}^{(h)} \circ \mathrm{T}^{(h-1)} \hdots \circ \mathrm{T}^1(x).
      ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>l</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="normal">T</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="normal">T</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mi>…</mml:mi><mml:mo>∘</mml:mo><mml:msup><mml:mi mathvariant="normal">T</mml:mi><mml:mn>1</mml:mn></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mi>.</mml:mi></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
  <p>Here, <inline-formula><alternatives>
  <tex-math><![CDATA[l : \mathbb{R}^{n_h} \rightarrow \mathbb{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>l</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:msup><mml:mo>→</mml:mo><mml:mi>ℝ</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  is a linear mapping in the output layer and
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathrm{T}^{(i)}(\cdot) = \sigma(W^{(i)} \times \cdot + b^{(i)})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi mathvariant="normal">T</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>⋅</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>⋅</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is a non-linear mapping in the <inline-formula><alternatives>
  <tex-math><![CDATA[i^{th}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  layer <inline-formula><alternatives>
  <tex-math><![CDATA[( i= 1, 2, \cdots , h)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>⋯</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
  with the non-linear activation function <inline-formula><alternatives>
  <tex-math><![CDATA[\sigma]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>σ</mml:mi></mml:math></alternatives></inline-formula>
  and the weights <inline-formula><alternatives>
  <tex-math><![CDATA[W^{(i)}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  and biases <inline-formula><alternatives>
  <tex-math><![CDATA[b^{(i)}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  of the respective layers.</p>
  <p>Physics-informed neural networks (PINNs), introduced by Raissi et
  al.
  (<xref alt="2019" rid="ref-raissi2019physics" ref-type="bibr">2019</xref>),
  work by incorporating the governing equations of the physical systems
  and the boundary conditions as physics-based constrains to train the
  neural networks. The core idea of PINNs lies in the ability of
  obtaining the gradients of the solutions with respect to the input
  using the automatic differention routines available within deep
  learning frameworks such as TensorFlow
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>). The loss function for the PINNs can be written
  as follows:</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[\begin{aligned}
      \begin{split}
          L_p(W,b) &= \frac{1}{N_T}\sum_{t=1}^{N_T}\left((-\nabla^2 u_{\text{NN}}(x_t;W,b) - f(x_t))\right)^2,\\
          L_b(W,b) &= \frac{1}{N_D}\sum_{d=1}^{N_{D}}\left(u_{\text{NN}}(x_d; W, b) - g(x_d)\right)^2,\\
          L_{\text{PINN}}(W,b) &= L_p + \tau L_b.
      \end{split}
  \end{aligned}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>−</mml:mi><mml:msup><mml:mi>∇</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="normal">PINN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
  <p>Here, the output of the neural network,
  <inline-formula><alternatives>
  <tex-math><![CDATA[u_{\text{NN}}(x; W, b)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  is used to approximate the unknown solution. In addition,
  <inline-formula><alternatives>
  <tex-math><![CDATA[N_T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the total number of training points in the interior of the domain
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Ω</mml:mi></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[N_D]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the total number of training points on the boundary
  <inline-formula><alternatives>
  <tex-math><![CDATA[\partial\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>∂</mml:mi><mml:mi>Ω</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\tau]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>τ</mml:mi></mml:math></alternatives></inline-formula>
  is a scaling factor applied to control the penalty on the boundary
  term, and <inline-formula><alternatives>
  <tex-math><![CDATA[L_{\text{PINN}}(W,b)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="normal">PINN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the loss function of the PINNs.</p>
  <p>Variational Physics informed neural networks (VPINNs)
  (<xref alt="Kharazmi et al., 2019" rid="ref-kharazmi2019variational" ref-type="bibr">Kharazmi
  et al., 2019</xref>) are an extension of PINNs, where the weak form of
  the PDE is used in the loss function of the neural network. The weak
  form of the PDE is obtained by multiplying the PDE with a test
  function, integrating over the domain, and applying integration by
  parts to the higher order derivative terms. The method of hp-VPINNs
  (<xref alt="Kharazmi et al., 2021" rid="ref-kharazmi2021hp" ref-type="bibr">Kharazmi
  et al., 2021</xref>) was subsequently developed to increase the
  accuracy using h-refinement (increasing number of elements) and
  p-refinement (increasing the number of test functions). The domain
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Ω</mml:mi></mml:math></alternatives></inline-formula>
  is divided into an array of non-overlapping cells, labeled as
  <inline-formula><alternatives>
  <tex-math><![CDATA[K_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[k=1,2,\ldots,\texttt{N\_{elem}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mtext mathvariant="monospace">𝙽_</mml:mtext><mml:mtext mathvariant="monospace">𝚎𝚕𝚎𝚖</mml:mtext></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  ensuring that the complete union <inline-formula><alternatives>
  <tex-math><![CDATA[\bigcup_{k=1}^\texttt{N\_{elem}}K_k = \Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mo>⋃</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext mathvariant="monospace">𝙽_</mml:mtext><mml:mtext mathvariant="monospace">𝚎𝚕𝚎𝚖</mml:mtext></mml:mrow></mml:msubsup><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>Ω</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  covers the entire domain <inline-formula><alternatives>
  <tex-math><![CDATA[\Omega]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Ω</mml:mi></mml:math></alternatives></inline-formula>.
  The hp-VPINNs framework utilizes specific test functions
  <inline-formula><alternatives>
  <tex-math><![CDATA[v_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>k</mml:mi></mml:math></alternatives></inline-formula>
  ranges from 1 to N_elem, that are localized and defined within
  individual non-overlapping element across the domain.
  <named-content id="eqnU003Atest_function_summation" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[v_k= 
      \begin{cases}
        v^p \neq 0, & \text{over $K_k$,} \\
        0, & \text{elsewhere.}
      \end{cases}
      ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">{</mml:mo><mml:mtable><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:msup><mml:mi>v</mml:mi><mml:mi>p</mml:mi></mml:msup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mrow><mml:mtext mathvariant="normal">over </mml:mtext><mml:mspace width="0.333em"></mml:mspace></mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mtext mathvariant="normal">,</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left" style="text-align: left"><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mtext mathvariant="normal">elsewhere.</mml:mtext></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></named-content></p>
  <p>The loss function of hp-VPINNs with <inline-formula><alternatives>
  <tex-math><![CDATA[\texttt{N\_{elem}}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="monospace">𝙽_</mml:mtext><mml:mtext mathvariant="monospace">𝚎𝚕𝚎𝚖</mml:mtext></mml:mrow></mml:math></alternatives></inline-formula>
  elements in the domain can be written as follows:
  <disp-formula><alternatives>
  <tex-math><![CDATA[\begin{aligned}
      \begin{split}
          L_v(W,b) &= \frac{1}{\texttt{N\_elem}}\sum_{k=1}^{\texttt{N\_elem}}  \left( \int_{K_k} \nabla u_{\text{NN}}(x;W,b) \cdot \nabla v_k dK  ~ - ~ \int_{K_k} f\,v_k\,dK ~\right)^2,\\
          L_b(W,b) &= \frac{1}{N_D}\sum_{d=1}^{N_{D}}\left(u_{\text{NN}}(x; W, b) - g(x)\right)^2,\\
          L_{\text{VPINN}}(W,b) &= L_v + \tau L_b.
      \end{split}
  \end{aligned}]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:mtable><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mtext mathvariant="monospace">𝙽_𝚎𝚕𝚎𝚖</mml:mtext></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mtext mathvariant="monospace">𝙽_𝚎𝚕𝚎𝚖</mml:mtext></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>∇</mml:mi><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>∇</mml:mi><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi>d</mml:mi><mml:mi>K</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mo>−</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:msub><mml:mo>∫</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mi>f</mml:mi><mml:mspace width="0.167em"></mml:mspace><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mspace width="0.167em"></mml:mspace><mml:mi>d</mml:mi><mml:mi>K</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:munderover><mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mtext mathvariant="normal">NN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right" style="text-align: right"><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="normal">VPINN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mtd><mml:mtd columnalign="left" style="text-align: left"><mml:mo>=</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>τ</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math></alternatives></disp-formula></p>
  <p>Where, <inline-formula><alternatives>
  <tex-math><![CDATA[K_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>K</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the <inline-formula><alternatives>
  <tex-math><![CDATA[k^{th}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
  element in the domain and <inline-formula><alternatives>
  <tex-math><![CDATA[v_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>v</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  is the test function in the respective element. Further,
  <inline-formula><alternatives>
  <tex-math><![CDATA[L_v(W,b)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the weak form PDE residual and <inline-formula><alternatives>
  <tex-math><![CDATA[L_{\text{VPINN}}(W,b)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>L</mml:mi><mml:mtext mathvariant="normal">VPINN</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the loss function of the hp-VPINNs. For more information on the
  derivation of the weak form of the PDE and the loss function of
  hp-VPINNs, refer to Anandh et al.
  (<xref alt="2024" rid="ref-anandh2024fastvpinns" ref-type="bibr">2024</xref>)
  and Ganesan &amp; Tobiska
  (<xref alt="2017" rid="ref-ganesan2017finite" ref-type="bibr">2017</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>The existing implementation of hp-VPINNs framework
  (<xref alt="Kharazmi, 2023" rid="ref-hp_vpinns_github" ref-type="bibr">Kharazmi,
  2023</xref>) suffers from two major challenges. One is the inabilty of
  the framework to handle complex geometries and the other is the
  increased training time associated with the increase in number of
  elements within the domain. In the work Anandh et al.
  (<xref alt="2024" rid="ref-anandh2024fastvpinns" ref-type="bibr">2024</xref>),
  we presented FastVPINNs, which addresses both of these challenges.
  FastVPINNs handles complex geometries by using bilinear
  transformation, and it uses a tensor-based loss computation to reduce
  the dependency of training time on number of elements. The current
  implementation of FastVPINNs can acheive an speed-up of up to 100
  times when compared with the existing implementation of hp-VPINNs. We
  have also shown that with proper hyperparameter selection, FastVPINNs
  can outperform PINNs both in terms of accuracy and training time,
  especially for problems with high frequency solutions.</p>
  <p>In this work, we present the Python based implementation of the
  novel FastVPINNs framework which is built using TensorFlow-v2.0
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015-whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>). FastVPINNs provides an elegant API for users to
  solve both forward and inverse problems for PDEs like the Poisson,
  Helmholtz, and Convection-Diffusion equations. With the current level
  of API abstraction, users should be able to solve PDEs with less than
  six API calls as shown in the minimal working example section. The
  framework is well-documented with examples, which can enable users to
  get started with the framework with ease. As per the authors’
  knowledge, FastVPINNs is the first open-source implementation of the
  hp-VPINNs framework with tensor-based loss computation and support for
  complex geometries.</p>
  <p>The ability of the framework to allow users to train a hp-VPINNs to
  solve a PDE both faster and with minimal code, can result in
  widespread application of this method on several real-world problems,
  which often require complex geometries with a large number of elements
  within the domain.</p>
</sec>
<sec id="modules">
  <title>Modules</title>
  <p>The FastVPINNs framework consists of five core modules, which are
  Geometry, FE, Data, Physics, and Model.</p>
  <fig>
    <caption><p>FastVPINNs Modules</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="fastvpinns_modules.png" />
  </fig>
  <sec id="geometry-module">
    <title>Geometry Module</title>
    <p>This module provides the functionality to define the geometry of
    the domain. The user can either generate a quadrilateral mesh
    internally or read an external <monospace>.mesh</monospace> file.
    The module also provides the functionality to obtain boundary points
    for complex geometries.</p>
  </sec>
  <sec id="fe-module">
    <title>FE Module</title>
    <p>The FE module is responsible for handling the finite element test
    functions and their gradients. The module’s functionality can be
    broadly classified into into four categories:</p>
    <list list-type="bullet">
      <list-item>
        <p><bold>Test functions</bold>: Provides the test function
        values and its gradients for a given reference element. In our
        framework, we use Lagrange polynomials as the test
        functions.</p>
      </list-item>
      <list-item>
        <p><bold>Quadrature functions</bold>: Provides the quadrature
        points and weights for a given element based on the quadrature
        order and the quadrature method selected by the user. This
        method will be used for performing the numerical integration of
        the weak form residual of the PDE.</p>
      </list-item>
      <list-item>
        <p><bold>Transformation functions</bold>: Provides the
        implementation of transformation routines such as affine
        transformation and bilinear transformation. This can be used to
        transform the test function values and gradients from the
        reference element to the actual element.</p>
      </list-item>
      <list-item>
        <p><bold>Finite Element Setup</bold>: Provides the functionality
        to set up the test functions, quadrature rules and the
        transformation for every element and save them in a common class
        to access them. Further, it also hosts functions to plot the
        mesh with quadrature points, assign boundary values based on the
        boundary points obtained from the geometry module and calculate
        the forcing term in the residual.</p>
      </list-item>
    </list>
    <p><italic>Remark: The module is named FE (Finite Element) Module
    because of its similarities with classical FEM routines, such as
    test functions, numerical quadratures, and transformations. However,
    we would like to state that our framework is not an FEM solver, but
    an hp-VPINNs solver.</italic></p>
  </sec>
  <sec id="data-module">
    <title>Data Module:</title>
    <p>The Data module collects data from all modules that are required
    for training and converts them to a tensor data type with the user
    specified precision (for example, <monospace>tf.float32</monospace>
    or <monospace>tf.float64</monospace>). It also assembles the test
    function values and gradients to form a three-dimensional tensor,
    which will be used during the loss computation.</p>
  </sec>
  <sec id="physics-module">
    <title>Physics Module:</title>
    <p>This module contains functions that are used to compute the
    variational loss function for different PDEs. These functions accept
    the test function tensors and the predicted gradients from the
    neural network along with the forcing matrix to compute the PDE
    residuals using tensor-based operations.</p>
  </sec>
  <sec id="model-module">
    <title>Model Module:</title>
    <p>This module contains custom subclasses of the
    <monospace>tensorflow.keras.Model</monospace> class, which are used
    to train the neural network. The module provides the functionality
    to train the neural network using the tensor based loss computation
    and also provides the functionality to predict the solution of the
    PDE for a given input.</p>
  </sec>
</sec>
<sec id="secU003Aminimal-working-example">
  <title>Minimal Working Example</title>
  <p>With the higher level of abstraction provided by the FastVPINNs
  framework, users can solve a PDE with just six API calls. A Minimal
  working example to solve the Poisson equation using the FastVPINNs
  framework can be found
  <ext-link ext-link-type="uri" xlink:href="https://cmgcds.github.io/fastvpinns/#usage">here</ext-link>.
  The example files with detailed documentation can be found in the
  <ext-link ext-link-type="uri" xlink:href="https://cmgcds.github.io/fastvpinns/_rst/_tutorial.html">Tutorials
  section</ext-link> of the documentation.</p>
</sec>
<sec id="testing">
  <title>Testing</title>
  <p>The FastVPINNs framework has a strong testing suite, which tests
  the core functionalities of the framework. The testing suite is built
  using the <monospace>pytest</monospace> framework and is integrated
  with the continuous integration pipeline provided by Github Actions.
  The testing functionalites can be broadly classified into the three
  categories:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Unit Testing</bold>: Covers the testing of individual
      modules and their functionalities.</p>
    </list-item>
    <list-item>
      <p><bold>Integration Testing</bold>: Covers the overall flow of
      the framework. Different PDEs are solved with different parameters
      to check if the accuracy after training is within the acceptable
      limits. This ensures that the collection of modules work together
      as expected.</p>
    </list-item>
    <list-item>
      <p><bold>Compatibility Testing</bold>: The framework is tested
      with different versions of Python such as 3.8, 3.9, 3.10, 3.11 and
      on different versions of OS such as Ubuntu, MacOS, and Windows to
      ensure the compatibility of the framework across different
      platforms.</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Shell Research, India, for providing partial funding for
  this project. We are thankful to the MHRD Grant No.STARS-1/388 (SPADE)
  for partial support.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-raissi2019physics">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Raissi</surname><given-names>Maziar</given-names></name>
        <name><surname>Perdikaris</surname><given-names>Paris</given-names></name>
        <name><surname>Karniadakis</surname><given-names>George E</given-names></name>
      </person-group>
      <article-title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>
      <source>Journal of Computational physics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>378</volume>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2018.10.045</pub-id>
      <fpage>686</fpage>
      <lpage>707</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hp_vpinns_github">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kharazmi</surname><given-names>Ehsan</given-names></name>
      </person-group>
      <article-title>hp-VPINNs: High-performance variational physics-informed neural networks</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://github.com/ehsankharazmi/hp-VPINNs</uri>
    </element-citation>
  </ref>
  <ref id="ref-anandh2024fastvpinns">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Anandh</surname><given-names>Thivin</given-names></name>
        <name><surname>Ghose</surname><given-names>Divij</given-names></name>
        <name><surname>Jain</surname><given-names>Himanshu</given-names></name>
        <name><surname>Ganesan</surname><given-names>Sashikumaar</given-names></name>
      </person-group>
      <article-title>FastVPINNs: Tensor-driven acceleration of VPINNs for complex geometries</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2404.12063</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2404.12063</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kharazmi2021hp">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kharazmi</surname><given-names>Ehsan</given-names></name>
        <name><surname>Zhang</surname><given-names>Zhongqiang</given-names></name>
        <name><surname>Karniadakis</surname><given-names>George Em</given-names></name>
      </person-group>
      <article-title>hp-VPINNs: Variational physics-informed neural networks with domain decomposition</article-title>
      <source>Computer Methods in Applied Mechanics and Engineering</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>374</volume>
      <pub-id pub-id-type="doi">10.1016/j.cma.2020.113547</pub-id>
      <fpage>113547</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kharazmi2019variational">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kharazmi</surname><given-names>Ehsan</given-names></name>
        <name><surname>Zhang</surname><given-names>Zhongqiang</given-names></name>
        <name><surname>Karniadakis</surname><given-names>George Em</given-names></name>
      </person-group>
      <article-title>Variational physics-informed neural networks for solving partial differential equations</article-title>
      <source>arXiv preprint arXiv:1912.00873</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.00873</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow2015-whitepaper">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Mané</surname><given-names>Dandelion</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://www.tensorflow.org/</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1603.04467</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ganesan2017finite">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Ganesan</surname><given-names>Sashikumaar</given-names></name>
        <name><surname>Tobiska</surname><given-names>Lutz</given-names></name>
      </person-group>
      <source>Finite elements: Theory and algorithms</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.1017/9781108235013</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
