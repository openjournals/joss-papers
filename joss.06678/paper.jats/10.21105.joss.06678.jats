<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6678</article-id>
<article-id pub-id-type="doi">10.21105/joss.06678</article-id>
<title-group>
<article-title>VisualTorch: Streamlining Visualization for PyTorch
Neural Network Architectures</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6209-3981</contrib-id>
<name>
<surname>Hendria</surname>
<given-names>Willy Fitra</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2667-9333</contrib-id>
<name>
<surname>Gavrikov</surname>
<given-names>Paul</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Researcher</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>IMLA, Offenburg University, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>University of Mannheim, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-02-18">
<day>18</day>
<month>2</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>101</issue>
<fpage>6678</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PyTorch</kwd>
<kwd>Visualization</kwd>
<kwd>Neural Networks</kwd>
<kwd>Artificial Intelligence</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>VisualTorch is a library designed for visualizing neural network
  architectures in PyTorch. It offers support for multiple visualization
  styles, such as layered-style, graph-style, and the newly added
  LeNet-like visualization. When provided with a sequential or custom
  PyTorch model, alongside the input shape and visualization
  specifications, VisualTorch automatically translates the model
  structure into an architectural diagram. The resulting diagram can be
  refined using various configurations, including style, color, opacity,
  size, and a legend. VisualTorch is particularly valuable for projects
  involving PyTorch-based neural networks. By facilitating the
  generation of graphics with a single function call, it streamlines the
  process of visualizing neural network architectures. This ensures that
  the produced results are suitable for publication with minimal
  additional modifications. Moreover, owing to its diverse customization
  options, VisualTorch empowers users to generate polished figures
  suitable for publication.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Neural network architecture visualization plays an important role
  in the scientific process within the realm of artificial intelligence
  and machine learning. While mathematical equations and descriptive
  paragraphs provide detailed information about architectures, effective
  visualizations can significantly aid scientists in communicating their
  proposed architectures to others.</p>
  <p>In deep learning projects based on Keras
  (<xref alt="Chollet &amp; others, 2015" rid="ref-CholletU003A2015" ref-type="bibr">Chollet
  &amp; others, 2015</xref>), the visualkeras project
  (<xref alt="Gavrikov, 2020" rid="ref-GavrikovU003A2020" ref-type="bibr">Gavrikov,
  2020</xref>) has been gaining traction over nearly four years of
  development. It offers visualization of Keras-based neural network
  architectures in two styles: layered and graph. The visualtorch
  library provides visualization for PyTorch-based architectures
  (<xref alt="Paszke et al., 2019" rid="ref-PaszkeU003A2019" ref-type="bibr">Paszke
  et al., 2019</xref>). PyTorch itself has gained popularity among deep
  learning frameworks within the scientific community in recent years
  (<xref alt="Aoun et al., 2022" rid="ref-AounU003A2022" ref-type="bibr">Aoun
  et al., 2022</xref>). VisualTorch expands the functionality of
  VisualKeras to PyTorch and offers more visualization styles, improved
  usability, and a development environment by providing web-based
  documentation and CI/CD pipelines for seamless future contributions
  and collaborations.</p>
</sec>
<sec id="introduction">
  <title>Introduction</title>
  <p>Recent advancements in artificial intelligence have sparked
  widespread interest among researchers, particularly in exploring
  innovative algorithmic approaches such as neural networks or deep
  learning architectures. These architectures have demonstrated
  remarkable utility across various AI applications, including computer
  vision, natural language processing, and robotics. To implement neural
  network architectures, many researchers and practitioners often
  utilize established deep learning frameworks, such as PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-PaszkeU003A2019" ref-type="bibr">Paszke
  et al., 2019</xref>), TensorFlow
  (<xref alt="Abadi &amp; others, 2016" rid="ref-AbadiU003A2016" ref-type="bibr">Abadi
  &amp; others, 2016</xref>), and Keras
  (<xref alt="Chollet &amp; others, 2015" rid="ref-CholletU003A2015" ref-type="bibr">Chollet
  &amp; others, 2015</xref>)</p>
  <p>To effectively communicate their ideas, practitioners often employ
  architecture diagrams as aids for comprehension. While detailed
  mathematical descriptions help in understanding the intricacies of
  algorithms, visual representations of architectures offer an
  additional means of conveying concepts, enabling individuals to grasp
  the overall visual representation. VisualTorch is designed to
  facilitate the visualization of PyTorch-based neural network
  architectures. Instead of manually constructing diagrams from scratch,
  practitioners can simply leverage our library to generate
  visualizations. With a variety of customization options, users can
  tailor visualizations to suit their preferences.</p>
  <p>One of the important features of VisualTorch is its ability to
  automatically map a neural network model to visualizations using
  various styles such as layered, graph, and LeNet-like visualization
  (<xref alt="Lecun et al., 1998" rid="ref-LecunU003A1998" ref-type="bibr">Lecun
  et al., 1998</xref>). Users can further refine these visualizations by
  adjusting attributes such as color, opacity, and size. VisualTorch
  aims to offer a solution for rapidly visualizing a wide range of
  neural network architectures in PyTorch. Inspired by the visualkeras
  (<xref alt="Gavrikov, 2020" rid="ref-GavrikovU003A2020" ref-type="bibr">Gavrikov,
  2020</xref>) project, our VisualTorch library shares a similar
  motivation: to assist in visualizing neural network architectures.
  Unlike visualkeras, our library offers enhanced functionality
  specifically tailored for PyTorch-based architectures, supporting
  models defined with <monospace>torch.nn.Module</monospace> and
  <monospace>torch.nn.Sequential</monospace>. In addition to providing
  more visualization styles, including the recent addition of LeNet
  style, our library also provides online web-based documentation and
  streamlined CI/CD workflows, which improve usability and facilitate
  future development.</p>
</sec>
<sec id="usage-example">
  <title>Usage Example</title>
  <p>In this section, we provide a usage example for each of the
  layered, LeNet, and graph-style visualizations. The graphic
  visualizations, displayed using the Matplotlib library
  (<xref alt="Hunter, 2007" rid="ref-HunterU003A2007" ref-type="bibr">Hunter,
  2007</xref>), are shown in
  <xref alt="[fig:layered]" rid="figU003Alayered">[fig:layered]</xref>,
  <xref alt="[fig:lenet]" rid="figU003Alenet">[fig:lenet]</xref>, and
  <xref alt="[fig:graph]" rid="figU003Agraph">[fig:graph]</xref>.</p>
  <sec id="layered-style">
    <title>Layered Style</title>
    <code language="python">import matplotlib.pyplot as plt
import visualtorch
from torch import nn

model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(16, 32, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Flatten(),
    nn.Linear(64 * 28 * 28, 256),
    nn.ReLU(),
    nn.Linear(256, 10),
)

input_shape = (1, 3, 224, 224)

img = visualtorch.layered_view(model, input_shape=input_shape, legend=True)

plt.axis(&quot;off&quot;)
plt.tight_layout()
plt.imshow(img)
plt.show()</code>
    <fig>
      <caption><p>An example of layered style visualization generated
      using
      VisualTorch.<styled-content id="figU003Alayered"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figure-layered.png" />
    </fig>
  </sec>
  <sec id="lenet-style">
    <title>LeNet Style</title>
    <code language="python">import matplotlib.pyplot as plt
import visualtorch
from torch import nn

model = nn.Sequential(
    nn.Conv2d(3, 8, kernel_size=3, padding=1),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(8, 16, kernel_size=3, padding=1),
    nn.MaxPool2d(2, 2),
)

input_shape = (1, 3, 128, 128)

img = visualtorch.lenet_view(model, input_shape=input_shape)

plt.axis(&quot;off&quot;)
plt.tight_layout()
plt.imshow(img)
plt.show()</code>
    <fig>
      <caption><p>An example of LeNet style visualization generated
      using
      VisualTorch.<styled-content id="figU003Alenet"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figure-lenet.png" />
    </fig>
  </sec>
  <sec id="graph-style">
    <title>Graph Style</title>
    <code language="python">import matplotlib.pyplot as plt
import torch
import visualtorch
from torch import nn

class SimpleDense(nn.Module):
    def __init__(self) -&gt; None:
        super().__init__()
        self.h0 = nn.Linear(4, 8)
        self.h1 = nn.Linear(8, 8)
        self.h2 = nn.Linear(8, 4)
        self.out = nn.Linear(4, 2)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        x = self.h0(x)
        x = self.h1(x)
        x = self.h2(x)
        return self.out(x)

model = SimpleDense()

input_shape = (1, 4)

img = visualtorch.graph_view(model, input_shape)

plt.axis(&quot;off&quot;)
plt.tight_layout()
plt.imshow(img)
plt.show()</code>
    <fig>
      <caption><p>An example of graph style visualization generated
      using
      VisualTorch.<styled-content id="figU003Agraph"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="figure-graph.png" />
    </fig>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We extend our appreciation to our community that we anticipate will
  contribute to the advancement of VisualTorch. Your future
  contributions, whether through questions, bug reports, or code
  submissions, will be instrumental in shaping the development of this
  project. Together, we look forward to fostering a collaborative
  environment that drives innovation and progress. Thank you for your
  anticipated dedication and commitment to excellence.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-AounU003A2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Aoun</surname><given-names>Mohamed Raed El</given-names></name>
        <name><surname>Tidjon</surname><given-names>Lionel Nganyewou</given-names></name>
        <name><surname>Rombaut</surname><given-names>Ben</given-names></name>
        <name><surname>Khomh</surname><given-names>Foutse</given-names></name>
        <name><surname>Hassan</surname><given-names>Ahmed E.</given-names></name>
      </person-group>
      <article-title>An empirical study of library usage and dependency in deep learning frameworks</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2211.15733</uri>
      <pub-id pub-id-type="doi">10.48550/arxiv.2211.15733</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-GavrikovU003A2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gavrikov</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>Visualkeras</article-title>
      <source>GitHub repository</source>
      <publisher-name>https://github.com/paulgavrikov/visualkeras; GitHub</publisher-name>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-CholletU003A2015">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Chollet</surname><given-names>François</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Keras</article-title>
      <publisher-name>https://github.com/fchollet/keras; GitHub</publisher-name>
      <year iso-8601-date="2015">2015</year>
    </element-citation>
  </ref>
  <ref id="ref-PaszkeU003A2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Köpf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zach</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/abs/1912.01703</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-AbadiU003A2016">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martín</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>TensorFlow: A system for large-scale machine learning</article-title>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/abs/1605.08695</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1605.08695</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-HunterU003A2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hunter</surname><given-names>J. D.</given-names></name>
      </person-group>
      <article-title>Matplotlib: A 2D graphics environment</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE COMPUTER SOC</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>9</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
      <fpage>90</fpage>
      <lpage>95</lpage>
    </element-citation>
  </ref>
  <ref id="ref-LecunU003A1998">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lecun</surname><given-names>Y.</given-names></name>
        <name><surname>Bottou</surname><given-names>L.</given-names></name>
        <name><surname>Bengio</surname><given-names>Y.</given-names></name>
        <name><surname>Haffner</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>Gradient-based learning applied to document recognition</article-title>
      <source>Proceedings of the IEEE</source>
      <year iso-8601-date="1998">1998</year>
      <volume>86</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1109/5.726791</pub-id>
      <fpage>2278</fpage>
      <lpage>2324</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
