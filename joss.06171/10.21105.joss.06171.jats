<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6171</article-id>
<article-id pub-id-type="doi">10.21105/joss.06171</article-id>
<title-group>
<article-title><inline-formula><alternatives>
<tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>:
Intuitive Scientific Computing with Dimension Types for Jax, PyTorch,
TensorFlow &amp; NumPy</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9246-5195</contrib-id>
<name>
<surname>Holl</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6647-8910</contrib-id>
<name>
<surname>Thuerey</surname>
<given-names>Nils</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>School of Computation, Information and Technology,
Technical University of Munich, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-08-01">
<day>1</day>
<month>8</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>95</issue>
<fpage>6171</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Machine Learning</kwd>
<kwd>Jax</kwd>
<kwd>TensorFlow</kwd>
<kwd>PyTorch</kwd>
<kwd>NumPy</kwd>
<kwd>Differentiable simulations</kwd>
<kwd>Sparse linear systems</kwd>
<kwd>Preconditioners</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  is a math and neural network library designed for science
  applications. It enables users to quickly evaluate many network
  architectures on their data sets, perform (sparse) linear and
  non-linear optimization, and write differentiable simulations that
  scale to <italic>n</italic> dimensions. <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  is compatible with Jax, PyTorch, TensorFlow and NumPy, and user code
  can be executed on all of these backends. The project is hosted at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/tum-pbs/PhiML">https://github.com/tum-pbs/PhiML</ext-link>
  under the MIT license.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Machine learning (ML) has become an essential tool for scientific
  research. In recent years, ML has been used to make significant
  advances in a wide range of scientific fields, including chemistry
  (<xref alt="Butler et al., 2018" rid="ref-Molecular2018" ref-type="bibr">Butler
  et al., 2018</xref>), materials science
  (<xref alt="Wei et al., 2019" rid="ref-Materials2019" ref-type="bibr">Wei
  et al., 2019</xref>), weather and climate prediction
  (<xref alt="Bochenek &amp; Ustrnul, 2022" rid="ref-Weather2022" ref-type="bibr">Bochenek
  &amp; Ustrnul, 2022</xref>;
  <xref alt="Rolnick et al., 2022" rid="ref-Climate2022" ref-type="bibr">Rolnick
  et al., 2022</xref>), computational fluid dynamics
  (<xref alt="Brunton et al., 2020" rid="ref-CFD2020" ref-type="bibr">Brunton
  et al., 2020</xref>), drug discovery
  (<xref alt="Jumper et al., 2021" rid="ref-AlphaFold2021" ref-type="bibr">Jumper
  et al., 2021</xref>;
  <xref alt="Vamathevan et al., 2019" rid="ref-DrugDiscovery2019" ref-type="bibr">Vamathevan
  et al., 2019</xref>), astrophysics
  (<xref alt="De La Calleja &amp; Fuentes, 2004" rid="ref-Galaxy2004" ref-type="bibr">De
  La Calleja &amp; Fuentes, 2004</xref>;
  <xref alt="Ntampaka et al., 2015" rid="ref-Galaxy2015" ref-type="bibr">Ntampaka
  et al., 2015</xref>;
  <xref alt="Petroff et al., 2020" rid="ref-CMB2020" ref-type="bibr">Petroff
  et al., 2020</xref>), geology
  (<xref alt="Rodriguez-Galiano et al., 2015" rid="ref-Mineral2015" ref-type="bibr">Rodriguez-Galiano
  et al., 2015</xref>), and many more. The use of ML for scientific
  applications is still in its early stages, but it has the potential to
  revolutionize the way that science is done. ML can help researchers to
  make new discoveries and insights that were previously impossible.</p>
  <p>The availability of domain knowledge sets science applications
  apart from other ML fields like computer vision or language modelling.
  Domain knowledge often allows for explicit modelling of known dynamics
  by simulating them with handwritten algorithms, which has been shown
  to improve results when training ML models
  (<xref alt="Raissi et al., 2019" rid="ref-PINN2019" ref-type="bibr">Raissi
  et al., 2019</xref>;
  <xref alt="Um et al., 2020" rid="ref-SolverInTheLoop2020" ref-type="bibr">Um
  et al., 2020</xref>). Implementing differentiable simulations into ML
  frameworks requires different functions and concepts than classical ML
  tasks. The major differences are:</p>
  <list list-type="bullet">
    <list-item>
      <p>Data typically represent objects or signals that exist in space
      and time. Data dimensions are interpretable, e.g. vector
      components, time series, <italic>n</italic>-dimensional
      lattices.</p>
    </list-item>
    <list-item>
      <p>Information transfer is usually local, resulting in sparsity in
      the dependency matrix between objects (particles, elements or
      cells).</p>
    </list-item>
    <list-item>
      <p>A high numerical accuracy is desirable for some operations,
      often requiring 64-bit and 32-bit floating point calculations.</p>
    </list-item>
  </list>
  <p>However, current machine learning frameworks have been designed for
  the core ML tasks which reflects in their priorities and design
  choices. This can result in overly verbose code when implementing
  scientific applications and may require implementing custom operators,
  since many common functions like sparse-sparse matrix multiplication,
  periodic padding or sparse linear solvers are not available in all
  libraries.</p>
  <p><inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  is a scientific computing library based on Python 3
  (<xref alt="Van Rossum &amp; Drake, 2009" rid="ref-Python3" ref-type="bibr">Van
  Rossum &amp; Drake, 2009</xref>) targeting scientific applications
  that use machine learning methods. Its main goals are:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Reusability.</bold> Code based on
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      should be able to run in many settings without modification. It
      should be agnostic towards the dimensionality of simulated systems
      and the employed discretization. All code should be trivially
      vectorizable.</p>
    </list-item>
    <list-item>
      <p><bold>Compatibility.</bold> Users should be free to choose
      whatever ML or third-party library they desire without modifying
      their simulation code. <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      should support Linux, Windows and Mac.</p>
    </list-item>
    <list-item>
      <p><bold>Usability.</bold> <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      should be easy to learn and use, matching existing APIs where
      possible. It should encourage users to write concise and
      expressive code.</p>
    </list-item>
    <list-item>
      <p><bold>Maintainability.</bold> All high-level source code of
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      should be easy to understand. Continuous testing should be used to
      ensure that future updates do not break existing code.</p>
    </list-item>
    <list-item>
      <p><bold>Performance.</bold> <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      should be able to make use of hardware accelerators, such as GPUs
      and TPUs, where possible. During development, we prioritize rapid
      code iterations over execution speed but the completed code should
      run as fast as if written directly against the chosen ML
      library.</p>
    </list-item>
  </list>
  <p>In the following, we explain the architecture and major features
  that help <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{Flow}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">Flow</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  reach these goals. <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  consists of a high-level NumPy-like API geared towards writing
  easy-to-read and scalable simulation code, as well as a neural network
  API designed to allow users to quickly iterate over many network
  architectures and hyperparameter settings. Similar to eagerpy
  (<xref alt="Rauber et al., 2020" rid="ref-rauber2020eagerpy" ref-type="bibr">Rauber
  et al., 2020</xref>), <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  integrates with Jax
  (<xref alt="Bradbury et al., 2018" rid="ref-Jax2018" ref-type="bibr">Bradbury
  et al., 2018</xref>), PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-PyTorch2019" ref-type="bibr">Paszke
  et al., 2019</xref>), TensorFlow
  (<xref alt="Abadi et al., 2016" rid="ref-TensorFlow2016" ref-type="bibr">Abadi
  et al., 2016</xref>) and NumPy
  (<xref alt="Harris et al., 2020" rid="ref-NumPy2020" ref-type="bibr">Harris
  et al., 2020</xref>) and provides a custom
  <monospace>Tensor</monospace> class. However,
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  adds additional functionality.</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Dimension names.</bold> Tensor dimensions are always
      referenced by their user-defined name, not their index. We support
      the syntax <monospace>tensor.dim</monospace> for operations like
      indexing or unstacking to make using dimension names as simple as
      possible.</p>
    </list-item>
    <list-item>
      <p><bold>Automatic reshaping.</bold>
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      automatically transposes tensors and inserts singleton dimensions
      to match arguments. Consequently, user code is agnostic to the
      dimension order by default.</p>
    </list-item>
    <list-item>
      <p><bold>Element names.</bold> Slices or <italic>items</italic>
      along dimensions can be named as well, e.g. allowing users to
      specify that a dimension lists the values
      <monospace>(x,y,z)</monospace> or <monospace>(r,g,b)</monospace>.
      These names can be used in slicing, gathering and scattering
      operations.</p>
    </list-item>
    <list-item>
      <p><bold>Dimension types.</bold> Tensor dimensions are grouped
      into five different types: <italic>batch</italic>,
      <italic>spatial</italic>, <italic>instance</italic>,
      <italic>channel</italic>, and <italic>dual</italic>. This allows
      tensor-related functions to automatically select dimensions to
      operate on, without requiring the user to specify individual
      dimensions.</p>
    </list-item>
    <list-item>
      <p><bold>Non-uniform tensors.</bold> Stacking tensors with
      different dimension sizes yields non-uniform tensors.
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      keeps track of the resulting shape, allowing users to operate on
      non-uniform tensors the same way as uniform ones.</p>
    </list-item>
    <list-item>
      <p><bold>Floating-point precision by context.</bold> All tensor
      operations determine the desired floating point precision from the
      operation context, not the data types of its inputs. This is much
      simpler and more predictable than the systems used by other
      libraries.</p>
    </list-item>
    <list-item>
      <p><bold>Lazy stacking.</bold> New memory is only allocated once
      stacked data is required as a block. Consequently, functions can
      unstack the components, operate on them individually, and restack
      them, without worrying about unnecessary memory allocations.</p>
    </list-item>
    <list-item>
      <p><bold>Sparse matrices from linear functions.</bold>
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      can transform linear functions into their corresponding sparse
      matrix representation. This makes solving linear systems of
      equations more performant and enables computation of
      preconditioners.</p>
    </list-item>
    <list-item>
      <p><bold>Compute device from Inputs.</bold> Tensor operations
      execute on the device on which the tensors reside. This prevents
      unintentional copies and transfers, as users have to explicitly
      declare them.</p>
    </list-item>
    <list-item>
      <p><bold>Custom CUDA Operatorions.</bold>
      <inline-formula><alternatives>
      <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
      provides custom CUDA kernels for specific operations that could
      bottleneck simulations, such as grid sampling for TensorFlow or
      linear solves.</p>
    </list-item>
  </list>
</sec>
<sec id="research-projects">
  <title>Research Projects</title>
  <p><inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  has been in development since 2019 as part of the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/tum-pbs/PhiFlow">PhiFlow</ext-link>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{Flow}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">Flow</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>)
  project where it originated as a unified API for TensorFlow and NumPy,
  used to run differentiable fluid simulations.
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{Flow}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">Flow</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  includes geometry, physics, and visualization modules, all of which
  use the <monospace>math</monospace> API of
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  to benefit from its reusability, compatibility, and performance.</p>
  <p>It was first used to show that differentiable PDE simulations can
  be used to train neural networks that steer the dynamics towards
  desired outcomes
  (<xref alt="Holl et al., 2019" rid="ref-phiflow" ref-type="bibr">Holl
  et al., 2019</xref>). Differentiable PDEs, implemented against
  <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>’s
  API, were later shown to benefit learning corrections for
  low-resolution or incomplete physics models
  (<xref alt="Um et al., 2020" rid="ref-SolverInTheLoop2020" ref-type="bibr">Um
  et al., 2020</xref>). These findings were summarized and formalized in
  Thuerey et al.
  (<xref alt="2022" rid="ref-PBDL2021" ref-type="bibr">2022</xref>),
  along with many additional examples.</p>
  <p>The library was also used in network optimization publications,
  such as showing that inverted simulations can be used to train
  networks
  (<xref alt="Holl et al., 2022" rid="ref-ScaleInvariant2022" ref-type="bibr">Holl
  et al., 2022</xref>) and that gradient inversion benefits learning the
  solutions to inverse problems
  (<xref alt="Schnell et al., 2021" rid="ref-HalfInverse2022" ref-type="bibr">Schnell
  et al., 2021</xref>).</p>
  <p>Simulations powered by <inline-formula><alternatives>
  <tex-math><![CDATA[\Phi_\textrm{ML}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>Φ</mml:mi><mml:mtext mathvariant="normal">ML</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
  have since been used in open data sets
  (<xref alt="Gupta &amp; Brandstetter, 2022" rid="ref-PDEArena" ref-type="bibr">Gupta
  &amp; Brandstetter, 2022</xref>;
  <xref alt="Takamoto et al., 2022" rid="ref-PDEBench" ref-type="bibr">Takamoto
  et al., 2022</xref>) and in publications from various research groups
  (<xref alt="Brandstetter et al., 2021" rid="ref-brandstetter2021message" ref-type="bibr">Brandstetter
  et al., 2021</xref>,
  <xref alt="2023" rid="ref-brandstetter2023clifford" ref-type="bibr">2023</xref>;
  <xref alt="Li et al., 2023" rid="ref-li2023latent" ref-type="bibr">Li
  et al., 2023</xref>;
  <xref alt="Parekh et al., 1993" rid="ref-parekh1993sex" ref-type="bibr">Parekh
  et al., 1993</xref>;
  <xref alt="Ramos et al., 2022" rid="ref-ramos2022control" ref-type="bibr">Ramos
  et al., 2022</xref>;
  <xref alt="Sengar et al., 2021" rid="ref-sengar2021multi" ref-type="bibr">Sengar
  et al., 2021</xref>;
  <xref alt="Wandel et al., 2020" rid="ref-wandel2020learning" ref-type="bibr">Wandel
  et al., 2020</xref>,
  <xref alt="2021" rid="ref-wandel2021teaching" ref-type="bibr">2021</xref>;
  <xref alt="P. Wang, 2023" rid="ref-wang2023applications" ref-type="bibr">P.
  Wang, 2023</xref>;
  <xref alt="R. Wang et al., 2022a" rid="ref-wang2022approximately" ref-type="bibr">R.
  Wang et al., 2022a</xref>,
  <xref alt="2022b" rid="ref-wang2022meta" ref-type="bibr">2022b</xref>;
  <xref alt="Wu et al., 2022" rid="ref-wu2022learning" ref-type="bibr">Wu
  et al., 2022</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank Robin Greif, Kartik Bali, Elias Djossou and
  Brener Ramos for their contributions, as well as everyone who
  contributed to the project on GitHub.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-rauber2020eagerpy">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Rauber</surname><given-names>Jonas</given-names></name>
        <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
        <name><surname>Brendel</surname><given-names>Wieland</given-names></name>
      </person-group>
      <article-title>EagerPy: Writing code that works natively with PyTorch, TensorFlow, JAX, and NumPy</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2008.04175</uri>
    </element-citation>
  </ref>
  <ref id="ref-phiflow">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Holl</surname><given-names>Philipp</given-names></name>
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
        <name><surname>Koltun</surname><given-names>Vladlen</given-names></name>
      </person-group>
      <article-title>Learning to control PDEs with differentiable physics</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-Python3">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Van Rossum</surname><given-names>Guido</given-names></name>
        <name><surname>Drake</surname><given-names>Fred L.</given-names></name>
      </person-group>
      <source>Python 3 reference manual</source>
      <publisher-name>CreateSpace</publisher-name>
      <publisher-loc>Scotts Valley, CA</publisher-loc>
      <year iso-8601-date="2009">2009</year>
      <isbn>1441412697</isbn>
    </element-citation>
  </ref>
  <ref id="ref-DrugDiscovery2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Vamathevan</surname><given-names>Jessica</given-names></name>
        <name><surname>Clark</surname><given-names>Dominic</given-names></name>
        <name><surname>Czodrowski</surname><given-names>Paul</given-names></name>
        <name><surname>Dunham</surname><given-names>Ian</given-names></name>
        <name><surname>Ferran</surname><given-names>Edgardo</given-names></name>
        <name><surname>Lee</surname><given-names>George</given-names></name>
        <name><surname>Li</surname><given-names>Bin</given-names></name>
        <name><surname>Madabhushi</surname><given-names>Anant</given-names></name>
        <name><surname>Shah</surname><given-names>Parantu</given-names></name>
        <name><surname>Spitzer</surname><given-names>Michaela</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Applications of machine learning in drug discovery and development</article-title>
      <source>Nature reviews Drug discovery</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>18</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1038/s41573-019-0024-5</pub-id>
      <fpage>463</fpage>
      <lpage>477</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AlphaFold2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jumper</surname><given-names>John</given-names></name>
        <name><surname>Evans</surname><given-names>Richard</given-names></name>
        <name><surname>Pritzel</surname><given-names>Alexander</given-names></name>
        <name><surname>Green</surname><given-names>Tim</given-names></name>
        <name><surname>Figurnov</surname><given-names>Michael</given-names></name>
        <name><surname>Ronneberger</surname><given-names>Olaf</given-names></name>
        <name><surname>Tunyasuvunakool</surname><given-names>Kathryn</given-names></name>
        <name><surname>Bates</surname><given-names>Russ</given-names></name>
        <name><surname>Žı́dek</surname><given-names>Augustin</given-names></name>
        <name><surname>Potapenko</surname><given-names>Anna</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Highly accurate protein structure prediction with AlphaFold</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>596</volume>
      <issue>7873</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id>
      <fpage>583</fpage>
      <lpage>589</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Weather2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bochenek</surname><given-names>Bogdan</given-names></name>
        <name><surname>Ustrnul</surname><given-names>Zbigniew</given-names></name>
      </person-group>
      <article-title>Machine learning in weather prediction and climate analyses—applications and perspectives</article-title>
      <source>Atmosphere</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>13</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.3390/atmos13020180</pub-id>
      <fpage>180</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Climate2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rolnick</surname><given-names>David</given-names></name>
        <name><surname>Donti</surname><given-names>Priya L</given-names></name>
        <name><surname>Kaack</surname><given-names>Lynn H</given-names></name>
        <name><surname>Kochanski</surname><given-names>Kelly</given-names></name>
        <name><surname>Lacoste</surname><given-names>Alexandre</given-names></name>
        <name><surname>Sankaran</surname><given-names>Kris</given-names></name>
        <name><surname>Ross</surname><given-names>Andrew Slavin</given-names></name>
        <name><surname>Milojevic-Dupont</surname><given-names>Nikola</given-names></name>
        <name><surname>Jaques</surname><given-names>Natasha</given-names></name>
        <name><surname>Waldman-Brown</surname><given-names>Anna</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Tackling climate change with machine learning</article-title>
      <source>ACM Computing Surveys (CSUR)</source>
      <publisher-name>ACM New York, NY</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>55</volume>
      <issue>2</issue>
      <fpage>1</fpage>
      <lpage>96</lpage>
    </element-citation>
  </ref>
  <ref id="ref-CFD2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brunton</surname><given-names>Steven L</given-names></name>
        <name><surname>Noack</surname><given-names>Bernd R</given-names></name>
        <name><surname>Koumoutsakos</surname><given-names>Petros</given-names></name>
      </person-group>
      <article-title>Machine learning for fluid mechanics</article-title>
      <source>Annual review of fluid mechanics</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>52</volume>
      <pub-id pub-id-type="doi">10.1146/annurev-fluid-010719-060214</pub-id>
      <fpage>477</fpage>
      <lpage>508</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Materials2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wei</surname><given-names>Jing</given-names></name>
        <name><surname>Chu</surname><given-names>Xuan</given-names></name>
        <name><surname>Sun</surname><given-names>Xiang-Yu</given-names></name>
        <name><surname>Xu</surname><given-names>Kun</given-names></name>
        <name><surname>Deng</surname><given-names>Hui-Xiong</given-names></name>
        <name><surname>Chen</surname><given-names>Jigen</given-names></name>
        <name><surname>Wei</surname><given-names>Zhongming</given-names></name>
        <name><surname>Lei</surname><given-names>Ming</given-names></name>
      </person-group>
      <article-title>Machine learning in materials science</article-title>
      <source>InfoMat</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>1</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1002/inf2.12028</pub-id>
      <fpage>338</fpage>
      <lpage>358</lpage>
    </element-citation>
  </ref>
  <ref id="ref-CMB2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Petroff</surname><given-names>Matthew A</given-names></name>
        <name><surname>Addison</surname><given-names>Graeme E</given-names></name>
        <name><surname>Bennett</surname><given-names>Charles L</given-names></name>
        <name><surname>Weiland</surname><given-names>Janet L</given-names></name>
      </person-group>
      <article-title>Full-sky cosmic microwave background foreground cleaning using machine learning</article-title>
      <source>The Astrophysical Journal</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>903</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.3847/1538-4357/abb9a7</pub-id>
      <fpage>104</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Galaxy2004">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>De La Calleja</surname><given-names>Jorge</given-names></name>
        <name><surname>Fuentes</surname><given-names>Olac</given-names></name>
      </person-group>
      <article-title>Machine learning and image analysis for morphological galaxy classification</article-title>
      <source>Monthly Notices of the Royal Astronomical Society</source>
      <publisher-name>Blackwell Science Ltd Oxford, UK</publisher-name>
      <year iso-8601-date="2004">2004</year>
      <volume>349</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1111/j.1365-2966.2004.07442.x</pub-id>
      <fpage>87</fpage>
      <lpage>93</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Galaxy2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ntampaka</surname><given-names>Michelle</given-names></name>
        <name><surname>Trac</surname><given-names>Hy</given-names></name>
        <name><surname>Sutherland</surname><given-names>Dougal J</given-names></name>
        <name><surname>Battaglia</surname><given-names>Nicholas</given-names></name>
        <name><surname>Póczos</surname><given-names>Barnabás</given-names></name>
        <name><surname>Schneider</surname><given-names>Jeff</given-names></name>
      </person-group>
      <article-title>A machine learning approach for dynamical mass measurements of galaxy clusters</article-title>
      <source>The Astrophysical Journal</source>
      <publisher-name>IOP Publishing</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>803</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1088/0004-637X/803/2/50</pub-id>
      <fpage>50</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Mineral2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rodriguez-Galiano</surname><given-names>V</given-names></name>
        <name><surname>Sanchez-Castillo</surname><given-names>M</given-names></name>
        <name><surname>Chica-Olmo</surname><given-names>M</given-names></name>
        <name><surname>Chica-Rivas</surname><given-names>MJOGR</given-names></name>
      </person-group>
      <article-title>Machine learning predictive models for mineral prospectivity: An evaluation of neural networks, random forest, regression trees and support vector machines</article-title>
      <source>Ore Geology Reviews</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>71</volume>
      <pub-id pub-id-type="doi">10.1016/j.oregeorev.2015.01.001</pub-id>
      <fpage>804</fpage>
      <lpage>818</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Molecular2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Butler</surname><given-names>Keith T</given-names></name>
        <name><surname>Davies</surname><given-names>Daniel W</given-names></name>
        <name><surname>Cartwright</surname><given-names>Hugh</given-names></name>
        <name><surname>Isayev</surname><given-names>Olexandr</given-names></name>
        <name><surname>Walsh</surname><given-names>Aron</given-names></name>
      </person-group>
      <article-title>Machine learning for molecular and materials science</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>559</volume>
      <issue>7715</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-018-0337-2</pub-id>
      <fpage>547</fpage>
      <lpage>555</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SolverInTheLoop2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Um</surname><given-names>Kiwon</given-names></name>
        <name><surname>Brand</surname><given-names>Robert</given-names></name>
        <name><surname>Fei</surname><given-names>Yun Raymond</given-names></name>
        <name><surname>Holl</surname><given-names>Philipp</given-names></name>
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
      </person-group>
      <article-title>Solver-in-the-loop: Learning from differentiable physics to interact with iterative PDE-solvers</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <fpage>6111</fpage>
      <lpage>6122</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PINN2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Raissi</surname><given-names>Maziar</given-names></name>
        <name><surname>Perdikaris</surname><given-names>Paris</given-names></name>
        <name><surname>Karniadakis</surname><given-names>George E</given-names></name>
      </person-group>
      <article-title>Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</article-title>
      <source>Journal of Computational physics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>378</volume>
      <pub-id pub-id-type="doi">10.1016/j.jcp.2018.10.045</pub-id>
      <fpage>686</fpage>
      <lpage>707</lpage>
    </element-citation>
  </ref>
  <ref id="ref-TensorFlow2016">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Martin</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Chen</surname><given-names>Jianmin</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Tensorflow: A system for large-scale machine learning</article-title>
      <source>12th USENIX symposium on operating systems design and implementation (OSDI 16)</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi"></pub-id>
      <fpage>265</fpage>
      <lpage>283</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PyTorch2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-NumPy2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>Stéfan J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Kerkwijk</surname><given-names>Marten H. van</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>Río</surname><given-names>Jaime Fernández del</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Gérard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <volume>585</volume>
      <issue>7825</issue>
      <uri>https://doi.org/10.1038/s41586-020-2649-2</uri>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Jax2018">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-ScaleInvariant2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Holl</surname><given-names>Philipp</given-names></name>
        <name><surname>Koltun</surname><given-names>Vladlen</given-names></name>
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
      </person-group>
      <article-title>Scale-invariant learning by physics inversion</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>35</volume>
      <fpage>5390</fpage>
      <lpage>5403</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HalfInverse2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schnell</surname><given-names>Patrick</given-names></name>
        <name><surname>Holl</surname><given-names>Philipp</given-names></name>
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
      </person-group>
      <article-title>Half-inverse gradients for physical deep learning</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-PBDL2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
        <name><surname>Holl</surname><given-names>Philipp</given-names></name>
        <name><surname>Mueller</surname><given-names>Maximilian</given-names></name>
        <name><surname>Schnell</surname><given-names>Patrick</given-names></name>
        <name><surname>Trost</surname><given-names>Felix</given-names></name>
        <name><surname>Um</surname><given-names>Kiwon</given-names></name>
      </person-group>
      <article-title>Physics-based deep learning</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2109.05237</uri>
    </element-citation>
  </ref>
  <ref id="ref-PDEBench">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Takamoto</surname><given-names>Makoto</given-names></name>
        <name><surname>Praditia</surname><given-names>Timothy</given-names></name>
        <name><surname>Leiteritz</surname><given-names>Raphael</given-names></name>
        <name><surname>MacKinlay</surname><given-names>Daniel</given-names></name>
        <name><surname>Alesiani</surname><given-names>Francesco</given-names></name>
        <name><surname>Pflüger</surname><given-names>Dirk</given-names></name>
        <name><surname>Niepert</surname><given-names>Mathias</given-names></name>
      </person-group>
      <article-title>PDEBench: An extensive benchmark for scientific machine learning</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>35</volume>
      <fpage>1596</fpage>
      <lpage>1611</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PDEArena">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gupta</surname><given-names>Jayesh K.</given-names></name>
        <name><surname>Brandstetter</surname><given-names>Johannes</given-names></name>
      </person-group>
      <article-title>Towards multi-spatiotemporal-scale generalized PDE modeling</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2209.15616</uri>
    </element-citation>
  </ref>
  <ref id="ref-brandstetter2021message">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Brandstetter</surname><given-names>Johannes</given-names></name>
        <name><surname>Worrall</surname><given-names>Daniel E</given-names></name>
        <name><surname>Welling</surname><given-names>Max</given-names></name>
      </person-group>
      <article-title>Message passing neural PDE solvers</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-wandel2021teaching">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wandel</surname><given-names>Nils</given-names></name>
        <name><surname>Weinmann</surname><given-names>Michael</given-names></name>
        <name><surname>Klein</surname><given-names>Reinhard</given-names></name>
      </person-group>
      <article-title>Teaching the incompressible Navier–Stokes equations to fast neural surrogate models in three dimensions</article-title>
      <source>Physics of Fluids</source>
      <publisher-name>AIP Publishing</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>33</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1063/5.0047428</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-brandstetter2023clifford">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Brandstetter</surname><given-names>Johannes</given-names></name>
        <name><surname>Berg</surname><given-names>Rianne van den</given-names></name>
        <name><surname>Welling</surname><given-names>Max</given-names></name>
        <name><surname>Gupta</surname><given-names>Jayesh K.</given-names></name>
      </person-group>
      <article-title>Clifford neural layers for PDE modeling</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2209.04934</uri>
    </element-citation>
  </ref>
  <ref id="ref-wandel2020learning">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wandel</surname><given-names>Nils</given-names></name>
        <name><surname>Weinmann</surname><given-names>Michael</given-names></name>
        <name><surname>Klein</surname><given-names>Reinhard</given-names></name>
      </person-group>
      <article-title>Learning incompressible fluid dynamics from scratch-towards fast, differentiable fluid models that generalize</article-title>
      <source>International conference on learning representations</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-sengar2021multi">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sengar</surname><given-names>Vartika</given-names></name>
        <name><surname>Seemakurthy</surname><given-names>Karthik</given-names></name>
        <name><surname>Gubbi</surname><given-names>Jayavardhana</given-names></name>
        <name><surname>P</surname><given-names>Balamuralidhar</given-names></name>
      </person-group>
      <article-title>Multi-task learning based approach for surgical video desmoking</article-title>
      <source>Proceedings of the twelfth indian conference on computer vision, graphics and image processing</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1145/3490035.3490283</pub-id>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-parekh1993sex">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Parekh</surname><given-names>NIRANJAN</given-names></name>
        <name><surname>Zou</surname><given-names>AP</given-names></name>
        <name><surname>Jungling</surname><given-names>ILSE</given-names></name>
        <name><surname>Endlich</surname><given-names>KARLHANS</given-names></name>
        <name><surname>Sadowski</surname><given-names>JANUSZ</given-names></name>
        <name><surname>Steinhausen</surname><given-names>MICHAEL</given-names></name>
      </person-group>
      <article-title>Sex differences in control of renal outer medullary circulation in rats: Role of prostaglandins</article-title>
      <source>American Journal of Physiology-Renal Physiology</source>
      <publisher-name>American Physiological Society Bethesda, MD</publisher-name>
      <year iso-8601-date="1993">1993</year>
      <volume>264</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1152/ajprenal.1993.264.4.F629</pub-id>
      <fpage>F629</fpage>
      <lpage>F636</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ramos2022control">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ramos</surname><given-names>Brener</given-names></name>
        <name><surname>Trost</surname><given-names>Felix</given-names></name>
        <name><surname>Thuerey</surname><given-names>Nils</given-names></name>
      </person-group>
      <article-title>Control of two-way coupled fluid systems with differentiable solvers</article-title>
      <source>ICLR 2022 workshop on generalizable policy learning in physical world</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2206.00342</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wang2022approximately">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Rui</given-names></name>
        <name><surname>Walters</surname><given-names>Robin</given-names></name>
        <name><surname>Yu</surname><given-names>Rose</given-names></name>
      </person-group>
      <article-title>Approximately equivariant networks for imperfectly symmetric dynamics</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <fpage>23078</fpage>
      <lpage>23091</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2022meta">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Rui</given-names></name>
        <name><surname>Walters</surname><given-names>Robin</given-names></name>
        <name><surname>Yu</surname><given-names>Rose</given-names></name>
      </person-group>
      <article-title>Meta-learning dynamics forecasting using task inference</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>35</volume>
      <fpage>21640</fpage>
      <lpage>21653</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2023applications">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Peiyan</given-names></name>
      </person-group>
      <article-title>The applications of generative adversarial network in surgical videos</article-title>
      <source>Third international conference on intelligent computing and human-computer interaction (ICHCI 2022)</source>
      <publisher-name>SPIE</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>12509</volume>
      <pub-id pub-id-type="doi">10.1117/12.2656026</pub-id>
      <fpage>300</fpage>
      <lpage>305</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wu2022learning">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Tailin</given-names></name>
        <name><surname>Maruyama</surname><given-names>Takashi</given-names></name>
        <name><surname>Leskovec</surname><given-names>Jure</given-names></name>
      </person-group>
      <article-title>Learning to accelerate partial differential equations via latent global evolution</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2022">2022</year>
      <volume>35</volume>
      <fpage>2240</fpage>
      <lpage>2253</lpage>
    </element-citation>
  </ref>
  <ref id="ref-li2023latent">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Zijie</given-names></name>
        <name><surname>Patil</surname><given-names>Saurabh</given-names></name>
        <name><surname>Shu</surname><given-names>Dule</given-names></name>
        <name><surname>Farimani</surname><given-names>Amir Barati</given-names></name>
      </person-group>
      <article-title>Latent neural PDE solver for time-dependent systems</article-title>
      <source>NeurIPS 2023 AI for science workshop</source>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
