<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3124</article-id>
<article-id pub-id-type="doi">10.21105/joss.03124</article-id>
<title-group>
<article-title>Logodetect: One-shot detection of logos in image and
video data</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Davila-Chacon</surname>
<given-names>Jorge</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pumperla</surname>
<given-names>Max</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Heldenkombinat Technologies GmbH</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>IUBH Internationale Hochschule</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Pathmind Inc.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-02-05">
<day>5</day>
<month>2</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>72</issue>
<fpage>3124</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>object detection</kwd>
<kwd>object recognition</kwd>
<kwd>computer vision</kwd>
<kwd>image processing</kwd>
<kwd>video processing</kwd>
<kwd>one-shot learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>logodetect</monospace> allows the detection of logos in
  images and videos after being trained with just one example of a logo.
  One-shot learning systems are not only great because they require
  little data, but also because they practically remove the need for
  specialists whenever the user wants to extend or re-purpose their
  functionality. This means that the benefits are manifold: the user
  requires little effort to collect and label training data, there is
  practically no time or economic costs for the training procedure, and
  it also provides a strategic benefit for business as they become
  self-sufficient for a large part of the system maintenance.
  <monospace>logodetect</monospace> comes with pretrained models, data
  and an interface that allows users to detect logos in images and
  videos with a single command.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>There is plenty of literature on the use of deep-learning for
  detecting logos, so, additionally to sharing pretrained algorithms
  with the community to get started with one-shot logo detection and a
  simple interface to use such detectors, the aim of
  <monospace>logodetect</monospace> is to provide a flexible
  architecture to facilitate the comparison of different algorithms for
  one-shot object recognition. While
  (<xref alt="Hsieh et al., 2019" rid="ref-NEURIPS2019_92af93f7" ref-type="bibr">Hsieh
  et al., 2019</xref>) published a reference implementation with their
  paper, there are generally few high-quality object recognizer
  architectures available to researchers.</p>
  <p><monospace>logodetect</monospace> works by first performing
  <italic>object detection</italic> on input images and then running
  <italic>object recognition</italic> on detection results. The idea is
  that the user can use a generic detector for a single class of objects
  (e.g. logos, traffic signs or faces) and then compare each of its
  detections with the exemplar, i.e., the sub-class that the user is
  trying to recognize, to determine if both belong to the same sub-class
  (e.g. a concrete brand, a stop sign or the face of a loved one).</p>
</sec>
<sec id="architecture">
  <title>Architecture</title>
  <p>The inference pipeline of <monospace>logodetect</monospace>
  supports architectures with one or two stages. One-stage architectures
  directly perform object recognition on the raw images, and two-stage
  architectures first perform object detection and then object
  recognition in a second stage.</p>
  <p>To get started, we include one <monospace>Detector</monospace>
  based on the Faster-RCNN architecture
  (<xref alt="Ren et al., 2015" rid="ref-NIPS2015_14bfa6bb" ref-type="bibr">Ren
  et al., 2015</xref>) for the object-detection phase, and two
  <monospace>Classifier</monospace>s for the object-recognition
  phase.</p>
  <p>As a baseline for recognition, in the first
  <monospace>Classifier</monospace> provided by
  <monospace>logodetect</monospace> we embed the provided exemplars and
  the detection results from the detection stage into the same latent
  space, and then simply measure the Euclidean or Cosine distance
  between the two embeddings. Both inputs are considered to belong to
  the same sub-class if their distance is below a threshold, determined
  in a preliminary analysis of the training dataset. The properties of
  the embedding can be modified by the user.</p>
  <p>As a first reference against the baseline, in the second of
  <monospace>logodetect</monospace>’s recognizers, we provide a modified
  ResNet
  (<xref alt="He et al., 2016" rid="ref-deep-residual-reco" ref-type="bibr">He
  et al., 2016</xref>) for object-recognition that directly takes the
  exemplars and the detections from the first stage and predicts if both
  belong to the same sub-class. Similarly to
  (<xref alt="Hsieh et al., 2019" rid="ref-NEURIPS2019_92af93f7" ref-type="bibr">Hsieh
  et al., 2019</xref>), this network infers a distance metric after
  being trained with examples of different sub-classes, but instead of
  sharing the same weights and processing each input in a separate pass
  as in
  (<xref alt="Koch et al., 2015" rid="ref-Koch2015SiameseNN" ref-type="bibr">Koch
  et al., 2015</xref>), it concatenates both inputs and processes them
  in one pass. This concept follows more closely the architecture
  proposed by
  (<xref alt="Bhunia et al., 2019" rid="ref-BHUNIA2019106965" ref-type="bibr">Bhunia
  et al., 2019</xref>), where the assumption is that the exemplars often
  have more high-frequency components than the detections, and therefore
  the model can increase its accuracy by learning a separate set of
  weights for each input. However, our proposed architecture splits the
  detection and classification tasks in two separate stages, which
  allows the use, and comparison, of different classifiers for the
  second stage.</p>
  <p>The code also provides functionality to add various
  transformations, so the user has the option to augment each exemplar
  with different transformations if desired. The user simply adds one or
  more exemplars and is good to go.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank <italic>NullConvergence</italic> and all the
  contributors to the open-source framework
  <monospace>mtorch</monospace>
  (<xref alt="NullConvergence, 2018" rid="ref-mtorch" ref-type="bibr">NullConvergence,
  2018</xref>) which serves as the foundation of our training
  pipeline.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-NIPS2015_14bfa6bb">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ren</surname><given-names>Shaoqing</given-names></name>
        <name><surname>He</surname><given-names>Kaiming</given-names></name>
        <name><surname>Girshick</surname><given-names>Ross</given-names></name>
        <name><surname>Sun</surname><given-names>Jian</given-names></name>
      </person-group>
      <article-title>Faster r-CNN: Towards real-time object detection with region proposal networks</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Cortes</surname><given-names>C.</given-names></name>
        <name><surname>Lawrence</surname><given-names>N.</given-names></name>
        <name><surname>Lee</surname><given-names>D.</given-names></name>
        <name><surname>Sugiyama</surname><given-names>M.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <uri>https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf</uri>
      <pub-id pub-id-type="doi">10.1109/tpami.2016.2577031</pub-id>
      <fpage>91</fpage>
      <lpage>99</lpage>
    </element-citation>
  </ref>
  <ref id="ref-deep-residual-reco">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>K.</given-names></name>
        <name><surname>Zhang</surname><given-names>X.</given-names></name>
        <name><surname>Ren</surname><given-names>S.</given-names></name>
        <name><surname>Sun</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Deep residual learning for image recognition</article-title>
      <source>2016 IEEE conference on computer vision and pattern recognition (CVPR)</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id>
      <fpage>770</fpage>
      <lpage>778</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NEURIPS2019_92af93f7">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hsieh</surname><given-names>Ting-I</given-names></name>
        <name><surname>Lo</surname><given-names>Yi-Chen</given-names></name>
        <name><surname>Chen</surname><given-names>Hwann-Tzong</given-names></name>
        <name><surname>Liu</surname><given-names>Tyng-Luh</given-names></name>
      </person-group>
      <article-title>One-shot object detection with co-attention and co-excitation</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Wallach</surname><given-names>H.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>dAlché-Buc</surname><given-names>F.</given-names></name>
        <name><surname>Fox</surname><given-names>E.</given-names></name>
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <uri>https://proceedings.neurips.cc/paper/2019/file/92af93f73faf3cefc129b6bc55a748a9-Paper.pdf</uri>
      <fpage>2725</fpage>
      <lpage>2734</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Koch2015SiameseNN">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Koch</surname><given-names>Gregory</given-names></name>
        <name><surname>Zemel</surname><given-names>Richard</given-names></name>
        <name><surname>Salakhutdinov</surname><given-names>Ruslan</given-names></name>
      </person-group>
      <article-title>Siamese neural networks for one-shot image recognition</article-title>
      <source>Proceedings of the 32nd international conference on machine learning (ICML)</source>
      <publisher-name>W&amp;CP</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>37</volume>
      <uri>https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf</uri>
    </element-citation>
  </ref>
  <ref id="ref-BHUNIA2019106965">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bhunia</surname><given-names>Ayan Kumar</given-names></name>
        <name><surname>Bhunia</surname><given-names>Ankan Kumar</given-names></name>
        <name><surname>Ghose</surname><given-names>Shuvozit</given-names></name>
        <name><surname>Das</surname><given-names>Abhirup</given-names></name>
        <name><surname>Roy</surname><given-names>Partha Pratim</given-names></name>
        <name><surname>Pal</surname><given-names>Umapada</given-names></name>
      </person-group>
      <article-title>A deep one-shot network for query-based logo retrieval</article-title>
      <source>Pattern Recognition</source>
      <year iso-8601-date="2019">2019</year>
      <volume>96</volume>
      <issn>0031-3203</issn>
      <uri>http://www.sciencedirect.com/science/article/pii/S0031320319302626</uri>
      <pub-id pub-id-type="doi">10.1016/j.patcog.2019.106965</pub-id>
      <fpage>106965</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-mtorch">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>NullConvergence</surname></name>
      </person-group>
      <article-title>Mtorch</article-title>
      <publisher-name>Open-source software under MIT License</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://github.com/NullConvergence/mtorch</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
