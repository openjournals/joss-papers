<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20240111T224534-86267b54d8313fd537e467ea647e05069eb61ede</doi_batch_id>
    <timestamp>20240111224534</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2024</year>
        </publication_date>
        <journal_volume>
          <volume>9</volume>
        </journal_volume>
        <issue>93</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>PhysioLabXR: A Python Platform for Real-Time,
Multi-modal, Brain–Computer Interfaces and Extended Reality
Experiments</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Ziheng ‘Leo’</given_name>
            <surname>Li</surname>
            <ORCID>https://orcid.org/0000-0001-5187-200X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Haowen ‘John’</given_name>
            <surname>Wei</surname>
            <ORCID>https://orcid.org/0000-0003-1856-5627</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Ziwen</given_name>
            <surname>Xie</surname>
            <ORCID>https://orcid.org/0009-0006-2304-7591</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Yunxiang</given_name>
            <surname>Peng</surname>
            <ORCID>https://orcid.org/0009-0000-1824-970X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>June Pyo</given_name>
            <surname>Suh</surname>
            <ORCID>https://orcid.org/0009-0005-1211-6101</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Steven</given_name>
            <surname>Feiner</surname>
            <ORCID>https://orcid.org/0000-0001-9978-7090</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Paul</given_name>
            <surname>Sajda</surname>
            <ORCID>https://orcid.org/0000-0002-9738-1342</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>11</day>
          <year>2024</year>
        </publication_date>
        <pages>
          <first_page>5854</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05854</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.10471500</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5854</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05854</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05854</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05854.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="renard2010openvibe">
            <article_title>Openvibe: An open-source software platform to
design, test, and use brain–computer interfaces in real and virtual
environments</article_title>
            <author>Renard</author>
            <journal_title>Presence</journal_title>
            <issue>1</issue>
            <volume>19</volume>
            <doi>10.1162/pres.19.1.35</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Renard, Y., Lotte, F., Gibert, G.,
Congedo, M., Maby, E., Delannoy, V., Bertrand, O., &amp; Lécuyer, A.
(2010). Openvibe: An open-source software platform to design, test, and
use brain–computer interfaces in real and virtual environments.
Presence, 19(1), 35–53.
https://doi.org/10.1162/pres.19.1.35</unstructured_citation>
          </citation>
          <citation key="esch2018mne">
            <article_title>MNE scan: Software for real-time processing
of electrophysiological data</article_title>
            <author>Esch</author>
            <journal_title>Journal of neuroscience
methods</journal_title>
            <volume>303</volume>
            <doi>10.1016/j.jneumeth.2018.03.020</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Esch, L., Sun, L., Klüber, V., Lew,
S., Baumgarten, D., Grant, P. E., Okada, Y., Haueisen, J., Hämäläinen,
M. S., &amp; Dinh, C. (2018). MNE scan: Software for real-time
processing of electrophysiological data. Journal of Neuroscience
Methods, 303, 55–67.
https://doi.org/10.1016/j.jneumeth.2018.03.020</unstructured_citation>
          </citation>
          <citation key="Unity">
            <article_title>Unity</article_title>
            <author>Unity Technologies</author>
            <cYear>2005</cYear>
            <unstructured_citation>Unity Technologies. (2005). Unity.
https://unity.com/</unstructured_citation>
          </citation>
          <citation key="neuropixels">
            <article_title>Neuropixels</article_title>
            <author>Interuniversity Microelectronics Centre</author>
            <cYear>2023</cYear>
            <unstructured_citation>Interuniversity Microelectronics
Centre. (2023). Neuropixels.
https://www.neuropixels.org/</unstructured_citation>
          </citation>
          <citation key="matlab">
            <article_title>MATLAB version: R2021b</article_title>
            <author>MathWorks Inc.</author>
            <cYear>2021</cYear>
            <unstructured_citation>MathWorks Inc. (2021). MATLAB
version: R2021b. The MathWorks Inc.
https://www.mathworks.com</unstructured_citation>
          </citation>
          <citation key="kothe2014labstreaminglayer">
            <article_title>A software framework for synchronizing a
large array of data collection and stimulation devices.</article_title>
            <author>Kothe</author>
            <unstructured_citation>Kothe, C., &amp; Mandel, C. (n.d.). A
software framework for synchronizing a large array of data collection
and stimulation devices.
https://github.com/sccn/labstreaminglayer</unstructured_citation>
          </citation>
          <citation key="rama2010eye">
            <article_title>Eye fixation-related potentials (EFRPs)
during object identification</article_title>
            <author>Rämä</author>
            <journal_title>Visual Neuroscience</journal_title>
            <issue>5-6</issue>
            <volume>27</volume>
            <doi>10.1017/S0952523810000283</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Rämä, P., &amp; Baccino, T. (2010).
Eye fixation-related potentials (EFRPs) during object identification.
Visual Neuroscience, 27(5-6), 187–192.
https://doi.org/10.1017/S0952523810000283</unstructured_citation>
          </citation>
          <citation key="zeromq">
            <article_title>ZeroMQ - the intelligent transport
layer</article_title>
            <author>ZeroMQ</author>
            <cYear>2021</cYear>
            <unstructured_citation>ZeroMQ. (2021). ZeroMQ - the
intelligent transport layer. https://zeromq.org/</unstructured_citation>
          </citation>
          <citation key="peirce2007psychopy">
            <article_title>PsychoPy—psychophysics software in
python</article_title>
            <author>Peirce</author>
            <journal_title>Journal of neuroscience
methods</journal_title>
            <issue>1-2</issue>
            <volume>162</volume>
            <doi>10.1016/j.jneumeth.2006.11.017</doi>
            <cYear>2007</cYear>
            <unstructured_citation>Peirce, J. W. (2007).
PsychoPy—psychophysics software in python. Journal of Neuroscience
Methods, 162(1-2), 8–13.
https://doi.org/10.1016/j.jneumeth.2006.11.017</unstructured_citation>
          </citation>
          <citation key="tobii">
            <article_title>Tobii</article_title>
            <author>Tobii AB</author>
            <cYear>2023</cYear>
            <unstructured_citation>Tobii AB. (2023). Tobii. Tobii AB.
https://www.tobii.com/</unstructured_citation>
          </citation>
          <citation key="nirx">
            <article_title>NIRx</article_title>
            <unstructured_citation>NIRx. (n.d.).
https://www.nirx.net/.</unstructured_citation>
          </citation>
          <citation key="michalareas2022scalable">
            <article_title>A scalable and robust system for audience EEG
recordings</article_title>
            <author>Michalareas</author>
            <journal_title>bioRxiv</journal_title>
            <doi>10.1101/2022.12.16.520764</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Michalareas, G., Rudwan, I. M., Lehr,
C., Gessini, P., Tavano, A., &amp; Grabenhorst, M. (2022). A scalable
and robust system for audience EEG recordings. bioRxiv, 2022–2012.
https://doi.org/10.1101/2022.12.16.520764</unstructured_citation>
          </citation>
          <citation key="macinnes2020pyneal">
            <article_title>Pyneal: Open source real-time fMRI
software</article_title>
            <author>MacInnes</author>
            <journal_title>Frontiers in neuroscience</journal_title>
            <volume>14</volume>
            <doi>10.3389/fnins.2020.00900</doi>
            <cYear>2020</cYear>
            <unstructured_citation>MacInnes, J. J., Adcock, R. A.,
Stocco, A., Prat, C. S., Rao, R. P., &amp; Dickerson, K. C. (2020).
Pyneal: Open source real-time fMRI software. Frontiers in Neuroscience,
14, 900.
https://doi.org/10.3389/fnins.2020.00900</unstructured_citation>
          </citation>
          <citation key="baltruvsaitis2016openface">
            <article_title>Openface: An open source facial behavior
analysis toolkit</article_title>
            <author>Baltrušaitis</author>
            <journal_title>2016 IEEE winter conference on applications
of computer vision (WACV)</journal_title>
            <doi>10.1109/WACV.2016.7477553</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Baltrušaitis, T., Robinson, P., &amp;
Morency, L.-P. (2016). Openface: An open source facial behavior analysis
toolkit. 2016 IEEE Winter Conference on Applications of Computer Vision
(WACV), 1–10.
https://doi.org/10.1109/WACV.2016.7477553</unstructured_citation>
          </citation>
          <citation key="neuropype">
            <article_title>Neuropype</article_title>
            <author>Neuropype</author>
            <cYear>2023</cYear>
            <unstructured_citation>Neuropype. (2023). Neuropype.
https://www.neuropype.io/</unstructured_citation>
          </citation>
          <citation key="iMotion">
            <article_title>iMotion</article_title>
            <author>iMotions</author>
            <cYear>2023</cYear>
            <unstructured_citation>iMotions. (2023). iMotion.
https://imotions.com/</unstructured_citation>
          </citation>
          <citation key="srinath2017python">
            <article_title>Python–the fastest growing programming
language</article_title>
            <author>Srinath</author>
            <journal_title>International Research Journal of Engineering
and Technology</journal_title>
            <issue>12</issue>
            <volume>4</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Srinath, K. (2017). Python–the
fastest growing programming language. International Research Journal of
Engineering and Technology, 4(12), 354–357.</unstructured_citation>
          </citation>
          <citation key="wang2023scoping">
            <article_title>A scoping review of the use of lab streaming
layer framework in virtual and augmented reality
research</article_title>
            <author>Wang</author>
            <journal_title>Virtual Reality</journal_title>
            <doi>10.1007/s10055-023-00799-8</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Wang, Q., Zhang, Q., Sun, W., Boulay,
C., Kim, K., &amp; Barmaki, R. L. (2023). A scoping review of the use of
lab streaming layer framework in virtual and augmented reality research.
Virtual Reality, 1–16.
https://doi.org/10.1007/s10055-023-00799-8</unstructured_citation>
          </citation>
          <citation key="behnel2010cython">
            <article_title>Cython: The best of both
worlds</article_title>
            <author>Behnel</author>
            <journal_title>Computing in Science &amp;
Engineering</journal_title>
            <issue>2</issue>
            <volume>13</volume>
            <doi>10.1109/MCSE.2010.118</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Behnel, S., Bradshaw, R., Citro, C.,
Dalcin, L., Seljebotn, D. S., &amp; Smith, K. (2010). Cython: The best
of both worlds. Computing in Science &amp; Engineering, 13(2), 31–39.
https://doi.org/10.1109/MCSE.2010.118</unstructured_citation>
          </citation>
          <citation key="nikolaev2016combining">
            <article_title>Combining EEG and eye movement recording in
free viewing: Pitfalls and possibilities</article_title>
            <author>Nikolaev</author>
            <journal_title>Brain and cognition</journal_title>
            <volume>107</volume>
            <doi>10.1016/j.bandc.2016.06.004</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Nikolaev, A. R., Meghanathan, R. N.,
&amp; Leeuwen, C. van. (2016). Combining EEG and eye movement recording
in free viewing: Pitfalls and possibilities. Brain and Cognition, 107,
55–83.
https://doi.org/10.1016/j.bandc.2016.06.004</unstructured_citation>
          </citation>
          <citation key="murphy2014pupil">
            <article_title>Pupil diameter covaries with BOLD activity in
human locus coeruleus</article_title>
            <author>Murphy</author>
            <journal_title>Human brain mapping</journal_title>
            <issue>8</issue>
            <volume>35</volume>
            <doi>10.1002/hbm.22466</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Murphy, P. R., O’connell, R. G.,
O’sullivan, M., Robertson, I. H., &amp; Balsters, J. H. (2014). Pupil
diameter covaries with BOLD activity in human locus coeruleus. Human
Brain Mapping, 35(8), 4140–4154.
https://doi.org/10.1002/hbm.22466</unstructured_citation>
          </citation>
          <citation key="koelstra2011deap">
            <article_title>Deap: A database for emotion analysis; using
physiological signals</article_title>
            <author>Koelstra</author>
            <journal_title>IEEE transactions on affective
computing</journal_title>
            <issue>1</issue>
            <volume>3</volume>
            <doi>10.1109/T-AFFC.2011.15</doi>
            <cYear>2011</cYear>
            <unstructured_citation>Koelstra, S., Muhl, C., Soleymani,
M., Lee, J.-S., Yazdani, A., Ebrahimi, T., Pun, T., Nijholt, A., &amp;
Patras, I. (2011). Deap: A database for emotion analysis; using
physiological signals. IEEE Transactions on Affective Computing, 3(1),
18–31. https://doi.org/10.1109/T-AFFC.2011.15</unstructured_citation>
          </citation>
          <citation key="he2020advances">
            <article_title>Advances in multimodal emotion recognition
based on brain–computer interfaces</article_title>
            <author>He</author>
            <journal_title>Brain sciences</journal_title>
            <issue>10</issue>
            <volume>10</volume>
            <doi>10.3390/brainsci10100687</doi>
            <cYear>2020</cYear>
            <unstructured_citation>He, Z., Li, Z., Yang, F., Wang, L.,
Li, J., Zhou, C., &amp; Pan, J. (2020). Advances in multimodal emotion
recognition based on brain–computer interfaces. Brain Sciences, 10(10),
687. https://doi.org/10.3390/brainsci10100687</unstructured_citation>
          </citation>
          <citation key="sollfrank2016effect">
            <article_title>The effect of multimodal and enriched
feedback on SMR-BCI performance</article_title>
            <author>Sollfrank</author>
            <journal_title>Clinical Neurophysiology</journal_title>
            <issue>1</issue>
            <volume>127</volume>
            <doi>10.1016/j.clinph.2015.06.004</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Sollfrank, T., Ramsay, A., Perdikis,
S., Williamson, J., Murray-Smith, R., Leeb, R., Millán, J., &amp;
Kübler, A. (2016). The effect of multimodal and enriched feedback on
SMR-BCI performance. Clinical Neurophysiology, 127(1), 490–498.
https://doi.org/10.1016/j.clinph.2015.06.004</unstructured_citation>
          </citation>
          <citation key="OpenBCI">
            <article_title>OpenBCI</article_title>
            <unstructured_citation>OpenBCI. (n.d.).
https://openbci.com/.</unstructured_citation>
          </citation>
          <citation key="lapborisuth2023pupil">
            <article_title>Pupil-linked arousal modulates network-level
EEG signatures of attention reorienting during immersive
multitasking</article_title>
            <author>Lapborisuth</author>
            <journal_title>Journal of Neural Engineering</journal_title>
            <doi>10.1088/1741-2552/acf1cb</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Lapborisuth, P., Koorathota, S.,
&amp; Sajda, P. (2023). Pupil-linked arousal modulates network-level EEG
signatures of attention reorienting during immersive multitasking.
Journal of Neural Engineering.
https://doi.org/10.1088/1741-2552/acf1cb</unstructured_citation>
          </citation>
          <citation key="koorathota2023multimodal">
            <article_title>Multimodal deep learning systems for analysis
of human behavior, preference, and state</article_title>
            <author>Koorathota</author>
            <cYear>2023</cYear>
            <unstructured_citation>Koorathota, S. C. (2023). Multimodal
deep learning systems for analysis of human behavior, preference, and
state [PhD thesis]. Columbia University.</unstructured_citation>
          </citation>
          <citation key="wei2022indexpen">
            <article_title>IndexPen: Two-finger text input with
millimeter-wave radar</article_title>
            <author>Wei</author>
            <journal_title>Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies</journal_title>
            <issue>2</issue>
            <volume>6</volume>
            <doi>10.1145/3534601</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Wei, H., Li, Z., Galvan, A. D., Su,
Z., Zhang, X., Pahlavan, K., &amp; Solovey, E. T. (2022). IndexPen:
Two-finger text input with millimeter-wave radar. Proceedings of the ACM
on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(2),
1–39. https://doi.org/10.1145/3534601</unstructured_citation>
          </citation>
          <citation key="paszke2019pytorch">
            <article_title>Pytorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>32</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., &amp; others. (2019). Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information
Processing Systems, 32.</unstructured_citation>
          </citation>
          <citation key="pedregosa2011scikit">
            <article_title>Scikit-learn: Machine learning in
python</article_title>
            <author>Pedregosa</author>
            <journal_title>the Journal of machine Learning
research</journal_title>
            <volume>12</volume>
            <cYear>2011</cYear>
            <unstructured_citation>Pedregosa, F., Varoquaux, G.,
Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,
Prettenhofer, P., Weiss, R., Dubourg, V., &amp; others. (2011).
Scikit-learn: Machine learning in python. The Journal of Machine
Learning Research, 12, 2825–2830.</unstructured_citation>
          </citation>
          <citation key="jangraw2014nede">
            <article_title>NEDE: An open-source scripting suite for
developing experiments in 3D virtual environments</article_title>
            <author>Jangraw</author>
            <journal_title>Journal of neuroscience
methods</journal_title>
            <volume>235</volume>
            <doi>10.1016/j.jneumeth.2014.06.033</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Jangraw, D. C., Johri, A., Gribetz,
M., &amp; Sajda, P. (2014). NEDE: An open-source scripting suite for
developing experiments in 3D virtual environments. Journal of
Neuroscience Methods, 235, 245–251.
https://doi.org/10.1016/j.jneumeth.2014.06.033</unstructured_citation>
          </citation>
          <citation key="luhrs2017turbo">
            <article_title>Turbo-satori: A neurofeedback and
brain–computer interface toolbox for real-time functional near-infrared
spectroscopy</article_title>
            <author>Lührs</author>
            <journal_title>Neurophotonics</journal_title>
            <issue>4</issue>
            <volume>4</volume>
            <doi>10.1117/1.NPh.4.4.041504</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Lührs, M., &amp; Goebel, R. (2017).
Turbo-satori: A neurofeedback and brain–computer interface toolbox for
real-time functional near-infrared spectroscopy. Neurophotonics, 4(4),
041504–041504.
https://doi.org/10.1117/1.NPh.4.4.041504</unstructured_citation>
          </citation>
          <citation key="saffaryazdi2022octopus">
            <article_title>Octopus sensing: A python library for human
behavior studies</article_title>
            <author>Saffaryazdi</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>71</issue>
            <volume>7</volume>
            <doi>10.21105/joss.04045</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Saffaryazdi, N., Gharibnavaz, A.,
&amp; Billinghurst, M. (2022). Octopus sensing: A python library for
human behavior studies. Journal of Open Source Software, 7(71), 4045.
https://doi.org/10.21105/joss.04045</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
