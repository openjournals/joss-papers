<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5854</article-id>
<article-id pub-id-type="doi">10.21105/joss.05854</article-id>
<title-group>
<article-title>PhysioLabXR: A Python Platform for Real-Time,
Multi-modal, Brain–Computer Interfaces and Extended Reality
Experiments</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5187-200X</contrib-id>
<name>
<surname>Li</surname>
<given-names>Ziheng ‘Leo’</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1856-5627</contrib-id>
<name>
<surname>Wei</surname>
<given-names>Haowen ‘John’</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-2304-7591</contrib-id>
<name>
<surname>Xie</surname>
<given-names>Ziwen</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-1824-970X</contrib-id>
<name>
<surname>Peng</surname>
<given-names>Yunxiang</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-1211-6101</contrib-id>
<name>
<surname>Suh</surname>
<given-names>June Pyo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9978-7090</contrib-id>
<name>
<surname>Feiner</surname>
<given-names>Steven</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9738-1342</contrib-id>
<name>
<surname>Sajda</surname>
<given-names>Paul</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Columbia University, New York, New York, United States of
America</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-09-08">
<day>8</day>
<month>9</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>93</issue>
<fpage>5854</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>neuroscience</kwd>
<kwd>human–computer interaction</kwd>
<kwd>brain-computer interface</kwd>
<kwd>multi-modality</kwd>
<kwd>virtual augmented reality</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <fig>
    <caption><p>PhysioLabXR includes various visualization methods,
    digital signal processing modules, support for recording and
    replaying experiments, and a scripting interface to deploy custom
    pipelines.<styled-content id="figU003Ateaser"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/physiolabxr%20teaser.png" />
  </fig>
  <p><italic>PhysioLabXR</italic> is a Python-based open-source software
  platform for developing experiments for neuroscience and
  human–computer interaction (HCI) that involve real-time and
  multi-modal physiological data processing and interactive interfaces.
  <italic>PhysioLabXR</italic> provides native support for data sources
  such as electrophysiological sensors (e.g., EEG, EMG, and EOG), fNIRS,
  eye trackers, cameras, microphones, and screen capture, and implements
  the popular data transfer protocols Lab Streaming Layer (LSL;
  <xref alt="Kothe &amp; Mandel, n.d." rid="ref-kothe2014labstreaminglayer" ref-type="bibr">Kothe
  &amp; Mandel, n.d.</xref>) and ZeroMQ (ZMQ;
  <xref alt="ZeroMQ, 2021" rid="ref-zeromq" ref-type="bibr">ZeroMQ,
  2021</xref>). It features multi-stream visualization methods,
  real-time digital signal processing (DSP) modules, support for
  recording and replay experiments, and a Python-based scripting
  interface for creating custom pipelines.</p>
  <p><italic>PhysioLabXR</italic> has an architecture optimized through
  concurrency and parallelism to ensure efficient performance. We
  provide a set of detailed tutorials covering all features and example
  applications, such as a P300 speller with a Unity frontend
  (<xref alt="Unity Technologies, 2005" rid="ref-Unity" ref-type="bibr">Unity
  Technologies, 2005</xref>) and a mental arithmetic experiment
  interfacing with PsychoPy
  (<xref alt="Peirce, 2007" rid="ref-peirce2007psychopy" ref-type="bibr">Peirce,
  2007</xref>). An accompanying set of benchmarks demonstrates the
  ability of <italic>PhysioLabXR</italic> to handle high-throughput and
  multi-stream data reliably and efficiently. Published use cases show
  its versatility for VR and screen-based experiments
  (<xref alt="Koorathota, 2023" rid="ref-koorathota2023multimodal" ref-type="bibr">Koorathota,
  2023</xref>;
  <xref alt="Lapborisuth et al., 2023" rid="ref-lapborisuth2023pupil" ref-type="bibr">Lapborisuth
  et al., 2023</xref>) and sensor fusion studies
  (<xref alt="Wei et al., 2022" rid="ref-wei2022indexpen" ref-type="bibr">Wei
  et al., 2022</xref>) <xref ref-type="fn" rid="fn1">1</xref>. </p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Recent years have seen a growing interest in multi-modal
  experiments, often involving closed-loop interaction systems, in
  neuroscience and human–computer interaction (HCI). Many emerging
  paradigms have found new roots in extended reality (XR) environments,
  including virtual reality (VR) and augmented reality (AR). Such
  experiments are increasingly fusing multiple modalities and combining
  different physiological measurements. For example, one sensor can
  generate events to extract meaningful data intervals from other
  sensors, such as fixation-related potential (FRP) studies in which EEG
  epochs are locked to visual fixations from eye trackers
  (<xref alt="Nikolaev et al., 2016" rid="ref-nikolaev2016combining" ref-type="bibr">Nikolaev
  et al., 2016</xref>). Multiple physiological signals can also be
  combined to enhance their predictive power for use in applications
  ranging from emotion recognition
  (<xref alt="He et al., 2020" rid="ref-he2020advances" ref-type="bibr">He
  et al., 2020</xref>;
  <xref alt="Koelstra et al., 2011" rid="ref-koelstra2011deap" ref-type="bibr">Koelstra
  et al., 2011</xref>) to movement actuation via sensorimotor rhythms
  (<xref alt="Sollfrank et al., 2016" rid="ref-sollfrank2016effect" ref-type="bibr">Sollfrank
  et al., 2016</xref>). Further, multi-modal paradigms can facilitate
  the exploration of how different physiological systems interact; for
  example, pupil dilation can be used as a proxy for the locus coeruleus
  activity as measured via functional magnetic resonance imaging (fMRI;
  <xref alt="Murphy et al., 2014" rid="ref-murphy2014pupil" ref-type="bibr">Murphy
  et al., 2014</xref>).</p>
  <p>Despite the prevalence of these experiments, software tools for
  real-time physiological data handling are surprisingly few and far
  between. They can be categorized into two groups: device-specific
  tools and device-independent tools. Device-specific tools, which are
  typically proprietary, offer data visualization and analysis
  (<xref alt="Lührs &amp; Goebel, 2017" rid="ref-luhrs2017turbo" ref-type="bibr">Lührs
  &amp; Goebel, 2017</xref>;
  <xref alt="NIRx, n.d." rid="ref-nirx" ref-type="bibr"><italic>NIRx</italic>,
  n.d.</xref>;
  <xref alt="Tobii AB, 2023" rid="ref-tobii" ref-type="bibr">Tobii AB,
  2023</xref>) for the hardware to which they are tied. However, they
  often lack support for multi-modal experiments. To address this,
  researchers have created custom data pipelines aided by third-party
  data transfer protocols such as LSL and ZMQ
  (<xref alt="Baltrušaitis et al., 2016" rid="ref-baltruvsaitis2016openface" ref-type="bibr">Baltrušaitis
  et al., 2016</xref>;
  <xref alt="Kothe &amp; Mandel, n.d." rid="ref-kothe2014labstreaminglayer" ref-type="bibr">Kothe
  &amp; Mandel, n.d.</xref>;
  <xref alt="MacInnes et al., 2020" rid="ref-macinnes2020pyneal" ref-type="bibr">MacInnes
  et al., 2020</xref>;
  <xref alt="Michalareas et al., 2022" rid="ref-michalareas2022scalable" ref-type="bibr">Michalareas
  et al., 2022</xref>;
  <xref alt="Wang et al., 2023" rid="ref-wang2023scoping" ref-type="bibr">Wang
  et al., 2023</xref>). This approach is typically time-consuming and
  requires substantial effort to adapt to new experiments. In addition,
  the data transfer middleware typically does not allow researchers to
  visually inspect data streams in real-time. This can be a crucial
  feature for many experiments, particularly those involving devices
  prone to failure and artifacts during operation, such as in EEG and
  fNIRS. Real-time visualization allows experimenters to react promptly
  to sensor failures and prevents wasting valuable participant time.</p>
  <p>Device-independent tools, including popular platforms, such as
  OpenVibe
  (<xref alt="Renard et al., 2010" rid="ref-renard2010openvibe" ref-type="bibr">Renard
  et al., 2010</xref>), MNE Scan
  (<xref alt="Esch et al., 2018" rid="ref-esch2018mne" ref-type="bibr">Esch
  et al., 2018</xref>), NeuroPype
  (<xref alt="Neuropype, 2023" rid="ref-neuropype" ref-type="bibr">Neuropype,
  2023</xref>), and iMotion
  (<xref alt="iMotions, 2023" rid="ref-iMotion" ref-type="bibr">iMotions,
  2023</xref>), support real-time visualization. However, they are
  primarily written in statically compiled languages, limiting
  customization, and some are closed-source commercial products, such as
  NeuroPype and iMotion. Python’s rise in popularity as a programming
  language
  (<xref alt="Srinath, 2017" rid="ref-srinath2017python" ref-type="bibr">Srinath,
  2017</xref>) has made it an obvious choice for developing new
  device-independent tools that allow customization through rapid
  prototyping. However, using Python as a backbone language for
  high-precision and high-throughput data necessitates significant
  optimization to match the performance level of a compiled language.
  Octopus-sensing
  (<xref alt="Saffaryazdi et al., 2022" rid="ref-saffaryazdi2022octopus" ref-type="bibr">Saffaryazdi
  et al., 2022</xref>) is an example of a Python-based platform that
  supports the acquisition and visualization of multi-modal data.</p>
  <p>Nevertheless, there remains a gap for an all-in-one open-source
  platform that supports multi-modal data visualization, and rapid
  prototyping for developing experiment pipelines in complex XR
  environments, while addressing the optimization challenges of basing
  on an interpreted language such as Python.</p>
</sec>
<sec id="benefits">
  <title>Benefits</title>
  <p><italic>PhysioLabXR</italic> is a complete all-in-one GUI
  application for visualizing, recording, and replaying neuroscience and
  HCI experiments, and deploying end-to-end DSP &amp; machine learning
  (ML) pipelines in XR. It offers the following benefits: (1) a
  user-friendly graphical user interface (GUI) for working with both
  physiological and behavioral data; (2) a reliable, robust,
  high-performance backend capable of synchronizing and processing
  multi-modal and high-throughput data in a scalable manner; (3) a
  workflow that streamlines the hitherto time-consuming and challenging
  steps in experiment cycles, including visualizing, recording, and
  analyzing data offline (e.g., to understand physiological phenomena)
  or online (e.g., to provide neurofeedback in a brain-computer
  interface); (4) flexibility and ease of setup as a cross-platform
  solution; and (5) an extensive developer API, which encourages users
  to extend the platform with custom hardware and real-time processing
  scripts.</p>
  <p>Python dominates the implementation from frontend GUI to backend
  servers without sacrificing performance, thanks in part to its
  concurrent runtime architecture. Selected portions, such as real-time
  DSP, are written in Cython
  (<xref alt="Behnel et al., 2010" rid="ref-behnel2010cython" ref-type="bibr">Behnel
  et al., 2010</xref>), a statically compiled language with Python-like
  syntax, to further improve performance. Users can use
  <italic>PhysioLabXR</italic> as a scaffold and leverage Python’s
  extensive APIs to shape the platform according to their needs.</p>
  <p>Users can write Python scripts to interact with any data stream and
  communicate processed results with built-in I/O modules. This
  flexibility allows users to design closed-loop systems, including
  deploying ML models and sending predictions to and from
  <italic>PhysioLabXR</italic>.</p>
  <p><italic>PhysioLabXR</italic> can be used with popular
  stimulus-presentation software such as Unity
  (<xref alt="Unity Technologies, 2005" rid="ref-Unity" ref-type="bibr">Unity
  Technologies, 2005</xref>), PsychoPy
  (<xref alt="Peirce, 2007" rid="ref-peirce2007psychopy" ref-type="bibr">Peirce,
  2007</xref>) and other analysis software, including MATLAB
  (<xref alt="MathWorks Inc., 2021" rid="ref-matlab" ref-type="bibr">MathWorks
  Inc., 2021</xref>). For experiments already utilizing LSL and ZMQ for
  data transfer, the software provides convenient network stream
  connectivity with these two widely-used data middleware. As its name
  implies, <italic>PhysioLabXR</italic> has extensive support for XR,
  including headset-based VR and AR. This builds on our previous work,
  where we developed an environment to support neuroscience experiments
  that utilize Unity and other advanced stimulus paradigms
  (<xref alt="Jangraw et al., 2014" rid="ref-jangraw2014nede" ref-type="bibr">Jangraw
  et al., 2014</xref>).</p>
  <p><italic>PhysioLabXR</italic> adheres to industry-standard software
  development guidelines, including continuous integration. Its modular
  software architecture simplifies the learning curve for users who wish
  to add custom functionality, such as support for new sensors.</p>
</sec>
<sec id="physiolabxr-working-with-streams">
  <title>PhysioLabXR: Working with Streams</title>
  <p>All functionality in <italic>PhysioLabXR</italic> is based on
  <italic>Streams</italic>. A stream is a sequence of data points that
  arrive in real-time, with each frame of data carrying a timestamp,
  whether it is from physiological sensors, video cameras, microphones,
  screen capture, or software-generated data.
  <italic>PhysioLabXR</italic> provides a unified interface for working
  with streams for visualization, recording, replaying, and DSP. Each
  feature addresses different requirements in experiments involving
  real-time data collection and processing. Here, we provide a brief
  overview of these features:</p>
  <list list-type="bullet">
    <list-item>
      <p><italic>Data stream API</italic> establishes a connection with
      data sources, either through native plugins or network protocols
      (LSL or ZMQ).</p>
    </list-item>
    <list-item>
      <p><italic>Visualization</italic> helps users visually inspect
      their data in real-time to understand their data better.</p>
    </list-item>
    <list-item>
      <p><italic>Recording</italic> lets users capture experimental data
      in real-time and export them for further analysis.</p>
    </list-item>
    <list-item>
      <p><italic>Replaying</italic> enables users to play back data
      streams from past experiments and, if needed, test their data
      processing script and algorithm in real-time as if the experiment
      is running live.</p>
    </list-item>
    <list-item>
      <p><italic>DSP</italic> is another powerful feature allowing users
      to apply predefined signal processing algorithms to their data
      streams.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Example use case of <italic>PhysioLabXR</italic> in a
    memory formation and retrieval experiment involving real-time
    processing of pupillometry and fMRI streams. This example
    demonstrates the diverse visualization options provided. In this
    experiment, the participant is asked to navigate a virtual shopping
    mall and respond verbally during their task. (A) The 3D fMRI
    visualizer shows fMRI data streamed in real-time. (B) The
    experimenter uses PhysioLabXR to monitor and record the scene from
    the participant’s first-person view while they perform the task. (C)
    The participant’s speech is captured using a microphone connected to
    the software that visualizes the audio data as a spectrogram. (D)
    Eye movement and pupillometry data are recorded through an eye
    tracker outside the scanner that receives the participant’s eye
    image via a mirror. The time series of the eye-tracking data are
    plotted in a line chart. (E) Simultaneously, an ML model deployed
    through PhysioLabXR’s scripting interface predicts from the fMRI
    data and pupillometry if a target memory is retrieved, with the
    two-class inference result visualized as a bar plot.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/AllPlottingFormat.png" />
  </fig>
  <sec id="scripting-interface">
    <title>Scripting Interface</title>
    <fig>
      <caption><p>Example script setup for a fixation-related potential
      (FRP) experiment. The FixationDetection script (upper right)
      identifies fixations from the eye-tracking stream, while the
      P300Detector script (bottom right) decodes EEG data locked to
      detected fixations, and determines if a target object elicits an
      FRP. This setup is similar to the experiment conducted by Rämä and
      Baccino
      (<xref alt="Rämä &amp; Baccino, 2010" rid="ref-rama2010eye" ref-type="bibr">Rämä
      &amp; Baccino, 2010</xref>). (A) Eye-tracking data is processed by
      the “FixationDetection” script. (B) The fixation results are
      streamed through the output LSL outlet “Fixations.” (C) In the
      <italic>.init</italic> function of P300Detector, the P300
      classifier model is loaded from the file system path in the script
      parameter “model_path.” (D–E) If a fixation is detected, the model
      takes the EEG epoch time-locked to the fixation and makes a
      prediction. A loop call is completed by writing the prediction
      results to the output stream “P300Detect.”</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/scripting%20example.png" />
    </fig>
    <p>The scripting interface allows researchers to build diverse
    experiment paradigms. It enables the execution of user-defined
    Python scripts, empowering users to create and deploy custom data
    processing pipelines. With Python’s versatility and open-source
    libraries encouraging exploration of novel applications such as
    closed-loop neurofeedback, users can train and run ML models in
    real-time, using PyTorch
    (<xref alt="Paszke et al., 2019" rid="ref-paszke2019pytorch" ref-type="bibr">Paszke
    et al., 2019</xref>), scikit-learn
    (<xref alt="Pedregosa et al., 2011" rid="ref-pedregosa2011scikit" ref-type="bibr">Pedregosa
    et al., 2011</xref>), and other libraries. Users can communicate
    results from scripts to external applications using built-in
    networking APIs, including LSL and ZMQ. The script widget offers a
    straightforward way to add and run scripts, adjust attributes that
    influence the data processing pipeline’s behavior, and monitor
    performance. These attributes include defining the streams to use as
    inputs, setting input buffer duration, controlling run frequency,
    creating outputs to visualize pipeline results or communicate with
    other programs, and utilizing exposed parameters that allow variable
    adjustments during runtime. A script in <italic>PhysioLabXR</italic>
    consists of three abstract methods—<monospace>init</monospace>,
    <monospace>loop</monospace>, and
    <monospace>cleanup</monospace>—which users can override to specify
    behavior. Built-in scripts support commonly used algorithms such as
    fixation detection and band-power computation and connection to
    popular devices such as Tobii eye trackers
    (<xref alt="Tobii AB, 2023" rid="ref-tobii" ref-type="bibr">Tobii
    AB, 2023</xref>) and OpenBCI EEG caps
    (<xref alt="OpenBCI, n.d." rid="ref-OpenBCI" ref-type="bibr"><italic>OpenBCI</italic>,
    n.d.</xref>).</p>
    <p>All these features can be used in combination, enhancing the
    overall potential of PhysioLabXR. For example, filters can first be
    applied to EEG data, and the user can visualize the filtered stream
    as it drives a BCI with a custom script.</p>
  </sec>
  <sec id="software-design-principles">
    <title>Software Design Principles</title>
    <p>Creating an efficient runtime experience is a significant
    challenge for large-scale Python software when dealing with
    high-throughput data, complex graphics, and frequent I/O operation
    from serialization, given the interpreted nature of the language. To
    make this possible, <italic>PhysioLabXR</italic> is optimized
    through a combination of concurrency, parallelism, and a modular
    software architecture.</p>
    <fig>
      <caption><p>Sequence diagram showing the information exchange
      between <italic>PhysioLabXR</italic>’s threads and processes. The
      main process contains concurrent threads of the main GUI
      controller, serialization, data worker, and visualization. When
      the user adds a new data source, the GUI thread forks a data
      worker and a visualization thread. More demanding operations run
      on separate server processes, including replay and scripting. User
      commands such as <italic>StartStreaming</italic>,
      <italic>StartReplay</italic>, and <italic>StopRecording</italic>
      are passed from the main GUI to the corresponding threads or
      processes.</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="media/PhysioLabXR%20Sequence%20Diagram.png" />
    </fig>
    <p>The architecture of <italic>PhysioLabXR</italic> follows rigorous
    software design patterns, minimizing maintenance efforts while
    maximizing scalability. Unit testing is at the core of our
    development process; each software feature, backend, and GUI
    frontend, is tested on both functionality and performance. The
    software is built using continuous integration: each commit to the
    main branch triggers a full test routine to ensure
    compatibility.</p>
  </sec>
</sec>
<sec id="limitation-and-future-scope">
  <title>Limitation and Future Scope</title>
  <p><italic>PhysioLabXR</italic> aims to be a versatile platform for
  real-time experiments, primarily for, but not limited to, HCI and
  neuroscience. It is designed to be a community-driven project, with
  our core team of developers maintaining architectural integrity and
  functional correctness. At the same time, we welcome contributions
  from researchers and practitioners in related fields to build on this
  scaffolding and expand its capabilities. While the stream interface
  supports data sent through LSL or ZMQ, we are currently developing
  native plugins for sensors that lack network support. These plugins
  will enable the use of certain brands of fMRI, TMS (transcranial
  magnetic stimulation), and invasive neuroimaging devices (e.g.,
  Neuropixels,
  <xref alt="Interuniversity Microelectronics Centre, 2023" rid="ref-neuropixels" ref-type="bibr">Interuniversity
  Microelectronics Centre, 2023</xref>). We are also adding real-time
  analysis and processing modules for more modalities, such as real-time
  source localization for EEG and speech recognition for audio.
  Moreover, the current scripting interface is designed to provide
  maximum flexibility, thus requiring users to write Python code for
  their pipelines. In coming releases, we plan to make the scripting
  features more accessible to users with less programming experience by
  providing a visual programming interface and code generation.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p><italic>PhysioLabXR</italic> is a pioneering tool in the era of
  spatial and physiological computing, addressing the increasing demand
  for a comprehensive software platform that drives multi-modal data
  integration and close-loop interaction. <italic>PhysioLabXR</italic>
  offers an array of interconnected functionalities, including
  visualization, recording, replay, real-time DSP modules, and a
  scripting interface. These all empower researchers and practitioners
  to explore novel experiment paradigms and design intricate feedback
  loops. With its Python-based frontend and C++-powered backend, the
  framework is built with scalability in mind while having optimized
  performance. The continued development of <italic>PhysioLabXR</italic>
  will be driven by the community and aims to fuel research insights
  into the intersection between the brain, behavior, and human-computer
  interaction.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>This project was partly funded by the Columbia/ARL Human-Guided
  Intelligent Systems (HGIS) Program ( W911NF-23-2-0067), a Vannevar
  Bush Faculty Fellowship from the US Department of Defense
  (N00014-20-1-2027) and a Center of Excellence grant from the Air Force
  Office of Scientific Research (FA9550-22-1-0337). This work was also
  supported by the Army Research Laboratory STRONG Program
  (W911NF-19-2-0135). We would like to express our gratitude for the
  support from our colleagues at the Laboratory for Intelligent Imaging
  and Neural Computing (LIINC), the Computer Graphics and User
  Interfaces (CGUI) Lab and the Artificial Intelligence for Vision
  Science (AI4VS) Lab at Columbia University, and Human-Computer
  Interaction (HCI) Lab at Worcester Polytechnic Institute. We would
  also like to thank all the community members who have contributed to
  <italic>PhysioLabXR</italic>.</p>
</sec>
<sec id="licensing-and-availability">
  <title>Licensing and Availability</title>
  <p><italic>PhysioLabXR</italic> is an open-source project distributed
  under the BSD 3-Clause License. Researchers are welcome to modify the
  software to meet their specific needs and share their modifications
  with the community. We provide the following links to help access
  related resources:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Website:</bold> The official <italic>PhysioLabXR</italic>
      website serves as a central hub for its information and updates.
      It can be accessed at
      <ext-link ext-link-type="uri" xlink:href="https://www.physiolabxr.org">https://www.physiolabxr.org</ext-link>.</p>
    </list-item>
    <list-item>
      <p><bold>Documentation:</bold> The documentation includes
      tutorials for its features, tutorials, example use cases, and
      developer guides. It is hosted at
      <ext-link ext-link-type="uri" xlink:href="https://physiolabxrdocs.readthedocs.io/en/latest/index.html">https://physiolabxrdocs.readthedocs.io/en/latest/index.html</ext-link>.
      For testing and demonstration purposes, the documentation links to
      (otherwise unpublished) example recordings performed on co-authors
      of this manuscript.</p>
    </list-item>
    <list-item>
      <p><bold>GitHub Repository:</bold> Users can access the repository
      at
      <ext-link ext-link-type="uri" xlink:href="https://github.com/PhysioLabXR/PhysioLabXR">https://github.com/PhysioLabXR/PhysioLabXR</ext-link>.
      Users can submit bug reports and feature requests through the
      GitHub issue tracker.</p>
    </list-item>
  </list>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-renard2010openvibe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Renard</surname><given-names>Yann</given-names></name>
        <name><surname>Lotte</surname><given-names>Fabien</given-names></name>
        <name><surname>Gibert</surname><given-names>Guillaume</given-names></name>
        <name><surname>Congedo</surname><given-names>Marco</given-names></name>
        <name><surname>Maby</surname><given-names>Emmanuel</given-names></name>
        <name><surname>Delannoy</surname><given-names>Vincent</given-names></name>
        <name><surname>Bertrand</surname><given-names>Olivier</given-names></name>
        <name><surname>Lécuyer</surname><given-names>Anatole</given-names></name>
      </person-group>
      <article-title>Openvibe: An open-source software platform to design, test, and use brain–computer interfaces in real and virtual environments</article-title>
      <source>Presence</source>
      <publisher-name>MIT press</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>19</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1162/pres.19.1.35</pub-id>
      <fpage>35</fpage>
      <lpage>53</lpage>
    </element-citation>
  </ref>
  <ref id="ref-esch2018mne">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Esch</surname><given-names>Lorenz</given-names></name>
        <name><surname>Sun</surname><given-names>Limin</given-names></name>
        <name><surname>Klüber</surname><given-names>Viktor</given-names></name>
        <name><surname>Lew</surname><given-names>Seok</given-names></name>
        <name><surname>Baumgarten</surname><given-names>Daniel</given-names></name>
        <name><surname>Grant</surname><given-names>P Ellen</given-names></name>
        <name><surname>Okada</surname><given-names>Yoshio</given-names></name>
        <name><surname>Haueisen</surname><given-names>Jens</given-names></name>
        <name><surname>Hämäläinen</surname><given-names>Matti S</given-names></name>
        <name><surname>Dinh</surname><given-names>Christoph</given-names></name>
      </person-group>
      <article-title>MNE scan: Software for real-time processing of electrophysiological data</article-title>
      <source>Journal of neuroscience methods</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>303</volume>
      <pub-id pub-id-type="doi">10.1016/j.jneumeth.2018.03.020</pub-id>
      <fpage>55</fpage>
      <lpage>67</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Unity">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Unity Technologies</string-name>
      </person-group>
      <article-title>Unity</article-title>
      <year iso-8601-date="2005">2005</year>
      <uri>https://unity.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-neuropixels">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Interuniversity Microelectronics Centre</string-name>
      </person-group>
      <article-title>Neuropixels</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.neuropixels.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-matlab">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <string-name>MathWorks Inc.</string-name>
      </person-group>
      <article-title>MATLAB version: R2021b</article-title>
      <publisher-name>The MathWorks Inc.</publisher-name>
      <publisher-loc>Natick, Massachusetts, United States</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <uri>https://www.mathworks.com</uri>
    </element-citation>
  </ref>
  <ref id="ref-kothe2014labstreaminglayer">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Kothe</surname><given-names>Christian</given-names></name>
        <name><surname>Mandel</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>A software framework for synchronizing a large array of data collection and stimulation devices.</article-title>
      <uri>https://github.com/sccn/labstreaminglayer</uri>
    </element-citation>
  </ref>
  <ref id="ref-rama2010eye">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rämä</surname><given-names>PIA</given-names></name>
        <name><surname>Baccino</surname><given-names>Thierry</given-names></name>
      </person-group>
      <article-title>Eye fixation-related potentials (EFRPs) during object identification</article-title>
      <source>Visual Neuroscience</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>27</volume>
      <issue>5-6</issue>
      <pub-id pub-id-type="doi">10.1017/S0952523810000283</pub-id>
      <fpage>187</fpage>
      <lpage>192</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zeromq">
    <element-citation publication-type="webpage">
      <person-group person-group-type="author">
        <name><surname>ZeroMQ</surname></name>
      </person-group>
      <article-title>ZeroMQ - the intelligent transport layer</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://zeromq.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-peirce2007psychopy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peirce</surname><given-names>Jonathan W</given-names></name>
      </person-group>
      <article-title>PsychoPy—psychophysics software in python</article-title>
      <source>Journal of neuroscience methods</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>162</volume>
      <issue>1-2</issue>
      <pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id>
      <fpage>8</fpage>
      <lpage>13</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tobii">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Tobii AB</string-name>
      </person-group>
      <article-title>Tobii</article-title>
      <publisher-name>Tobii AB</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.tobii.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-nirx">
    <element-citation>
      <article-title>NIRx</article-title>
      <publisher-name>https://www.nirx.net/</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-michalareas2022scalable">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Michalareas</surname><given-names>Georgios</given-names></name>
        <name><surname>Rudwan</surname><given-names>Ismat MA</given-names></name>
        <name><surname>Lehr</surname><given-names>Claudia</given-names></name>
        <name><surname>Gessini</surname><given-names>Paolo</given-names></name>
        <name><surname>Tavano</surname><given-names>Alessandro</given-names></name>
        <name><surname>Grabenhorst</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>A scalable and robust system for audience EEG recordings</article-title>
      <source>bioRxiv</source>
      <publisher-name>Cold Spring Harbor Laboratory</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1101/2022.12.16.520764</pub-id>
      <fpage>2022</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-macinnes2020pyneal">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>MacInnes</surname><given-names>Jeff J</given-names></name>
        <name><surname>Adcock</surname><given-names>R Alison</given-names></name>
        <name><surname>Stocco</surname><given-names>Andrea</given-names></name>
        <name><surname>Prat</surname><given-names>Chantel S</given-names></name>
        <name><surname>Rao</surname><given-names>Rajesh PN</given-names></name>
        <name><surname>Dickerson</surname><given-names>Kathryn C</given-names></name>
      </person-group>
      <article-title>Pyneal: Open source real-time fMRI software</article-title>
      <source>Frontiers in neuroscience</source>
      <publisher-name>Frontiers Media SA</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>14</volume>
      <pub-id pub-id-type="doi">10.3389/fnins.2020.00900</pub-id>
      <fpage>900</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-baltruvsaitis2016openface">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Baltrušaitis</surname><given-names>Tadas</given-names></name>
        <name><surname>Robinson</surname><given-names>Peter</given-names></name>
        <name><surname>Morency</surname><given-names>Louis-Philippe</given-names></name>
      </person-group>
      <article-title>Openface: An open source facial behavior analysis toolkit</article-title>
      <source>2016 IEEE winter conference on applications of computer vision (WACV)</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1109/WACV.2016.7477553</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-neuropype">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Neuropype</surname></name>
      </person-group>
      <article-title>Neuropype</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.neuropype.io/</uri>
    </element-citation>
  </ref>
  <ref id="ref-iMotion">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>iMotions</surname></name>
      </person-group>
      <article-title>iMotion</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://imotions.com/</uri>
    </element-citation>
  </ref>
  <ref id="ref-srinath2017python">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Srinath</surname><given-names>KR</given-names></name>
      </person-group>
      <article-title>Python–the fastest growing programming language</article-title>
      <source>International Research Journal of Engineering and Technology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>4</volume>
      <issue>12</issue>
      <fpage>354</fpage>
      <lpage>357</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2023scoping">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Qile</given-names></name>
        <name><surname>Zhang</surname><given-names>Qinqi</given-names></name>
        <name><surname>Sun</surname><given-names>Weitong</given-names></name>
        <name><surname>Boulay</surname><given-names>Chadwick</given-names></name>
        <name><surname>Kim</surname><given-names>Kangsoo</given-names></name>
        <name><surname>Barmaki</surname><given-names>Roghayeh Leila</given-names></name>
      </person-group>
      <article-title>A scoping review of the use of lab streaming layer framework in virtual and augmented reality research</article-title>
      <source>Virtual Reality</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1007/s10055-023-00799-8</pub-id>
      <fpage>1</fpage>
      <lpage>16</lpage>
    </element-citation>
  </ref>
  <ref id="ref-behnel2010cython">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Behnel</surname><given-names>Stefan</given-names></name>
        <name><surname>Bradshaw</surname><given-names>Robert</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Dalcin</surname><given-names>Lisandro</given-names></name>
        <name><surname>Seljebotn</surname><given-names>Dag Sverre</given-names></name>
        <name><surname>Smith</surname><given-names>Kurt</given-names></name>
      </person-group>
      <article-title>Cython: The best of both worlds</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>13</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id>
      <fpage>31</fpage>
      <lpage>39</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nikolaev2016combining">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nikolaev</surname><given-names>Andrey R</given-names></name>
        <name><surname>Meghanathan</surname><given-names>Radha Nila</given-names></name>
        <name><surname>Leeuwen</surname><given-names>Cees van</given-names></name>
      </person-group>
      <article-title>Combining EEG and eye movement recording in free viewing: Pitfalls and possibilities</article-title>
      <source>Brain and cognition</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>107</volume>
      <pub-id pub-id-type="doi">10.1016/j.bandc.2016.06.004</pub-id>
      <fpage>55</fpage>
      <lpage>83</lpage>
    </element-citation>
  </ref>
  <ref id="ref-murphy2014pupil">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Murphy</surname><given-names>Peter R</given-names></name>
        <name><surname>O’connell</surname><given-names>Redmond G</given-names></name>
        <name><surname>O’sullivan</surname><given-names>Michael</given-names></name>
        <name><surname>Robertson</surname><given-names>Ian H</given-names></name>
        <name><surname>Balsters</surname><given-names>Joshua H</given-names></name>
      </person-group>
      <article-title>Pupil diameter covaries with BOLD activity in human locus coeruleus</article-title>
      <source>Human brain mapping</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>35</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1002/hbm.22466</pub-id>
      <fpage>4140</fpage>
      <lpage>4154</lpage>
    </element-citation>
  </ref>
  <ref id="ref-koelstra2011deap">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Koelstra</surname><given-names>Sander</given-names></name>
        <name><surname>Muhl</surname><given-names>Christian</given-names></name>
        <name><surname>Soleymani</surname><given-names>Mohammad</given-names></name>
        <name><surname>Lee</surname><given-names>Jong-Seok</given-names></name>
        <name><surname>Yazdani</surname><given-names>Ashkan</given-names></name>
        <name><surname>Ebrahimi</surname><given-names>Touradj</given-names></name>
        <name><surname>Pun</surname><given-names>Thierry</given-names></name>
        <name><surname>Nijholt</surname><given-names>Anton</given-names></name>
        <name><surname>Patras</surname><given-names>Ioannis</given-names></name>
      </person-group>
      <article-title>Deap: A database for emotion analysis; using physiological signals</article-title>
      <source>IEEE transactions on affective computing</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <volume>3</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id>
      <fpage>18</fpage>
      <lpage>31</lpage>
    </element-citation>
  </ref>
  <ref id="ref-he2020advances">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>He</surname><given-names>Zhipeng</given-names></name>
        <name><surname>Li</surname><given-names>Zina</given-names></name>
        <name><surname>Yang</surname><given-names>Fuzhou</given-names></name>
        <name><surname>Wang</surname><given-names>Lei</given-names></name>
        <name><surname>Li</surname><given-names>Jingcong</given-names></name>
        <name><surname>Zhou</surname><given-names>Chengju</given-names></name>
        <name><surname>Pan</surname><given-names>Jiahui</given-names></name>
      </person-group>
      <article-title>Advances in multimodal emotion recognition based on brain–computer interfaces</article-title>
      <source>Brain sciences</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>10</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.3390/brainsci10100687</pub-id>
      <fpage>687</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-sollfrank2016effect">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sollfrank</surname><given-names>T</given-names></name>
        <name><surname>Ramsay</surname><given-names>A</given-names></name>
        <name><surname>Perdikis</surname><given-names>S</given-names></name>
        <name><surname>Williamson</surname><given-names>J</given-names></name>
        <name><surname>Murray-Smith</surname><given-names>R</given-names></name>
        <name><surname>Leeb</surname><given-names>R</given-names></name>
        <name><surname>Millán</surname><given-names>JDR</given-names></name>
        <name><surname>Kübler</surname><given-names>A</given-names></name>
      </person-group>
      <article-title>The effect of multimodal and enriched feedback on SMR-BCI performance</article-title>
      <source>Clinical Neurophysiology</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>127</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1016/j.clinph.2015.06.004</pub-id>
      <fpage>490</fpage>
      <lpage>498</lpage>
    </element-citation>
  </ref>
  <ref id="ref-OpenBCI">
    <element-citation>
      <article-title>OpenBCI</article-title>
      <publisher-name>https://openbci.com/</publisher-name>
    </element-citation>
  </ref>
  <ref id="ref-lapborisuth2023pupil">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lapborisuth</surname><given-names>Pawan</given-names></name>
        <name><surname>Koorathota</surname><given-names>Sharath</given-names></name>
        <name><surname>Sajda</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>Pupil-linked arousal modulates network-level EEG signatures of attention reorienting during immersive multitasking</article-title>
      <source>Journal of Neural Engineering</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1088/1741-2552/acf1cb</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-koorathota2023multimodal">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Koorathota</surname><given-names>Sharath Chandra</given-names></name>
      </person-group>
      <article-title>Multimodal deep learning systems for analysis of human behavior, preference, and state</article-title>
      <publisher-name>Columbia University</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-wei2022indexpen">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wei</surname><given-names>Haowen</given-names></name>
        <name><surname>Li</surname><given-names>Ziheng</given-names></name>
        <name><surname>Galvan</surname><given-names>Alexander D</given-names></name>
        <name><surname>Su</surname><given-names>Zhuoran</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiao</given-names></name>
        <name><surname>Pahlavan</surname><given-names>Kaveh</given-names></name>
        <name><surname>Solovey</surname><given-names>Erin T</given-names></name>
      </person-group>
      <article-title>IndexPen: Two-finger text input with millimeter-wave radar</article-title>
      <source>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</source>
      <publisher-name>ACM New York, NY, USA</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>6</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1145/3534601</pub-id>
      <fpage>1</fpage>
      <lpage>39</lpage>
    </element-citation>
  </ref>
  <ref id="ref-paszke2019pytorch">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
    </element-citation>
  </ref>
  <ref id="ref-pedregosa2011scikit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Michel</surname><given-names>Vincent</given-names></name>
        <name><surname>Thirion</surname><given-names>Bertrand</given-names></name>
        <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
        <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
        <name><surname>Weiss</surname><given-names>Ron</given-names></name>
        <name><surname>Dubourg</surname><given-names>Vincent</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in python</article-title>
      <source>the Journal of machine Learning research</source>
      <publisher-name>JMLR. org</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jangraw2014nede">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jangraw</surname><given-names>David C</given-names></name>
        <name><surname>Johri</surname><given-names>Ansh</given-names></name>
        <name><surname>Gribetz</surname><given-names>Meron</given-names></name>
        <name><surname>Sajda</surname><given-names>Paul</given-names></name>
      </person-group>
      <article-title>NEDE: An open-source scripting suite for developing experiments in 3D virtual environments</article-title>
      <source>Journal of neuroscience methods</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>235</volume>
      <pub-id pub-id-type="doi">10.1016/j.jneumeth.2014.06.033</pub-id>
      <fpage>245</fpage>
      <lpage>251</lpage>
    </element-citation>
  </ref>
  <ref id="ref-luhrs2017turbo">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lührs</surname><given-names>Michael</given-names></name>
        <name><surname>Goebel</surname><given-names>Rainer</given-names></name>
      </person-group>
      <article-title>Turbo-satori: A neurofeedback and brain–computer interface toolbox for real-time functional near-infrared spectroscopy</article-title>
      <source>Neurophotonics</source>
      <publisher-name>Society of Photo-Optical Instrumentation Engineers</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>4</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1117/1.NPh.4.4.041504</pub-id>
      <fpage>041504</fpage>
      <lpage>041504</lpage>
    </element-citation>
  </ref>
  <ref id="ref-saffaryazdi2022octopus">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Saffaryazdi</surname><given-names>Nastaran</given-names></name>
        <name><surname>Gharibnavaz</surname><given-names>Aidin</given-names></name>
        <name><surname>Billinghurst</surname><given-names>Mark</given-names></name>
      </person-group>
      <article-title>Octopus sensing: A python library for human behavior studies</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2022">2022</year>
      <volume>7</volume>
      <issue>71</issue>
      <pub-id pub-id-type="doi">10.21105/joss.04045</pub-id>
      <fpage>4045</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><italic>PhysioLabXR</italic> was formerly called
    <italic>RealityNavigation</italic>, and <italic>RNApp</italic> in
    older publications.</p>
  </fn>
</fn-group>
</back>
</article>
