<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9415</article-id>
<article-id pub-id-type="doi">10.21105/joss.09415</article-id>
<title-group>
<article-title>torch_blue: A Flexible Python Package for Bayesian Neural
Networks in PyTorch</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2684-0927</contrib-id>
<name>
<surname>Weyrauch</surname>
<given-names>Arvid</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7949-1858</contrib-id>
<name>
<surname>Heyen</surname>
<given-names>Lars H.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8439-7145</contrib-id>
<name>
<surname>Muriedas</surname>
<given-names>Juan Pedro Guti√©rrez Hermosillo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-5440-4211</contrib-id>
<name>
<surname>Hsia</surname>
<given-names>Pei-Hsuan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0003-2808-4744</contrib-id>
<name>
<surname>√ñzdemir</surname>
<given-names>Asena Karolin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5065-469X</contrib-id>
<name>
<surname>Streit</surname>
<given-names>Achim</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2233-1041</contrib-id>
<name>
<surname>G√∂tz</surname>
<given-names>Markus</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7156-2022</contrib-id>
<name>
<surname>Debus</surname>
<given-names>Charlotte</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Karlsruhe Institute of Technology, Germany</institution>
<institution-id institution-id-type="ROR">04t3en479</institution-id>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Helmholtz AI</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2026-01-16">
<day>16</day>
<month>1</month>
<year>2026</year>
</pub-date>
<volume>11</volume>
<issue>117</issue>
<fpage>9415</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bayesian Neural Networks</kwd>
<kwd>Variational Inference</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Bayesian Neural Networks (BNN) integrate uncertainty quantification
  in all steps of the training and prediction process, thereby enabling
  better-informed decisions
  (<xref alt="Arbel et al., 2023" rid="ref-arbel2023primer" ref-type="bibr">Arbel
  et al., 2023</xref>). Among the different approaches to implementing
  BNNs, Variational Inference (VI)
  (<xref alt="Hoffman et al., 2013" rid="ref-hoffman2013svi" ref-type="bibr">Hoffman
  et al., 2013</xref>) specifically strikes a balance between the
  ability to consider a large variety of distributions while maintaining
  low enough compute requirements to allow scaling to larger models.</p>
  <p>However, setting up and training BNNs is quite complicated, and
  existing libraries all either lack flexibility, lack scalability, or
  tackle Bayesian computation in general, adding even more complexity
  and therefore a huge barrier to entry. Moreover, no existing framework
  directly supports straightforward BNN model prototyping by offering
  pre-programmed Bayesian network layer types, similar to PyTorch‚Äôs
  <monospace>nn</monospace> module. This forces any BNNs to be
  implemented from scratch, which can be challenging even for
  non-Bayesian networks.</p>
  <p><monospace>torch_blue</monospace> addresses this by providing an
  interface that is almost identical to the widely used PyTorch
  (<xref alt="Ansel et al., 2024" rid="ref-ansel2024pytorch" ref-type="bibr">Ansel
  et al., 2024</xref>) for basic use, providing a low barrier to entry,
  as well as an advanced interface designed for exploration and
  research. Overall, this allows users to set up models and even custom
  layers without worrying about the Bayesian intricacies under the
  hood.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>To represent uncertainty, BNNs do not consider their weights as
  point values, but random variables, i.e., distributions. The
  optimization goal becomes adapting the weight distributions to
  minimize their distance to the true distribution. This requires two
  assumptions. For one, the distance between distributions needs to be
  defined, for which the Kullback-Leibler divergence
  (<xref alt="Kullback &amp; Leibler, 1951" rid="ref-kullback1951information" ref-type="bibr">Kullback
  &amp; Leibler, 1951</xref>) is typically used. Secondly, optimizing an
  object as complex as a distribution is a non-trivial task. To overcome
  this, VI specifies a parametrized distribution and optimizes its
  parameters. Thus, the Kullback-Leibler criterion can be simplified to
  the ELBO (<bold>E</bold>vidence <bold>L</bold>ower <bold>BO</bold>und)
  loss
  (<xref alt="Jordan et al., 1999" rid="ref-jordan1999introduction" ref-type="bibr">Jordan
  et al., 1999</xref>): <disp-formula><alternatives>
  <tex-math><![CDATA[\mathrm{ELBO} = \mathbb{E}_{W\sim q}[\underbrace{\log p(Y|X, W)}_\mathrm{Data~fitting} - \underbrace{(\log q(W|\lambda) - \log p(W))}_\mathrm{Prior~matching}] \quad ,]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">B</mml:mi><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>ùîº</mml:mi><mml:mrow><mml:mi>W</mml:mi><mml:mo>‚àº</mml:mo><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false" form="prefix">[</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mi>log</mml:mi><mml:mo>&#8289;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false" form="prefix">(</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false" form="postfix">)</mml:mo></mml:mrow><mml:mo accent="true">‚èü</mml:mo></mml:munder><mml:mrow><mml:mi mathvariant="normal">D</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:munder><mml:mo>‚àí</mml:mo><mml:munder><mml:munder><mml:mrow><mml:mo stretchy="false" form="prefix">(</mml:mo><mml:mi>log</mml:mi><mml:mo>&#8289;</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>Œª</mml:mi><mml:mo stretchy="false" form="postfix">)</mml:mo><mml:mo>‚àí</mml:mo><mml:mi>log</mml:mi><mml:mo>&#8289;</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false" form="prefix">(</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false" form="postfix">)</mml:mo><mml:mo stretchy="false" form="postfix">)</mml:mo></mml:mrow><mml:mo accent="true">‚èü</mml:mo></mml:munder><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="0.222em"></mml:mspace><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow></mml:munder><mml:mo stretchy="false" form="postfix">]</mml:mo><mml:mspace width="1.0em"></mml:mspace><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[(X, Y)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="false" form="prefix">(</mml:mo><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>Y</mml:mi><mml:mo stretchy="false" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  are the training inputs and labels, <inline-formula><alternatives>
  <tex-math><![CDATA[W]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>W</mml:mi></mml:math></alternatives></inline-formula>
  the network weights, <inline-formula><alternatives>
  <tex-math><![CDATA[q]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>q</mml:mi></mml:math></alternatives></inline-formula>
  the variational distribution and <inline-formula><alternatives>
  <tex-math><![CDATA[\lambda]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œª</mml:mi></mml:math></alternatives></inline-formula>
  its current best fit parameters.</p>
  <p>While interest in uncertainty quantification and BNNs has been
  growing, support for users with little to no experience in Bayesian
  statistics is still limited. Probabilistic programming languages, such
  as Pyro
  (<xref alt="Bingham et al., 2019" rid="ref-bingham2019pyro" ref-type="bibr">Bingham
  et al., 2019</xref>) and Stan
  (<xref alt="Stan Development Team, 2025" rid="ref-stan2025stan" ref-type="bibr">Stan
  Development Team, 2025</xref>), are very powerful and versatile,
  allowing the implementation of many approaches beyond VI. However,
  their interfaces are structured around Bayesian concepts ‚Äì like plate
  notation ‚Äì which will be unfamiliar to many primary machine learning
  users.</p>
  <fig>
    <caption><p>Code example of a three-layer Bayesian MLP with
    cross-entropy loss in <monospace>torch_blue</monospace>. The
    highlight colors relate user-facing components to their position in
    <xref alt="[design_graph]" rid="design_graph">[design_graph]</xref>.
    <styled-content id="code"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="code_example.png" />
  </fig>
  <fig>
    <caption><p>Design graph of <monospace>torch_blue</monospace>.
    Colored highlights correspond to their practical applications in the
    code example (<xref alt="[code]" rid="code">[code]</xref>).
    <styled-content id="design_graph"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="design_graph.png" />
  </fig>
  <p><monospace>torch_blue</monospace> sacrifices this extreme
  flexibility to allow nearly fully automatic VI with reparametrization
  (Bayes by Backprop)
  (<xref alt="Blundell et al., 2015" rid="ref-blundell15bbb" ref-type="bibr">Blundell
  et al., 2015</xref>). The ability to use multiple independent sampling
  dimensions is removed, which allows for fully automating a single
  sampling dimension in the outermost instance of the new base class
  <monospace>VIModule</monospace>. To control the number of samples this
  module also captures the optional keyword argument
  <monospace>samples</monospace>. The log likelihoods typically needed
  for loss calculation are automatically calculated whenever weights are
  sampled, aggregated, and returned once again by the outermost
  <monospace>VIModule</monospace>.</p>
</sec>
<sec id="core-design-and-features">
  <title>Core design and features</title>
  <p><monospace>torch_blue</monospace> is designed around two core
  aims:</p>
  <list list-type="order">
    <list-item>
      <p>Ease of use, even for users with little to no experience with
      Bayesian statistics</p>
    </list-item>
    <list-item>
      <p>Flexibility and extensibility as required for research and
      exploration</p>
    </list-item>
  </list>
  <p>While ease of use influences all design decisions, it features most
  prominently in the PyTorch-like interface. While currently only the
  most common layer types provided by PyTorch are supported,
  corresponding Bayesian layers follow an analogous naming pattern and
  accept the same arguments as their PyTorch version. Additionally,
  while there are minor differences, the process of implementing custom
  layers is also very similar to PyTorch. To illustrate this
  <xref alt="Figures " rid="code">Figures
  </xref><xref alt="[code]" rid="code">[code]</xref> and
  <xref alt="[design_graph]" rid="design_graph">[design_graph]</xref>
  show an application example and internal interactions of
  <monospace>torch_blue</monospace> with the colors connecting the
  abstract and applied components.</p>
  <p>The additional arguments required to modify the Bayesian aspects of
  the layers are collected on a common group of keyword arguments called
  <monospace>VIkwargs</monospace>. The default settings use mean field
  Gaussian variational inference with a Gaussian prior, allowing
  beginner users to implement simple, unoptimized models without
  worrying about Bayesian settings.</p>
  <p>An overview of the currently supported user-facing components is
  given in <xref alt="[overview]" rid="overview">[overview]</xref>.
  While modular priors and predictive distributions are quite common
  even for packages with simpler interfaces, flexible variational
  distributions are much more challenging and are often restricted to
  mean-field Gaussian. This is likely due to the fact that a generic
  variational distribution might require any number of different
  parameters, and the number and shape of weight matrices can only be
  determined with knowledge of the specific combination of layer and
  variational distribution. This is overcome in
  <monospace>torch_blue</monospace> by having the layer provide the
  names and shapes of the required random variables (e.g., mean and
  bias) and dynamically creating the associated class attributes during
  initialization, when the variational distribution is known. The
  modules also provide methods to sample from the variational
  distribution and access its parameters.</p>
  <fig>
    <caption><p>Overview of the major components of
    <monospace>torch_blue</monospace> and corresponding non-Bayesian
    components of PyTorch.
    <styled-content id="overview"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="content_overview.png" xlink:title="Content overview for `torch_blue` and comparison with the interface of `torch.nn`" />
  </fig>
  <p>Another challenge is introduced by the prior term of the ELBO loss.
  It can only be calculated analytically for a very limited set of
  priors and variational distributions. However, like the rest of the
  ELBO it can be estimated from the log probability of the sampled
  weights under these two distributions. Therefore,
  <monospace>torch_blue</monospace> provides the option to return these
  as part of the forward pass in the form of a
  <monospace>Tensor</monospace> containing an additional
  <monospace>log_probs</monospace> attribute similar to gradient
  tracking. As a result, the only requirement on custom distributions is
  that there needs to be a method to differentiably sample from a
  variational distribution and, for both priors and variational
  distributions, a method to compute the log probability of a given
  sample.</p>
  <p>Finally, in the age of large neural networks, scalability and
  efficiency are always a concern. While BNNs are not currently scaled
  to very large models and this is not a primary target of
  <monospace>torch_blue</monospace>, it is kept in mind wherever
  possible. A core feature for this purpose is GPU compatibility, which
  comes with the challenge of various backends and device types. We
  address this by performing all core operations, in particular the
  layer forward passes, with the methods from
  <monospace>torch.nn.functional</monospace>. This outsources backend
  maintenance to a large, community-supported library.</p>
  <p>Another efficiency optimization is the automatic vectorization of
  the sampling process. <monospace>torch_blue</monospace> adds an
  additional wrapper around the forward pass, which catches the optional
  <monospace>samples</monospace> argument, creates the specified number
  of samples (default: 10), and vectorizes the forward pass via
  <monospace>PyTorch</monospace>s <monospace>vmap</monospace>
  method.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work is supported by the German Federal Ministry of Research,
  Technology and Space under the 01IS22068 - EQUIPE grant. The authors
  gratefully acknowledge the computing time made available to them
  through the HAICORE@KIT partition and on the high-performance computer
  HoreKa at the NHR Center KIT. This center is jointly supported by the
  Federal Ministry of Education and Research and the state governments
  participating in the NHR
  (<ext-link ext-link-type="uri" xlink:href="www.nhr-verein.de/en/our-partners">www.nhr-verein.de/en/our-partners</ext-link>).</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-bingham2019pyro">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bingham</surname><given-names>Eli</given-names></name>
        <name><surname>Chen</surname><given-names>Jonathan P.</given-names></name>
        <name><surname>Jankowiak</surname><given-names>Martin</given-names></name>
        <name><surname>Obermeyer</surname><given-names>Fritz</given-names></name>
        <name><surname>Pradhan</surname><given-names>Neeraj</given-names></name>
        <name><surname>Karaletsos</surname><given-names>Theofanis</given-names></name>
        <name><surname>Singh</surname><given-names>Rohit</given-names></name>
        <name><surname>Szerlip</surname><given-names>Paul A.</given-names></name>
        <name><surname>Horsfall</surname><given-names>Paul</given-names></name>
        <name><surname>Goodman</surname><given-names>Noah D.</given-names></name>
      </person-group>
      <article-title>Pyro: Deep universal probabilistic programming</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2019">2019</year>
      <volume>20</volume>
      <issue>28</issue>
      <uri>http://jmlr.org/papers/v20/18-403.html</uri>
      <fpage>28:1</fpage>
      <lpage>28:6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-stan2025stan">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <string-name>Stan Development Team</string-name>
      </person-group>
      <article-title>Stan reference manual, 2.37</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://mc-stan.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-ansel2024pytorch">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ansel</surname><given-names>Jason</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>He</surname><given-names>Horace</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Jain</surname><given-names>Animesh</given-names></name>
        <name><surname>Voznesensky</surname><given-names>Michael</given-names></name>
        <name><surname>Bao</surname><given-names>Bin</given-names></name>
        <name><surname>Bell</surname><given-names>Peter</given-names></name>
        <name><surname>Berard</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Chauhan</surname><given-names>Geeta</given-names></name>
        <name><surname>Chourdia</surname><given-names>Anjali</given-names></name>
        <name><surname>Constable</surname><given-names>Will</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Ellison</surname><given-names>Elias</given-names></name>
        <name><surname>Feng</surname><given-names>Will</given-names></name>
        <name><surname>Gong</surname><given-names>Jiong</given-names></name>
        <name><surname>Gschwind</surname><given-names>Michael</given-names></name>
        <name><surname>Hirsh</surname><given-names>Brian</given-names></name>
        <name><surname>Huang</surname><given-names>Sherlock</given-names></name>
        <name><surname>Kalambarkar</surname><given-names>Kshiteej</given-names></name>
        <name><surname>Kirsch</surname><given-names>Laurent</given-names></name>
        <name><surname>Lazos</surname><given-names>Michael</given-names></name>
        <name><surname>Lezcano</surname><given-names>Mario</given-names></name>
        <name><surname>Liang</surname><given-names>Yanbo</given-names></name>
        <name><surname>Liang</surname><given-names>Jason</given-names></name>
        <name><surname>Lu</surname><given-names>Yinghai</given-names></name>
        <name><surname>Luk</surname><given-names>C. K.</given-names></name>
        <name><surname>Maher</surname><given-names>Bert</given-names></name>
        <name><surname>Pan</surname><given-names>Yunjie</given-names></name>
        <name><surname>Puhrsch</surname><given-names>Christian</given-names></name>
        <name><surname>Reso</surname><given-names>Matthias</given-names></name>
        <name><surname>Saroufim</surname><given-names>Mark</given-names></name>
        <name><surname>Siraichi</surname><given-names>Marcos Yukio</given-names></name>
        <name><surname>Suk</surname><given-names>Helen</given-names></name>
        <name><surname>Zhang</surname><given-names>Shunting</given-names></name>
        <name><surname>Suo</surname><given-names>Michael</given-names></name>
        <name><surname>Tillet</surname><given-names>Phil</given-names></name>
        <name><surname>Zhao</surname><given-names>Xu</given-names></name>
        <name><surname>Wang</surname><given-names>Eikan</given-names></name>
        <name><surname>Zhou</surname><given-names>Keren</given-names></name>
        <name><surname>Zou</surname><given-names>Richard</given-names></name>
        <name><surname>Wang</surname><given-names>Xiaodong</given-names></name>
        <name><surname>Mathews</surname><given-names>Ajit</given-names></name>
        <name><surname>Wen</surname><given-names>William</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Wu</surname><given-names>Peng</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch¬†2: Faster machine learning through dynamic Python bytecode transformation and graph compilation</article-title>
      <source>Proceedings of the 29th ACM international conference on architectural support for programming languages and operating systems, volume 2</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2024">2024</year>
      <isbn>9798400703850</isbn>
      <uri>https://doi.org/10.1145/3620665.3640366</uri>
      <pub-id pub-id-type="doi">10.1145/3620665.3640366</pub-id>
      <fpage>929</fpage>
      <lpage>947</lpage>
    </element-citation>
  </ref>
  <ref id="ref-blundell15bbb">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Blundell</surname><given-names>Charles</given-names></name>
        <name><surname>Cornebise</surname><given-names>Julien</given-names></name>
        <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
        <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
      </person-group>
      <article-title>Weight uncertainty in neural network</article-title>
      <source>Proceedings of the 32nd international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>Bach</surname><given-names>Francis</given-names></name>
        <name><surname>Blei</surname><given-names>David</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <publisher-loc>Lille, France</publisher-loc>
      <year iso-8601-date="2015-07">2015</year><month>07</month>
      <volume>37</volume>
      <uri>https://proceedings.mlr.press/v37/blundell15.html</uri>
      <fpage>1613</fpage>
      <lpage>1622</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kullback1951information">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kullback</surname><given-names>Solomon</given-names></name>
        <name><surname>Leibler</surname><given-names>Richard A.</given-names></name>
      </person-group>
      <article-title>On information and sufficiency</article-title>
      <source>The Annals of Mathematical Statistics</source>
      <year iso-8601-date="1951-03">1951</year><month>03</month>
      <volume>22</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1214/aoms/1177729694</pub-id>
      <fpage>79</fpage>
      <lpage>86</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jordan1999introduction">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jordan</surname><given-names>Michael I.</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Jaakkola</surname><given-names>Tommi S.</given-names></name>
        <name><surname>Saul</surname><given-names>Lawrence K.</given-names></name>
      </person-group>
      <article-title>An introduction to variational methods for graphical models</article-title>
      <source>Machine Learning</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="1999-11">1999</year><month>11</month>
      <volume>37</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1023/A:1007665907178</pub-id>
      <fpage>183</fpage>
      <lpage>233</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hoffman2013svi">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matthew D.</given-names></name>
        <name><surname>Blei</surname><given-names>David M.</given-names></name>
        <name><surname>Wang</surname><given-names>Chong</given-names></name>
        <name><surname>Paisley</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>Stochastic variational inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2013">2013</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-09-04">2025</year><month>09</month><day>04</day></date-in-citation>
      <volume>14</volume>
      <issue>4</issue>
      <issn>1533-7928</issn>
      <uri>http://jmlr.org/papers/v14/hoffman13a.html</uri>
      <fpage>1303</fpage>
      <lpage>1347</lpage>
    </element-citation>
  </ref>
  <ref id="ref-arbel2023primer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arbel</surname><given-names>Julyan</given-names></name>
        <name><surname>Pitas</surname><given-names>Konstantinos</given-names></name>
        <name><surname>Vladimirova</surname><given-names>Mariia</given-names></name>
        <name><surname>Fortuin</surname><given-names>Vincent</given-names></name>
      </person-group>
      <article-title>A primer on Bayesian neural networks: Review and debates</article-title>
      <source>arXiv preprint arXiv:2309.16314</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2309.16314</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
