<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7570</article-id>
<article-id pub-id-type="doi">10.21105/joss.07570</article-id>
<title-group>
<article-title>LangFair: A Python Package for Assessing Bias and
Fairness in Large Language Model Use Cases</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0004-9233-2324</contrib-id>
<name>
<surname>Bouchard</surname>
<given-names>Dylan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7817-0427</contrib-id>
<name>
<surname>Chauhan</surname>
<given-names>Mohit Singh</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-0005-0408</contrib-id>
<name>
<surname>Skarbrevik</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9984-1293</contrib-id>
<name>
<surname>Bajaj</surname>
<given-names>Viren</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-1478-2940</contrib-id>
<name>
<surname>Ahmad</surname>
<given-names>Zeya</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>CVS Health Corporation</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-06-01">
<day>1</day>
<month>6</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>105</issue>
<fpage>7570</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Large Language Model</kwd>
<kwd>Bias</kwd>
<kwd>Fairness</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Large Language Models (LLMs) have been observed to exhibit bias in
  numerous ways, potentially creating or worsening outcomes for specific
  groups identified by protected attributes such as sex, race, sexual
  orientation, or age. To help address this gap, we introduce
  <monospace>langfair</monospace>, an open-source Python package that
  aims to equip LLM practitioners with the tools to evaluate bias and
  fairness risks relevant to their specific use
  cases.<xref ref-type="fn" rid="fn1">1</xref> The package offers
  functionality to easily generate evaluation datasets, comprised of LLM
  responses to use-case-specific prompts, and subsequently calculate
  applicable metrics for the practitioner’s use case. To guide in metric
  selection, LangFair offers an actionable decision framework, discussed
  in detail in the project’s companion paper, Bouchard
  (<xref alt="2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">2024</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Traditional machine learning (ML) fairness toolkits like AIF360
  (<xref alt="Bellamy et al., 2018" rid="ref-aif360-oct-2018" ref-type="bibr">Bellamy
  et al., 2018</xref>), Fairlearn
  (<xref alt="Weerts et al., 2023" rid="ref-Weerts_Fairlearn_Assessing_and_2023" ref-type="bibr">Weerts
  et al., 2023</xref>), Aequitas
  (<xref alt="Saleiro et al., 2018" rid="ref-2018aequitas" ref-type="bibr">Saleiro
  et al., 2018</xref>) and others
  (<xref alt="Tensorflow, 2020" rid="ref-tensorflow-no-date" ref-type="bibr">Tensorflow,
  2020</xref>;
  <xref alt="Vasudevan &amp; Kenthapadi, 2020" rid="ref-vasudevan20lift" ref-type="bibr">Vasudevan
  &amp; Kenthapadi, 2020</xref>;
  <xref alt="Wexler et al., 2019" rid="ref-DBLPU003AjournalsU002FcorrU002Fabs-1907-04135" ref-type="bibr">Wexler
  et al., 2019</xref>) have laid crucial groundwork. These toolkits
  offer various metrics and algorithms that focus on assessing and
  mitigating bias and fairness through different stages of the ML
  lifecycle. While the fairness assessments offered by these toolkits
  include a wide variety of generic fairness metrics, which can also
  apply to certain LLM use cases, they are not tailored to the
  generative and context-dependent nature of
  LLMs.<xref ref-type="fn" rid="fn2">2</xref></p>
  <p>LLMs are used in systems that solve tasks such as recommendation,
  classification, text generation, and summarization. In practice, these
  systems try to restrict the responses of the LLM to the task at hand,
  often by including task-specific instructions in system or user
  prompts. When the LLM is evaluated without taking the set of
  task-specific prompts into account, the evaluation metrics are not
  representative of the system’s true performance. Representing the
  system’s actual performance is especially important when evaluating
  its outputs for bias and fairness risks because they pose real harm to
  the user and, by way of repercussions, the system developer.</p>
  <p>Most evaluation tools, including those that assess bias and
  fairness risk, evaluate LLMs at the model-level by calculating metrics
  based on the responses of the LLMs to static benchmark datasets of
  prompts
  (<xref alt="Barikeri et al., 2021" rid="ref-barikeri2021redditbiasrealworldresourcebias" ref-type="bibr">Barikeri
  et al., 2021</xref>;
  <xref alt="Bartl et al., 2020" rid="ref-bartl2020unmasking" ref-type="bibr">Bartl
  et al., 2020</xref>;
  <xref alt="Dhamala et al., 2021" rid="ref-bold_2021" ref-type="bibr">Dhamala
  et al., 2021</xref>;
  <xref alt="Felkner et al., 2024" rid="ref-felkner2024winoqueercommunityintheloopbenchmarkantilgbtq" ref-type="bibr">Felkner
  et al., 2024</xref>;
  <xref alt="Gehman et al., 2020" rid="ref-Gehman2020RealToxicityPromptsEN" ref-type="bibr">Gehman
  et al., 2020</xref>;
  <xref alt="Y. Huang et al., 2023" rid="ref-huang2023trustgptbenchmarktrustworthyresponsible" ref-type="bibr">Y.
  Huang et al., 2023</xref>;
  <xref alt="Kiritchenko &amp; Mohammad, 2018" rid="ref-kiritchenko2018examininggenderracebias" ref-type="bibr">Kiritchenko
  &amp; Mohammad, 2018</xref>;
  <xref alt="Krieg et al., 2023" rid="ref-10.1145U002F3576840.3578295" ref-type="bibr">Krieg
  et al., 2023</xref>;
  <xref alt="Levy et al., 2021" rid="ref-levy2021collecting" ref-type="bibr">Levy
  et al., 2021</xref>;
  <xref alt="Li et al., 2020" rid="ref-li-etal-2020-unqovering" ref-type="bibr">Li
  et al., 2020</xref>;
  <xref alt="Nadeem et al., 2020" rid="ref-nadeem2020stereoset" ref-type="bibr">Nadeem
  et al., 2020</xref>;
  <xref alt="Nangia et al., 2020" rid="ref-nangia2020crows" ref-type="bibr">Nangia
  et al., 2020</xref>;
  <xref alt="Nozza et al., 2021" rid="ref-nozza-etal-2021-honest" ref-type="bibr">Nozza
  et al., 2021</xref>;
  <xref alt="Parrish et al., 2022" rid="ref-parrish-etal-2022-bbq" ref-type="bibr">Parrish
  et al., 2022</xref>;
  <xref alt="Qian et al., 2022" rid="ref-qian2022perturbationaugmentationfairernlp" ref-type="bibr">Qian
  et al., 2022</xref>;
  <xref alt="Rudinger et al., 2018" rid="ref-rudinger-EtAlU003A2018U003AN18" ref-type="bibr">Rudinger
  et al., 2018</xref>;
  <xref alt="Webster et al., 2018" rid="ref-webster-etal-2018-mind" ref-type="bibr">Webster
  et al., 2018</xref>;
  <xref alt="Zhao et al., 2018" rid="ref-zhao-2018" ref-type="bibr">Zhao
  et al., 2018</xref>) that do not consider prompt-specific risks and
  are often independent of the task at hand. Holistic Evaluation of
  Language Models (HELM)
  (<xref alt="Liang et al., 2023" rid="ref-liang2023holisticevaluationlanguagemodels" ref-type="bibr">Liang
  et al., 2023</xref>), DecodingTrust
  (<xref alt="Wang et al., 2023" rid="ref-wang2023decodingtrust" ref-type="bibr">Wang
  et al., 2023</xref>), and several other toolkits
  (<xref alt="Gao et al., 2024" rid="ref-eval-harness" ref-type="bibr">Gao
  et al., 2024</xref>;
  <xref alt="Y. Huang et al., 2024" rid="ref-huang2024trustllm" ref-type="bibr">Y.
  Huang et al., 2024</xref>;
  <xref alt="Huggingface, 2022" rid="ref-huggingface-no-date" ref-type="bibr">Huggingface,
  2022</xref>;
  <xref alt="Nazir et al., 2024" rid="ref-Arshaan_Nazir" ref-type="bibr">Nazir
  et al., 2024</xref>;
  <xref alt="Srivastava et al., 2022" rid="ref-srivastava2022beyond" ref-type="bibr">Srivastava
  et al., 2022</xref>) follow this paradigm.</p>
  <p>LangFair complements the aforementioned frameworks because it
  follows a bring your own prompts (BYOP) approach, which allows users
  to tailor the bias and fairness evaluation to their use case by
  computing metrics using LLM responses to user-provided prompts. This
  addresses the need for a task-based bias and fairness evaluation tool
  that accounts for prompt-specific risk for
  LLMs.<xref ref-type="fn" rid="fn3">3</xref></p>
  <p>Furthermore, LangFair is designed for real-world LLM-based systems
  that require governance audits. LangFair focuses on calculating
  metrics from LLM responses only, which is more practical for
  real-world testing where access to internal states of model to
  retrieve embeddings or token probabilities is difficult. An added
  benefit is that output-based metrics, which are focused on the
  downstream task, have shown to be potentially more reliable than
  metrics derived from embeddings or token probabilities
  (<xref alt="Delobelle et al., 2022" rid="ref-delobelle-etal-2022-measuring" ref-type="bibr">Delobelle
  et al., 2022</xref>;
  <xref alt="Goldfarb-Tarrant et al., 2021" rid="ref-goldfarbtarrant2021intrinsicbiasmetricscorrelate" ref-type="bibr">Goldfarb-Tarrant
  et al., 2021</xref>).</p>
</sec>
<sec id="generation-of-evaluation-datasets">
  <title>Generation of Evaluation Datasets</title>
  <p>The <monospace>langfair.generator</monospace> module offers two
  classes, <monospace>ResponseGenerator</monospace> and
  <monospace>Counterfactual</monospace>-
  <monospace>Generator</monospace>, which aim to enable user-friendly
  construction of evaluation datasets for text generation use cases.</p>
  <sec id="responsegenerator-class">
    <title><monospace>ResponseGenerator</monospace> class</title>
    <p>To streamline generation of evaluation datasets, the
    <monospace>ResponseGenerator</monospace> class wraps an instance of
    a <monospace>langchain</monospace> LLM and leverages asynchronous
    generation with <monospace>asyncio</monospace>. To implement, users
    simply pass a list of prompts (strings) to the
    <monospace>ResponseGenerator.generate_responses</monospace> method,
    which returns a dictionary containing prompts, responses, and
    applicable metadata.</p>
  </sec>
  <sec id="counterfactualgenerator-class">
    <title><monospace>CounterfactualGenerator</monospace> class</title>
    <p>In the context of LLMs, counterfactual fairness can be assessed
    by constructing counterfactual input pairs
    (<xref alt="Bouchard, 2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">Bouchard,
    2024</xref>;
    <xref alt="Gallegos et al., 2024" rid="ref-gallegos2024biasfairnesslargelanguage" ref-type="bibr">Gallegos
    et al., 2024</xref>), comprised of prompt pairs that mention
    different protected attribute groups but are otherwise identical,
    and measuring the differences in the corresponding generated output
    pairs. These assessments are applicable to use cases that do not
    satisfy fairness through unawareness (FTU), meaning prompts contain
    mentions of protected attribute groups. To address this, the
    <monospace>CounterfactualGenerator</monospace> class offers
    functionality to check for FTU, construct counterfactual input
    pairs, and generate corresponding pairs of responses asynchronously
    using a <monospace>langchain</monospace> LLM
    instance.<xref ref-type="fn" rid="fn4">4</xref> Off the shelf, the
    FTU check and creation of counterfactual input pairs can be done for
    gender and race/ethnicity, but users may also provide a custom
    mapping of protected attribute words to enable this functionality
    for other attributes as well.</p>
  </sec>
</sec>
<sec id="bias-and-fairness-evaluations-for-focused-use-cases">
  <title>Bias and Fairness Evaluations for Focused Use Cases</title>
  <p>Following Bouchard
  (<xref alt="2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">2024</xref>),
  evaluation metrics are categorized according to the risks they assess
  (toxicity, stereotypes, counterfactual unfairness, and allocational
  harms), as well as the use case task (text generation, classification,
  and recommendation).<xref ref-type="fn" rid="fn5">5</xref> Table 1
  maps the classes contained in the
  <monospace>langfair.metrics</monospace> module to these risks. These
  classes are discussed in detail below.</p>
  <table-wrap>
    <table>
      <thead>
        <tr>
          <th>Class</th>
          <th>Risk Assessed</th>
          <th>Applicable Tasks</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td><monospace>ToxicityMetrics</monospace></td>
          <td>Toxicity</td>
          <td>Text generation</td>
        </tr>
        <tr>
          <td><monospace>StereotypeMetrics</monospace></td>
          <td>Stereotypes</td>
          <td>Text generation</td>
        </tr>
        <tr>
          <td><monospace>CounterfactualMetrics</monospace></td>
          <td>Counterfactual fairness</td>
          <td>Text generation</td>
        </tr>
        <tr>
          <td><monospace>RecommendationMetrics</monospace></td>
          <td>Counterfactual fairness</td>
          <td>Recommendation</td>
        </tr>
        <tr>
          <td><monospace>ClassificationMetrics</monospace></td>
          <td>Allocational harms</td>
          <td>Classification</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p><bold>Table 1</bold> : Classes for Computing Evaluation Metrics in
  langfair.metrics</p>
  <sec id="toxicity-metrics">
    <title>Toxicity Metrics</title>
    <p>The <monospace>ToxicityMetrics</monospace> class facilitates
    simple computation of toxicity metrics from a user-provided list of
    LLM responses. These metrics leverage a pre-trained toxicity
    classifier that maps a text input to a toxicity score ranging from 0
    to 1
    (<xref alt="Gehman et al., 2020" rid="ref-Gehman2020RealToxicityPromptsEN" ref-type="bibr">Gehman
    et al., 2020</xref>;
    <xref alt="Liang et al., 2023" rid="ref-liang2023holisticevaluationlanguagemodels" ref-type="bibr">Liang
    et al., 2023</xref>). For off-the-shelf toxicity classifiers, the
    <monospace>ToxicityMetrics</monospace> class provides four options:
    two classifiers from the <monospace>detoxify</monospace> package,
    <monospace>roberta-hate-speech-dynabench-r4-target</monospace> from
    the <monospace>evaluate</monospace> package, and
    <monospace>toxigen</monospace> available on
    HuggingFace.<xref ref-type="fn" rid="fn6">6</xref> For additional
    flexibility, users can specify an ensemble of the off-the-shelf
    classifiers offered or provide a custom toxicity classifier
    object.</p>
  </sec>
  <sec id="stereotype-metrics">
    <title>Stereotype Metrics</title>
    <p>To measure stereotypes in LLM responses, the
    <monospace>StereotypeMetrics</monospace> class offers two categories
    of metrics: metrics based on word cooccurrences and metrics that
    leverage a pre-trained stereotype classifier. Metrics based on word
    cooccurrences aim to assess relative cooccurrence of stereotypical
    words with certain protected attribute words. On the other hand,
    stereotype-classifier-based metrics leverage the
    <monospace>wu981526092/Sentence-Level-Stereotype-Detector</monospace>
    classifier available on HuggingFace
    (<xref alt="Zekun et al., 2023" rid="ref-zekun2023auditinglargelanguagemodels" ref-type="bibr">Zekun
    et al., 2023</xref>) and compute analogs of the aforementioned
    toxicity-classifier-based metrics
    (<xref alt="Bouchard, 2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">Bouchard,
    2024</xref>).<xref ref-type="fn" rid="fn7">7</xref></p>
  </sec>
  <sec id="counterfactual-fairness-metrics-for-text-generation">
    <title>Counterfactual Fairness Metrics for Text Generation</title>
    <p>The <monospace>CounterfactualMetrics</monospace> class offers two
    groups of metrics to assess counterfactual fairness in text
    generation use cases. The first group of metrics leverage a
    pre-trained sentiment classifier to measure sentiment disparities in
    counterfactually generated outputs (see P.-S. Huang et al.
    (<xref alt="2020" rid="ref-huang2020reducingsentimentbiaslanguage" ref-type="bibr">2020</xref>)
    for further details). This class uses the
    <monospace>vaderSentiment</monospace> classifier by default but also
    gives users the option to provide a custom sentiment classifier
    object.<xref ref-type="fn" rid="fn8">8</xref> The second group of
    metrics addresses a stricter desiderata and measures overall
    similarity in counterfactually generated outputs using
    well-established text similarity metrics
    (<xref alt="Bouchard, 2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">Bouchard,
    2024</xref>).</p>
  </sec>
  <sec id="counterfactual-fairness-metrics-for-recommendation">
    <title>Counterfactual Fairness Metrics for Recommendation</title>
    <p>The <monospace>RecommendationMetrics</monospace> class is
    designed to assess counterfactual fairness for recommendation use
    cases. Specifically, these metrics measure similarity in generated
    lists of recommendations from counterfactual input pairs. Metrics
    may be computed pairwise
    (<xref alt="Bouchard, 2024" rid="ref-bouchard2024actionableframeworkassessingbias" ref-type="bibr">Bouchard,
    2024</xref>), or attribute-wise
    (<xref alt="Zhang et al., 2023" rid="ref-Zhang_2023" ref-type="bibr">Zhang
    et al., 2023</xref>).</p>
  </sec>
  <sec id="fairness-metrics-for-classification">
    <title>Fairness Metrics for Classification</title>
    <p>When LLMs are used to solve classification problems, traditional
    machine learning fairness metrics may be applied, provided that
    inputs can be mapped to a protected attribute. To this end, the
    <monospace>ClassificationMetrics</monospace> class offers a suite of
    metrics to address unfair classification by measuring disparities in
    predicted prevalence, false negatives, or false positives. When
    computing metrics using the
    <monospace>ClassificationMetrics</monospace> class, the user may
    specify whether to compute these metrics as pairwise differences
    (<xref alt="Bellamy et al., 2018" rid="ref-aif360-oct-2018" ref-type="bibr">Bellamy
    et al., 2018</xref>) or pairwise ratios
    (<xref alt="Saleiro et al., 2018" rid="ref-2018aequitas" ref-type="bibr">Saleiro
    et al., 2018</xref>).</p>
  </sec>
</sec>
<sec id="semi-automated-evaluation">
  <title>Semi-Automated Evaluation</title>
  <sec id="autoeval-class">
    <title><monospace>AutoEval</monospace> class</title>
    <p>To streamline assessments for text generation use cases, the
    <monospace>AutoEval</monospace> class conducts a multi-step process
    (each step is described in detail above) for a comprehensive
    fairness assessment. Specifically, these steps include metric
    selection (based on whether FTU is satsified), evaluation dataset
    generation from user-provided prompts with a user-provided LLM, and
    computation of applicable fairness metrics. To implement, the user
    is required to supply a list of prompts and an instance of
    <monospace>langchain</monospace> LLM. Below we provide a basic
    example demonstrating the execution of
    <monospace>AutoEval.evaluate</monospace> with a
    <monospace>gemini-pro</monospace>
    instance.<xref ref-type="fn" rid="fn9">9</xref></p>
    <code language="python">from langchain_google_vertexai import ChatVertexAI
from langfair.auto import AutoEval

llm = ChatVertexAI(model_name='gemini-pro')
auto_object = AutoEval(prompts=prompts, langchain_llm=llm)
results = await auto_object.evaluate()</code>
    <p>Under the hood, the <monospace>AutoEval.evaluate</monospace>
    method 1) checks for FTU, 2) generates responses and counterfactual
    responses (if FTU is not satisfied), and 3) calculates applicable
    metrics for the use case.<xref ref-type="fn" rid="fn10">10</xref>
    This process flow is depicted in Figure 1.
    <inline-graphic mimetype="image" mime-subtype="png" xlink:href="AutoEval_flowchart_colored.png">
      <alt-text>AutoEval_flowchart</alt-text>
    </inline-graphic></p>
    <p><bold>Figure 1</bold>:Flowchart of internal design of
    Autoeval.evaluate method</p>
  </sec>
</sec>
<sec id="author-contributions">
  <title>Author Contributions</title>
  <p>Dylan Bouchard was the principal developer and researcher of the
  LangFair project, responsible for conceptualization, methodology, and
  software development of the <monospace>langfair</monospace> library.
  Mohit Singh Chauhan was the architect behind the structural design of
  the <monospace>langfair</monospace> library and helped lead the
  software development efforts. David Skarbrevik was the primary author
  of LangFair’s documentation, helped implement software engineering
  best practices, and contributed to software development. Viren Bajaj
  wrote unit tests, contributed to the software development, and helped
  implement software engineering best practices. Zeya Ahmad contributed
  to the software development.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We wish to thank Piero Ferrante, Blake Aber, Xue (Crystal) Gu, and
  Zirui Xu for their helpful suggestions.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-rudinger-EtAlU003A2018U003AN18">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rudinger</surname><given-names>Rachel</given-names></name>
        <name><surname>Naradowsky</surname><given-names>Jason</given-names></name>
        <name><surname>Leonard</surname><given-names>Brian</given-names></name>
        <name><surname>Van Durme</surname><given-names>Benjamin</given-names></name>
      </person-group>
      <article-title>Gender bias in coreference resolution</article-title>
      <source>Proceedings of the 2018 conference of the north american chapter of the association for computational linguistics: Human language technologies</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>New Orleans, Louisiana</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1804.09301</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zhao-2018">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zhao</surname><given-names>Jieyu</given-names></name>
        <name><surname>Wang</surname><given-names>Tianlu</given-names></name>
        <name><surname>Yatskar</surname><given-names>Mark</given-names></name>
        <name><surname>Ordonez</surname><given-names>Vicente</given-names></name>
        <name><surname>Chang</surname><given-names>Kai-Wei</given-names></name>
      </person-group>
      <article-title>Gender Bias in Coreference Resolution: Evaluation and Debiasing methods</article-title>
      <year iso-8601-date="2018-04">2018</year><month>04</month>
      <uri>https://arxiv.org/abs/1804.06876</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1804.06876</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-webster-etal-2018-mind">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Webster</surname><given-names>Kellie</given-names></name>
        <name><surname>Recasens</surname><given-names>Marta</given-names></name>
        <name><surname>Axelrod</surname><given-names>Vera</given-names></name>
        <name><surname>Baldridge</surname><given-names>Jason</given-names></name>
      </person-group>
      <article-title>Mind the GAP: A balanced corpus of gendered ambiguous pronouns</article-title>
      <source>Transactions of the Association for Computational Linguistics</source>
      <person-group person-group-type="editor">
        <name><surname>Lee</surname><given-names>Lillian</given-names></name>
        <name><surname>Johnson</surname><given-names>Mark</given-names></name>
        <name><surname>Toutanova</surname><given-names>Kristina</given-names></name>
        <name><surname>Roark</surname><given-names>Brian</given-names></name>
      </person-group>
      <publisher-name>MIT Press</publisher-name>
      <publisher-loc>Cambridge, MA</publisher-loc>
      <year iso-8601-date="2018">2018</year>
      <volume>6</volume>
      <uri>https://aclanthology.org/Q18-1042</uri>
      <pub-id pub-id-type="doi">10.1162/tacl_a_00240</pub-id>
      <fpage>605</fpage>
      <lpage>617</lpage>
    </element-citation>
  </ref>
  <ref id="ref-levy2021collecting">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Levy</surname><given-names>Shahar</given-names></name>
        <name><surname>Lazar</surname><given-names>Koren</given-names></name>
        <name><surname>Stanovsky</surname><given-names>Gabriel</given-names></name>
      </person-group>
      <article-title>Collecting a large-scale gender bias dataset for coreference resolution and machine translation</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2109.03858</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2109.03858</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-nadeem2020stereoset">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Nadeem</surname><given-names>Moin</given-names></name>
        <name><surname>Bethke</surname><given-names>Anna</given-names></name>
        <name><surname>Reddy</surname><given-names>Siva</given-names></name>
      </person-group>
      <article-title>StereoSet: Measuring stereotypical bias in pretrained language models</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/2004.09456</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2004.09456</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bartl2020unmasking">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bartl</surname><given-names>Marion</given-names></name>
        <name><surname>Nissim</surname><given-names>Malvina</given-names></name>
        <name><surname>Gatt</surname><given-names>Albert</given-names></name>
      </person-group>
      <article-title>Unmasking contextual stereotypes: Measuring and mitigating BERT’s gender bias</article-title>
      <source>Proceedings of the second workshop on gender bias in natural language processing</source>
      <person-group person-group-type="editor">
        <name><surname>Costa-jussà</surname><given-names>Marta R.</given-names></name>
        <name><surname>Hardmeier</surname><given-names>Christian</given-names></name>
        <name><surname>Webster</surname><given-names>Kellie</given-names></name>
        <name><surname>Radford</surname><given-names>Will</given-names></name>
      </person-group>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2010.14534</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-nangia2020crows">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nangia</surname><given-names>Nikita</given-names></name>
        <name><surname>Vania</surname><given-names>Clara</given-names></name>
        <name><surname>Bhalerao</surname><given-names>Rasika</given-names></name>
        <name><surname>Bowman</surname><given-names>Samuel R.</given-names></name>
      </person-group>
      <article-title>CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</article-title>
      <source>Proceedings of the 2020 conference on empirical methods in natural language processing</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-11">2020</year><month>11</month>
      <pub-id pub-id-type="doi">10.48550/arXiv.2010.00133</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-felkner2024winoqueercommunityintheloopbenchmarkantilgbtq">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Felkner</surname><given-names>Virginia K.</given-names></name>
        <name><surname>Chang</surname><given-names>Ho-Chun Herbert</given-names></name>
        <name><surname>Jang</surname><given-names>Eugene</given-names></name>
        <name><surname>May</surname><given-names>Jonathan</given-names></name>
      </person-group>
      <article-title>WinoQueer: A community-in-the-loop benchmark for anti-LGBTQ+ bias in large language models</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2306.15087</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2306.15087</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-barikeri2021redditbiasrealworldresourcebias">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Barikeri</surname><given-names>Soumya</given-names></name>
        <name><surname>Lauscher</surname><given-names>Anne</given-names></name>
        <name><surname>Vulić</surname><given-names>Ivan</given-names></name>
        <name><surname>Glavaš</surname><given-names>Goran</given-names></name>
      </person-group>
      <article-title>RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2106.03521</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2106.03521</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-qian2022perturbationaugmentationfairernlp">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Qian</surname><given-names>Rebecca</given-names></name>
        <name><surname>Ross</surname><given-names>Candace</given-names></name>
        <name><surname>Fernandes</surname><given-names>Jude</given-names></name>
        <name><surname>Smith</surname><given-names>Eric</given-names></name>
        <name><surname>Kiela</surname><given-names>Douwe</given-names></name>
        <name><surname>Williams</surname><given-names>Adina</given-names></name>
      </person-group>
      <article-title>Perturbation augmentation for fairer NLP</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://arxiv.org/abs/2205.12586</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2205.12586</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kiritchenko2018examininggenderracebias">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kiritchenko</surname><given-names>Svetlana</given-names></name>
        <name><surname>Mohammad</surname><given-names>Saif M.</given-names></name>
      </person-group>
      <article-title>Examining gender and race bias in two hundred sentiment analysis systems</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://arxiv.org/abs/1805.04508</uri>
      <pub-id pub-id-type="doi">10.18653/v1/S18-2005</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Gehman2020RealToxicityPromptsEN">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gehman</surname><given-names>Samuel</given-names></name>
        <name><surname>Gururangan</surname><given-names>Suchin</given-names></name>
        <name><surname>Sap</surname><given-names>Maarten</given-names></name>
        <name><surname>Choi</surname><given-names>Yejin</given-names></name>
        <name><surname>Smith</surname><given-names>Noah A.</given-names></name>
      </person-group>
      <article-title>RealToxicityPrompts: Evaluating neural toxic degeneration in language models</article-title>
      <source>Findings</source>
      <year iso-8601-date="2020">2020</year>
      <uri>https://api.semanticscholar.org/CorpusID:221878771</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2020.findings-emnlp.301</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-bold_2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dhamala</surname><given-names>Jwala</given-names></name>
        <name><surname>Sun</surname><given-names>Tony</given-names></name>
        <name><surname>Kumar</surname><given-names>Varun</given-names></name>
        <name><surname>Krishna</surname><given-names>Satyapriya</given-names></name>
        <name><surname>Pruksachatkun</surname><given-names>Yada</given-names></name>
        <name><surname>Chang</surname><given-names>Kai-Wei</given-names></name>
        <name><surname>Gupta</surname><given-names>Rahul</given-names></name>
      </person-group>
      <article-title>BOLD: Dataset and metrics for measuring biases in open-ended language generation</article-title>
      <source>Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <isbn>9781450383097</isbn>
      <uri>https://doi.org/10.1145/3442188.3445924</uri>
      <pub-id pub-id-type="doi">10.1145/3442188.3445924</pub-id>
      <fpage>862</fpage>
      <lpage>872</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huang2023trustgptbenchmarktrustworthyresponsible">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Yue</given-names></name>
        <name><surname>Zhang</surname><given-names>Qihui</given-names></name>
        <name><surname>Y</surname><given-names>Philip S.</given-names></name>
        <name><surname>Sun</surname><given-names>Lichao</given-names></name>
      </person-group>
      <article-title>TrustGPT: A benchmark for trustworthy and responsible large language models</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2306.11507</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2306.11507</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-nozza-etal-2021-honest">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Nozza</surname><given-names>Debora</given-names></name>
        <name><surname>Bianchi</surname><given-names>Federico</given-names></name>
        <name><surname>Hovy</surname><given-names>Dirk</given-names></name>
      </person-group>
      <article-title>&quot;HONEST: Measuring hurtful sentence completion in language models&quot;</article-title>
      <source>Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies</source>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021-06">2021</year><month>06</month>
      <uri>https://aclanthology.org/2021.naacl-main.191</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2021.naacl-main.191</pub-id>
      <fpage>2398</fpage>
      <lpage>2406</lpage>
    </element-citation>
  </ref>
  <ref id="ref-parrish-etal-2022-bbq">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Parrish</surname><given-names>Alicia</given-names></name>
        <name><surname>Chen</surname><given-names>Angelica</given-names></name>
        <name><surname>Nangia</surname><given-names>Nikita</given-names></name>
        <name><surname>Padmakumar</surname><given-names>Vishakh</given-names></name>
        <name><surname>Phang</surname><given-names>Jason</given-names></name>
        <name><surname>Thompson</surname><given-names>Jana</given-names></name>
        <name><surname>Htut</surname><given-names>Phu Mon</given-names></name>
        <name><surname>Bowman</surname><given-names>Samuel</given-names></name>
      </person-group>
      <article-title>BBQ: A hand-built bias benchmark for question answering</article-title>
      <source>Findings of the association for computational linguistics: ACL 2022</source>
      <person-group person-group-type="editor">
        <name><surname>Muresan</surname><given-names>Smaranda</given-names></name>
        <name><surname>Nakov</surname><given-names>Preslav</given-names></name>
        <name><surname>Villavicencio</surname><given-names>Aline</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Dublin, Ireland</publisher-loc>
      <year iso-8601-date="2022-05">2022</year><month>05</month>
      <uri>https://aclanthology.org/2022.findings-acl.165</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2022.findings-acl.165</pub-id>
      <fpage>2086</fpage>
      <lpage>2105</lpage>
    </element-citation>
  </ref>
  <ref id="ref-li-etal-2020-unqovering">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Tao</given-names></name>
        <name><surname>Khashabi</surname><given-names>Daniel</given-names></name>
        <name><surname>Khot</surname><given-names>Tushar</given-names></name>
        <name><surname>Sabharwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Srikumar</surname><given-names>Vivek</given-names></name>
      </person-group>
      <article-title>UNQOVERing stereotyping biases via underspecified questions</article-title>
      <source>Findings of the association for computational linguistics: EMNLP 2020</source>
      <person-group person-group-type="editor">
        <name><surname>Cohn</surname><given-names>Trevor</given-names></name>
        <name><surname>He</surname><given-names>Yulan</given-names></name>
        <name><surname>Liu</surname><given-names>Yang</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2020-11">2020</year><month>11</month>
      <uri>https://aclanthology.org/2020.findings-emnlp.311</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2020.findings-emnlp.311</pub-id>
      <fpage>3475</fpage>
      <lpage>3489</lpage>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3576840.3578295">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Krieg</surname><given-names>Klara</given-names></name>
        <name><surname>Parada-Cabaleiro</surname><given-names>Emilia</given-names></name>
        <name><surname>Medicus</surname><given-names>Gertraud</given-names></name>
        <name><surname>Lesota</surname><given-names>Oleg</given-names></name>
        <name><surname>Schedl</surname><given-names>Markus</given-names></name>
        <name><surname>Rekabsaz</surname><given-names>Navid</given-names></name>
      </person-group>
      <article-title>Grep-BiasIR: A dataset for investigating gender representation bias in information retrieval results</article-title>
      <source>Proceedings of the 2023 conference on human information interaction and retrieval</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <isbn>9798400700354</isbn>
      <uri>https://doi.org/10.1145/3576840.3578295</uri>
      <pub-id pub-id-type="doi">10.1145/3576840.3578295</pub-id>
      <fpage>444</fpage>
      <lpage>448</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liang2023holisticevaluationlanguagemodels">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Liang</surname><given-names>Percy</given-names></name>
        <name><surname>Bommasani</surname><given-names>Rishi</given-names></name>
        <name><surname>Lee</surname><given-names>Tony</given-names></name>
        <name><surname>Tsipras</surname><given-names>Dimitris</given-names></name>
        <name><surname>Soylu</surname><given-names>Dilara</given-names></name>
        <name><surname>Yasunaga</surname><given-names>Michihiro</given-names></name>
        <name><surname>Zhang</surname><given-names>Yian</given-names></name>
        <name><surname>Narayanan</surname><given-names>Deepak</given-names></name>
        <name><surname>Wu</surname><given-names>Yuhuai</given-names></name>
        <name><surname>Kumar</surname><given-names>Ananya</given-names></name>
        <name><surname>Newman</surname><given-names>Benjamin</given-names></name>
        <name><surname>Yuan</surname><given-names>Binhang</given-names></name>
        <name><surname>Yan</surname><given-names>Bobby</given-names></name>
        <name><surname>Zhang</surname><given-names>Ce</given-names></name>
        <name><surname>Cosgrove</surname><given-names>Christian</given-names></name>
        <name><surname>Manning</surname><given-names>Christopher D.</given-names></name>
        <name><surname>Ré</surname><given-names>Christopher</given-names></name>
        <name><surname>Acosta-Navas</surname><given-names>Diana</given-names></name>
        <name><surname>Hudson</surname><given-names>Drew A.</given-names></name>
        <name><surname>Zelikman</surname><given-names>Eric</given-names></name>
        <name><surname>Durmus</surname><given-names>Esin</given-names></name>
        <name><surname>Ladhak</surname><given-names>Faisal</given-names></name>
        <name><surname>Rong</surname><given-names>Frieda</given-names></name>
        <name><surname>Ren</surname><given-names>Hongyu</given-names></name>
        <name><surname>Yao</surname><given-names>Huaxiu</given-names></name>
        <name><surname>Wang</surname><given-names>Jue</given-names></name>
        <name><surname>Santhanam</surname><given-names>Keshav</given-names></name>
        <name><surname>Orr</surname><given-names>Laurel</given-names></name>
        <name><surname>Zheng</surname><given-names>Lucia</given-names></name>
        <name><surname>Yuksekgonul</surname><given-names>Mert</given-names></name>
        <name><surname>Suzgun</surname><given-names>Mirac</given-names></name>
        <name><surname>Kim</surname><given-names>Nathan</given-names></name>
        <name><surname>Guha</surname><given-names>Neel</given-names></name>
        <name><surname>Chatterji</surname><given-names>Niladri</given-names></name>
        <name><surname>Khattab</surname><given-names>Omar</given-names></name>
        <name><surname>Henderson</surname><given-names>Peter</given-names></name>
        <name><surname>Huang</surname><given-names>Qian</given-names></name>
        <name><surname>Chi</surname><given-names>Ryan</given-names></name>
        <name><surname>Xie</surname><given-names>Sang Michael</given-names></name>
        <name><surname>Santurkar</surname><given-names>Shibani</given-names></name>
        <name><surname>Ganguli</surname><given-names>Surya</given-names></name>
        <name><surname>Hashimoto</surname><given-names>Tatsunori</given-names></name>
        <name><surname>Icard</surname><given-names>Thomas</given-names></name>
        <name><surname>Zhang</surname><given-names>Tianyi</given-names></name>
        <name><surname>Chaudhary</surname><given-names>Vishrav</given-names></name>
        <name><surname>Wang</surname><given-names>William</given-names></name>
        <name><surname>Li</surname><given-names>Xuechen</given-names></name>
        <name><surname>Mai</surname><given-names>Yifan</given-names></name>
        <name><surname>Zhang</surname><given-names>Yuhui</given-names></name>
        <name><surname>Koreeda</surname><given-names>Yuta</given-names></name>
      </person-group>
      <article-title>Holistic evaluation of language models</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2211.09110</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2211.09110</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-wang2023decodingtrust">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Boxin</given-names></name>
        <name><surname>Chen</surname><given-names>Weixin</given-names></name>
        <name><surname>Pei</surname><given-names>Hengzhi</given-names></name>
        <name><surname>Xie</surname><given-names>Chulin</given-names></name>
        <name><surname>Kang</surname><given-names>Mintong</given-names></name>
        <name><surname>Zhang</surname><given-names>Chenhui</given-names></name>
        <name><surname>Xu</surname><given-names>Chejian</given-names></name>
        <name><surname>Xiong</surname><given-names>Zidi</given-names></name>
        <name><surname>Dutta</surname><given-names>Ritik</given-names></name>
        <name><surname>Schaeffer</surname><given-names>Rylan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>DecodingTrust: A comprehensive assessment of trustworthiness in GPT models</article-title>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2306.11698</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-huggingface-no-date">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Huggingface</surname></name>
      </person-group>
      <article-title>GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/huggingface/evaluate</uri>
    </element-citation>
  </ref>
  <ref id="ref-Arshaan_Nazir">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Nazir</surname><given-names>Arshaan</given-names></name>
        <name><surname>Chakravarthy</surname><given-names>Thadaka Kalyan</given-names></name>
        <name><surname>Cecchini</surname><given-names>David Amore</given-names></name>
        <name><surname>Chakravarthy</surname><given-names>Thadaka Kalyan</given-names></name>
        <name><surname>Khajuria</surname><given-names>Rakshit</given-names></name>
        <name><surname>Sharma</surname><given-names>Prikshit</given-names></name>
        <name><surname>Mirik</surname><given-names>Ali Tarik</given-names></name>
        <name><surname>Kocaman</surname><given-names>Veysel</given-names></name>
        <name><surname>Talby</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>LangTest: A comprehensive evaluation library for custom LLM and NLP models</article-title>
      <source>Software Impacts</source>
      <year iso-8601-date="2024">2024</year>
      <volume>19</volume>
      <issue>100619</issue>
      <pub-id pub-id-type="doi">10.1016/j.simpa.2024.100619</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-srivastava2022beyond">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Srivastava</surname><given-names>Aarohi</given-names></name>
        <name><surname>Rastogi</surname><given-names>Abhinav</given-names></name>
        <name><surname>Rao</surname><given-names>Abhishek</given-names></name>
        <name><surname>Shoeb</surname><given-names>Abu Awal Md</given-names></name>
        <name><surname>Abid</surname><given-names>Abubakar</given-names></name>
        <name><surname>Fisch</surname><given-names>Adam</given-names></name>
        <name><surname>Brown</surname><given-names>Adam R</given-names></name>
        <name><surname>Santoro</surname><given-names>Adam</given-names></name>
        <name><surname>Gupta</surname><given-names>Aditya</given-names></name>
        <name><surname>Garriga-Alonso</surname><given-names>Adri‘a</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</article-title>
      <source>arXiv preprint arXiv:2206.04615</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2206.04615</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-eval-harness">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gao</surname><given-names>Leo</given-names></name>
        <name><surname>Tow</surname><given-names>Jonathan</given-names></name>
        <name><surname>Abbasi</surname><given-names>Baber</given-names></name>
        <name><surname>Biderman</surname><given-names>Stella</given-names></name>
        <name><surname>Black</surname><given-names>Sid</given-names></name>
        <name><surname>DiPofi</surname><given-names>Anthony</given-names></name>
        <name><surname>Foster</surname><given-names>Charles</given-names></name>
        <name><surname>Golding</surname><given-names>Laurence</given-names></name>
        <name><surname>Hsu</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Le Noac’h</surname><given-names>Alain</given-names></name>
        <name><surname>Li</surname><given-names>Haonan</given-names></name>
        <name><surname>McDonell</surname><given-names>Kyle</given-names></name>
        <name><surname>Muennighoff</surname><given-names>Niklas</given-names></name>
        <name><surname>Ociepa</surname><given-names>Chris</given-names></name>
        <name><surname>Phang</surname><given-names>Jason</given-names></name>
        <name><surname>Reynolds</surname><given-names>Laria</given-names></name>
        <name><surname>Schoelkopf</surname><given-names>Hailey</given-names></name>
        <name><surname>Skowron</surname><given-names>Aviya</given-names></name>
        <name><surname>Sutawika</surname><given-names>Lintang</given-names></name>
        <name><surname>Tang</surname><given-names>Eric</given-names></name>
        <name><surname>Thite</surname><given-names>Anish</given-names></name>
        <name><surname>Wang</surname><given-names>Ben</given-names></name>
        <name><surname>Wang</surname><given-names>Kevin</given-names></name>
        <name><surname>Zou</surname><given-names>Andy</given-names></name>
      </person-group>
      <article-title>A framework for few-shot language model evaluation</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2024-07">2024</year><month>07</month>
      <uri>https://zenodo.org/records/12608602</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.12608602</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-huang2024trustllm">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Yue</given-names></name>
        <name><surname>Sun</surname><given-names>Lichao</given-names></name>
        <name><surname>Wang</surname><given-names>Haoran</given-names></name>
        <name><surname>Wu</surname><given-names>Siyuan</given-names></name>
        <name><surname>Zhang</surname><given-names>Qihui</given-names></name>
        <name><surname>Li</surname><given-names>Yuan</given-names></name>
        <name><surname>Gao</surname><given-names>Chujie</given-names></name>
        <name><surname>Huang</surname><given-names>Yixin</given-names></name>
        <name><surname>Lyu</surname><given-names>Wenhan</given-names></name>
        <name><surname>Zhang</surname><given-names>Yixuan</given-names></name>
        <name><surname>Li</surname><given-names>Xiner</given-names></name>
        <name><surname>Sun</surname><given-names>Hanchi</given-names></name>
        <name><surname>Liu</surname><given-names>Zhengliang</given-names></name>
        <name><surname>Liu</surname><given-names>Yixin</given-names></name>
        <name><surname>Wang</surname><given-names>Yijue</given-names></name>
        <name><surname>Zhang</surname><given-names>Zhikun</given-names></name>
        <name><surname>Vidgen</surname><given-names>Bertie</given-names></name>
        <name><surname>Kailkhura</surname><given-names>Bhavya</given-names></name>
        <name><surname>Xiong</surname><given-names>Caiming</given-names></name>
        <name><surname>Xiao</surname><given-names>Chaowei</given-names></name>
        <name><surname>Li</surname><given-names>Chunyuan</given-names></name>
        <name><surname>Xing</surname><given-names>Eric P.</given-names></name>
        <name><surname>Huang</surname><given-names>Furong</given-names></name>
        <name><surname>Liu</surname><given-names>Hao</given-names></name>
        <name><surname>Ji</surname><given-names>Heng</given-names></name>
        <name><surname>Wang</surname><given-names>Hongyi</given-names></name>
        <name><surname>Zhang</surname><given-names>Huan</given-names></name>
        <name><surname>Yao</surname><given-names>Huaxiu</given-names></name>
        <name><surname>Kellis</surname><given-names>Manolis</given-names></name>
        <name><surname>Zitnik</surname><given-names>Marinka</given-names></name>
        <name><surname>Jiang</surname><given-names>Meng</given-names></name>
        <name><surname>Bansal</surname><given-names>Mohit</given-names></name>
        <name><surname>Zou</surname><given-names>James</given-names></name>
        <name><surname>Pei</surname><given-names>Jian</given-names></name>
        <name><surname>Liu</surname><given-names>Jian</given-names></name>
        <name><surname>Gao</surname><given-names>Jianfeng</given-names></name>
        <name><surname>Han</surname><given-names>Jiawei</given-names></name>
        <name><surname>Zhao</surname><given-names>Jieyu</given-names></name>
        <name><surname>Tang</surname><given-names>Jiliang</given-names></name>
        <name><surname>Wang</surname><given-names>Jindong</given-names></name>
        <name><surname>Vanschoren</surname><given-names>Joaquin</given-names></name>
        <name><surname>Mitchell</surname><given-names>John</given-names></name>
        <name><surname>Shu</surname><given-names>Kai</given-names></name>
        <name><surname>Xu</surname><given-names>Kaidi</given-names></name>
        <name><surname>Chang</surname><given-names>Kai-Wei</given-names></name>
        <name><surname>He</surname><given-names>Lifang</given-names></name>
        <name><surname>Huang</surname><given-names>Lifu</given-names></name>
        <name><surname>Backes</surname><given-names>Michael</given-names></name>
        <name><surname>Gong</surname><given-names>Neil Zhenqiang</given-names></name>
        <name><surname>Yu</surname><given-names>Philip S.</given-names></name>
        <name><surname>Chen</surname><given-names>Pin-Yu</given-names></name>
        <name><surname>Gu</surname><given-names>Quanquan</given-names></name>
        <name><surname>Xu</surname><given-names>Ran</given-names></name>
        <name><surname>Ying</surname><given-names>Rex</given-names></name>
        <name><surname>Ji</surname><given-names>Shuiwang</given-names></name>
        <name><surname>Jana</surname><given-names>Suman</given-names></name>
        <name><surname>Chen</surname><given-names>Tianlong</given-names></name>
        <name><surname>Liu</surname><given-names>Tianming</given-names></name>
        <name><surname>Zhou</surname><given-names>Tianyi</given-names></name>
        <name><surname>Wang</surname><given-names>William Yang</given-names></name>
        <name><surname>Li</surname><given-names>Xiang</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiangliang</given-names></name>
        <name><surname>Wang</surname><given-names>Xiao</given-names></name>
        <name><surname>Xie</surname><given-names>Xing</given-names></name>
        <name><surname>Chen</surname><given-names>Xun</given-names></name>
        <name><surname>Wang</surname><given-names>Xuyu</given-names></name>
        <name><surname>Liu</surname><given-names>Yan</given-names></name>
        <name><surname>Ye</surname><given-names>Yanfang</given-names></name>
        <name><surname>Cao</surname><given-names>Yinzhi</given-names></name>
        <name><surname>Chen</surname><given-names>Yong</given-names></name>
        <name><surname>Zhao</surname><given-names>Yue</given-names></name>
      </person-group>
      <article-title>TrustLLM: Trustworthiness in large language models</article-title>
      <source>Forty-first international conference on machine learning</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://openreview.net/forum?id=bWUU0LwwMp</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2401.05561</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-aif360-oct-2018">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bellamy</surname><given-names>Rachel K. E.</given-names></name>
        <name><surname>Dey</surname><given-names>Kuntal</given-names></name>
        <name><surname>Hind</surname><given-names>Michael</given-names></name>
        <name><surname>Hoffman</surname><given-names>Samuel C.</given-names></name>
        <name><surname>Houde</surname><given-names>Stephanie</given-names></name>
        <name><surname>Kannan</surname><given-names>Kalapriya</given-names></name>
        <name><surname>Lohia</surname><given-names>Pranay</given-names></name>
        <name><surname>Martino</surname><given-names>Jacquelyn</given-names></name>
        <name><surname>Mehta</surname><given-names>Sameep</given-names></name>
        <name><surname>Mojsilovic</surname><given-names>Aleksandra</given-names></name>
        <name><surname>Nagar</surname><given-names>Seema</given-names></name>
        <name><surname>Ramamurthy</surname><given-names>Karthikeyan Natesan</given-names></name>
        <name><surname>Richards</surname><given-names>John</given-names></name>
        <name><surname>Saha</surname><given-names>Diptikalyan</given-names></name>
        <name><surname>Sattigeri</surname><given-names>Prasanna</given-names></name>
        <name><surname>Singh</surname><given-names>Moninder</given-names></name>
        <name><surname>Varshney</surname><given-names>Kush R.</given-names></name>
        <name><surname>Zhang</surname><given-names>Yunfeng</given-names></name>
      </person-group>
      <article-title>AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias</article-title>
      <year iso-8601-date="2018-10">2018</year><month>10</month>
      <uri>https://arxiv.org/abs/1810.01943</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1810.01943</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Weerts_Fairlearn_Assessing_and_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Weerts</surname><given-names>Hilde</given-names></name>
        <name><surname>Dudík</surname><given-names>Miroslav</given-names></name>
        <name><surname>Edgar</surname><given-names>Richard</given-names></name>
        <name><surname>Jalali</surname><given-names>Adrin</given-names></name>
        <name><surname>Lutz</surname><given-names>Roman</given-names></name>
        <name><surname>Madaio</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Fairlearn: Assessing and Improving Fairness of AI Systems</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2023">2023</year>
      <volume>24</volume>
      <uri>http://jmlr.org/papers/v24/23-0389.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-2018aequitas">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Saleiro</surname><given-names>Pedro</given-names></name>
        <name><surname>Kuester</surname><given-names>Benedict</given-names></name>
        <name><surname>Stevens</surname><given-names>Abby</given-names></name>
        <name><surname>Anisfeld</surname><given-names>Ari</given-names></name>
        <name><surname>Hinkson</surname><given-names>Loren</given-names></name>
        <name><surname>London</surname><given-names>Jesse</given-names></name>
        <name><surname>Ghani</surname><given-names>Rayid</given-names></name>
      </person-group>
      <article-title>Aequitas: A bias and fairness audit toolkit</article-title>
      <source>arXiv preprint arXiv:1811.05577</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1811.05577</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-DBLPU003AjournalsU002FcorrU002Fabs-1907-04135">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wexler</surname><given-names>James</given-names></name>
        <name><surname>Pushkarna</surname><given-names>Mahima</given-names></name>
        <name><surname>Bolukbasi</surname><given-names>Tolga</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Viégas</surname><given-names>Fernanda B.</given-names></name>
        <name><surname>Wilson</surname><given-names>Jimbo</given-names></name>
      </person-group>
      <article-title>The what-if tool: Interactive probing of machine learning models</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2019">2019</year>
      <volume>abs/1907.04135</volume>
      <uri>http://arxiv.org/abs/1907.04135</uri>
      <pub-id pub-id-type="doi">10.1109/TVCG.2019.2934619</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow-no-date">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tensorflow</surname></name>
      </person-group>
      <article-title>GitHub - tensorflow/fairness-indicators: Tensorflow’s Fairness Evaluation and Visualization Toolkit</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://github.com/tensorflow/fairness-indicators</uri>
    </element-citation>
  </ref>
  <ref id="ref-vasudevan20lift">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vasudevan</surname><given-names>Sriram</given-names></name>
        <name><surname>Kenthapadi</surname><given-names>Krishnaram</given-names></name>
      </person-group>
      <article-title>LiFT: A scalable framework for measuring fairness in ML applications</article-title>
      <source>Proceedings of the 29th ACM international conference on information and knowledge management</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1145/3340531.3412705</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-bouchard2024actionableframeworkassessingbias">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Bouchard</surname><given-names>Dylan</given-names></name>
      </person-group>
      <article-title>An actionable framework for assessing bias and fairness in large language model use cases</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2407.10853</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2407.10853</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-zekun2023auditinglargelanguagemodels">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Zekun</surname><given-names>Wu</given-names></name>
        <name><surname>Bulathwela</surname><given-names>Sahan</given-names></name>
        <name><surname>Koshiyama</surname><given-names>Adriano Soares</given-names></name>
      </person-group>
      <article-title>Towards auditing large language models: Improving text-based stereotype detection</article-title>
      <year iso-8601-date="2023">2023</year>
      <uri>https://arxiv.org/abs/2311.14126</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2311.14126</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-gallegos2024biasfairnesslargelanguage">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gallegos</surname><given-names>Isabel O.</given-names></name>
        <name><surname>Rossi</surname><given-names>Ryan A.</given-names></name>
        <name><surname>Barrow</surname><given-names>Joe</given-names></name>
        <name><surname>Tanjim</surname><given-names>Md Mehrab</given-names></name>
        <name><surname>Kim</surname><given-names>Sungchul</given-names></name>
        <name><surname>Dernoncourt</surname><given-names>Franck</given-names></name>
        <name><surname>Yu</surname><given-names>Tong</given-names></name>
        <name><surname>Zhang</surname><given-names>Ruiyi</given-names></name>
        <name><surname>Ahmed</surname><given-names>Nesreen K.</given-names></name>
      </person-group>
      <article-title>Bias and fairness in large language models: A survey</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2309.00770</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2309.00770</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-huang2020reducingsentimentbiaslanguage">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>Po-Sen</given-names></name>
        <name><surname>Zhang</surname><given-names>Huan</given-names></name>
        <name><surname>Jiang</surname><given-names>Ray</given-names></name>
        <name><surname>Stanforth</surname><given-names>Robert</given-names></name>
        <name><surname>Welbl</surname><given-names>Johannes</given-names></name>
        <name><surname>Rae</surname><given-names>Jack</given-names></name>
        <name><surname>Maini</surname><given-names>Vishal</given-names></name>
        <name><surname>Yogatama</surname><given-names>Dani</given-names></name>
        <name><surname>Kohli</surname><given-names>Pushmeet</given-names></name>
      </person-group>
      <article-title>Reducing sentiment bias in language models via counterfactual evaluation</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>https://arxiv.org/abs/1911.03064</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2020.findings-emnlp.7</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Zhang_2023">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Jizhi</given-names></name>
        <name><surname>Bao</surname><given-names>Keqin</given-names></name>
        <name><surname>Zhang</surname><given-names>Yang</given-names></name>
        <name><surname>Wang</surname><given-names>Wenjie</given-names></name>
        <name><surname>Feng</surname><given-names>Fuli</given-names></name>
        <name><surname>He</surname><given-names>Xiangnan</given-names></name>
      </person-group>
      <article-title>Is ChatGPT fair for recommendation? Evaluating fairness in large language model recommendation</article-title>
      <source>Proceedings of the 17th ACM conference on recommender systems</source>
      <publisher-name>ACM</publisher-name>
      <year iso-8601-date="2023-09">2023</year><month>09</month>
      <volume>2012</volume>
      <uri>http://dx.doi.org/10.1145/3604915.3608860</uri>
      <pub-id pub-id-type="doi">10.1145/3604915.3608860</pub-id>
      <fpage>993</fpage>
      <lpage>999</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goldfarbtarrant2021intrinsicbiasmetricscorrelate">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Goldfarb-Tarrant</surname><given-names>Seraphina</given-names></name>
        <name><surname>Marchant</surname><given-names>Rebecca</given-names></name>
        <name><surname>Sanchez</surname><given-names>Ricardo Muñoz</given-names></name>
        <name><surname>Pandya</surname><given-names>Mugdha</given-names></name>
        <name><surname>Lopez</surname><given-names>Adam</given-names></name>
      </person-group>
      <article-title>Intrinsic bias metrics do not correlate with application bias</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2012.15859</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2021.acl-long.150</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-delobelle-etal-2022-measuring">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Delobelle</surname><given-names>Pieter</given-names></name>
        <name><surname>Tokpo</surname><given-names>Ewoenam</given-names></name>
        <name><surname>Calders</surname><given-names>Toon</given-names></name>
        <name><surname>Berendt</surname><given-names>Bettina</given-names></name>
      </person-group>
      <article-title>Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models</article-title>
      <source>Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: Human language technologies</source>
      <person-group person-group-type="editor">
        <name><surname>Carpuat</surname><given-names>Marine</given-names></name>
        <name><surname>Marneffe</surname><given-names>Marie-Catherine de</given-names></name>
        <name><surname>Meza Ruiz</surname><given-names>Ivan Vladimir</given-names></name>
      </person-group>
      <publisher-name>Association for Computational Linguistics</publisher-name>
      <publisher-loc>Seattle, United States</publisher-loc>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <uri>https://aclanthology.org/2022.naacl-main.122</uri>
      <pub-id pub-id-type="doi">10.18653/v1/2022.naacl-main.122</pub-id>
      <fpage>1693</fpage>
      <lpage>1706</lpage>
    </element-citation>
  </ref>
  <ref id="ref-huggingface-no-date">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Huggingface</surname></name>
      </person-group>
      <article-title>GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/huggingface/evaluate</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>The repository for
    <monospace>langfair</monospace> can be found at
    https://github.com/cvs-health/langfair.</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>The toolkits mentioned here offer fairness
    metrics for classification. In a similar vein, the recommendation
    fairness metrics offered in FaiRLLM
    (<xref alt="Zhang et al., 2023" rid="ref-Zhang_2023" ref-type="bibr">Zhang
    et al., 2023</xref>) can be applied to ML recommendation systems as
    well as LLM recommendation use cases.</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>Experiments in Wang et al.
    (<xref alt="2023" rid="ref-wang2023decodingtrust" ref-type="bibr">2023</xref>)
    demonstrate that prompt content has substantial influence on the
    likelihood of biased LLM responses.</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>In practice, a FTU check consists of parsing use
    case prompts for mentions of protected attribute groups.</p>
  </fn>
  <fn id="fn5">
    <label>5</label><p>Note that text generation encompasses all use
    cases for which output is text, but does not belong to a predefined
    set of elements (as with classification and recommendation).</p>
  </fn>
  <fn id="fn6">
    <label>6</label><p>https://github.com/unitaryai/detoxify;
    https://github.com/huggingface/evaluate;
    https://github.com/microsoft/TOXIGEN</p>
  </fn>
  <fn id="fn7">
    <label>7</label><p>https://huggingface.co/wu981526092/Sentence-Level-Stereotype-Detector</p>
  </fn>
  <fn id="fn8">
    <label>8</label><p>https://github.com/cjhutto/vaderSentiment</p>
  </fn>
  <fn id="fn9">
    <label>9</label><p>Note that this example assumes the user has
    already set up their VertexAI credentials and sampled a list of
    prompts from their use case prompts.</p>
  </fn>
  <fn id="fn10">
    <label>10</label><p>The ‘AutoEval‘ class is designed specifically
    for text generation use cases. Applicable metrics include toxicity
    metrics, stereotype metrics, and, if FTU is not satisfied,
    counterfactual fairness metrics.</p>
  </fn>
</fn-group>
</back>
</article>
