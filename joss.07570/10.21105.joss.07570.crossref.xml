<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20250123152025-3ef170cac43c7813ed7df6ef593960f4f46ec998</doi_batch_id>
    <timestamp>20250123152025</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>01</month>
          <year>2025</year>
        </publication_date>
        <journal_volume>
          <volume>10</volume>
        </journal_volume>
        <issue>105</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Dylan</given_name>
            <surname>Bouchard</surname>
            <affiliations>
              <institution><institution_name>CVS Health Corporation</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0004-9233-2324</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Mohit Singh</given_name>
            <surname>Chauhan</surname>
            <affiliations>
              <institution><institution_name>CVS Health Corporation</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-7817-0427</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>David</given_name>
            <surname>Skarbrevik</surname>
            <affiliations>
              <institution><institution_name>CVS Health Corporation</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0005-0005-0408</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Viren</given_name>
            <surname>Bajaj</surname>
            <affiliations>
              <institution><institution_name>CVS Health Corporation</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0000-0002-9984-1293</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Zeya</given_name>
            <surname>Ahmad</surname>
            <affiliations>
              <institution><institution_name>CVS Health Corporation</institution_name></institution>
            </affiliations>
            <ORCID>https://orcid.org/0009-0009-1478-2940</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>01</month>
          <day>23</day>
          <year>2025</year>
        </publication_date>
        <pages>
          <first_page>7570</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.07570</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.14622998</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/7570</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.07570</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.07570</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.07570.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="rudinger-EtAl:2018:N18">
            <article_title>Gender bias in coreference resolution</article_title>
            <author>Rudinger</author>
            <journal_title>Proceedings of the 2018 conference of the north american chapter of the association for computational linguistics: Human language technologies</journal_title>
            <doi>10.48550/arXiv.1804.09301</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Rudinger, R., Naradowsky, J., Leonard, B., &amp; Van Durme, B. (2018). Gender bias in coreference resolution. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. https://doi.org/10.48550/arXiv.1804.09301</unstructured_citation>
          </citation>
          <citation key="zhao-2018">
            <article_title>Gender Bias in Coreference Resolution: Evaluation and Debiasing methods</article_title>
            <author>Zhao</author>
            <doi>10.48550/arXiv.1804.06876</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Zhao, J., Wang, T., Yatskar, M., Ordonez, V., &amp; Chang, K.-W. (2018). Gender Bias in Coreference Resolution: Evaluation and Debiasing methods. https://doi.org/10.48550/arXiv.1804.06876</unstructured_citation>
          </citation>
          <citation key="webster-etal-2018-mind">
            <article_title>Mind the GAP: A balanced corpus of gendered ambiguous pronouns</article_title>
            <author>Webster</author>
            <journal_title>Transactions of the Association for Computational Linguistics</journal_title>
            <volume>6</volume>
            <doi>10.1162/tacl_a_00240</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Webster, K., Recasens, M., Axelrod, V., &amp; Baldridge, J. (2018). Mind the GAP: A balanced corpus of gendered ambiguous pronouns. Transactions of the Association for Computational Linguistics, 6, 605–617. https://doi.org/10.1162/tacl_a_00240</unstructured_citation>
          </citation>
          <citation key="levy2021collecting">
            <article_title>Collecting a large-scale gender bias dataset for coreference resolution and machine translation</article_title>
            <author>Levy</author>
            <doi>10.48550/arXiv.2109.03858</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Levy, S., Lazar, K., &amp; Stanovsky, G. (2021). Collecting a large-scale gender bias dataset for coreference resolution and machine translation. https://doi.org/10.48550/arXiv.2109.03858</unstructured_citation>
          </citation>
          <citation key="nadeem2020stereoset">
            <article_title>StereoSet: Measuring stereotypical bias in pretrained language models</article_title>
            <author>Nadeem</author>
            <doi>10.48550/arXiv.2004.09456</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Nadeem, M., Bethke, A., &amp; Reddy, S. (2020). StereoSet: Measuring stereotypical bias in pretrained language models. https://doi.org/10.48550/arXiv.2004.09456</unstructured_citation>
          </citation>
          <citation key="bartl2020unmasking">
            <article_title>Unmasking contextual stereotypes: Measuring and mitigating BERT’s gender bias</article_title>
            <author>Bartl</author>
            <journal_title>Proceedings of the second workshop on gender bias in natural language processing</journal_title>
            <doi>10.48550/arXiv.2010.14534</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Bartl, M., Nissim, M., &amp; Gatt, A. (2020). Unmasking contextual stereotypes: Measuring and mitigating BERT’s gender bias. In M. R. Costa-jussà, C. Hardmeier, K. Webster, &amp; W. Radford (Eds.), Proceedings of the second workshop on gender bias in natural language processing. https://doi.org/10.48550/arXiv.2010.14534</unstructured_citation>
          </citation>
          <citation key="nangia2020crows">
            <article_title>CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models</article_title>
            <author>Nangia</author>
            <journal_title>Proceedings of the 2020 conference on empirical methods in natural language processing</journal_title>
            <doi>10.48550/arXiv.2010.00133</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Nangia, N., Vania, C., Bhalerao, R., &amp; Bowman, S. R. (2020, November). CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. https://doi.org/10.48550/arXiv.2010.00133</unstructured_citation>
          </citation>
          <citation key="felkner2024winoqueercommunityintheloopbenchmarkantilgbtq">
            <article_title>WinoQueer: A community-in-the-loop benchmark for anti-LGBTQ+ bias in large language models</article_title>
            <author>Felkner</author>
            <doi>10.48550/arXiv.2306.15087</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Felkner, V. K., Chang, H.-C. H., Jang, E., &amp; May, J. (2024). WinoQueer: A community-in-the-loop benchmark for anti-LGBTQ+ bias in large language models. https://doi.org/10.48550/arXiv.2306.15087</unstructured_citation>
          </citation>
          <citation key="barikeri2021redditbiasrealworldresourcebias">
            <article_title>RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models</article_title>
            <author>Barikeri</author>
            <doi>10.48550/arXiv.2106.03521</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Barikeri, S., Lauscher, A., Vulić, I., &amp; Glavaš, G. (2021). RedditBias: A real-world resource for bias evaluation and debiasing of conversational language models. https://doi.org/10.48550/arXiv.2106.03521</unstructured_citation>
          </citation>
          <citation key="qian2022perturbationaugmentationfairernlp">
            <article_title>Perturbation augmentation for fairer NLP</article_title>
            <author>Qian</author>
            <doi>10.48550/arXiv.2205.12586</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Qian, R., Ross, C., Fernandes, J., Smith, E., Kiela, D., &amp; Williams, A. (2022). Perturbation augmentation for fairer NLP. https://doi.org/10.48550/arXiv.2205.12586</unstructured_citation>
          </citation>
          <citation key="kiritchenko2018examininggenderracebias">
            <article_title>Examining gender and race bias in two hundred sentiment analysis systems</article_title>
            <author>Kiritchenko</author>
            <doi>10.18653/v1/S18-2005</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Kiritchenko, S., &amp; Mohammad, S. M. (2018). Examining gender and race bias in two hundred sentiment analysis systems. https://doi.org/10.18653/v1/S18-2005</unstructured_citation>
          </citation>
          <citation key="Gehman2020RealToxicityPromptsEN">
            <article_title>RealToxicityPrompts: Evaluating neural toxic degeneration in language models</article_title>
            <author>Gehman</author>
            <journal_title>Findings</journal_title>
            <doi>10.18653/v1/2020.findings-emnlp.301</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Gehman, S., Gururangan, S., Sap, M., Choi, Y., &amp; Smith, N. A. (2020). RealToxicityPrompts: Evaluating neural toxic degeneration in language models. Findings. https://doi.org/10.18653/v1/2020.findings-emnlp.301</unstructured_citation>
          </citation>
          <citation key="bold_2021">
            <article_title>BOLD: Dataset and metrics for measuring biases in open-ended language generation</article_title>
            <author>Dhamala</author>
            <journal_title>Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</journal_title>
            <doi>10.1145/3442188.3445924</doi>
            <isbn>9781450383097</isbn>
            <cYear>2021</cYear>
            <unstructured_citation>Dhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-W., &amp; Gupta, R. (2021). BOLD: Dataset and metrics for measuring biases in open-ended language generation. Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 862–872. https://doi.org/10.1145/3442188.3445924</unstructured_citation>
          </citation>
          <citation key="huang2023trustgptbenchmarktrustworthyresponsible">
            <article_title>TrustGPT: A benchmark for trustworthy and responsible large language models</article_title>
            <author>Huang</author>
            <doi>10.48550/arXiv.2306.11507</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Huang, Y., Zhang, Q., Y, P. S., &amp; Sun, L. (2023). TrustGPT: A benchmark for trustworthy and responsible large language models. https://doi.org/10.48550/arXiv.2306.11507</unstructured_citation>
          </citation>
          <citation key="nozza-etal-2021-honest">
            <article_title>"HONEST: Measuring hurtful sentence completion in language models"</article_title>
            <author>Nozza</author>
            <journal_title>Proceedings of the 2021 conference of the north american chapter of the association for computational linguistics: Human language technologies</journal_title>
            <doi>10.18653/v1/2021.naacl-main.191</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Nozza, D., Bianchi, F., &amp; Hovy, D. (2021). "HONEST: Measuring hurtful sentence completion in language models". Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2398–2406. https://doi.org/10.18653/v1/2021.naacl-main.191</unstructured_citation>
          </citation>
          <citation key="parrish-etal-2022-bbq">
            <article_title>BBQ: A hand-built bias benchmark for question answering</article_title>
            <author>Parrish</author>
            <journal_title>Findings of the association for computational linguistics: ACL 2022</journal_title>
            <doi>10.18653/v1/2022.findings-acl.165</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., &amp; Bowman, S. (2022). BBQ: A hand-built bias benchmark for question answering. In S. Muresan, P. Nakov, &amp; A. Villavicencio (Eds.), Findings of the association for computational linguistics: ACL 2022 (pp. 2086–2105). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.findings-acl.165</unstructured_citation>
          </citation>
          <citation key="li-etal-2020-unqovering">
            <article_title>UNQOVERing stereotyping biases via underspecified questions</article_title>
            <author>Li</author>
            <journal_title>Findings of the association for computational linguistics: EMNLP 2020</journal_title>
            <doi>10.18653/v1/2020.findings-emnlp.311</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Li, T., Khashabi, D., Khot, T., Sabharwal, A., &amp; Srikumar, V. (2020). UNQOVERing stereotyping biases via underspecified questions. In T. Cohn, Y. He, &amp; Y. Liu (Eds.), Findings of the association for computational linguistics: EMNLP 2020 (pp. 3475–3489). Association for Computational Linguistics. https://doi.org/10.18653/v1/2020.findings-emnlp.311</unstructured_citation>
          </citation>
          <citation key="10.1145/3576840.3578295">
            <article_title>Grep-BiasIR: A dataset for investigating gender representation bias in information retrieval results</article_title>
            <author>Krieg</author>
            <journal_title>Proceedings of the 2023 conference on human information interaction and retrieval</journal_title>
            <doi>10.1145/3576840.3578295</doi>
            <isbn>9798400700354</isbn>
            <cYear>2023</cYear>
            <unstructured_citation>Krieg, K., Parada-Cabaleiro, E., Medicus, G., Lesota, O., Schedl, M., &amp; Rekabsaz, N. (2023). Grep-BiasIR: A dataset for investigating gender representation bias in information retrieval results. Proceedings of the 2023 Conference on Human Information Interaction and Retrieval, 444–448. https://doi.org/10.1145/3576840.3578295</unstructured_citation>
          </citation>
          <citation key="liang2023holisticevaluationlanguagemodels">
            <article_title>Holistic evaluation of language models</article_title>
            <author>Liang</author>
            <doi>10.48550/arXiv.2211.09110</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang, Y., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan, B., Zhang, C., Cosgrove, C., Manning, C. D., Ré, C., Acosta-Navas, D., Hudson, D. A., … Koreeda, Y. (2023). Holistic evaluation of language models. https://doi.org/10.48550/arXiv.2211.09110</unstructured_citation>
          </citation>
          <citation key="wang2023decodingtrust">
            <article_title>DecodingTrust: A comprehensive assessment of trustworthiness in GPT models</article_title>
            <author>Wang</author>
            <doi>10.48550/arXiv.2306.11698</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Wang, B., Chen, W., Pei, H., Xie, C., Kang, M., Zhang, C., Xu, C., Xiong, Z., Dutta, R., Schaeffer, R., &amp; others. (2023). DecodingTrust: A comprehensive assessment of trustworthiness in GPT models. https://doi.org/10.48550/arXiv.2306.11698</unstructured_citation>
          </citation>
          <citation key="huggingface-no-date">
            <article_title>GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets.</article_title>
            <author>Huggingface</author>
            <cYear>2022</cYear>
            <unstructured_citation>Huggingface. (2022). GitHub - huggingface/evaluate: Evaluate: A library for easily evaluating machine learning models and datasets. https://github.com/huggingface/evaluate</unstructured_citation>
          </citation>
          <citation key="Arshaan_Nazir">
            <article_title>LangTest: A comprehensive evaluation library for custom LLM and NLP models</article_title>
            <author>Nazir</author>
            <journal_title>Software Impacts</journal_title>
            <issue>100619</issue>
            <volume>19</volume>
            <doi>10.1016/j.simpa.2024.100619</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Nazir, A., Chakravarthy, T. K., Cecchini, D. A., Chakravarthy, T. K., Khajuria, R., Sharma, P., Mirik, A. T., Kocaman, V., &amp; Talby, D. (2024). LangTest: A comprehensive evaluation library for custom LLM and NLP models. Software Impacts, 19(100619). https://doi.org/10.1016/j.simpa.2024.100619</unstructured_citation>
          </citation>
          <citation key="srivastava2022beyond">
            <article_title>Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</article_title>
            <author>Srivastava</author>
            <journal_title>arXiv preprint arXiv:2206.04615</journal_title>
            <doi>10.48550/arXiv.2206.04615</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A., &amp; others. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv Preprint arXiv:2206.04615. https://doi.org/10.48550/arXiv.2206.04615</unstructured_citation>
          </citation>
          <citation key="eval-harness">
            <article_title>A framework for few-shot language model evaluation</article_title>
            <author>Gao</author>
            <doi>10.5281/zenodo.12608602</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff, N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika, L., … Zou, A. (2024). A framework for few-shot language model evaluation (Version v0.4.3). Zenodo. https://doi.org/10.5281/zenodo.12608602</unstructured_citation>
          </citation>
          <citation key="huang2024trustllm">
            <article_title>TrustLLM: Trustworthiness in large language models</article_title>
            <author>Huang</author>
            <journal_title>Forty-first international conference on machine learning</journal_title>
            <doi>10.48550/arXiv.2401.05561</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Huang, Y., Sun, L., Wang, H., Wu, S., Zhang, Q., Li, Y., Gao, C., Huang, Y., Lyu, W., Zhang, Y., Li, X., Sun, H., Liu, Z., Liu, Y., Wang, Y., Zhang, Z., Vidgen, B., Kailkhura, B., Xiong, C., … Zhao, Y. (2024). TrustLLM: Trustworthiness in large language models. Forty-First International Conference on Machine Learning. https://doi.org/10.48550/arXiv.2401.05561</unstructured_citation>
          </citation>
          <citation key="aif360-oct-2018">
            <article_title>AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias</article_title>
            <author>Bellamy</author>
            <doi>10.48550/arXiv.1810.01943</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Bellamy, R. K. E., Dey, K., Hind, M., Hoffman, S. C., Houde, S., Kannan, K., Lohia, P., Martino, J., Mehta, S., Mojsilovic, A., Nagar, S., Ramamurthy, K. N., Richards, J., Saha, D., Sattigeri, P., Singh, M., Varshney, K. R., &amp; Zhang, Y. (2018). AI Fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias. https://doi.org/10.48550/arXiv.1810.01943</unstructured_citation>
          </citation>
          <citation key="Weerts_Fairlearn_Assessing_and_2023">
            <article_title>Fairlearn: Assessing and Improving Fairness of AI Systems</article_title>
            <author>Weerts</author>
            <journal_title>Journal of Machine Learning Research</journal_title>
            <volume>24</volume>
            <cYear>2023</cYear>
            <unstructured_citation>Weerts, H., Dudík, M., Edgar, R., Jalali, A., Lutz, R., &amp; Madaio, M. (2023). Fairlearn: Assessing and Improving Fairness of AI Systems. Journal of Machine Learning Research, 24. http://jmlr.org/papers/v24/23-0389.html</unstructured_citation>
          </citation>
          <citation key="2018aequitas">
            <article_title>Aequitas: A bias and fairness audit toolkit</article_title>
            <author>Saleiro</author>
            <journal_title>arXiv preprint arXiv:1811.05577</journal_title>
            <doi>10.48550/arXiv.1811.05577</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Saleiro, P., Kuester, B., Stevens, A., Anisfeld, A., Hinkson, L., London, J., &amp; Ghani, R. (2018). Aequitas: A bias and fairness audit toolkit. arXiv Preprint arXiv:1811.05577. https://doi.org/10.48550/arXiv.1811.05577</unstructured_citation>
          </citation>
          <citation key="DBLP:journals/corr/abs-1907-04135">
            <article_title>The what-if tool: Interactive probing of machine learning models</article_title>
            <author>Wexler</author>
            <journal_title>CoRR</journal_title>
            <volume>abs/1907.04135</volume>
            <doi>10.1109/TVCG.2019.2934619</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Wexler, J., Pushkarna, M., Bolukbasi, T., Wattenberg, M., Viégas, F. B., &amp; Wilson, J. (2019). The what-if tool: Interactive probing of machine learning models. CoRR, abs/1907.04135. https://doi.org/10.1109/TVCG.2019.2934619</unstructured_citation>
          </citation>
          <citation key="tensorflow-no-date">
            <article_title>GitHub - tensorflow/fairness-indicators: Tensorflow’s Fairness Evaluation and Visualization Toolkit</article_title>
            <author>Tensorflow</author>
            <cYear>2020</cYear>
            <unstructured_citation>Tensorflow. (2020). GitHub - tensorflow/fairness-indicators: Tensorflow’s Fairness Evaluation and Visualization Toolkit. https://github.com/tensorflow/fairness-indicators</unstructured_citation>
          </citation>
          <citation key="vasudevan20lift">
            <article_title>LiFT: A scalable framework for measuring fairness in ML applications</article_title>
            <author>Vasudevan</author>
            <journal_title>Proceedings of the 29th ACM international conference on information and knowledge management</journal_title>
            <doi>10.1145/3340531.3412705</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Vasudevan, S., &amp; Kenthapadi, K. (2020). LiFT: A scalable framework for measuring fairness in ML applications. Proceedings of the 29th ACM International Conference on Information and Knowledge Management. https://doi.org/10.1145/3340531.3412705</unstructured_citation>
          </citation>
          <citation key="bouchard2024actionableframeworkassessingbias">
            <article_title>An actionable framework for assessing bias and fairness in large language model use cases</article_title>
            <author>Bouchard</author>
            <doi>10.48550/arXiv.2407.10853</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Bouchard, D. (2024). An actionable framework for assessing bias and fairness in large language model use cases. https://doi.org/10.48550/arXiv.2407.10853</unstructured_citation>
          </citation>
          <citation key="zekun2023auditinglargelanguagemodels">
            <article_title>Towards auditing large language models: Improving text-based stereotype detection</article_title>
            <author>Zekun</author>
            <doi>10.48550/arXiv.2311.14126</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Zekun, W., Bulathwela, S., &amp; Koshiyama, A. S. (2023). Towards auditing large language models: Improving text-based stereotype detection. https://doi.org/10.48550/arXiv.2311.14126</unstructured_citation>
          </citation>
          <citation key="gallegos2024biasfairnesslargelanguage">
            <article_title>Bias and fairness in large language models: A survey</article_title>
            <author>Gallegos</author>
            <doi>10.48550/arXiv.2309.00770</doi>
            <cYear>2024</cYear>
            <unstructured_citation>Gallegos, I. O., Rossi, R. A., Barrow, J., Tanjim, M. M., Kim, S., Dernoncourt, F., Yu, T., Zhang, R., &amp; Ahmed, N. K. (2024). Bias and fairness in large language models: A survey. https://doi.org/10.48550/arXiv.2309.00770</unstructured_citation>
          </citation>
          <citation key="huang2020reducingsentimentbiaslanguage">
            <article_title>Reducing sentiment bias in language models via counterfactual evaluation</article_title>
            <author>Huang</author>
            <doi>10.18653/v1/2020.findings-emnlp.7</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Huang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini, V., Yogatama, D., &amp; Kohli, P. (2020). Reducing sentiment bias in language models via counterfactual evaluation. https://doi.org/10.18653/v1/2020.findings-emnlp.7</unstructured_citation>
          </citation>
          <citation key="Zhang_2023">
            <article_title>Is ChatGPT fair for recommendation? Evaluating fairness in large language model recommendation</article_title>
            <author>Zhang</author>
            <journal_title>Proceedings of the 17th ACM conference on recommender systems</journal_title>
            <volume>2012</volume>
            <doi>10.1145/3604915.3608860</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Zhang, J., Bao, K., Zhang, Y., Wang, W., Feng, F., &amp; He, X. (2023). Is ChatGPT fair for recommendation? Evaluating fairness in large language model recommendation. Proceedings of the 17th ACM Conference on Recommender Systems, 2012, 993–999. https://doi.org/10.1145/3604915.3608860</unstructured_citation>
          </citation>
          <citation key="goldfarbtarrant2021intrinsicbiasmetricscorrelate">
            <article_title>Intrinsic bias metrics do not correlate with application bias</article_title>
            <author>Goldfarb-Tarrant</author>
            <doi>10.18653/v1/2021.acl-long.150</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Goldfarb-Tarrant, S., Marchant, R., Sanchez, R. M., Pandya, M., &amp; Lopez, A. (2021). Intrinsic bias metrics do not correlate with application bias. https://doi.org/10.18653/v1/2021.acl-long.150</unstructured_citation>
          </citation>
          <citation key="delobelle-etal-2022-measuring">
            <article_title>Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models</article_title>
            <author>Delobelle</author>
            <journal_title>Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: Human language technologies</journal_title>
            <doi>10.18653/v1/2022.naacl-main.122</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Delobelle, P., Tokpo, E., Calders, T., &amp; Berendt, B. (2022). Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In M. Carpuat, M.-C. de Marneffe, &amp; I. V. Meza Ruiz (Eds.), Proceedings of the 2022 conference of the North American chapter of the association for computational linguistics: Human language technologies (pp. 1693–1706). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.122</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
