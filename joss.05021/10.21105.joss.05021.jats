<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5021</article-id>
<article-id pub-id-type="doi">10.21105/joss.05021</article-id>
<title-group>
<article-title>flowMC: Normalizing flow enhanced sampling package for
probabilistic inference in JAX</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8432-7788</contrib-id>
<name>
<surname>Wong</surname>
<given-names>Kaze W. K.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5989-1018</contrib-id>
<name>
<surname>Gabri√©</surname>
<given-names>Marylou</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9328-5652</contrib-id>
<name>
<surname>Foreman-Mackey</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Center for Computational Astrophysics, Flatiron Institute,
New York, NY 10010, US</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>√âcole Polytechnique, Palaiseau 91120, France</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Center for Computational Mathematics, Flatiron Institute,
New York, NY 10010, US</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-09-30">
<day>30</day>
<month>9</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>83</issue>
<fpage>5021</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bayesian Inference</kwd>
<kwd>Machine Learning</kwd>
<kwd>JAX</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Across scientific fields, the Bayesian framework is used to account
  for uncertainties in inference
  (<xref alt="Ellison, 2004" rid="ref-ellison2004bayesian" ref-type="bibr">Ellison,
  2004</xref>;
  <xref alt="Lancaster, 2004" rid="ref-lancaster2004introduction" ref-type="bibr">Lancaster,
  2004</xref>;
  <xref alt="Von Toussaint, 2011" rid="ref-von2011bayesian" ref-type="bibr">Von
  Toussaint, 2011</xref>). However, for models with more than a few
  parameters, exact inference is intractable. A frequently used strategy
  is to approximately sample the posterior distribution on a model‚Äôs
  parameters with a Markov chain Monte Carlo (MCMC) method. Yet
  conventional MCMC methods relying on local updates can take a
  prohibitive time to converge when posterior distributions have complex
  geometries (see e.g., Rubinstein &amp; Kroese
  (<xref alt="2017" rid="ref-rubinstein_simulation_2017" ref-type="bibr">2017</xref>)).</p>
  <p><monospace>flowMC</monospace> is a Python library implementing
  accelerated MCMC leveraging deep generative modelling as proposed by
  Gabri√© et al.
  (<xref alt="2022" rid="ref-Gabrie2022" ref-type="bibr">2022</xref>),
  built on top of the machine learning libraries JAX
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>) and Flax
  (<xref alt="Heek et al., 2020" rid="ref-flax2020github" ref-type="bibr">Heek
  et al., 2020</xref>). At its core, <monospace>flowMC</monospace> uses
  a combination of Metropolis-Hastings Markov kernels using local and
  global proposed moves. While multiple chains are run using
  local-update Markov kernels to generate approximate samples over the
  region of interest in the target parameter space, these samples are
  used to train a normalizing flow (NF) model to approximate the
  samples‚Äô density. The NF is then used in an independent
  Metropolis-Hastings kernel to propose global jumps across the
  parameter space. The <monospace>flowMC</monospace> sampler can handle
  non-trivial geometry, such as multimodal distributions and
  distributions with local correlations.</p>
  <p>The key features of <monospace>flowMC</monospace> are summarized in
  the following list:</p>
  <list list-type="bullet">
    <list-item>
      <p>Since <monospace>flowMC</monospace> is built on top of JAX, it
      supports gradient-based samplers through automatic differentiation
      such as the Metropolis-adjusted Langevin algorithm (MALA) and
      Hamiltonian Monte Carlo (HMC).</p>
    </list-item>
    <list-item>
      <p><monospace>flowMC</monospace> uses state-of-the-art NF models
      such as rational quadratic splines (RQS) to power its global
      proposals. These models are efficient in capturing important
      features within a relatively short training time.</p>
    </list-item>
    <list-item>
      <p>Use of accelerators such as graphics processing units (GPUs)
      and tensor processing units (TPUs) are natively supported. The
      code also supports the use of multiple accelerators with SIMD
      parallelism.</p>
    </list-item>
    <list-item>
      <p>By default, just-in-time (JIT) compilations are used to further
      accelerate the sampling process.</p>
    </list-item>
    <list-item>
      <p>We provide a simple black-box interface for the users who want
      to use <monospace>flowMC</monospace> by its default parameters, as
      well as an extensive guide explaining trade-offs while tuning the
      sampler parameters.</p>
    </list-item>
  </list>
  <p>The tight integration of all the above features makes
  <monospace>flowMC</monospace> a highly performant yet simple-to-use
  package for statistical inference.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Bayesian inference requires computing expectations with respect to
  a posterior distribution on parameters <inline-formula><alternatives>
  <tex-math><![CDATA[\theta]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ∏</mml:mi></mml:math></alternatives></inline-formula>
  after collecting observations <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{D}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùíü</mml:mi></mml:math></alternatives></inline-formula>.
  This posterior is given by</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[
  p(\theta|\mathcal{D}) = \frac{\ell(\mathcal{D}|\theta) p_0(\theta)}{Z(\mathcal{D})},  
  ]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>ùíü</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùíü</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùíü</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[\ell(\mathcal{D}|\theta)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo>‚Ñì</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùíü</mml:mi><mml:mo stretchy="false" form="prefix">|</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is the likelihood induced by the model, <inline-formula><alternatives>
  <tex-math><![CDATA[p_0(\theta)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  the prior on the parameters and <inline-formula><alternatives>
  <tex-math><![CDATA[Z(\mathcal{D})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Z</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùíü</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  the model evidence. For parameter space with more than a few
  dimensions, it is necessary to resort to a robust sampling strategy
  such as MCMC. Drastic gains in computational efficiency can be
  obtained by a careful selection of the MCMC transition kernel, which
  can be assisted by machine learning methods and libraries.</p>
  <p><bold><italic>Gradient-based sampler</italic></bold> In a high
  dimensional space, sampling methods which leverage gradient
  information of the target distribution aid by proposing new samples
  likely to be accepted. <monospace>flowMC</monospace> supports
  gradient-based samplers such as MALA and HMC through automatic
  differentiation with JAX.</p>
  <p><bold><italic>Learned transition kernels with NFs</italic></bold>
  When the posterior distribution has a non-trivial geometry, such as
  multiple modes or spatially dependent correlation structures (e.g,
  (<xref alt="Neal, 2003" rid="ref-neal2003slice" ref-type="bibr">Neal,
  2003</xref>)), samplers based on local updates are inefficient. To
  address this problem, <monospace>flowMC</monospace> also uses a
  generative model, namely a NF
  (<xref alt="Kobyzev et al., 2021" rid="ref-Kobyzev2021" ref-type="bibr">Kobyzev
  et al., 2021</xref>;
  <xref alt="Papamakarios et al., 2021" rid="ref-Papamakarios2019" ref-type="bibr">Papamakarios
  et al., 2021</xref>), that is trained to mimic the posterior
  distribution and used as a proposal in Metropolis-Hastings MCMC steps.
  Variants of this idea have been explored in the past few years
  (<xref alt="Albergo et al., 2019" rid="ref-Albergo2019" ref-type="bibr">Albergo
  et al., 2019</xref>;
  <xref alt="Hoffman et al., 2019" rid="ref-Hoffman2019" ref-type="bibr">Hoffman
  et al., 2019</xref>; e.g.,
  <xref alt="Parno &amp; Marzouk, 2018" rid="ref-Parno2018" ref-type="bibr">Parno
  &amp; Marzouk, 2018</xref>). Despite the growing interest in these
  methods, few accessible implementations for practitioners exist,
  especially with GPU and TPU support. Notably, a version of the NeuTra
  sampler
  (<xref alt="Hoffman et al., 2019" rid="ref-Hoffman2019" ref-type="bibr">Hoffman
  et al., 2019</xref>) is available in Pyro
  (<xref alt="Bingham et al., 2019" rid="ref-bingham2019pyro" ref-type="bibr">Bingham
  et al., 2019</xref>), and the PocoMC package
  (<xref alt="Karamanis et al., 2022" rid="ref-karamanis_accelerating_2022" ref-type="bibr">Karamanis
  et al., 2022</xref>) implements a version of sequential Monte Carlo
  (SMC), including NFs.</p>
  <p><monospace>flowMC</monospace> implements the method proposed by
  Gabri√© et al.
  (<xref alt="2022" rid="ref-Gabrie2022" ref-type="bibr">2022</xref>).
  As individual chains explore their local neighborhood through
  gradient-based MCMC steps, multiple chains train the NF to learn the
  global landscape of the posterior distribution. In turn, the chains
  can be propagated with a Metropolis-Hastings kernel using the NF to
  propose globally in the parameter space. The cycle of local sampling,
  NF tuning, and global sampling is repeated until chains of the desired
  length are obtained. The entire algorithm belongs to the class of
  adaptive MCMC methods
  (<xref alt="Andrieu &amp; Thoms, 2008" rid="ref-Andrieu2008" ref-type="bibr">Andrieu
  &amp; Thoms, 2008</xref>), collecting information from the chains‚Äô
  previous steps to simultaneously improve the transition kernel. Usual
  MCMC diagnostics can be applied to assess the robustness of the
  inference results, thereby avoiding the common concern of validating
  the NF model. If further sampling from the posterior is necessary, the
  flow trained during a previous run can be reused without further
  training. The mathematical details of the method are explained in
  (<xref alt="Gabri√© et al., 2021" rid="ref-Gabrie2021a" ref-type="bibr">Gabri√©
  et al., 2021</xref>,
  <xref alt="2022" rid="ref-Gabrie2022" ref-type="bibr">2022</xref>).</p>
  <p><bold><italic>Use of accelerators</italic></bold>
  <monospace>flowMC</monospace> is built on top of JAX, which supports
  the use of GPU and TPU accelerators by default. Users can write code
  the same way as they would on a CPU, and the library will
  automatically detect the available accelerators and use them at run
  time. Furthermore, the library leverages JIT compilations to further
  improve the performance of the sampler.</p>
  <p><bold><italic>Simplicity and extensibility</italic></bold> We
  provide a black-box interface with a few tuning parameters for users
  who intend to use <monospace>flowMC</monospace> without too much
  customization on the sampler side. The only inputs we require from the
  users are (1) a function to evaluate the logarithm of the
  (unnormalized) density of the posterior distribution of interest and
  (2) the initial position of the chains. On top of the black-box
  interface, the package offers automatic tuning for the local samplers
  to reduce the number of hyperparameters users need to manage.</p>
  <p>While we provide a high-level interface suitable for most
  practitioners, the code is also designed to be extensible. Researchers
  with knowledge of more appropriate local and/or global sampling
  kernels for their application can integrate the kernels in the sampler
  module.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>M.G. acknowledges funding from Hi! PARIS.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Gabrie2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Gabri√©</surname><given-names>Marylou</given-names></name>
        <name><surname>Rotskoff</surname><given-names>Grant M.</given-names></name>
        <name><surname>Vanden-Eijnden</surname><given-names>Eric</given-names></name>
      </person-group>
      <article-title>Adaptive Monte Carlo augmented with normalizing flows</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <year iso-8601-date="2022-03">2022</year><month>03</month>
      <volume>119</volume>
      <issue>10</issue>
      <issn>0027-8424</issn>
      <uri>https://pnas.org/doi/full/10.1073/pnas.2109420119</uri>
      <pub-id pub-id-type="doi">10.1073/pnas.2109420119</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Hoffman2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hoffman</surname><given-names>Matthew D</given-names></name>
        <name><surname>Sountsov</surname><given-names>Pavel</given-names></name>
        <name><surname>Dillon</surname><given-names>Joshua V.</given-names></name>
        <name><surname>Langmore</surname><given-names>Ian</given-names></name>
        <name><surname>Tran</surname><given-names>Dustin</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Srinivas</given-names></name>
      </person-group>
      <article-title>NeuTra-lizing bad geometry in Hamiltonian Monte Carlo using neural transport</article-title>
      <source>1st symposium on advances in approximate bayesian inference, 2018 1‚Äì5</source>
      <year iso-8601-date="2019">2019</year>
      <uri>http://arxiv.org/abs/1903.03704</uri>
    </element-citation>
  </ref>
  <ref id="ref-Albergo2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Albergo</surname><given-names>M. S.</given-names></name>
        <name><surname>Kanwar</surname><given-names>G.</given-names></name>
        <name><surname>Shanahan</surname><given-names>P. E.</given-names></name>
      </person-group>
      <article-title>Flow-based generative models for Markov chain Monte Carlo in lattice field theory</article-title>
      <source>Physical Review D</source>
      <publisher-name>American Physical Society</publisher-name>
      <year iso-8601-date="2019-08">2019</year><month>08</month>
      <volume>100</volume>
      <issue>3</issue>
      <issn>2470-0010</issn>
      <uri>https://link.aps.org/doi/10.1103/PhysRevD.100.034515</uri>
      <pub-id pub-id-type="doi">10.1103/PhysRevD.100.034515</pub-id>
      <fpage>034515</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Gabrie2021a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gabri√©</surname><given-names>Marylou</given-names></name>
        <name><surname>Rotskoff</surname><given-names>Grant M.</given-names></name>
        <name><surname>Vanden-Eijnden</surname><given-names>Eric</given-names></name>
      </person-group>
      <article-title>Efficient Bayesian sampling using normalizing flows to assist Markov chain Monte Carlo methods</article-title>
      <source>Invertible neural networks, NormalizingFlows, and explicit likelihood models (ICML workshop).</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://arxiv.org/abs/2107.08001</uri>
    </element-citation>
  </ref>
  <ref id="ref-Kobyzev2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kobyzev</surname><given-names>Ivan</given-names></name>
        <name><surname>Prince</surname><given-names>Simon J. D.</given-names></name>
        <name><surname>Brubaker</surname><given-names>Marcus A.</given-names></name>
      </person-group>
      <article-title>Normalizing flows: An introduction and review of current methods</article-title>
      <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>43</volume>
      <issue>11</issue>
      <uri>https://arxiv.org/abs/1908.09257</uri>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2020.2992934</pub-id>
      <pub-id pub-id-type="pmid">32396070</pub-id>
      <fpage>3964</fpage>
      <lpage>3979</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Papamakarios2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Papamakarios</surname><given-names>George</given-names></name>
        <name><surname>Nalisnick</surname><given-names>Eric</given-names></name>
        <name><surname>Rezende</surname><given-names>Danilo Jimenez</given-names></name>
        <name><surname>Mohamed</surname><given-names>Shakir</given-names></name>
        <name><surname>Lakshminarayanan</surname><given-names>Balaji</given-names></name>
      </person-group>
      <article-title>Normalizing flows for probabilistic modeling and inference</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>57</issue>
      <uri>https://jmlr.org/papers/v22/19-1028.html</uri>
      <fpage>1</fpage>
      <lpage>64</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Andrieu2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Andrieu</surname><given-names>Christophe</given-names></name>
        <name><surname>Thoms</surname><given-names>Johannes</given-names></name>
      </person-group>
      <article-title>A tutorial on adaptive MCMC</article-title>
      <source>Statistics and Computing</source>
      <year iso-8601-date="2008">2008</year>
      <volume>18</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1007/s11222-008-9110-y</pub-id>
      <fpage>343</fpage>
      <lpage>373</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bingham2019pyro">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bingham</surname><given-names>Eli</given-names></name>
        <name><surname>Chen</surname><given-names>Jonathan P.</given-names></name>
        <name><surname>Jankowiak</surname><given-names>Martin</given-names></name>
        <name><surname>Obermeyer</surname><given-names>Fritz</given-names></name>
        <name><surname>Pradhan</surname><given-names>Neeraj</given-names></name>
        <name><surname>Karaletsos</surname><given-names>Theofanis</given-names></name>
        <name><surname>Singh</surname><given-names>Rohit</given-names></name>
        <name><surname>Szerlip</surname><given-names>Paul A.</given-names></name>
        <name><surname>Horsfall</surname><given-names>Paul</given-names></name>
        <name><surname>Goodman</surname><given-names>Noah D.</given-names></name>
      </person-group>
      <article-title>Pyro: Deep universal probabilistic programming</article-title>
      <source>J. Mach. Learn. Res.</source>
      <year iso-8601-date="2019">2019</year>
      <volume>20</volume>
      <uri>http://jmlr.org/papers/v20/18-403.html</uri>
      <fpage>28:1</fpage>
      <lpage>28:6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-karamanis_accelerating_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Karamanis</surname><given-names>Minas</given-names></name>
        <name><surname>Beutler</surname><given-names>Florian</given-names></name>
        <name><surname>Peacock</surname><given-names>John A</given-names></name>
        <name><surname>Nabergoj</surname><given-names>David</given-names></name>
        <name><surname>Seljak</surname><given-names>Uro≈°</given-names></name>
      </person-group>
      <article-title>Accelerating astronomical and cosmological inference with preconditioned Monte Carlo</article-title>
      <source>Monthly Notices of the Royal Astronomical Society</source>
      <year iso-8601-date="2022-08">2022</year><month>08</month>
      <volume>516</volume>
      <issue>2</issue>
      <issn>0035-8711</issn>
      <uri>https://doi.org/10.1093/mnras/stac2272</uri>
      <pub-id pub-id-type="doi">10.1093/mnras/stac2272</pub-id>
      <fpage>1644</fpage>
      <lpage>1653</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Parno2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Parno</surname><given-names>Matthew D.</given-names></name>
        <name><surname>Marzouk</surname><given-names>Youssef M.</given-names></name>
      </person-group>
      <article-title>Transport map accelerated Markov chain Monte Carlo</article-title>
      <source>SIAM-ASA Journal on Uncertainty Quantification</source>
      <year iso-8601-date="2018">2018</year>
      <volume>6</volume>
      <pub-id pub-id-type="doi">10.1137/17M1134640</pub-id>
      <fpage>645</fpage>
      <lpage>682</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jax2018github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Frostig</surname><given-names>Roy</given-names></name>
        <name><surname>Hawkins</surname><given-names>Peter</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew James</given-names></name>
        <name><surname>Leary</surname><given-names>Chris</given-names></name>
        <name><surname>Maclaurin</surname><given-names>Dougal</given-names></name>
        <name><surname>Necula</surname><given-names>George</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>Skye</given-names></name>
        <name><surname>Zhang</surname><given-names>Qiao</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-flax2020github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Heek</surname><given-names>Jonathan</given-names></name>
        <name><surname>Levskaya</surname><given-names>Anselm</given-names></name>
        <name><surname>Oliver</surname><given-names>Avital</given-names></name>
        <name><surname>Ritter</surname><given-names>Marvin</given-names></name>
        <name><surname>Rondepierre</surname><given-names>Bertrand</given-names></name>
        <name><surname>Steiner</surname><given-names>Andreas</given-names></name>
        <name><surname>Zee</surname><given-names>Marc van</given-names></name>
      </person-group>
      <article-title>Flax: A neural network library and ecosystem for JAX</article-title>
      <year iso-8601-date="2020">2020</year>
      <uri>http://github.com/google/flax</uri>
    </element-citation>
  </ref>
  <ref id="ref-ellison2004bayesian">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ellison</surname><given-names>Aaron M</given-names></name>
      </person-group>
      <article-title>Bayesian inference in ecology</article-title>
      <source>Ecology letters</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2004">2004</year>
      <volume>7</volume>
      <issue>6</issue>
      <fpage>509</fpage>
      <lpage>520</lpage>
    </element-citation>
  </ref>
  <ref id="ref-von2011bayesian">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Von Toussaint</surname><given-names>Udo</given-names></name>
      </person-group>
      <article-title>Bayesian inference in physics</article-title>
      <source>Reviews of Modern Physics</source>
      <publisher-name>APS</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <volume>83</volume>
      <issue>3</issue>
      <fpage>943</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-lancaster2004introduction">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Lancaster</surname><given-names>Tony</given-names></name>
      </person-group>
      <source>An introduction to modern bayesian econometrics</source>
      <publisher-name>Blackwell Oxford</publisher-name>
      <year iso-8601-date="2004">2004</year>
    </element-citation>
  </ref>
  <ref id="ref-rubinstein_simulation_2017">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Rubinstein</surname><given-names>Reuven Y.</given-names></name>
        <name><surname>Kroese</surname><given-names>Dirk P.</given-names></name>
      </person-group>
      <source>Simulation and the Monte Carlo method</source>
      <publisher-name>John Wiley &amp; Sons, Inc</publisher-name>
      <publisher-loc>Hoboken, New Jersey</publisher-loc>
      <year iso-8601-date="2017">2017</year>
      <edition>Third edition</edition>
      <isbn>978-1-118-63238-3</isbn>
    </element-citation>
  </ref>
  <ref id="ref-neal2003slice">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Neal</surname><given-names>Radford M</given-names></name>
      </person-group>
      <article-title>Slice sampling</article-title>
      <source>The annals of statistics</source>
      <publisher-name>Institute of Mathematical Statistics</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <volume>31</volume>
      <issue>3</issue>
      <fpage>705</fpage>
      <lpage>767</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
