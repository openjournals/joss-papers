body = <<-EOF
<meta name="citation_title" content="pyGPGO: Bayesian Optimization for Python">
<meta name="citation_author" content="Jim&#233;nez, Jos&#233;"><meta name="citation_author" content="Ginebra, Josep">
<meta name="citation_publication_date" content="2017//">
<meta name="citation_journal_title" content="The Journal of Open Source Software">
<meta name="citation_pdf_url" content="http://www.theoj.org/joss-papers/joss.00431/10.21105.joss.00431.pdf">
<meta name="citation_doi" content="10.21105/joss.00431">
<meta name="citation_issn" content="2475-9066">
<div class="accepted-paper">
  <h1>pyGPGO: Bayesian Optimization for Python</h1>
  <div class="columns links">
    <div class="column four-fifths" style="padding-bottom: 10px;">
      <strong>Authors</strong>
      <ul class="author-list">
            <li><a href="http://orcid.org/0000-0002-5335-7834" target="_blank">Jos&#233; Jim&#233;nez</a></li>
            <li><a href="http://orcid.org/0000-0001-9521-9635" target="_blank">Josep Ginebra</a></li>
            </ul>
    </div>
    <div class="one-third column">
      <span class="repo">Repository:<br /><a href="https://github.com/hawk31/pyGPGO">Repository link &raquo;</a></span>
    </div>
    <div class="one-third column">
      <span class="paper">Paper:<br /><a href="http://www.theoj.org/joss-papers/joss.00431/10.21105.joss.00431.pdf">PDF link &raquo;</a></span>
    </div>
    <div class="one-third column">
      <span class="paper">Review:<br /><a href="https://github.com/openjournals/openjournals/joss-reviews/issues/431">View review issue &raquo;</a></span>
    </div>

    <div class="one-third column" style="padding-top: 20px;">
      <span class="repo">DOI:<br /><a href="https://doi.org/10.21105/joss.00431">https://doi.org/10.21105/joss.00431</a></span>
    </div>
    <div class="one-third column" style="padding-top: 20px;">
      <span class="paper">Status badge:<br /><img src="http://joss.theoj.org/papers/10.21105/joss.00431/status.svg"></span>
    </div>
    <div class="one-third column" style="padding-top: 20px;">
      <span class="paper">Citation:<br />
      <small>Jim&#233;nez et al., (2017). pyGPGO: Bayesian Optimization for Python. <em>Journal of Open Source Software</em>, 2(19), 431, doi:10.21105/joss.00431</small>
    </div>
  </div>
  <div class="paper-body">
  <h1 id="summary">Summary</h1>
  <p>Bayesian optimization has risen over the last few years as a very attractive method to optimize expensive to evaluate, black box, derivative-free and possibly noisy functions <span class="citation">(Shahriari et al. 2016)</span>. This framework uses <em>surrogate models</em>, such as the likes of a Gaussian Process <span class="citation">(Rasmussen and Williams 2004)</span> which describe a prior belief over the possible objective functions in order to approximate them. The procedure itself is inherently sequential: our function is first evaluated a few times, a surrogate model is then fit with this information, which will later suggest the next point to be evaluated according to a predefined <em>acquisition function</em>. These strategies typically aim to balance exploitation and exploration, that is, areas where the posterior mean or variance of our surrogate model are high respectively.</p>
  <p>These strategies have recently grabbed the attention of machine learning researchers over simpler black-box optimization strategies, such as grid search or random search <span class="citation">(Bergstra James and Bengio Yoshua 2012)</span>. It is specially interesting in areas such as automatic machine-learning hyperparameter optimization <span class="citation">(Snoek, Larochelle, and Adams 2012)</span>, A/B testing <span class="citation">(Chapelle and Li 2011)</span> or recommender systems <span class="citation">(Vanchinathan et al. 2014)</span>, among others. Furthermore, the framework is entirely modular; there are many choices a user could take regarding the design of the optimization procedure: choice of surrogate model, covariance function, acquisition function behaviour or hyperparameter treatment, to name a few.</p>
  <p>Here we present <em>pyGPGO</em> , an open-source Python package for Bayesian Optimization, which embraces this modularity in its design. While additional Python packages exist for the same purpose, either they are restricted for non-commercial applications <span class="citation">(Snoek 2012)</span>, implement a small subset of the features <span class="citation">(Yelp 2014)</span>, or do not provide a modular interface <span class="citation">(team. 2016)</span>. <em>pyGPGO</em> on the other hand aims to provide the highest degree of freedom in the design and inference of a Bayesian optimization pipeline, while being feature-wise competitive with other existing software. <em>pyGPGO</em> currently supports:</p>
  <ul>
  <li>Different surrogate models: Gaussian Processes, Student-t Processes, Random Forests (&amp; variants) and Gradient Boosting Machines.</li>
  <li>Most usual covariance function structures, as well as their derivatives: squared exponential, Mat&#232;rn, gamma-exponential, rational-quadratic, exponential-sine and dot-product kernel.</li>
  <li>Several acquisition function behaviours: probability of improvement, expected improvement, upper confidence bound and entropy-based, as well as their integrated versions.</li>
  <li>Type II maximum-likelihood estimation of covariance hyperparameters.</li>
  <li>MCMC sampling for the full-bayesian treatment of hyperparameters (via <code>pyMC3</code> <span class="citation">(Salvatier, Wiecki, and C. 2016)</span>)</li>
  </ul>
  <p><em>pyGPGO</em> is MIT-licensed and can be retrieved from both <a href="https://github.com/hawk31/pyGPGO">GitHub</a> and <a href="https://pypi.python.org/pypi/pyGPGO/">PyPI</a>, with extensive documentation available at <a href="http://pygpgo.readthedocs.io/en/latest/">ReadTheDocs</a>. <em>pyGPGO</em> is built on top of other well known packages of the Python scientific ecosystem as dependencies, such as numpy, scikit-learn, pyMC3 and theano.</p>
  <div class="figure">
  <img src="franke.gif" alt="pyGPGO in action." />
  <p class="caption">pyGPGO in action.</p>
  </div>
  <h1 id="future-work">Future work</h1>
  <p><em>pyGPGO</em> is an ongoing project, and as such there are several improvements that will be tackled in the near future:</p>
  <ul>
  <li>Support for linear combinations of covariance functions, with automatic gradient computation.</li>
  <li>Support for more diverse acquisition functions, such as Predictive Entropy Search <span class="citation">(Hern&#225;ndez-Lobato, Hoffman, and Ghahramani 2014)</span>.</li>
  <li>A class for constrained Bayesian Optimization is planned for the near future. <span class="citation">(Gardner et al. 2014)</span></li>
  </ul>
  <h1 id="references" class="unnumbered">References</h1>
  <div id="refs" class="references">
  <div id="ref-Bergstra2012">
  <p>Bergstra James, James, and Umontrealca Bengio Yoshua. 2012. &#8220;Random Search for Hyper-Parameter Optimization.&#8221; <em>Journal of Machine Learning Research</em> 13: 281&#8211;305. doi:<a href="https://doi.org/10.1162/153244303322533223">10.1162/153244303322533223</a>.</p>
  </div>
  <div id="ref-Chapelle2011">
  <p>Chapelle, Olivier, and Lihong Li. 2011. &#8220;An Empirical Evaluation of Thompson Sampling.&#8221; <em>Advances in Neural Information Processing Systems</em>, 2249&#8212;&#8211;2257. <a href="http://explo.cs.ucl.ac.uk/wp-content/uploads/2011/05/An-Empirical-Evaluation-of-Thompson-Sampling-Chapelle-Li-2011.pdf" class="uri">http://explo.cs.ucl.ac.uk/wp-content/uploads/2011/05/An-Empirical-Evaluation-of-Thompson-Sampling-Chapelle-Li-2011.pdf</a>.</p>
  </div>
  <div id="ref-Gardner2014">
  <p>Gardner, Jacob R., Matt J. Kusner, Zhixiang Eddie Xu, Kilian Q. Weinberger, and John P. Cunningham. 2014. &#8220;Bayesian Optimization with Inequality Constraints.&#8221; <em>Proceedings of the 31st International Conference on Machine Learning</em> 32: 937&#8211;45.</p>
  </div>
  <div id="ref-Hernandez-Lobato2014">
  <p>Hern&#225;ndez-Lobato, Jos&#233; Miguel, Matthew W Hoffman, and Zoubin Ghahramani. 2014. &#8220;Predictive Entropy Search for Efficient Global Optimization of Black-box Functions.&#8221; <em>Advances in Neural Information Processing Systems 28</em>, 1&#8211;9. <a href="https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf" class="uri">https://jmhldotorg.files.wordpress.com/2014/10/pes-final.pdf</a>.</p>
  </div>
  <div id="ref-Rasmussen2004">
  <p>Rasmussen, Carl E., and Christopher K. I. Williams. 2004. <em>Gaussian processes for machine learning.</em> Vol. 14. 2. doi:<a href="https://doi.org/10.1142/S0129065704001899">10.1142/S0129065704001899</a>.</p>
  </div>
  <div id="ref-Salvatier2016">
  <p>Salvatier, J, TV Wiecki, and Fonnesbeck C. 2016. &#8220;Probabilistic programming in Python using PyMC3.&#8221; <em>PeerJ Computer Science</em>. <a href="https://doi.org/10.7717/peerj-cs.55" class="uri">https://doi.org/10.7717/peerj-cs.55</a>.</p>
  </div>
  <div id="ref-Shahriari2016">
  <p>Shahriari, Bobak, Kevin Swersky, Ziyu Wang, Ryan P. Adams, and Nando De Freitas. 2016. &#8220;Taking the human out of the loop: A review of Bayesian optimization.&#8221; <em>Proceedings of the IEEE</em> 104 (1): 148&#8211;75. doi:<a href="https://doi.org/10.1109/JPROC.2015.2494218">10.1109/JPROC.2015.2494218</a>.</p>
  </div>
  <div id="ref-SpearmintSnoek2012">
  <p>Snoek, Jasper. 2012. &#8220;Spearmint.&#8221; <a href="https://github.com/HIPS/Spearmint" class="uri">https://github.com/HIPS/Spearmint</a>.</p>
  </div>
  <div id="ref-Snoek2012">
  <p>Snoek, Jasper, Hugo Larochelle, and Rp Adams. 2012. &#8220;Practical Bayesian optimization of machine learning algorithms.&#8221; <em>Advances in Neural Information &#8230;</em>, 1&#8211;9. doi:<a href="https://doi.org/2012arXiv1206.2944S">2012arXiv1206.2944S</a>.</p>
  </div>
  <div id="ref-scikitoptimize">
  <p>team., The scikit-optimize. 2016. &#8220;Scikit-Optimize.&#8221; <a href="https://github.com/scikit-optimize/scikit-optimize" class="uri">https://github.com/scikit-optimize/scikit-optimize</a>.</p>
  </div>
  <div id="ref-Vanchinathan2014">
  <p>Vanchinathan, Hastagiri P., Isidor Nikolic, Fabio De Bona, and Andreas Krause. 2014. &#8220;Explore-exploit in top-N recommender systems via Gaussian processes.&#8221; In <em>Proceedings of the 8th Acm Conference on Recommender Systems - Recsys &#8217;14</em>, 225&#8211;32. doi:<a href="https://doi.org/10.1145/2645710.2645733">10.1145/2645710.2645733</a>.</p>
  </div>
  <div id="ref-yelpmoe">
  <p>Yelp. 2014. &#8220;MOE.&#8221; <a href="https://github.com/Yelp/MOE" class="uri">https://github.com/Yelp/MOE</a>.</p>
  </div>
  </div>
  </div>
</div>
EOF
