<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8077</article-id>
<article-id pub-id-type="doi">10.21105/joss.08077</article-id>
<title-group>
<article-title>StateSpaceDynamics.jl: A Julia package for probabilistic
state space models (SSMs)</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3776-4576</contrib-id>
<name>
<surname>Senne</surname>
<given-names>Ryan</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-2831-2641</contrib-id>
<name>
<surname>Loschinskey</surname>
<given-names>Zachary</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Fourie</surname>
<given-names>James</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Loughridge</surname>
<given-names>Carson</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-4508-0537</contrib-id>
<name>
<surname>DePasquale</surname>
<given-names>Brian D.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Biomedical Engineering, Boston
University</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Graduate Program for Neuroscience, Boston
University</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-08-05">
<day>5</day>
<month>8</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>115</issue>
<fpage>8077</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>State space models</kwd>
<kwd>Julia</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>State-space models (SSMs) are powerful tools for modeling time
  series data that naturally arise in a variety of domains, including
  neuroscience, finance, and engineering. The unifying principle of
  these models is they assume an observation sequence,
  <inline-formula><alternatives>
  <tex-math><![CDATA[Y_1, Y_2,...,Y_T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>Y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>Y</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
  is generated through an underlying Markovian latent sequence,
  <inline-formula><alternatives>
  <tex-math><![CDATA[X_1, X_2,...,X_T]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mi>.</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>.
  This framework encompasses two popular models for time series
  analysis: the hidden Markov model (HMM) and the (Gaussian) linear
  dynamical system (LDS, i.e., the Kalman filter). Thus, SSMs provide a
  probabilistic framework for describing the temporal evolution of many
  phenomena, and their generality naturally leads to a variety of use
  cases. We introduce <monospace>StateSpaceDynamics.jl</monospace>
  (<xref alt="Senne et al., 2025" rid="ref-SSDjl2024" ref-type="bibr">Senne
  et al., 2025</xref>), an open-source, modular package designed to be
  fast, readable, and self-contained for the purpose of easily fitting a
  plurality of SSMs in the Julia language.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Advances in neuroscience have enabled the collection of massive,
  multivariate, and complex time-series datasets, where simultaneous
  observations from hundreds to thousands of neurons are increasingly
  common. Interpreting these high-dimensional datasets presents
  significant challenges. Recent modeling approaches suggest that neural
  activity can be characterized by a set of latent factors evolving
  within a low-dimensional manifold. Consequently, there is a growing
  need for models that combine dimensionality reduction with temporal
  dynamics, for which state-space models provide a natural
  framework.</p>
  <p>While state-space model implementations exist in Python, such as
  the <monospace>ssm</monospace> package
  (<xref alt="S. Linderman, 2022" rid="ref-PySSM2022" ref-type="bibr">S.
  Linderman, 2022</xref>) and <monospace>Dynamax</monospace>
  (<xref alt="Scott W. Linderman et al., 2025" rid="ref-Linderman_Dynamax_A_Python_2025" ref-type="bibr">Scott
  W. Linderman et al., 2025</xref>), the Julia programming language
  lacks an equivalent that meets the needs of modern neuroscience.
  Existing Julia offerings, like
  <monospace>StateSpaceModels.jl</monospace>
  (<xref alt="Saavedra et al., 2019" rid="ref-SaavedraBodinSouto2019" ref-type="bibr">Saavedra
  et al., 2019</xref>), can accommodate continuous-state SSMs (e.g.,
  LDS) but are limited to Gaussian observation models and rely on
  analytical calculation of the marginal log-likelihood. This latter
  limitation precludes model inference and parameter learning for
  non-conjugate observations which are common in neuroscience, where
  neural activity follow Poisson or other discrete distributions.
  Packages for performing inference and learning using sampling-based
  methods exist in Julia (such as <monospace>Turing.jl</monospace>
  (<xref alt="Fjelde et al., 2025" rid="ref-10.1145U002F3711897" ref-type="bibr">Fjelde
  et al., 2025</xref>;
  <xref alt="Ge et al., 2018" rid="ref-pmlr-v84-ge18b" ref-type="bibr">Ge
  et al., 2018</xref>)) but are computationally inefficient compared to
  tailored approaches based on Expectation-Maximization (EM). For
  discrete SSMs, an existing Julia offering,
  <monospace>HiddenMarkovModels.jl</monospace>
  (<xref alt="Dalle, 2024" rid="ref-Dalle2024" ref-type="bibr">Dalle,
  2024</xref>), is efficient and scalable but not intentionally designed
  with the functionality for mixing models that contain both discrete
  and continuous latent variables, such as the switching linear
  dynamical system model (SLDS)
  (<xref alt="Ghahramani &amp; Hinton, 2000" rid="ref-slds" ref-type="bibr">Ghahramani
  &amp; Hinton, 2000</xref>;
  <xref alt="Scott W. Linderman et al., 2016" rid="ref-Linderman2016-xe" ref-type="bibr">Scott
  W. Linderman et al., 2016</xref>) increasingly used in neuroscience.
  Although our primary motivation arises from challenges in modeling
  high-dimensional neural population activity, the package is not
  specific to neuroscience. The algorithms and abstractions apply
  equally well to time-series problems in engineering, econometrics, and
  other fields where latent variable models and structured inference are
  required.</p>
</sec>
<sec id="package-design">
  <title>Package design</title>
  <p>To address these limitations, we developed
  <monospace>StateSpaceDynamics.jl</monospace>, which provides a
  flexible framework for fitting a variety of SSMs‚Äìincluding
  non-Gaussian observation models and models that mix discrete and
  continuous latents‚Äìwhile maintaining computational efficiency.</p>
  <p>For continuous latent-variable models, (e.g., LDS)
  <monospace>StateSpaceDynamics.jl</monospace> employs a previously
  advocated approach of directly maximizing the complete-data
  log-likelihood with respect to the hidden state path
  (<xref alt="Paninski et al., 2010" rid="ref-Paninski2010-ns" ref-type="bibr">Paninski
  et al., 2010</xref>). By leveraging the block tridiagonal structure of
  the Hessian matrix, this method allows for the exact computation of
  the Kalman smoother in <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  time
  (<xref alt="Paninski et al., 2010" rid="ref-Paninski2010-ns" ref-type="bibr">Paninski
  et al., 2010</xref>). Furthermore, it facilitates the generalization
  of the Rauch‚ÄìTung‚ÄìStriebel (RTS) smoother to accommodate other
  observation models (e.g., Poisson and Bernoulli), requiring only the
  computation of the gradient and Hessian of the new model to obtain an
  <italic>exact</italic> maximum a posteriori (MAP) path
  (<xref alt="Macke et al., 2011" rid="ref-NIPS2011_7143d7fb" ref-type="bibr">Macke
  et al., 2011</xref>).</p>
  <p>Using analytically computable Hessians,
  <monospace>StateSpaceDynamics.jl</monospace> performs approximate EM
  for non-Gaussian models via Laplace approximation of the latent
  posterior. Speed is maintained by using fast inversion algorithms of
  the negative Hessian (i.e., Fisher Information Matrix), which are
  block tridiagonal
  (<xref alt="Rybicki &amp; Hummer, 1990" rid="ref-Rybicki1990-ky" ref-type="bibr">Rybicki
  &amp; Hummer, 1990</xref>). From here
  <monospace>StateSpaceDynamics.jl</monospace> computes the approximate
  second moments of the posterior i.e., <inline-formula><alternatives>
  <tex-math><![CDATA[\text{Cov}(X_t, X_t)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">Cov</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\text{Cov}(X_t, X_{t-1})]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mtext mathvariant="normal">Cov</mml:mtext><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>‚àí</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and uses the analytical updates of the canonical LDS
  (<xref alt="Bishop, 2006" rid="ref-Bishop2006-kv" ref-type="bibr">Bishop,
  2006</xref>;
  <xref alt="Paninski et al., 2010" rid="ref-Paninski2010-ns" ref-type="bibr">Paninski
  et al., 2010</xref>). It is important to note that when the
  observations and state-evolution process are assumed to have Gaussian
  errors, this approach is exactly the same as using the standard Kalman
  Filter and RTS-Smoother, i.e., they will give the same results.</p>
  <p>Lastly, <monospace>StateSpaceDynamics.jl</monospace> provides
  implementations of discrete state-space models i.e., hidden Markov
  models, and the ability to fit these models using EM. While this is
  not the primary development target of the package, these models are
  necessary for the development of hierarchical models that mix discrete
  and continuous latents, e.g., the switching LDS (SLDS) and the
  recurrent switching LDS (rSLDS)
  (<xref alt="Ghahramani &amp; Hinton, 2000" rid="ref-slds" ref-type="bibr">Ghahramani
  &amp; Hinton, 2000</xref>;
  <xref alt="Scott W. Linderman et al., 2016" rid="ref-Linderman2016-xe" ref-type="bibr">Scott
  W. Linderman et al., 2016</xref>;
  <xref alt="Murphy, 1998" rid="ref-Murphy1998-bk" ref-type="bibr">Murphy,
  1998</xref>) which have become immensely popular in neuroscience and
  require similarly tailored computational routines for efficient
  inference and learning. To illustrate the functionality of
  <monospace>StateSpaceDynamics.jl</monospace> for this model class, we
  include an implementation of Variational Laplace EM (vLEM)
  (<xref alt="Zoltowski et al., 2020" rid="ref-pmlr-v119-zoltowski20a" ref-type="bibr">Zoltowski
  et al., 2020</xref>). The development of
  <monospace>HiddenMarkovModels.jl</monospace>, may make our approach to
  discrete model learning redundant, and future work may entail directly
  interfacing with this package
  (<xref alt="Dalle, 2024" rid="ref-Dalle2024" ref-type="bibr">Dalle,
  2024</xref>). Nonetheless, we provide a suite of HMM models popular in
  neuroscience including the classic Gaussian HMM and a variety of
  input-output HMMs
  (<xref alt="Bengio &amp; Frasconi, 1994" rid="ref-NIPS1994_8065d07d" ref-type="bibr">Bengio
  &amp; Frasconi, 1994</xref>), commonly referred to as generalized
  linear model-HMMs (GLM-HMMs)
  (<xref alt="Ashwood et al., 2022" rid="ref-Ashwood2022" ref-type="bibr">Ashwood
  et al., 2022</xref>) in neuroscience.</p>
  <p>By providing these features,
  <monospace>StateSpaceDynamics.jl</monospace> fills a critical gap in
  the Julia ecosystem, offering modern computational neuroscientists the
  tools to model complex neural data with state-space models that
  incorporate both dimensionality reduction and temporal dynamics.</p>
</sec>
<sec id="benchmarks">
  <title>Benchmarks</title>
  <p>To evaluate the performance of
  <monospace>StateSpaceDynamics.jl</monospace>, we conducted two
  benchmarking studies focusing on fitting a Gaussian LDS and a Gaussian
  HMM. For the Gaussian LDS benchmark, we compared our package against
  two alternatives: the NumPy-based Kalman filter-smoother package
  <monospace>pykalman</monospace> and the more recent JAX-based
  <monospace>Dynamax</monospace>. We intentionally excluded
  <monospace>StateSpaceModels.jl</monospace> from our comparison as its
  scope is geared towards structured time-series models.
  <monospace>Dynamax</monospace> was properly JIT-compiled using the
  <monospace>jax.jit</monospace> function prior to benchmarking to
  ensure fair comparison.</p>
  <p>For our Gaussian LDS experiments, we constructed a synthetic
  dataset as follows. The state transition matrix
  <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>
  was generated as a random <inline-formula><alternatives>
  <tex-math><![CDATA[n]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>-dimensional
  rotation matrix, while the observation matrix
  <inline-formula><alternatives>
  <tex-math><![CDATA[C]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>C</mml:mi></mml:math></alternatives></inline-formula>
  was created as a random <inline-formula><alternatives>
  <tex-math><![CDATA[m \times n]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>m</mml:mi><mml:mo>√ó</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
  matrix. Both the state noise covariance <inline-formula><alternatives>
  <tex-math><![CDATA[Q]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Q</mml:mi></mml:math></alternatives></inline-formula>
  and observation noise covariance <inline-formula><alternatives>
  <tex-math><![CDATA[R]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>R</mml:mi></mml:math></alternatives></inline-formula>
  were set to identity matrices. To ensure a fair comparison, all
  packages were initialized using identical random parameters, after
  which we executed the EM algorithm for 100 iterations. We conducted
  these benchmarks using <monospace>PythonCall.jl</monospace>
  (<xref alt="Doris, 2021" rid="ref-pythoncall" ref-type="bibr">Doris,
  2021</xref>) and <monospace>BenchmarkTools.jl</monospace>
  (<xref alt="Chen &amp; Revels, 2016" rid="ref-BenchmarkTools.jl-2016" ref-type="bibr">Chen
  &amp; Revels, 2016</xref>), with the assumption that Julia-to-Python
  overhead is negligible for these computationally intensive
  operations.</p>
  <p>To thoroughly assess performance across different scales, we tested
  three sequence lengths (<inline-formula><alternatives>
  <tex-math><![CDATA[T = 100, 500, 1000]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>)
  and explored multiple dimensionality settings, with state dimensions
  <inline-formula><alternatives>
  <tex-math><![CDATA[n = 2, 4, 6, 8]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  and observation dimensions <inline-formula><alternatives>
  <tex-math><![CDATA[m = 2, 4, 6, 8]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
  In all cases, we restricted evaluations to settings where the latent
  dimension was less than or equal to the observation dimension.
  Finally, it is worth noting that <monospace>Dynamax</monospace>
  includes a temporally parallel smoother with
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(\log T)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mo>log</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  complexity. We did not include this method in our comparisons because
  it is GPU-specific and incompatible with our direct optimization
  approach, which is designed for inference in non-conjugate models.</p>
  <fig>
    <graphic mimetype="image" mime-subtype="png" xlink:href="lds_bench.png" />
  </fig>
  <p>For the second benchmarking study, we compared
  <monospace>StateSpaceDynamics.jl</monospace>,
  <monospace>HiddenMarkovModels.jl</monospace>, and
  <monospace>Dynamax</monospace> in their ability to fit a Gaussian HMM.
  Once again, we ensured that <monospace>Dynamax</monospace> was
  JIT-compiled for a fair comparison. To construct synthetic datasets,
  we sampled from a Gaussian HMM with randomly selected emission models,
  transition matrices, and initial state distributions. Each package was
  initialized using identical random parameters to maintain consistency.
  EM was run for 100 iterations.</p>
  <fig>
    <graphic mimetype="image" mime-subtype="png" xlink:href="hmm_bench.png" />
  </fig>
  <p>In our benchmarking, we find that for the LDS, both
  <monospace>StateSpaceDynamics.jl</monospace> and
  <monospace>Dynamax</monospace> are faster than
  <monospace>pykalman</monospace> across all sequence lengths and
  dimension configurations. More generally,
  <monospace>StateSpaceDynamics.jl</monospace> and
  <monospace>Dynamax</monospace> exhibit similar performance at lower
  sequence lengths (with <monospace>Dynamax</monospace> slightly
  outperforming <monospace>StateSpaceDynamics.jl</monospace>). However,
  <monospace>Dynamax</monospace> exhibits superior scaling in both the
  dimensions of the state and observation matrices as well as the
  temporal sequnce length. In our current implementation, the Hessian is
  represented as a sparse matrix with block tridiagonal structure,
  resulting in <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{O}(Tn^2)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí™</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>T</mml:mi><mml:msup><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  memory scaling ‚Äî which is optimal. However, we do not yet exploit this
  structure fully during inference. In particular, our solver does not
  leverage specialized routines for block-banded systems (e.g., the
  block Thomas algorithm), which can result in unnecessary fill-in and
  degraded performance at large T. Future versions will use banded or
  block tridiagonal solvers to achieve truly linear-time inference.</p>
  <p>In our HMM benchmarks, <monospace>HiddenMarkovModels.jl</monospace>
  outperforms both <monospace>StateSpaceDynamics.jl</monospace> and
  <monospace>Dynamax</monospace> across most sequence lengths and state
  dimensions, with <monospace>Dynamax</monospace> only becoming slightly
  faster for high state dimensions and long sequence lengths.
  <monospace>StateSpaceDynamics.jl</monospace> outperforms
  <monospace>Dynamax</monospace> at low state dimensions for all
  sequence lengths but exhibits worse scaling with the number of states,
  allowing <monospace>Dynamax</monospace> to overtake it as the number
  of states increases. These results, combined with our primary
  development goals in hierarchical SSMs, highlight the benefits of
  interfacing with <monospace>HiddenMarkovModels.jl</monospace> for
  HMM-specific functionality. Efforts are currently underway to make
  this interface seamless.</p>
  <p>Taken together, these benchmarks demonstrate the competitiveness of
  <monospace>StateSpaceDynamics.jl</monospace> for fitting state-space
  models. Our benchmarks are available in the
  <monospace>benchmarking</monospace> folder of our repository, and
  instructions for running these are available in a
  <monospace>README.md</monospace> file.</p>
</sec>
<sec id="availability">
  <title>Availability</title>
  <p><monospace>StateSpaceDynamics.jl</monospace> is publicly available
  under the
  <ext-link ext-link-type="uri" xlink:href="https://github.com/depasquale-lab/StateSpaceDynamics.jl/blob/main/LICENSE">GNU
  license</ext-link> at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/depasquale-lab/StateSpaceDynamics.jl">https://github.com/depasquale-lab/StateSpaceDynamics.jl</ext-link>.</p>
</sec>
<sec id="future-directions">
  <title>Future Directions</title>
  <p>The current release of <monospace>StateSpaceDynamics.jl</monospace>
  emphasizes efficient CPU-based implementations and analytically
  derived gradients and Hessians for commonly used observation models.
  Several avenues of future development will broaden the scope and
  accessibility of the package. First, we plan to add optional support
  for automatic differentiation (AD) using Julia‚Äôs AD ecosystem (e.g.,
  <monospace>ForwardDiff.jl</monospace>
  (<xref alt="Revels et al., 2016" rid="ref-ForwardDiff" ref-type="bibr">Revels
  et al., 2016</xref>), <monospace>Zygote.jl</monospace>
  (<xref alt="Innes, 2018" rid="ref-Zygote" ref-type="bibr">Innes,
  2018</xref>), <monospace>DifferentiationInterface.jl</monospace>
  (<xref alt="Dalle &amp; Hill, 2025" rid="ref-dalle2025commoninterfaceautomaticdifferentiation" ref-type="bibr">Dalle
  &amp; Hill, 2025</xref>)). This will allow users to prototype new
  observation models without requiring hand-coded derivatives, while
  maintaining the existing optimized implementations for speed-critical
  cases. Second, we aim to extend hardware support to GPU backends by
  exploiting Julia‚Äôs GPU array abstractions and block-tridiagonal
  solvers, enabling large-scale inference with temporally parallel
  methods. Finally, we plan to expand parameter inference options beyond
  maximum likelihood and Laplace-EM, including Bayesian approaches via
  variational inference and interoperability with probabilistic
  programming frameworks such as <monospace>Turing.jl</monospace>.
  Together, these developments will further enhance the package‚Äôs
  flexibility, performance, and utility across scientific
  disciplines.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p><monospace>StateSpaceDynamics.jl</monospace> fills an existing gap
  in the Julia ecosystem for general state-space modeling that exists in
  Python. Importantly, our package‚Äôs approach is simple enough that
  other candidate state-space models can be easily implemented. Further,
  this work provides a foundation for future development of more
  advanced state-space models, such as the rSLDS, which are essential
  for modeling complex neural data. We expect that this package will be
  of interest to computational neuroscientists and other researchers
  working with high-dimensional time series data and we are currently
  using its functionality in three separate projects.</p>
</sec>
<sec id="author-contributions">
  <title>Author contributions</title>
  <p>RS (Ryan Senne) was the primary developer of
  <monospace>StateSpaceDynamics.jl</monospace>, implementing the core
  algorithms, designing the package architecture, and writing the
  manuscript. ZL (Zachary Loschinskey) was the secondary developer,
  whose contributions include optimizing and extending HMM/GLM-HMM
  functionality, implementing core multi-trial EM algorithms, and
  assisting with SLDS development. CL (Carson Loughridge) and JF (James
  Fourie) contributed to package development, including implementation
  of key features, testing, and documentation. BDD (Brian D. DePasquale)
  conceived the project, provided theoretical guidance and technical
  oversight throughout development, secured funding, and supervised the
  work. All authors reviewed and approved the final manuscript.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work was supported by the Biomedical Engineering Department at
  Boston University.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-PySSM2022">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Linderman</surname><given-names>Scott</given-names></name>
      </person-group>
      <article-title>SSM: Bayesian learning and inference for state space models</article-title>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/lindermanlab/ssm</uri>
    </element-citation>
  </ref>
  <ref id="ref-10.1145U002F3711897">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fjelde</surname><given-names>Tor Erlend</given-names></name>
        <name><surname>Xu</surname><given-names>Kai</given-names></name>
        <name><surname>Widmann</surname><given-names>David</given-names></name>
        <name><surname>Tarek</surname><given-names>Mohamed</given-names></name>
        <name><surname>Pfiffer</surname><given-names>Cameron</given-names></name>
        <name><surname>Trapp</surname><given-names>Martin</given-names></name>
        <name><surname>Axen</surname><given-names>Seth D.</given-names></name>
        <name><surname>Sun</surname><given-names>Xianda</given-names></name>
        <name><surname>Hauru</surname><given-names>Markus</given-names></name>
        <name><surname>Yong</surname><given-names>Penelope</given-names></name>
        <name><surname>Tebbutt</surname><given-names>Will</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Ge</surname><given-names>Hong</given-names></name>
      </person-group>
      <article-title>Turing.jl: A general-purpose probabilistic programming language</article-title>
      <source>ACM Trans. Probab. Mach. Learn.</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2025-02">2025</year><month>02</month>
      <uri>https://doi.org/10.1145/3711897</uri>
      <pub-id pub-id-type="doi">10.1145/3711897</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-pmlr-v84-ge18b">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ge</surname><given-names>Hong</given-names></name>
        <name><surname>Xu</surname><given-names>Kai</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
      </person-group>
      <article-title>Turing: A language for flexible probabilistic inference</article-title>
      <source>Proceedings of the twenty-first international conference on artificial intelligence and statistics</source>
      <person-group person-group-type="editor">
        <name><surname>Storkey</surname><given-names>Amos</given-names></name>
        <name><surname>Perez-Cruz</surname><given-names>Fernando</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>84</volume>
      <uri>https://proceedings.mlr.press/v84/ge18b.html</uri>
      <fpage>1682</fpage>
      <lpage>1690</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Dalle2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dalle</surname><given-names>Guillaume</given-names></name>
      </person-group>
      <article-title>HiddenMarkovModels.jl: Generic, fast and reliable state space modeling</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>9</volume>
      <issue>96</issue>
      <uri>https://doi.org/10.21105/joss.06436</uri>
      <pub-id pub-id-type="doi">10.21105/joss.06436</pub-id>
      <fpage>6436</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-SaavedraBodinSouto2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Saavedra</surname><given-names>Raphael</given-names></name>
        <name><surname>Bodin</surname><given-names>Guilherme</given-names></name>
        <name><surname>Souto</surname><given-names>Mario</given-names></name>
      </person-group>
      <article-title>StateSpaceModels.jl: A julia package for time-series analysis in a state-space framework</article-title>
      <source>arXiv preprint arXiv:1908.01757</source>
      <year iso-8601-date="2019">2019</year>
    </element-citation>
  </ref>
  <ref id="ref-NIPS2011_7143d7fb">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Macke</surname><given-names>Jakob H</given-names></name>
        <name><surname>Buesing</surname><given-names>Lars</given-names></name>
        <name><surname>Cunningham</surname><given-names>John P</given-names></name>
        <name><surname>Yu</surname><given-names>Byron M</given-names></name>
        <name><surname>Shenoy</surname><given-names>Krishna V</given-names></name>
        <name><surname>Sahani</surname><given-names>Maneesh</given-names></name>
      </person-group>
      <article-title>Empirical models of spiking in neural populations</article-title>
      <person-group person-group-type="editor">
        <name><surname>Shawe-Taylor</surname><given-names>J.</given-names></name>
        <name><surname>Zemel</surname><given-names>R.</given-names></name>
        <name><surname>Bartlett</surname><given-names>P.</given-names></name>
        <name><surname>Pereira</surname><given-names>F.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <volume>24</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2011/file/7143d7fbadfa4693b9eec507d9d37443-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Linderman_Dynamax_A_Python_2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Linderman</surname><given-names>Scott W.</given-names></name>
        <name><surname>Chang</surname><given-names>Peter</given-names></name>
        <name><surname>Harper-Donnelly</surname><given-names>Giles</given-names></name>
        <name><surname>Kara</surname><given-names>Aleyna</given-names></name>
        <name><surname>Li</surname><given-names>Xinglong</given-names></name>
        <name><surname>Duran-Martin</surname><given-names>Gerardo</given-names></name>
        <name><surname>Murphy</surname><given-names>Kevin</given-names></name>
      </person-group>
      <article-title>Dynamax: A Python package for probabilistic state space modeling with JAX</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2025-04">2025</year><month>04</month>
      <volume>10</volume>
      <issue>108</issue>
      <uri>https://joss.theoj.org/papers/10.21105/joss.07069</uri>
      <pub-id pub-id-type="doi">10.21105/joss.07069</pub-id>
      <fpage>7069</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Paninski2010-ns">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paninski</surname><given-names>Liam</given-names></name>
        <name><surname>Ahmadian</surname><given-names>Yashar</given-names></name>
        <name><surname>Ferreira</surname><given-names>Daniel Gil</given-names></name>
        <name><surname>Koyama</surname><given-names>Shinsuke</given-names></name>
        <name><surname>Rahnama Rad</surname><given-names>Kamiar</given-names></name>
        <name><surname>Vidne</surname><given-names>Michael</given-names></name>
        <name><surname>Vogelstein</surname><given-names>Joshua</given-names></name>
        <name><surname>Wu</surname><given-names>Wei</given-names></name>
      </person-group>
      <article-title>A new look at state-space models for neural data</article-title>
      <source>J. Comput. Neurosci.</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2010-08">2010</year><month>08</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-09-11">2024</year><month>09</month><day>11</day></date-in-citation>
      <volume>29</volume>
      <pub-id pub-id-type="doi">10.1007/s10827-009-0179-x</pub-id>
      <fpage>107</fpage>
      <lpage>126</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SSDjl2024">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Senne</surname><given-names>Ryan</given-names></name>
        <name><surname>Loschinskey</surname><given-names>Zachary</given-names></name>
        <name><surname>Loughridge</surname><given-names>Carson</given-names></name>
        <name><surname>DePasquale</surname><given-names>Brian</given-names></name>
      </person-group>
      <article-title>StateSpaceDynamics.jl: A julia package for probabilistic state space models</article-title>
      <publisher-name>https://github.com/depasquale-lab/StateSpaceDynamics.jl</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <uri>https://github.com/depasquale-lab/StateSpaceDynamics.jl</uri>
    </element-citation>
  </ref>
  <ref id="ref-Linderman2016-xe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Linderman</surname><given-names>Scott W</given-names></name>
        <name><surname>Miller</surname><given-names>Andrew C</given-names></name>
        <name><surname>Adams</surname><given-names>Ryan P</given-names></name>
        <name><surname>Blei</surname><given-names>David M</given-names></name>
        <name><surname>Paninski</surname><given-names>Liam</given-names></name>
        <name><surname>Johnson</surname><given-names>Matthew J</given-names></name>
      </person-group>
      <article-title>Recurrent switching linear dynamical systems</article-title>
      <source>arXiv [stat.ML]</source>
      <year iso-8601-date="2016-10-26">2016</year><month>10</month><day>26</day>
      <pub-id pub-id-type="doi">10.48550/arXiv.1610.08466</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Murphy1998-bk">
    <element-citation publication-type="report">
      <person-group person-group-type="author">
        <name><surname>Murphy</surname><given-names>Kevin P</given-names></name>
      </person-group>
      <article-title>Switching kalman filters</article-title>
      <year iso-8601-date="1998">1998</year>
    </element-citation>
  </ref>
  <ref id="ref-Rybicki1990-ky">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Rybicki</surname><given-names>G B</given-names></name>
        <name><surname>Hummer</surname><given-names>D G</given-names></name>
      </person-group>
      <article-title>Fast solution for the diagonal elements of the inverse of a tridiagonal matrix</article-title>
      <year iso-8601-date="1990">1990</year>
    </element-citation>
  </ref>
  <ref id="ref-Bishop2006-kv">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Bishop</surname><given-names>Christopher M</given-names></name>
      </person-group>
      <source>Pattern recognition and machine learning</source>
      <publisher-name>Springer</publisher-name>
      <publisher-loc>New York, NY</publisher-loc>
      <year iso-8601-date="2006-11-01">2006</year><month>11</month><day>01</day>
    </element-citation>
  </ref>
  <ref id="ref-slds">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ghahramani</surname><given-names>Zoubin</given-names></name>
        <name><surname>Hinton</surname><given-names>Geoffrey E.</given-names></name>
      </person-group>
      <article-title>Variational learning for switching state-space models</article-title>
      <source>Neural Computation</source>
      <year iso-8601-date="2000-04">2000</year><month>04</month>
      <volume>12</volume>
      <issue>4</issue>
      <issn>0899-7667</issn>
      <uri>https://doi.org/10.1162/089976600300015619</uri>
      <pub-id pub-id-type="doi">10.1162/089976600300015619</pub-id>
      <fpage>831</fpage>
      <lpage>864</lpage>
    </element-citation>
  </ref>
  <ref id="ref-NIPS1994_8065d07d">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
        <name><surname>Frasconi</surname><given-names>Paolo</given-names></name>
      </person-group>
      <article-title>An input output HMM architecture</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Tesauro</surname><given-names>G.</given-names></name>
        <name><surname>Touretzky</surname><given-names>D.</given-names></name>
        <name><surname>Leen</surname><given-names>T.</given-names></name>
      </person-group>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="1994">1994</year>
      <volume>7</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/1994/file/8065d07da4a77621450aa84fee5656d9-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Ashwood2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ashwood</surname><given-names>Zoe C.</given-names></name>
        <name><surname>Roy</surname><given-names>Nicholas A.</given-names></name>
        <name><surname>Stone</surname><given-names>Iris R.</given-names></name>
        <name><surname>Urai</surname><given-names>Anne E.</given-names></name>
        <name><surname>Churchland</surname><given-names>Anne K.</given-names></name>
        <name><surname>Pouget</surname><given-names>Alexandre</given-names></name>
        <name><surname>Pillow</surname><given-names>Jonathan W.</given-names></name>
      </person-group>
      <article-title>Mice alternate between discrete strategies during perceptual decision-making</article-title>
      <source>Nature Neuroscience</source>
      <year iso-8601-date="2022">2022</year>
      <volume>25</volume>
      <issue>2</issue>
      <uri>https://www.nature.com/articles/s41593-021-01007-z</uri>
      <pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id>
      <fpage>201</fpage>
      <lpage>212</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pythoncall">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Doris</surname><given-names>Christopher</given-names></name>
      </person-group>
      <article-title>PythonCall.jl</article-title>
      <publisher-name>https://github.com/JuliaPy/PythonCall.jl</publisher-name>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-BenchmarkTools.jl-2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Jiahao</given-names></name>
        <name><surname>Revels</surname><given-names>Jarrett</given-names></name>
      </person-group>
      <article-title>Robust benchmarking in noisy environments</article-title>
      <source>arXiv e-prints</source>
      <year iso-8601-date="2016-08">2016</year><month>08</month>
      <uri>https://arxiv.org/abs/1608.04295</uri>
    </element-citation>
  </ref>
  <ref id="ref-ForwardDiff">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Revels</surname><given-names>J.</given-names></name>
        <name><surname>Lubin</surname><given-names>M.</given-names></name>
        <name><surname>Papamarkou</surname><given-names>T.</given-names></name>
      </person-group>
      <article-title>Forward-mode automatic differentiation in Julia</article-title>
      <source>arXiv:1607.07892 [cs.MS]</source>
      <year iso-8601-date="2016">2016</year>
      <uri>https://arxiv.org/abs/1607.07892</uri>
    </element-citation>
  </ref>
  <ref id="ref-Zygote">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Innes</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Don‚Äôt unroll adjoint: Differentiating SSA-form programs</article-title>
      <source>CoRR</source>
      <year iso-8601-date="2018">2018</year>
      <volume>abs/1810.07951</volume>
      <uri>http://arxiv.org/abs/1810.07951</uri>
    </element-citation>
  </ref>
  <ref id="ref-dalle2025commoninterfaceautomaticdifferentiation">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Dalle</surname><given-names>Guillaume</given-names></name>
        <name><surname>Hill</surname><given-names>Adrian</given-names></name>
      </person-group>
      <article-title>A common interface for automatic differentiation</article-title>
      <year iso-8601-date="2025">2025</year>
      <uri>https://arxiv.org/abs/2505.05542</uri>
    </element-citation>
  </ref>
  <ref id="ref-pmlr-v119-zoltowski20a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Zoltowski</surname><given-names>David</given-names></name>
        <name><surname>Pillow</surname><given-names>Jonathan</given-names></name>
        <name><surname>Linderman</surname><given-names>Scott</given-names></name>
      </person-group>
      <article-title>A general recurrent state space framework for modeling neural dynamics during decision-making</article-title>
      <source>Proceedings of the 37th international conference on machine learning</source>
      <person-group person-group-type="editor">
        <name><surname>III</surname><given-names>Hal Daum√©</given-names></name>
        <name><surname>Singh</surname><given-names>Aarti</given-names></name>
      </person-group>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>119</volume>
      <uri>https://proceedings.mlr.press/v119/zoltowski20a.html</uri>
      <fpage>11680</fpage>
      <lpage>11691</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
