<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5093</article-id>
<article-id pub-id-type="doi">10.21105/joss.05093</article-id>
<title-group>
<article-title>Multi-view-AE: A Python package for multi-view
autoencoder models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0727-3274</contrib-id>
<name>
<surname>Aguila</surname>
<given-names>Ana Lawry</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Jayme</surname>
<given-names>Alejandra</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-5685-971X</contrib-id>
<name>
<surname>Montaña-Brown</surname>
<given-names>Nina</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Heuveline</surname>
<given-names>Vincent</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9265-2393</contrib-id>
<name>
<surname>Altmann</surname>
<given-names>Andre</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Centre for Medical Image Computing (CMIC), Medical Physics
and Biomedical Engineering, University College London (UCL), London,
UK</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Engineering Mathematics and Computing Lab (EMCL),
Heidelberg, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-11-25">
<day>25</day>
<month>11</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>85</issue>
<fpage>5093</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Autoencoders</kwd>
<kwd>Multi-view</kwd>
<kwd>Unsupervised learning</kwd>
<kwd>Representation learning</kwd>
<kwd>Data generation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Often, data can be naturally described via multiple views or
modalities. For example, we could consider an image and the
corresponding text as different modalities. These modalities contain
complementary information which can be modelled jointly using multi-view
methods. The joint modelling of multiple modalities has been explored in
many research fields such as medical imaging
(<xref alt="Serra et al., 2019" rid="ref-Serra2018" ref-type="bibr">Serra
et al., 2019</xref>), chemistry
(<xref alt="Sjöström et al., 1983" rid="ref-Sjostrom1983" ref-type="bibr">Sjöström
et al., 1983</xref>), and natural language processing
(<xref alt="Sadr et al., 2020" rid="ref-Sadr2020" ref-type="bibr">Sadr
et al., 2020</xref>).</p>
<p>Autoencoders are unsupervised models which learn low dimensional
latent representations of complex data. The autoencoder framework
consists of two mappings; the encoder which embeds information from the
input space into a latent space, and a decoder which transforms point
estimates from the latent space back into in the input space.
Autoencoders have been successful in downstream tasks such as
classification
(<xref alt="Creswell &amp; Bharath, 2017" rid="ref-Creswell2017" ref-type="bibr">Creswell
&amp; Bharath, 2017</xref>), outlier detection
(<xref alt="An &amp; Cho, 2015" rid="ref-An2015" ref-type="bibr">An
&amp; Cho, 2015</xref>), and data generation
(<xref alt="Wei &amp; Mahmood, 2021" rid="ref-Wei2020" ref-type="bibr">Wei
&amp; Mahmood, 2021</xref>).</p>
<p>There exist many software frameworks for extending autoencoders to
multiple modalities. Generally, this involves learning separate encoder
and decoder functions for each modality with the latent representations
being combined or associated in some way. By far the most popular group
of multi-view autoencoder models are multi-view extensions of
Variational Autoencoders (VAEs) where the latent space is regularised by
mapping the encoding distributions to a gaussian prior using a
Kullback–Leibler (KL) divergence term. However, there are also other
multi-view autoencoder frameworks, such as multi-view Adversarial
Autoencoders (AAEs)
(<xref alt="X. Wang et al., 2019" rid="ref-Wang2019" ref-type="bibr">X.
Wang et al., 2019</xref>). Here the latent space is regularised by
mapping the encoding distribution to a prior (here a gaussian) using an
auxiliary discriminator tasked with distinguishing samples from the
posterior and prior distributions. The choice of AAE or VAE model may be
influenced by various elements of the application process. For example,
the encoding distribution which best describes the data or stability
during training may impact the choice of model.</p>
<fig>
  <caption><p>Single view autoencoder frameworks; (a) vanilla
  autoencoder, (b) adversarial autoencoder, (c) variational
  autoencoder.</p></caption>
  <graphic mimetype="image" mime-subtype="png" xlink:href="media/84e492ecc51908d0ce8ac67e9cf6d5b9be186ebf.png" />
</fig>
<p>Even within these regularisation frameworks there are vast modelling
differences to be considered when choosing the best model for the task
at hand.
<xref alt="[fig:AElatent]" rid="figU003AAElatent">[fig:AElatent]</xref>
depicts two possible latent variable models for modelling two views of
data; <inline-formula><alternatives>
<tex-math><![CDATA[X]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
and <inline-formula><alternatives>
<tex-math><![CDATA[Y]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>.
<xref alt="[fig:AElatent]" rid="figU003AAElatent">[fig:AElatent]</xref>a
shows the joint latent variable model
(<xref alt="Suzuki &amp; Matsuo, 2022" rid="ref-Suzuki2022" ref-type="bibr">Suzuki
&amp; Matsuo, 2022</xref>) where both views,
<inline-formula><alternatives>
<tex-math><![CDATA[X]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
and <inline-formula><alternatives>
<tex-math><![CDATA[Y]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>,
share an underlying factor. The latent variable model in
<xref alt="[fig:AElatent]" rid="figU003AAElatent">[fig:AElatent]</xref>b
shows a coordinated model, which assumes some relationship between the
latent variables, <inline-formula><alternatives>
<tex-math><![CDATA[z_x]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>z</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
and <inline-formula><alternatives>
<tex-math><![CDATA[z_y]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>z</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
of <inline-formula><alternatives>
<tex-math><![CDATA[X]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
and <inline-formula><alternatives>
<tex-math><![CDATA[Y]]></tex-math>
<mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
respectively. Which latent variable model is most appropriate depends on
the desired outcome of the learning task. Example multi-view autoencoder
frameworks built for these two latent variable models are given in
<xref alt="[fig:AEexample]" rid="figU003AAEexample">[fig:AEexample]</xref>.</p>
<fig>
  <caption><p>Latent variable models for two input views. Latent
  variable model where data <inline-formula><alternatives>
  <tex-math><![CDATA[X]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[Y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
  (a) share an underlying latent factor <inline-formula><alternatives>
  <tex-math><![CDATA[z]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>z</mml:mi></mml:math></alternatives></inline-formula>
  (b) have associated latent factors <inline-formula><alternatives>
  <tex-math><![CDATA[z_x]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>z</mml:mi><mml:mi>x</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[z_y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>z</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></alternatives></inline-formula>.<styled-content id="figU003AAElatent"></styled-content></p></caption>
  <graphic mimetype="image" mime-subtype="png" xlink:href="media/af040642dd1127e797523c7b6c263e51ab7a99e6.png" />
</fig>
<fig>
  <caption><p>Example frameworks of a two-view autoencoder for data
  <inline-formula><alternatives>
  <tex-math><![CDATA[X]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>X</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[Y]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Y</mml:mi></mml:math></alternatives></inline-formula>
  for a (a) joint model, where the individual latent spaces are combined
  and the reconstruction is carried out from the joint latent space, and
  a (b) coordinated model, where the latent representations are
  coordinated either by cross view generation or an addition loss term
  for association between the latent
  variables.<styled-content id="figU003AAEexample"></styled-content></p></caption>
  <graphic mimetype="image" mime-subtype="png" xlink:href="media/e1f1bd3c38fb01ab1aede37f273f43ccaf72b70a.png" />
</fig>
<p>Given the large number of multi-view autoencoders and versatility of
architecture, it is important to consider which model would best suit
the use case. <monospace>multi-view-AE</monospace> is a Python library
which implements several variants of multi-view autoencoders in a
simple, flexible, and easy-to-use framework. We would like to highlight
the following benefits of our package.</p>
<p>Firstly, the <monospace>multi-view-AE</monospace> package is
implemented with a similar interface to
<monospace>scikit-learn</monospace>
(<xref alt="Buitinck et al., 2013" rid="ref-sklearn_api2013" ref-type="bibr">Buitinck
et al., 2013</xref>) with common and straight-forward functions
implemented for all models. This makes it simple for users to train and
evaluate models without requiring detailed knowledge of the methodology.
Secondly, all models follow a modular structure. This gives users the
flexibility to choose the class (such as the encoder or decoder network)
from the available implementations, or to contribute their own. As such,
the <monospace>multi-view-AE</monospace> package is accessible to both
beginners, with off-the-shelf models, and experts, who wish to adapt the
existing framework for further research purposes. Finally, the
<monospace>multi-view-AE</monospace> package uses the
<monospace>PyTorch-Lightning</monospace>
(<xref alt="Falcon &amp; others, 2019" rid="ref-Falcon2019" ref-type="bibr">Falcon
&amp; others, 2019</xref>) API which offers the same functionality as
raw <monospace>PyTorch</monospace>
(<xref alt="Paszke et al., 2019" rid="ref-Paszke2019" ref-type="bibr">Paszke
et al., 2019</xref>) in a more structured and streamlined way. This
offers users more flexibility, faster training and optimisation time,
and high scalability.</p>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Multi-view autoencoders have become a popular family of
  unsupervised learning methods in the field of multi-view learning. The
  flexibility of the form of the encoder and decoder functions, ease of
  extension to multiple views, generative properties, and adaptability
  to large scale datasets has contributed to the popularity of
  multi-view autoencoders compared to other multi-view methods.
  Subsequently, multi-view autoencoders have used to address challenges
  across a range of fields; such as anomaly detection from videos
  (<xref alt="Deepak et al., 2021" rid="ref-Deepak2021" ref-type="bibr">Deepak
  et al., 2021</xref>) or cross-modal generation of multi-omics data
  (<xref alt="“A Mixture-of-Experts Deep Generative Model for Integrated Analysis of Single-Cell Multiomics Data,” 2021" rid="ref-Minoura2021" ref-type="bibr">“A
  Mixture-of-Experts Deep Generative Model for Integrated Analysis of
  Single-Cell Multiomics Data,” 2021</xref>).</p>
  <p>There exist many different multi-view autoencoder frameworks with
  the best method of choice depending on the specific task. Existing
  code is often implemented using different Deep Learning frameworks or
  varied programming styles making it difficult for users to compare
  methods. The motivation for developing the
  <monospace>multi-view-AE</monospace> library is to widen the
  accessibility of these algorithms by allowing users to easily test
  methods on new datasets and enable developers to compare methods and
  extend code for new research applications. The modular structure of
  the <monospace>multi-view-AE</monospace> library allows developers to
  choose which element of the code to extend and swap out existing
  Python classes for new implementations whilst leaving the wider
  codebase untouched.</p>
  <p>There exists, as far as we are aware, no Python library that
  collates a large number of multi-view autoencoder models into one easy
  to use framework. The <monospace>Pixyz</monospace> library
  (<xref alt="Suzuki et al., 2021" rid="ref-Suzuki2021" ref-type="bibr">Suzuki
  et al., 2021</xref>) is probably the closest relative of
  <monospace>multi-view-AE</monospace>, implementing a number of
  multi-view autoencoder methods. However, <monospace>Pixyz</monospace>
  is designed for the wider field of deep generative modelling whereas
  <monospace>multi-view-AE</monospace> focuses specifically on
  multi-view autoencoder models. As such
  <monospace>multi-view-AE</monospace> builds upon
  <monospace>Pixyz</monospace>’s multi-view offering providing a wider
  range of multi-view methods.</p>
</sec>
<sec id="software-description">
  <title>Software description</title>
  <sec id="software-architecture">
    <title>Software architecture</title>
    <p>Following the <monospace>scikit-learn</monospace> interface, to
    train a multi-view autoencoder model with the
    <monospace>multi-view-AE</monospace> package, first a model object
    is initialised with relevant parameters in an easy-to-configure
    file. Next, the model is trained with the
    <monospace>fit()</monospace> method using the specified data.
    Following fitting, the saved model object can be used for further
    analysis: predicting the latent variables, using
    <monospace>predict_latent()</monospace>, or data reconstructions,
    using <monospace>predict_reconstruction()</monospace>.</p>
    <p>All models are implemented in <monospace>PyTorch</monospace>
    using the <monospace>PyTorch-Lightning</monospace> wrapper.</p>
  </sec>
  <sec id="parameter-settings">
    <title>Parameter settings</title>
    <p>The <monospace>multi-view-AE</monospace> package uses the Hydra
    API for configuration management. Most parameters are set in a
    configuration file and are loaded into the model object by
    <monospace>Hydra</monospace>. The combination of Hydra with the
    modular structure of models in the
    <monospace>multi-view-AE</monospace> package, makes it easy for the
    user to replace model elements with, either other available
    implementations or their own by editing the relevant section of the
    configuration file.</p>
  </sec>
  <sec id="implemented-models">
    <title>Implemented models</title>
    <p>A complete model list at the time of publication:</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="38%" />
          <col width="44%" />
          <col width="18%" />
        </colgroup>
        <thead>
          <tr>
            <th>Model class</th>
            <th align="center">Model name</th>
            <th align="right">Number of views</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>mcVAE</td>
            <td align="center">Multi-Channel Variational Autoencoder
            (mcVAE)
            (<xref alt="Antelmi et al., 2019" rid="ref-Antelmi2019" ref-type="bibr">Antelmi
            et al., 2019</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>AE</td>
            <td align="center">Multi-view Autoencoder</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>AAE</td>
            <td align="center">Multi-view Adversarial Autoencoder with
            separate latent representations</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>DVCCA</td>
            <td align="center">Deep Variational CCA
            (<xref alt="W. Wang et al., 2016" rid="ref-Wang2016" ref-type="bibr">W.
            Wang et al., 2016</xref>)</td>
            <td align="right">2</td>
          </tr>
          <tr>
            <td>jointAAE</td>
            <td align="center">Multi-view Adversarial Autoencoder with
            joint latent representation</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>wAAE</td>
            <td align="center">Multi-view Adversarial Autoencoder with
            joint latent representation and Wasserstein loss</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>mmVAE</td>
            <td align="center">Variational mixture-of-experts
            autoencoder (MMVAE)
            (<xref alt="Shi et al., 2019" rid="ref-Shi2019" ref-type="bibr">Shi
            et al., 2019</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>mVAE</td>
            <td align="center">Multimodal Variational Autoencoder (MVAE)
            (<xref alt="Wu &amp; Goodman, 2018" rid="ref-Wu2018" ref-type="bibr">Wu
            &amp; Goodman, 2018</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>me_mVAE</td>
            <td align="center">Multimodal Variational Autoencoder (MVAE)
            with separate ELBO terms for each view
            (<xref alt="Wu &amp; Goodman, 2018" rid="ref-Wu2018" ref-type="bibr">Wu
            &amp; Goodman, 2018</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>JMVAE</td>
            <td align="center">Joint Multimodal Variational
            Autoencoder(JMVAE-kl)
            (<xref alt="Suzuki et al., 2016" rid="ref-Suzuki2016" ref-type="bibr">Suzuki
            et al., 2016</xref>)</td>
            <td align="right">2</td>
          </tr>
          <tr>
            <td>MVTCAE</td>
            <td align="center">Multi-View Total Correlation Auto-Encoder
            (MVTCAE)
            (<xref alt="Hwang et al., 2021" rid="ref-Hwang2021" ref-type="bibr">Hwang
            et al., 2021</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>MoPoEVAE</td>
            <td align="center">Mixture-of-Products-of-Experts VAE
            (<xref alt="T. M. Sutter et al., 2021" rid="ref-Sutter2021" ref-type="bibr">T.
            M. Sutter et al., 2021</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
          <tr>
            <td>mmJSD</td>
            <td align="center">Multimodal Jensen-Shannon divergence
            model (mmJSD)
            (<xref alt="T. Sutter et al., 2021" rid="ref-Sutter2021b" ref-type="bibr">T.
            Sutter et al., 2021</xref>)</td>
            <td align="right">&gt;=1</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="documentation">
    <title>Documentation</title>
    <p>Documentation is available
    (https://multi-view-ae.readthedocs.io/en/latest/) for the
    <monospace>multi-view-AE</monospace> package as well as tutorial
    notebooks. These resources serve as both guides to the
    <monospace>multi-view-AE</monospace> package and educational
    material for multi-view autoencoder models.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank Thomas Sutter, HyeongJoo Hwang and, Marco
  Lorenzi, and Brooks Paige, for their help understanding the
  Mixture-of-Product-of-Experts VAE
  (<xref alt="T. M. Sutter et al., 2021" rid="ref-Sutter2021" ref-type="bibr">T.
  M. Sutter et al., 2021</xref>) and mmJSD
  (<xref alt="T. Sutter et al., 2021" rid="ref-Sutter2021b" ref-type="bibr">T.
  Sutter et al., 2021</xref>), MVTCAE
  (<xref alt="Hwang et al., 2021" rid="ref-Hwang2021" ref-type="bibr">Hwang
  et al., 2021</xref>), mcVAE
  (<xref alt="Antelmi et al., 2019" rid="ref-Antelmi2019" ref-type="bibr">Antelmi
  et al., 2019</xref>), and MMVAE
  (<xref alt="Shi et al., 2019" rid="ref-Shi2019" ref-type="bibr">Shi et
  al., 2019</xref>) models, respectively.</p>
  <p>ALA and NMB are supported by the EPSRC-funded UCL Centre for
  Doctoral Training in Intelligent, Integrated Imaging in Healthcare
  (i4health) and the Department of Health’s NIHR-funded Biomedical
  Research Centre at University College London Hospitals (EP/S021930/1).
  AJ is supported by the Engineering Mathematics and Computing Lab
  (EMCL), Heidelberg University, the Helmholtz Association under the
  joint research school “HIDSS4Health – Helmholtz Information and Data
  Science School for Health”, and the Heidelberg Institute for
  Theoretical Studies (HITS).</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Serra2018">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Serra</surname><given-names>A.</given-names></name>
        <name><surname>Galdi</surname><given-names>P.</given-names></name>
        <name><surname>Tagliaferri</surname><given-names>R.</given-names></name>
      </person-group>
      <article-title>Multiview learning in biomedical applications</article-title>
      <source>Artificial intelligence in the age of neural networks and brain computing</source>
      <publisher-name>Academic Press</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1016/B978-0-12-815480-9.00013-X</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Sjostrom1983">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sjöström</surname><given-names>Michael</given-names></name>
        <name><surname>Wold</surname><given-names>Svante</given-names></name>
        <name><surname>Lindberg</surname><given-names>Walter</given-names></name>
        <name><surname>Persson</surname><given-names>Jan-Åke</given-names></name>
        <name><surname>Martens</surname><given-names>Harald</given-names></name>
      </person-group>
      <article-title>A multivariate calibration problem in analytical chemistry solved by partial least-squares models in latent variables</article-title>
      <source>Analytica Chimica Acta</source>
      <year iso-8601-date="1983">1983</year>
      <volume>150</volume>
      <pub-id pub-id-type="doi">10.1016/S0003-2670(00)85460-4</pub-id>
      <fpage>61</fpage>
      <lpage>70</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Sadr2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sadr</surname><given-names>Hossein</given-names></name>
        <name><surname>Pedram</surname><given-names>Mir Mohsen</given-names></name>
        <name><surname>Teshnehlab</surname><given-names>Mohammad</given-names></name>
      </person-group>
      <article-title>Multi-view deep network: A deep model based on learning features from heterogeneous neural networks for sentiment analysis</article-title>
      <source>IEEE Access</source>
      <year iso-8601-date="2020-05">2020</year><month>05</month>
      <volume>8</volume>
      <pub-id pub-id-type="doi">10.1109/ACCESS.2020.2992063</pub-id>
      <fpage>86984</fpage>
      <lpage>86997</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Creswell2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Creswell</surname><given-names>Antonia</given-names></name>
        <name><surname>Bharath</surname><given-names>Anil</given-names></name>
      </person-group>
      <article-title>Denoising adversarial autoencoders</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <year iso-8601-date="2017-03">2017</year><month>03</month>
      <volume>30</volume>
      <uri>http://arxiv.org/abs/1703.01220</uri>
    </element-citation>
  </ref>
  <ref id="ref-An2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>An</surname><given-names>Jinwon</given-names></name>
        <name><surname>Cho</surname><given-names>Sungzoon</given-names></name>
      </person-group>
      <article-title>Variational autoencoder based anomaly detection using reconstruction probability</article-title>
      <source>Special Lecture on IE</source>
      <year iso-8601-date="2015">2015</year>
      <volume>2</volume>
      <issue>1</issue>
      <fpage>1</fpage>
      <lpage>18</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wei2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wei</surname><given-names>Ruoqi</given-names></name>
        <name><surname>Mahmood</surname><given-names>Ausif</given-names></name>
      </person-group>
      <article-title>Recent advances in variational autoencoders with representation learning for biomedical informatics a survey</article-title>
      <year iso-8601-date="2021">2021</year>
      <volume>9</volume>
      <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3048309</pub-id>
      <fpage>4939</fpage>
      <lpage>4956</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wang2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Xu</given-names></name>
        <name><surname>Peng</surname><given-names>Dezhong</given-names></name>
        <name><surname>Hu</surname><given-names>Peng</given-names></name>
        <name><surname>Sang</surname><given-names>Yongsheng</given-names></name>
      </person-group>
      <article-title>Adversarial correlated autoencoder for unsupervised multi-view representation learning</article-title>
      <source>Knowledge-Based Systems</source>
      <year iso-8601-date="2019">2019</year>
      <volume>168</volume>
      <pub-id pub-id-type="doi">10.1016/j.knosys.2019.01.017</pub-id>
      <fpage>109</fpage>
      <lpage>120</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sklearn_api2013">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Buitinck</surname><given-names>Lars</given-names></name>
        <name><surname>Louppe</surname><given-names>Gilles</given-names></name>
        <name><surname>Blondel</surname><given-names>Mathieu</given-names></name>
        <name><surname>Pedregosa</surname><given-names>Fabian</given-names></name>
        <name><surname>Mueller</surname><given-names>Andreas</given-names></name>
        <name><surname>Grisel</surname><given-names>Olivier</given-names></name>
        <name><surname>Niculae</surname><given-names>Vlad</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>Peter</given-names></name>
        <name><surname>Gramfort</surname><given-names>Alexandre</given-names></name>
        <name><surname>Grobler</surname><given-names>Jaques</given-names></name>
        <name><surname>Layton</surname><given-names>Robert</given-names></name>
        <name><surname>VanderPlas</surname><given-names>Jake</given-names></name>
        <name><surname>Joly</surname><given-names>Arnaud</given-names></name>
        <name><surname>Holt</surname><given-names>Brian</given-names></name>
        <name><surname>Varoquaux</surname><given-names>Gaël</given-names></name>
      </person-group>
      <article-title>API design for machine learning software: Experiences from the scikit-learn project</article-title>
      <source>ECML PKDD workshop: Languages for data mining and machine learning</source>
      <year iso-8601-date="2013">2013</year>
      <fpage>108</fpage>
      <lpage>122</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Suzuki2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>A survey of multimodal deep generative models</article-title>
      <source>Advanced Robotics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>36</volume>
      <issue>5-6</issue>
      <pub-id pub-id-type="doi">10.1080/01691864.2022.2035253</pub-id>
      <fpage>261</fpage>
      <lpage>278</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Suzuki2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Kaneko</surname><given-names>Takaaki</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Pixyz: A library for developing deep generative models</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2021">2021</year>
      <volume>abs/2107.13109</volume>
      <uri>https://arxiv.org/abs/2107.13109</uri>
    </element-citation>
  </ref>
  <ref id="ref-Antelmi2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Antelmi</surname><given-names>Luigi</given-names></name>
        <name><surname>Ayache</surname><given-names>Nicholas</given-names></name>
        <name><surname>Robert</surname><given-names>Philippe</given-names></name>
        <name><surname>Lorenzi</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>Sparse multi-channel variational autoencoder for the joint analysis of heterogeneous data</article-title>
      <source>Proceedings of the 36th international conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>97</volume>
      <uri>https://proceedings.mlr.press/v97/antelmi19a.html</uri>
      <fpage>302</fpage>
      <lpage>311</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wang2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Weiran</given-names></name>
        <name><surname>Lee</surname><given-names>Honglak</given-names></name>
        <name><surname>Livescu</surname><given-names>Karen</given-names></name>
      </person-group>
      <article-title>Deep variational canonical correlation analysis</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2016-10">2016</year><month>10</month>
      <volume>abs/1610.03454</volume>
      <uri>http://arxiv.org/abs/1610.03454</uri>
    </element-citation>
  </ref>
  <ref id="ref-Shi2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Shi</surname><given-names>Yuge</given-names></name>
        <name><surname>Narayanaswamy</surname><given-names>Siddharth</given-names></name>
        <name><surname>Paige</surname><given-names>Brooks</given-names></name>
        <name><surname>Torr</surname><given-names>Philip</given-names></name>
      </person-group>
      <article-title>Variational mixture-of-experts autoencoders for multi-modal deep generative models</article-title>
      <source>Neural information processing systems</source>
      <year iso-8601-date="2019-11">2019</year><month>11</month>
      <pub-id pub-id-type="doi">10.48550/ARXIV.1911.03393</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Wu2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Mike</given-names></name>
        <name><surname>Goodman</surname><given-names>Noah</given-names></name>
      </person-group>
      <article-title>Multimodal generative models for scalable weakly-supervised learning</article-title>
      <source>Proceedings of the 32nd international conference on neural information processing systems</source>
      <publisher-name>Curran Associates Inc.</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>http://arxiv.org/abs/1802.05335</uri>
      <fpage>5580</fpage>
      <lpage>5590</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Suzuki2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Suzuki</surname><given-names>Masahiro</given-names></name>
        <name><surname>Nakayama</surname><given-names>Kotaro</given-names></name>
        <name><surname>Matsuo</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Joint multimodal learning with deep generative models</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2016-11">2016</year><month>11</month>
      <pub-id pub-id-type="doi">10.48550/ARXIV.1611.01891</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Hwang2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hwang</surname><given-names>HyeongJoo</given-names></name>
        <name><surname>Kim</surname><given-names>Geon-Hyeong</given-names></name>
        <name><surname>Hong</surname><given-names>Seunghoon</given-names></name>
        <name><surname>Kim</surname><given-names>Kee-Eung</given-names></name>
      </person-group>
      <article-title>Multi-view representation learning via total correlation objective</article-title>
      <source>Advances in neural information processing systems</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
      <uri>https://proceedings.neurips.cc/paper/2021/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf</uri>
      <fpage>12194</fpage>
      <lpage>12207</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Sutter2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sutter</surname><given-names>Thomas M.</given-names></name>
        <name><surname>Daunhawer</surname><given-names>Imant</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia E.</given-names></name>
      </person-group>
      <article-title>Generalized multimodal ELBO</article-title>
      <source>ArXiv</source>
      <year iso-8601-date="2021">2021</year>
      <volume>abs/2105.02470</volume>
      <uri>https://arxiv.org/abs/2105.02470</uri>
    </element-citation>
  </ref>
  <ref id="ref-Sutter2021b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sutter</surname><given-names>Thomas</given-names></name>
        <name><surname>Daunhawer</surname><given-names>Imant</given-names></name>
        <name><surname>Vogt</surname><given-names>Julia</given-names></name>
      </person-group>
      <article-title>Multimodal generative learning utilizing jensen-shannon-divergence</article-title>
      <source>Advances in Neural Information Processing Systems</source>
      <year iso-8601-date="2021-03">2021</year><month>03</month>
      <volume>33</volume>
      <uri>https://arxiv.org/abs/2006.08242</uri>
    </element-citation>
  </ref>
  <ref id="ref-Minoura2021">
    <element-citation publication-type="article-journal">
      <article-title>A mixture-of-experts deep generative model for integrated analysis of single-cell multiomics data</article-title>
      <source>Cell Reports Methods</source>
      <year iso-8601-date="2021">2021</year>
      <volume>1</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1016/j.crmeth.2021.100071</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Deepak2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Deepak</surname><given-names>K. V.</given-names></name>
        <name><surname>Srivathsan</surname><given-names>G.</given-names></name>
        <name><surname>Roshan</surname><given-names>S.</given-names></name>
        <name><surname>Chandrakala</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>Deep multi-view representation learning for video anomaly detection using spatiotemporal autoencoders</article-title>
      <source>Circuits, Systems, and Signal Processing</source>
      <year iso-8601-date="2021">2021</year>
      <volume>40</volume>
      <pub-id pub-id-type="doi">10.1007/s00034-020-01522-7</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Paszke2019">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Kopf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zachary</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems 32</source>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</uri>
      <fpage>8024</fpage>
      <lpage>8035</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Falcon2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Falcon</surname><given-names>William</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch lightning</article-title>
      <source>GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning</source>
      <year iso-8601-date="2019">2019</year>
      <volume>3</volume>
      <issue>6</issue>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
