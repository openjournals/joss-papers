<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230516T143530-36b303a24b1f3545f0add9c751b1ac586536da7f</doi_batch_id>
    <timestamp>20230516143530</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>05</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>85</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Multi-view-AE: A Python package for multi-view
autoencoder models</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Ana Lawry</given_name>
            <surname>Aguila</surname>
            <ORCID>https://orcid.org/0000-0003-0727-3274</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Alejandra</given_name>
            <surname>Jayme</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Nina</given_name>
            <surname>Montaña-Brown</surname>
            <ORCID>https://orcid.org/0000-0001-5685-971X</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Vincent</given_name>
            <surname>Heuveline</surname>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Andre</given_name>
            <surname>Altmann</surname>
            <ORCID>https://orcid.org/0000-0002-9265-2393</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>05</month>
          <day>16</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5093</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05093</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7871099</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5093</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05093</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05093</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05093.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="Serra2018">
            <article_title>Multiview learning in biomedical
applications</article_title>
            <author>Serra</author>
            <journal_title>Artificial intelligence in the age of neural
networks and brain computing</journal_title>
            <doi>10.1016/B978-0-12-815480-9.00013-X</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Serra, A., Galdi, P., &amp;
Tagliaferri, R. (2019). Multiview learning in biomedical applications.
In Artificial intelligence in the age of neural networks and brain
computing. Academic Press.
https://doi.org/10.1016/B978-0-12-815480-9.00013-X</unstructured_citation>
          </citation>
          <citation key="Sjostrom1983">
            <article_title>A multivariate calibration problem in
analytical chemistry solved by partial least-squares models in latent
variables</article_title>
            <author>Sjöström</author>
            <journal_title>Analytica Chimica Acta</journal_title>
            <volume>150</volume>
            <doi>10.1016/S0003-2670(00)85460-4</doi>
            <cYear>1983</cYear>
            <unstructured_citation>Sjöström, M., Wold, S., Lindberg, W.,
Persson, J.-Å., &amp; Martens, H. (1983). A multivariate calibration
problem in analytical chemistry solved by partial least-squares models
in latent variables. Analytica Chimica Acta, 150, 61–70.
https://doi.org/10.1016/S0003-2670(00)85460-4</unstructured_citation>
          </citation>
          <citation key="Sadr2020">
            <article_title>Multi-view deep network: A deep model based
on learning features from heterogeneous neural networks for sentiment
analysis</article_title>
            <author>Sadr</author>
            <journal_title>IEEE Access</journal_title>
            <volume>8</volume>
            <doi>10.1109/ACCESS.2020.2992063</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Sadr, H., Pedram, M. M., &amp;
Teshnehlab, M. (2020). Multi-view deep network: A deep model based on
learning features from heterogeneous neural networks for sentiment
analysis. IEEE Access, 8, 86984–86997.
https://doi.org/10.1109/ACCESS.2020.2992063</unstructured_citation>
          </citation>
          <citation key="Creswell2017">
            <article_title>Denoising adversarial
autoencoders</article_title>
            <author>Creswell</author>
            <journal_title>IEEE Transactions on Neural Networks and
Learning Systems</journal_title>
            <volume>30</volume>
            <cYear>2017</cYear>
            <unstructured_citation>Creswell, A., &amp; Bharath, A.
(2017). Denoising adversarial autoencoders. IEEE Transactions on Neural
Networks and Learning Systems, 30.
http://arxiv.org/abs/1703.01220</unstructured_citation>
          </citation>
          <citation key="An2015">
            <article_title>Variational autoencoder based anomaly
detection using reconstruction probability</article_title>
            <author>An</author>
            <journal_title>Special Lecture on IE</journal_title>
            <issue>1</issue>
            <volume>2</volume>
            <cYear>2015</cYear>
            <unstructured_citation>An, J., &amp; Cho, S. (2015).
Variational autoencoder based anomaly detection using reconstruction
probability. Special Lecture on IE, 2(1), 1–18.</unstructured_citation>
          </citation>
          <citation key="Wei2020">
            <article_title>Recent advances in variational autoencoders
with representation learning for biomedical informatics a
survey</article_title>
            <author>Wei</author>
            <volume>9</volume>
            <doi>10.1109/ACCESS.2020.3048309</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Wei, R., &amp; Mahmood, A. (2021).
Recent advances in variational autoencoders with representation learning
for biomedical informatics a survey. 9, 4939–4956.
https://doi.org/10.1109/ACCESS.2020.3048309</unstructured_citation>
          </citation>
          <citation key="Wang2019">
            <article_title>Adversarial correlated autoencoder for
unsupervised multi-view representation learning</article_title>
            <author>Wang</author>
            <journal_title>Knowledge-Based Systems</journal_title>
            <volume>168</volume>
            <doi>10.1016/j.knosys.2019.01.017</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Wang, X., Peng, D., Hu, P., &amp;
Sang, Y. (2019). Adversarial correlated autoencoder for unsupervised
multi-view representation learning. Knowledge-Based Systems, 168,
109–120.
https://doi.org/10.1016/j.knosys.2019.01.017</unstructured_citation>
          </citation>
          <citation key="sklearn_api2013">
            <article_title>API design for machine learning software:
Experiences from the scikit-learn project</article_title>
            <author>Buitinck</author>
            <journal_title>ECML PKDD workshop: Languages for data mining
and machine learning</journal_title>
            <cYear>2013</cYear>
            <unstructured_citation>Buitinck, L., Louppe, G., Blondel,
M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer,
P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A.,
Holt, B., &amp; Varoquaux, G. (2013). API design for machine learning
software: Experiences from the scikit-learn project. ECML PKDD Workshop:
Languages for Data Mining and Machine Learning,
108–122.</unstructured_citation>
          </citation>
          <citation key="Suzuki2022">
            <article_title>A survey of multimodal deep generative
models</article_title>
            <author>Suzuki</author>
            <journal_title>Advanced Robotics</journal_title>
            <issue>5-6</issue>
            <volume>36</volume>
            <doi>10.1080/01691864.2022.2035253</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Suzuki, M., &amp; Matsuo, Y. (2022).
A survey of multimodal deep generative models. Advanced Robotics,
36(5-6), 261–278.
https://doi.org/10.1080/01691864.2022.2035253</unstructured_citation>
          </citation>
          <citation key="Suzuki2021">
            <article_title>Pixyz: A library for developing deep
generative models</article_title>
            <author>Suzuki</author>
            <journal_title>ArXiv</journal_title>
            <volume>abs/2107.13109</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Suzuki, M., Kaneko, T., &amp; Matsuo,
Y. (2021). Pixyz: A library for developing deep generative models.
ArXiv, abs/2107.13109.
https://arxiv.org/abs/2107.13109</unstructured_citation>
          </citation>
          <citation key="Antelmi2019">
            <article_title>Sparse multi-channel variational autoencoder
for the joint analysis of heterogeneous data</article_title>
            <author>Antelmi</author>
            <journal_title>Proceedings of the 36th international
conference on machine learning</journal_title>
            <volume>97</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Antelmi, L., Ayache, N., Robert, P.,
&amp; Lorenzi, M. (2019). Sparse multi-channel variational autoencoder
for the joint analysis of heterogeneous data. Proceedings of the 36th
International Conference on Machine Learning, 97, 302–311.
https://proceedings.mlr.press/v97/antelmi19a.html</unstructured_citation>
          </citation>
          <citation key="Wang2016">
            <article_title>Deep variational canonical correlation
analysis</article_title>
            <author>Wang</author>
            <journal_title>ArXiv</journal_title>
            <volume>abs/1610.03454</volume>
            <cYear>2016</cYear>
            <unstructured_citation>Wang, W., Lee, H., &amp; Livescu, K.
(2016). Deep variational canonical correlation analysis. ArXiv,
abs/1610.03454. http://arxiv.org/abs/1610.03454</unstructured_citation>
          </citation>
          <citation key="Shi2019">
            <article_title>Variational mixture-of-experts autoencoders
for multi-modal deep generative models</article_title>
            <author>Shi</author>
            <journal_title>Neural information processing
systems</journal_title>
            <doi>10.48550/ARXIV.1911.03393</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Shi, Y., Narayanaswamy, S., Paige,
B., &amp; Torr, P. (2019, November). Variational mixture-of-experts
autoencoders for multi-modal deep generative models. Neural Information
Processing Systems.
https://doi.org/10.48550/ARXIV.1911.03393</unstructured_citation>
          </citation>
          <citation key="Wu2018">
            <article_title>Multimodal generative models for scalable
weakly-supervised learning</article_title>
            <author>Wu</author>
            <journal_title>Proceedings of the 32nd international
conference on neural information processing systems</journal_title>
            <cYear>2018</cYear>
            <unstructured_citation>Wu, M., &amp; Goodman, N. (2018).
Multimodal generative models for scalable weakly-supervised learning.
Proceedings of the 32nd International Conference on Neural Information
Processing Systems, 5580–5590.
http://arxiv.org/abs/1802.05335</unstructured_citation>
          </citation>
          <citation key="Suzuki2016">
            <article_title>Joint multimodal learning with deep
generative models</article_title>
            <author>Suzuki</author>
            <journal_title>arXiv</journal_title>
            <doi>10.48550/ARXIV.1611.01891</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Suzuki, M., Nakayama, K., &amp;
Matsuo, Y. (2016). Joint multimodal learning with deep generative
models. arXiv.
https://doi.org/10.48550/ARXIV.1611.01891</unstructured_citation>
          </citation>
          <citation key="Hwang2021">
            <article_title>Multi-view representation learning via total
correlation objective</article_title>
            <author>Hwang</author>
            <journal_title>Advances in neural information processing
systems</journal_title>
            <volume>34</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Hwang, H., Kim, G.-H., Hong, S.,
&amp; Kim, K.-E. (2021). Multi-view representation learning via total
correlation objective. Advances in Neural Information Processing
Systems, 34, 12194–12207.
https://proceedings.neurips.cc/paper/2021/file/65a99bb7a3115fdede20da98b08a370f-Paper.pdf</unstructured_citation>
          </citation>
          <citation key="Sutter2021">
            <article_title>Generalized multimodal ELBO</article_title>
            <author>Sutter</author>
            <journal_title>ArXiv</journal_title>
            <volume>abs/2105.02470</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Sutter, T. M., Daunhawer, I., &amp;
Vogt, J. E. (2021). Generalized multimodal ELBO. ArXiv, abs/2105.02470.
https://arxiv.org/abs/2105.02470</unstructured_citation>
          </citation>
          <citation key="Sutter2021b">
            <article_title>Multimodal generative learning utilizing
jensen-shannon-divergence</article_title>
            <author>Sutter</author>
            <journal_title>Advances in Neural Information Processing
Systems</journal_title>
            <volume>33</volume>
            <cYear>2021</cYear>
            <unstructured_citation>Sutter, T., Daunhawer, I., &amp;
Vogt, J. (2021). Multimodal generative learning utilizing
jensen-shannon-divergence. Advances in Neural Information Processing
Systems, 33. https://arxiv.org/abs/2006.08242</unstructured_citation>
          </citation>
          <citation key="Minoura2021">
            <article_title>A mixture-of-experts deep generative model
for integrated analysis of single-cell multiomics data</article_title>
            <journal_title>Cell Reports Methods</journal_title>
            <issue>5</issue>
            <volume>1</volume>
            <doi>10.1016/j.crmeth.2021.100071</doi>
            <cYear>2021</cYear>
            <unstructured_citation>A mixture-of-experts deep generative
model for integrated analysis of single-cell multiomics data. (2021).
Cell Reports Methods, 1(5).
https://doi.org/10.1016/j.crmeth.2021.100071</unstructured_citation>
          </citation>
          <citation key="Deepak2021">
            <article_title>Deep multi-view representation learning for
video anomaly detection using spatiotemporal
autoencoders</article_title>
            <author>Deepak</author>
            <journal_title>Circuits, Systems, and Signal
Processing</journal_title>
            <volume>40</volume>
            <doi>10.1007/s00034-020-01522-7</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Deepak, K. V., Srivathsan, G.,
Roshan, S., &amp; Chandrakala, S. (2021). Deep multi-view representation
learning for video anomaly detection using spatiotemporal autoencoders.
Circuits, Systems, and Signal Processing, 40.
https://doi.org/10.1007/s00034-020-01522-7</unstructured_citation>
          </citation>
          <citation key="Paszke2019">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems 32</journal_title>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. In Advances in neural information processing systems 32 (pp.
8024–8035). Curran Associates, Inc.
https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</unstructured_citation>
          </citation>
          <citation key="Falcon2019">
            <article_title>Pytorch lightning</article_title>
            <author>Falcon</author>
            <journal_title>GitHub. Note:
https://github.com/PyTorchLightning/pytorch-lightning</journal_title>
            <issue>6</issue>
            <volume>3</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Falcon, W., &amp; others. (2019).
Pytorch lightning. GitHub. Note:
Https://Github.com/PyTorchLightning/Pytorch-Lightning,
3(6).</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
