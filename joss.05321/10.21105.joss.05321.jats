<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5321</article-id>
<article-id pub-id-type="doi">10.21105/joss.05321</article-id>
<title-group>
<article-title><monospace>stimupy</monospace>: A Python package for
creating stimuli in vision science</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3621-9576</contrib-id>
<name>
<surname>Schmittwilken</surname>
<given-names>Lynn</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Maertens</surname>
<given-names>Marianne</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6882-5584</contrib-id>
<name>
<surname>Vincent</surname>
<given-names>Joris</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Science of Intelligence, Technische Universität Berlin,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Computational Psychology, Technische Universität Berlin,
Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-03-24">
<day>24</day>
<month>3</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>86</issue>
<fpage>5321</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python package</kwd>
<kwd>vision science</kwd>
<kwd>stimulus creation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Visual stimuli are at the heart of perception research. They may
  come as visual illusions which demonstrate the striking differences
  between the perceptual and physical world, they may involve minuscule
  stimulus changes which are used to probe the limits of visual
  sensitivity, or they may be used to probe any other aspect of visual
  processing. <monospace>stimupy</monospace> is a free and open-source
  Python package which allows the user to create visual stimuli of
  different complexity as they are commonly used in the study of visual
  perception
  (<xref alt="[fig:overview]" rid="figU003Aoverview">[fig:overview]</xref>).</p>
  <p><monospace>stimupy</monospace> provides functions to generate:</p>
  <list list-type="bullet">
    <list-item>
      <p>basic
      <ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/_api/stimupy.components.html">components</ext-link>,
      including shapes, lines, gratings, checkerboards, and
      Gaussians</p>
    </list-item>
    <list-item>
      <p>different types of visual
      <ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/_api/stimupy.noises.html">noise</ext-link>
      textures</p>
    </list-item>
    <list-item>
      <p>visual
      <ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/_api/stimupy.stimuli.html">stimuli</ext-link>
      such as Gabors, plaids, edges, and a variety of so-called
      illusions (e.g., Simultaneous Brightness Contrast, White’s
      illusion, Hermann grid, Ponzo illusion), and many more</p>
    </list-item>
    <list-item>
      <p>stimulus sets from prior research
      <ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/_api/stimupy.papers.html">papers</ext-link>,
      providing exact stimulus recreations (e.g., ModelFest, Carney et
      al.
      (<xref alt="1999" rid="ref-carney1999" ref-type="bibr">1999</xref>))</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/_api/stimupy.utils.html">utility
      functions</ext-link> for stimulus import, export, manipulation
      (e.g., contrast, size), or plotting</p>
    </list-item>
    <list-item>
      <p><ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/index.html">documentation</ext-link>,
      including
      <ext-link ext-link-type="uri" xlink:href="https://stimupy.readthedocs.io/en/latest/reference/demos.html">interactive
      demonstrations</ext-link> of stimulus functions</p>
    </list-item>
    <list-item>
      <p>unit and integration
      <ext-link ext-link-type="uri" xlink:href="https://github.com/computational-psychology/stimupy/actions/workflows/test.yml">tests</ext-link></p>
    </list-item>
  </list>
  <p><monospace>stimupy</monospace> has been designed to:</p>
  <list list-type="bullet">
    <list-item>
      <p>generate (novel) visual stimuli in a reproducible, flexible,
      and easy way</p>
    </list-item>
    <list-item>
      <p>recreate exact stimuli as they have been used in prior vision
      research</p>
    </list-item>
    <list-item>
      <p>explore large parameter spaces to reveal relations between
      formerly unconnected stimuli</p>
    </list-item>
    <list-item>
      <p>provide classic stimulus sets (e.g., ModelFest), exactly as
      described in the original manuscripts (including experimental
      data)</p>
    </list-item>
    <list-item>
      <p>build new stimulus sets or benchmarks (e.g., for testing
      computational models), and easily add them to
      <monospace>stimupy</monospace></p>
    </list-item>
    <list-item>
      <p>support vision science by providing a large, openly-available
      and flexible battery of relevant stimulus functions</p>
    </list-item>
    <list-item>
      <p>unify and automate stimulus creation</p>
    </list-item>
  </list>
  <fig>
    <caption><p>A small fraction of the stimulus variety that
    <monospace>stimupy</monospace> can produce
    <styled-content id="figU003Aoverview"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/overview.png" />
  </fig>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Creating visual stimuli is a central task in vision research. To
  generate stimuli, it is common practice to either write your own
  stimulus functions from scratch; reuse existing code; or import a
  static stimulus version from an image or data file (see e.g., Carney
  et al.
  (<xref alt="1999" rid="ref-carney1999" ref-type="bibr">1999</xref>),
  Murray
  (<xref alt="2020" rid="ref-murray2020" ref-type="bibr">2020</xref>)).
  The alternative to these idiosyncratic approaches is to use existing
  software which provides more flexible stimulus functions.</p>
  <p>We are currently aware of</p>
  <list list-type="bullet">
    <list-item>
      <p>Psychtoolbox
      (<xref alt="Brainard, 1997" rid="ref-brainard1997" ref-type="bibr">Brainard,
      1997</xref>),</p>
    </list-item>
    <list-item>
      <p>Psychopy
      (<xref alt="Peirce et al., 2019" rid="ref-peirce2019" ref-type="bibr">Peirce
      et al., 2019</xref>),</p>
    </list-item>
    <list-item>
      <p>Pyllusion
      (<xref alt="Makowski et al., 2021" rid="ref-makowski2021" ref-type="bibr">Makowski
      et al., 2021</xref>),</p>
    </list-item>
    <list-item>
      <p>OCTA
      (<xref alt="Van Geert et al., 2022" rid="ref-OCTA" ref-type="bibr">Van
      Geert et al., 2022</xref>).</p>
    </list-item>
  </list>
  <p>Psychtoolbox and Psychopy both provide functions to generate a
  number of visual stimuli. However, stimulus generation is integrated
  into their main purpose which is to run psychophysical experiments.
  The design focus of both Psychtoolbox and Psychopy has therefore been
  to support the user to interface between computer hardware and MATLAB
  and Python, respectively, to enable temporal precision and high
  dynamic range stimulus delivery.</p>
  <p>The design focus of <monospace>stimupy</monospace> is on stimulus
  creation. This allows <monospace>stimupy</monospace> to include many
  more stimuli than included in Psychtoolbox or Psychopy. It also allows
  the user to interact with the stimulus arrays directly. This makes it
  easy to manipulate the stimulus and use it for other purposes than
  psychophysical experimentation (e.g., computational modeling,
  visualization). This also means that in order to present the stimuli
  on a computer monitor, the user may still want to use Psychopy,
  Psychtoolbox or another delivery system for hardware control.</p>
  <p>Pyllusion is a Python package to generate a number of well-known
  illusions such as the Müller-Lyer, Ponzo or Zöllner illusions, and
  more. Pyllusion provides functions for each of these illusions using
  high-level parameters (e.g., illusion strength). The parametric
  approach of Pyllusion is similar in spirit to
  <monospace>stimupy</monospace>. However, in Pyllusion each
  illusion-function stands alone: it produces only that stimulus, and
  its arguments are unique to that stimulus. In contrast,
  <monospace>stimupy</monospace> provides a unified interface to
  stimulus creation, where many functions share the same —intuitive—
  parameters. This makes it easier to explore parameters and to create
  novel stimuli.</p>
  <p>OCTA is also a Python package to generate stimuli, specifically
  grids of multiple elements that show regularity and variety along
  various stimulus dimensions. These stimuli are of particular use to
  studies on Gestalt vision, aesthetics and complexity. The parametric
  variation of stimulus dimensions as well as the compositionality of
  displays are features found in both OCTA and
  <monospace>stimupy</monospace>. Both packages also have a strong focus
  on ease-of-use, replicability, and open science.
  <monospace>stimupy</monospace> currently focuses on a different class
  of stimuli: mainly displays used to study early and low-level visual
  processes, as well as visual features such as brightness, contrast,
  and orientation. Thus, OCTA and <monospace>stimupy</monospace> cover
  complementary use cases.</p>
  <p>Another design decision that sets <monospace>stimupy</monospace>
  apart from existing software such as OCTA and Pyllusion, is that all
  <monospace>stimupy</monospace> stimuli are generated as
  <monospace>NumPy</monospace>-arrays representing pixel-based
  raster-graphics (NumPy, Harris et al.
  (<xref alt="2020" rid="ref-harris2020" ref-type="bibr">2020</xref>)).
  This has several advantages over existing, vector-graphics or custom
  object-based approaches, mainly that any standard array-manipulation
  tooling can be used to further process a stimulus.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Many visual stimuli are used time and again. Despite their
  relevance, there is no standard way of implementing stimuli which
  considers function parameters in a way that is targeted towards vision
  science. Hence, in practice, researchers implement their own stimuli
  from scratch or are lucky enough to find some existing implementation
  online, from colleagues or in the above mentioned software packages.
  Depending on the complexity or specificity of the desired stimulus
  manipulation, this endeavor is (1) time-consuming, (2) prone to error,
  and (3) makes comparisons with other research difficult. Hence, we
  developed <monospace>stimupy</monospace> to simplify, unify and
  automate visual stimulus generation while at the same time allowing
  the flexibility to create entirely new stimuli and build stimulus
  benchmarks.</p>
  <p>As far as we know <monospace>stimupy</monospace> is the only
  package that:</p>
  <list list-type="bullet">
    <list-item>
      <p>contains a wide variety of visual stimuli, from simple
      geometric shapes to complex illusions</p>
    </list-item>
    <list-item>
      <p>includes ready-to-use replications of existing stimulus sets
      (e.g., ModelFest)</p>
    </list-item>
    <list-item>
      <p>makes it easy to create new stimuli because (1) stimulus
      functions use parameters which are familiar to vision scientists,
      and (2) it provides building blocks and masks which can be used to
      assemble more complicated geometries</p>
    </list-item>
    <list-item>
      <p>uses flexible output structures (NumPy arrays, and Python
      dictionaries) and hence makes it easy to interact with the
      stimulus arrays and store additional information (e.g., stimulus
      descriptions, stimulus masks, experimental data)</p>
    </list-item>
    <list-item>
      <p>is modular and therefore easy to extend with new stimulus
      functions, and new stimulus sets</p>
    </list-item>
    <list-item>
      <p>is hierarchical in a sense that more complex stimulus functions
      (e.g., visual illusions) use more basic stimulus functions (e.g.,
      components)</p>
    </list-item>
    <list-item>
      <p>comes with application-oriented documentation, including
      interactive Jupyter Notebooks
      (<xref alt="Kluyver et al., 2016" rid="ref-kluyver2016" ref-type="bibr">Kluyver
      et al., 2016</xref>)</p>
    </list-item>
  </list>
  <p><monospace>stimupy</monospace> is a free and open-source Python
  package which can be easily downloaded and installed via standard
  package managers, or directly from its
  <ext-link ext-link-type="uri" xlink:href="https://github.com/computational-psychology/stimupy">GitHub
  source</ext-link>. We think that using <monospace>stimupy</monospace>
  will improve the consistency and accessibility of visual stimuli while
  helping to avoid bugs. A key feature in <monospace>stimupy</monospace>
  is that its functions are parameterized with parameters that are
  relevant to vision scientists (e.g., visual angle, spatial frequency,
  target placements). Moreover, <monospace>stimupy</monospace> is
  designed in a modular fashion, i.e. more complex stimuli are composed
  of less complex stimuli, which supports the understanding of existing
  stimuli, makes connections between stimuli explicit, and facilitates
  the creation of novel stimuli. The output of all stimulus functions is
  a dictionary which contains the stimulus-image as a NumPy-array
  together with other useful stimulus information (e.g., masks, stimulus
  parameters, and experimental data). Having the stimulus-image as a
  NumPy-array makes it easy to work and interact with the stimulus,
  e.g., using common NumPy tooling and/or utility functions provided by
  <monospace>stimupy</monospace>. This is useful for manipulating the
  stimulus as well as for using the stimulus for other purposes than
  psychophysical experimentation on a computer screen (e.g., for
  visualizations or for computational modeling). The main advantage of
  using dictionaries as function outputs is that Python dictionaries are
  mutable data structures which allow you to add additional information
  easily. Moreover, the stimulus dictionaries contain all parameter
  information relevant for describing a stimulus (e.g., size,
  resolution, spatial frequency, orientation, phase, etc), and hence
  help to report stimulus parameters, e.g. in a research paper.
  <monospace>stimupy</monospace> also provides utility-functions to
  strip the stimulus dictionaries down to just the raw parameters that
  the original stimulus function expects. Hence, it is sufficient to
  share only the stimulus dictionaries to recreate exact stimulus
  replications, which makes it easy to create slight variations of this
  stimulus. Taken together, these design choices make
  <monospace>stimupy</monospace> a flexible and versatile Python package
  which facilitates the (re)creation and use of visual stimuli for a
  variety of purposes.</p>
  <p>Another important use case for <monospace>stimupy</monospace> is
  the evaluation of computational vision models. A common strategy to
  validate computational vision models is to test them with benchmark
  datasets; e.g. in spatial vision
  (<xref alt="Carney et al., 1999" rid="ref-carney1999" ref-type="bibr">Carney
  et al., 1999</xref>), lightness perception
  (<xref alt="Murray, 2021" rid="ref-murray2021" ref-type="bibr">Murray,
  2021</xref>), object recognition
  (<xref alt="Deng et al., 2009" rid="ref-deng2009" ref-type="bibr">Deng
  et al., 2009</xref>), or object segmentation
  (<xref alt="Martin et al., 2001" rid="ref-martin2001" ref-type="bibr">Martin
  et al., 2001</xref>). However, visual stimuli from prior research are
  not always publicly available and it is thus difficult and
  time-consuming to test model performance on stimuli from prior
  research. On top of that, creating and agreeing on benchmark datasets
  is a challenging task. Hence, to support the accessibility of
  previously used stimuli and encourage the creation of stimulus
  benchmarks, <monospace>stimupy</monospace> provides a collection of
  existing stimulus sets (including ModelFest) as they have been used in
  the original manuscripts. Due to <monospace>stimupy</monospace>’s
  versatility, entire stimulus sets (including experimental findings)
  can be accessed via a single line of code, and more stimulus sets can
  be added at any point in time.</p>
  <fig>
    <caption><p>Samples from a parameter space of a single
    <monospace>stimupy</monospace> stimulus function:
    <styled-content id="figU003Astimspace"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/stimspace.png" />
  </fig>
  <p><monospace>stimupy</monospace>’s high degree of parameterizability
  allows for extensive explorations of stimulus parameter spaces
  (<xref alt="[fig:stimspace]" rid="figU003Astimspace">[fig:stimspace]</xref>).
  On the one hand, this can be useful for vision experimentation because
  varying stimuli along one or multiple dimensions can be directly
  mapped to certain experimental designs and research questions. On the
  other hand, this feature can also guide theoretical work because among
  other things it allows to find so-called maximally-differentiable
  stimuli
  (<xref alt="Wang &amp; Simoncelli, 2008" rid="ref-wang2008" ref-type="bibr">Wang
  &amp; Simoncelli, 2008</xref>). The basic idea of maximum
  differentiation is to analyze model predictions for systematically
  varied stimuli to find the ones which differentiate best (maximally)
  between models. Like this, the number of stimuli tested experimentally
  can be reduced to the most relevant stimuli. Since collecting (human)
  data is resourceful, maximal differentiation is a useful method to
  reduce theoretically large stimulus parameter spaces to a testable
  number of stimuli.</p>
  <p>Last but not least, <monospace>stimupy</monospace> can be a useful
  aid in teaching contexts because it provides students with a basic
  framework in which they can design and interact with stimuli in a
  playful way. Since <monospace>stimupy</monospace> is focused on
  stimulus creation rather than stimulus presentation, a user can
  quickly generate complex and innovative stimuli – even with beginner
  knowledge of Python. The parameterized functions and the interactive
  documentation allow for easy teaching and communication of how various
  stimulus parameters affect perception.</p>
</sec>
<sec id="projects-using-the-software">
  <title>Projects Using the Software</title>
  <p>As stimulus creation is relevant for many vision science projects,
  stimulus functions which are part of <monospace>stimupy</monospace> or
  a pre-release version of the software have been used in almost all of
  the work of our laboratory within the last two years. Some of
  <monospace>stimupy</monospace>’s noise functions have been used to
  generate the narrowband noise masks of varying center frequency in
  (<xref alt="Schmittwilken &amp; Maertens, 2022b" rid="ref-schmittwilken2022b" ref-type="bibr">Schmittwilken
  &amp; Maertens, 2022b</xref>). A pre-release version was used in
  multiple conference contributions in which we compared structural
  elements between existing models of brightness perception
  (<xref alt="Vincent &amp; Maertens, 2021a" rid="ref-vincent.maertens2021a" ref-type="bibr">Vincent
  &amp; Maertens, 2021a</xref>); in which we compared existing models of
  human brightness perception on a large battery of brightness stimuli
  (<xref alt="Schmittwilken et al., 2022" rid="ref-schmittwilken2022a" ref-type="bibr">Schmittwilken
  et al., 2022</xref>); in which we evaluated how to quantitatively link
  output of computational models to human brightness perception data
  (<xref alt="Vincent &amp; Maertens, 2021b" rid="ref-vincent.maertens2021" ref-type="bibr">Vincent
  &amp; Maertens, 2021b</xref>); in which we demonstrate that a family
  of computational models fails to account for novel brightenss
  perception data
  (<xref alt="Aguilar et al., 2022" rid="ref-aguilar.maertens.ea2022" ref-type="bibr">Aguilar
  et al., 2022</xref>;
  <xref alt="Vincent et al., 2022b" rid="ref-vincent.maertens.ea2021" ref-type="bibr">Vincent
  et al., 2022b</xref>,
  <xref alt="2022a" rid="ref-vincent.maertens.ea2022" ref-type="bibr">2022a</xref>)
  and in which we studied human edge processing with Cornsweet stimuli
  in various kinds of noise (white, pink, brown, several narrowband
  noises)
  (<xref alt="Schmittwilken &amp; Maertens, 2022a" rid="ref-schmittwilken2022c" ref-type="bibr">Schmittwilken
  &amp; Maertens, 2022a</xref>). All these stimuli were created with
  <monospace>stimupy</monospace> or functions that are included in the
  software. Moreover, we are using <monospace>stimupy</monospace> in
  ongoing work in our laboratory and in many student projects.</p>
</sec>
<sec id="future-work">
  <title>Future Work</title>
  <p>In theory, there is an infinite number of stimuli which are or
  could be interesting in the future of vision research. Hence,
  <monospace>stimupy</monospace> will by default remain under active
  development. In future versions, we want to add more visual stimuli
  and more stimulus sets – particularly dynamic stimuli which are
  currently not included. Finally, we want to foster the development of
  stimulus benchmarks in vision science which will be added to
  <monospace>stimupy</monospace>.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Funded by the Deutsche Forschungsgemeinschaft (DFG, German Research
  Foundation) under Germany’s Excellence Strategy -EXC 2002/1 “Science
  of Intelligence”- project number 390523135 and individual grants MA
  5127/4-1 and 5-1 to M. Maertens</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-aguilar.maertens.ea2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Aguilar</surname><given-names>Guillermo</given-names></name>
        <name><surname>Maertens</surname><given-names>Marianne</given-names></name>
        <name><surname>Vincent</surname><given-names>Joris</given-names></name>
      </person-group>
      <article-title>Characterizing perceptual brightness scales for White’s effect using conjoint measurement</article-title>
      <source>Journal of Vision</source>
      <publisher-loc>St. Pete Beach, FL</publisher-loc>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <volume>22</volume>
      <pub-id pub-id-type="doi">10.1167/jov.22.14.3519</pub-id>
      <fpage>3519</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-brainard1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brainard</surname><given-names>D. H.</given-names></name>
      </person-group>
      <article-title>The psychophysics toolbox</article-title>
      <source>Spatial vision</source>
      <publisher-name>Netherlands Publisher: VSP</publisher-name>
      <year iso-8601-date="1997">1997</year>
      <volume>10</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id>
      <fpage>433</fpage>
      <lpage>436</lpage>
    </element-citation>
  </ref>
  <ref id="ref-carney1999">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Carney</surname><given-names>T.</given-names></name>
        <name><surname>Klein</surname><given-names>S. A.</given-names></name>
        <name><surname>Tyler</surname><given-names>C. W.</given-names></name>
        <name><surname>Silverstein</surname><given-names>A. D.</given-names></name>
        <name><surname>Beutter</surname><given-names>B.</given-names></name>
        <name><surname>Levi</surname><given-names>D.</given-names></name>
        <name><surname>Watson</surname><given-names>A. B.</given-names></name>
        <name><surname>Reeves</surname><given-names>A. J.</given-names></name>
        <name><surname>Norcia</surname><given-names>A. M.</given-names></name>
        <name><surname>Chen</surname><given-names>C.-C.</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Development of an image/threshold database for designing and testing human vision models</article-title>
      <source>Human vision and electronic imaging IV</source>
      <publisher-name>SPIE</publisher-name>
      <year iso-8601-date="1999">1999</year>
      <volume>3644</volume>
      <pub-id pub-id-type="doi">10.1117/12.348473</pub-id>
      <fpage>542</fpage>
      <lpage>551</lpage>
    </element-citation>
  </ref>
  <ref id="ref-deng2009">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Deng</surname><given-names>J.</given-names></name>
        <name><surname>Dong</surname><given-names>W.</given-names></name>
        <name><surname>Socher</surname><given-names>R.</given-names></name>
        <name><surname>Li</surname><given-names>L.-J.</given-names></name>
        <name><surname>Li</surname><given-names>K.</given-names></name>
        <name><surname>Fei-Fei</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>Imagenet: A large-scale hierarchical image database</article-title>
      <source>IEEE conference on computer vision and pattern recognition</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id>
      <fpage>248</fpage>
      <lpage>255</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. Jarrod</given-names></name>
        <name><surname>Walt</surname><given-names>Stéfan J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J.</given-names></name>
        <name><surname>Kern</surname><given-names>Robert</given-names></name>
        <name><surname>Picus</surname><given-names>Matti</given-names></name>
        <name><surname>Hoyer</surname><given-names>Stephan</given-names></name>
        <name><surname>Kerkwijk</surname><given-names>Marten H. van</given-names></name>
        <name><surname>Brett</surname><given-names>Matthew</given-names></name>
        <name><surname>Haldane</surname><given-names>Allan</given-names></name>
        <name><surname>Río</surname><given-names>Jaime Fernández del</given-names></name>
        <name><surname>Wiebe</surname><given-names>Mark</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Gérard-Marchant</surname><given-names>Pierre</given-names></name>
        <name><surname>Sheppard</surname><given-names>Kevin</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Abbasi</surname><given-names>Hameer</given-names></name>
        <name><surname>Gohlke</surname><given-names>Christoph</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Springer Science; Business Media LLC</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kluyver2016">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kluyver</surname><given-names>Thomas</given-names></name>
        <name><surname>Ragan-Kelley</surname><given-names>Benjamin</given-names></name>
        <name><surname>Pérez</surname><given-names>Fernando</given-names></name>
        <name><surname>Granger</surname><given-names>Brian</given-names></name>
        <name><surname>Bussonnier</surname><given-names>Matthias</given-names></name>
        <name><surname>Frederic</surname><given-names>Jonathan</given-names></name>
        <name><surname>Kelley</surname><given-names>Kyle</given-names></name>
        <name><surname>Hamrick</surname><given-names>Jessica</given-names></name>
        <name><surname>Grout</surname><given-names>Jason</given-names></name>
        <name><surname>Corlay</surname><given-names>Sylvain</given-names></name>
        <name><surname>Ivanov</surname><given-names>Paul</given-names></name>
        <name><surname>Avila</surname><given-names>Damián</given-names></name>
        <name><surname>Abdalla</surname><given-names>Safia</given-names></name>
        <name><surname>Willing</surname><given-names>Carol</given-names></name>
      </person-group>
      <article-title>Jupyter notebooks – a publishing format for reproducible computational workflows</article-title>
      <source>Positioning and power in academic publishing: Players, agents and agendas</source>
      <person-group person-group-type="editor">
        <name><surname>Loizides</surname><given-names>F.</given-names></name>
        <name><surname>Schmidt</surname><given-names>B.</given-names></name>
      </person-group>
      <publisher-name>IOS Press</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.3233/978-1-61499-649-1-87</pub-id>
      <fpage>87 </fpage>
      <lpage> 90</lpage>
    </element-citation>
  </ref>
  <ref id="ref-OCTA">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Van Geert</surname><given-names>E.</given-names></name>
        <name><surname>Bossens</surname><given-names>C.</given-names></name>
        <name><surname>Wagemans</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>The order &amp; complexity toolbox for aesthetics (OCTA): A systematic approach to study the relations between order, complexity, and aesthetic appreciation</article-title>
      <source>Behavior Research Methods</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.3758/s13428-022-01900-w</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-makowski2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Makowski</surname><given-names>D.</given-names></name>
        <name><surname>Lau</surname><given-names>Z. J.</given-names></name>
        <name><surname>Pham</surname><given-names>T.</given-names></name>
        <name><surname>Paul B.</surname><given-names>W.</given-names></name>
        <name><surname>Annabel C.</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>A parametric framework to generate visual illusions using python</article-title>
      <source>Perception</source>
      <publisher-name>Sage Publications Sage UK: London, England</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>50</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1177/03010066211057347</pub-id>
      <fpage>950</fpage>
      <lpage>965</lpage>
    </element-citation>
  </ref>
  <ref id="ref-martin2001">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Martin</surname><given-names>D.</given-names></name>
        <name><surname>Fowlkes</surname><given-names>C.</given-names></name>
        <name><surname>Tal</surname><given-names>D.</given-names></name>
        <name><surname>Malik</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</article-title>
      <source>8th IEEE internatinonal conference on computer vision</source>
      <year iso-8601-date="2001-07">2001</year><month>07</month>
      <volume>2</volume>
      <pub-id pub-id-type="doi">10.1109/ICCV.2001.937655</pub-id>
      <fpage>416</fpage>
      <lpage>423</lpage>
    </element-citation>
  </ref>
  <ref id="ref-murray2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Murray</surname><given-names>R. F.</given-names></name>
      </person-group>
      <article-title>A model of lightness perception guided by probabilistic assumptions about lighting and reflectance</article-title>
      <source>Journal of Vision</source>
      <year iso-8601-date="2020-07">2020</year><month>07</month>
      <volume>20</volume>
      <issue>7</issue>
      <issn>1534-7362</issn>
      <pub-id pub-id-type="doi">10.1167/jov.20.7.28</pub-id>
      <fpage>28</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-murray2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Murray</surname><given-names>R. F.</given-names></name>
      </person-group>
      <article-title>Lightness perception in complex scenes</article-title>
      <source>Annual Review of Vision Science</source>
      <publisher-name>Annual Reviews</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>7</volume>
      <pub-id pub-id-type="doi">10.1146/annurev-vision-093019-115159</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-peirce2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Peirce</surname><given-names>J.</given-names></name>
        <name><surname>Gray</surname><given-names>J. R.</given-names></name>
        <name><surname>Simpson</surname><given-names>S.</given-names></name>
        <name><surname>MacAskill</surname><given-names>M.</given-names></name>
        <name><surname>Hoechenberger</surname><given-names>R.</given-names></name>
        <name><surname>Sogo</surname><given-names>H.</given-names></name>
        <name><surname>Kastman</surname><given-names>E.</given-names></name>
        <name><surname>Lindelov</surname><given-names>J. K.</given-names></name>
      </person-group>
      <article-title>PsychoPy2: Experiments in behavior made easy</article-title>
      <source>Behavior research methods</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>51</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>
      <fpage>195</fpage>
      <lpage>203</lpage>
    </element-citation>
  </ref>
  <ref id="ref-schmittwilken2022a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schmittwilken</surname><given-names>L.</given-names></name>
        <name><surname>Matic</surname><given-names>M.</given-names></name>
        <name><surname>Maertens</surname><given-names>M.</given-names></name>
        <name><surname>Vincent</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>BRENCH: An open-source framework for b(r)enchmarking brightness models</article-title>
      <source>Journal of vision</source>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2022-02">2022</year><month>02</month>
      <volume>22</volume>
      <pub-id pub-id-type="doi">10.1167/jov.22.3.36</pub-id>
      <fpage>36</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-schmittwilken2022b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schmittwilken</surname><given-names>L.</given-names></name>
        <name><surname>Maertens</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Fixational eye movements enable robust edge detection</article-title>
      <source>Journal of Vision</source>
      <year iso-8601-date="2022">2022</year>
      <volume>22</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.1167/jov.22.8.5</pub-id>
      <fpage>1</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-schmittwilken2022c">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Schmittwilken</surname><given-names>L.</given-names></name>
        <name><surname>Maertens</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Medium spatial frequencies mask edges most effectively</article-title>
      <source>Journal of vision</source>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <volume>22</volume>
      <pub-id pub-id-type="doi">10.1167/jov.22.14.4041</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-vincent.maertens.ea2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vincent</surname><given-names>Joris</given-names></name>
        <name><surname>Maertens</surname><given-names>Marianne</given-names></name>
        <name><surname>Aguilar</surname><given-names>Guillermo</given-names></name>
      </person-group>
      <article-title>Perceptual brightness scales in a White’s effect stimulus are not captured by multiscale spatial filtering models of brightness perception</article-title>
      <source>Journal of Vision</source>
      <year iso-8601-date="2022-02">2022</year><month>02</month>
      <volume>22</volume>
      <pub-id pub-id-type="doi">10.1167/jov.22.3.20</pub-id>
      <fpage>20</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vincent.maertens.ea2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vincent</surname><given-names>Joris</given-names></name>
        <name><surname>Maertens</surname><given-names>Marianne</given-names></name>
        <name><surname>Aguilar</surname><given-names>Guillermo</given-names></name>
      </person-group>
      <article-title>Perceptual Brightness Scales for White’s Effect Constrain Computational Models of Brightness Perception</article-title>
      <source>Journal of Vision</source>
      <publisher-loc>St. Pete Beach, FL</publisher-loc>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <volume>22</volume>
      <pub-id pub-id-type="doi">10.1167/jov.22.14.4160</pub-id>
      <fpage>4160</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-vincent.maertens2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Vincent</surname><given-names>Joris</given-names></name>
        <name><surname>Maertens</surname><given-names>Marianne</given-names></name>
      </person-group>
      <article-title>The missing linking functions in computational models of brightness perception</article-title>
      <publisher-name>OSF</publisher-name>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021">2021</year>
      <uri>osf.io/9bca7</uri>
    </element-citation>
  </ref>
  <ref id="ref-vincent.maertens2021a">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Vincent</surname><given-names>Joris</given-names></name>
        <name><surname>Maertens</surname><given-names>Marianne</given-names></name>
      </person-group>
      <article-title>A history and modular future of multiscale spatial filtering models</article-title>
      <source>Journal of Vision</source>
      <publisher-loc>Online</publisher-loc>
      <year iso-8601-date="2021-05">2021</year><month>05</month>
      <volume>21</volume>
      <pub-id pub-id-type="doi">10.1167/jov.21.9.2824</pub-id>
      <fpage>2824</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Z.</given-names></name>
        <name><surname>Simoncelli</surname><given-names>E. P.</given-names></name>
      </person-group>
      <article-title>Maximum differentiation (MAD) competition: A methodology for comparing computational models of perceptual quantities</article-title>
      <source>Journal of Vision</source>
      <publisher-name>The Association for Research in Vision and Ophthalmology</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>8</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1167/8.12.8</pub-id>
      <fpage>8</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
