<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4515</article-id>
<article-id pub-id-type="doi">10.21105/joss.04515</article-id>
<title-group>
<article-title>pyndl: Naïve Discriminative Learning in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Sering</surname>
<given-names>Konstantin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Weitz</surname>
<given-names>Marc</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Shafaei-Bajestan</surname>
<given-names>Elnaz</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Künstle</surname>
<given-names>David-Elias</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Tübingen</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>UiT The Arctic University of Norway</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>International Max Planck Research School for Intelligent
Systems</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-03-31">
<day>31</day>
<month>3</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>80</issue>
<fpage>4515</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Delta-rule</kwd>
<kwd>Rescorla-Wagner</kwd>
<kwd>NDL</kwd>
<kwd>Naïve Discrimination Learning</kwd>
<kwd>Language model</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The <italic>pyndl</italic> package implements Naïve Discriminative
  Learning (NDL) in Python. NDL is an incremental learning algorithm
  grounded in the principles of discrimination learning
  (<xref alt="Rescorla &amp; Wagner, 1972" rid="ref-rescorla1972theory" ref-type="bibr">Rescorla
  &amp; Wagner, 1972</xref>;
  <xref alt="Widrow &amp; Hoff, 1960" rid="ref-widrow1960adaptive" ref-type="bibr">Widrow
  &amp; Hoff, 1960</xref>) and motivated by animal and human learning
  research (e.g.
  <xref alt="Baayen et al., 2011" rid="ref-Baayen_2011" ref-type="bibr">Baayen
  et al., 2011</xref>;
  <xref alt="Rescorla, 1988" rid="ref-Rescorla1988PavlovianCI" ref-type="bibr">Rescorla,
  1988</xref>). Lately, NDL has become a popular tool in language
  research to examine large corpora and vocabularies, with 750,000
  spoken word tokens
  (<xref alt="Shafaei-Bajestan et al., 2022" rid="ref-Shafaei_2022" ref-type="bibr">Shafaei-Bajestan
  et al., 2022</xref>) and a vocabulary size of 52,402 word types
  (<xref alt="Sering et al., 2018" rid="ref-Sering_2018" ref-type="bibr">Sering
  et al., 2018</xref>). In contrast to previous implementations,
  <italic>pyndl</italic> allows for a broader range of analysis,
  including non-English languages, adds further learning rules and
  provides better maintainability while having the same fast processing
  speed. As of today, it supports multiple research groups in their work
  and led to several scientific publications.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Naïve Discriminative Learning (NDL) is a computational modelling
  framework that phrases language comprehension as a multiple label
  classification problem
  (<xref alt="Baayen et al., 2011" rid="ref-Baayen_2011" ref-type="bibr">Baayen
  et al., 2011</xref>;
  <xref alt="Sering et al., 2018" rid="ref-Sering_2018" ref-type="bibr">Sering
  et al., 2018</xref>). It is grounded in the simple but powerful
  principles of discrimination learning implemented on a 2-layer
  symbolic network combined with the Rescorla-Wagner learning rule
  (<xref alt="Rescorla &amp; Wagner, 1972" rid="ref-rescorla1972theory" ref-type="bibr">Rescorla
  &amp; Wagner, 1972</xref>). This learning rule explains phenomena,
  where animals associate co-occurring cues and outcomes, e.g. a
  flashing light and food, only if the cue is effective in predicting
  the outcome (see
  <xref alt="Baayen &amp; Ramscar, 2015" rid="ref-Baayen_2015" ref-type="bibr">Baayen
  &amp; Ramscar, 2015</xref>, for an introduction). In linguistics, NDL
  models are trained on large corpora to investigate the language’s
  structure and to provide insights into the learning process of life
  long language acquisition (please find an extensive list of examples
  in
  <xref alt="Baayen &amp; Ramscar, 2015, sec. 5" rid="ref-Baayen_2015" ref-type="bibr">Baayen
  &amp; Ramscar, 2015, sec. 5</xref>).</p>
  <p>Several implementations of NDL have been created over time but are
  struggling with modern challenges of linguistic research like
  multi-language support, increasing model sizes, and open science
  principles. The first implementation was the R package
  <italic>ndl</italic>
  (<xref alt="Arppe et al., 2018" rid="ref-ndl" ref-type="bibr">Arppe et
  al., 2018</xref>), which could solve the Danks equilibrium equations
  (<xref alt="Danks, 2003" rid="ref-Danks_2003" ref-type="bibr">Danks,
  2003</xref>), but did not provide an exact iterative solver. An
  iterative solver was added to the R package <italic>ndl2</italic>
  (<xref alt="Shaoul et al., 2014" rid="ref-ndl2" ref-type="bibr">Shaoul
  et al., 2014</xref>). <italic>ndl</italic> and <italic>ndl2</italic>
  made efficient implementations to learning algorithms available to
  language researchers.</p>
  <p>However, the code of <italic>ndl2</italic> was only available upon
  request until end of the year 2022. One reason for this has been that
  it only runs on Linux and CRAN’s guidelines make it difficult to
  publish single platform packages
  (<xref alt="R Core Team, 2022" rid="ref-R_project" ref-type="bibr">R
  Core Team, 2022</xref>). Another reason is the limited maintainability
  through low level C and C++ code next to high level R code. A severe
  limitation of <italic>ndl</italic> and <italic>ndl2</italic> is that
  both packages have difficulties with non-ASCII input, causing problems
  in the analysis of non-English text corpora due to special characters
  or non-Latin alphabets. An example would be the processing of Arabic
  or Slavic languages; even German umlauts are inconvenient to use in
  <italic>ndl</italic> and <italic>ndl2</italic>. Furthermore, in
  <italic>ndl2</italic>, it is impossible to conveniently access huge
  weight matrices due to a size limitation of arrays in the R
  programming language
  (<xref alt="R Core Team, 2022" rid="ref-R_project" ref-type="bibr">R
  Core Team, 2022</xref>). This limit does not allow for more than
  46,340 word types in a word-type to word-type model, which is too
  small to capture the full lexicon in most languages.</p>
</sec>
<sec id="implementation-and-use-in-research">
  <title>Implementation and use in research</title>
  <p><italic>pyndl</italic> reimplements the learning rule of NDL mainly
  in Python with small code chunks outsourced to Cython to speed up the
  processing. This allows the processing of UTF-8 encoded corpora
  enabling the analysis of many non-European languages and alphabets
  (e.g. Mandarin or Cyrillic,
  <xref alt="Milin et al., 2020" rid="ref-milin2020keeping" ref-type="bibr">Milin
  et al., 2020</xref>). Using the Python ecosystem, the size of weight
  matrices in <italic>pyndl</italic> is only limited by the memory
  available. Computed weights using the <italic>xarray</italic> format
  (<xref alt="Hoyer &amp; Hamman, 2017" rid="ref-hoyer2017xarray" ref-type="bibr">Hoyer
  &amp; Hamman, 2017</xref>) can be easily integrated into down-stream
  tasks like analyzing the association strength between grapheme
  clusters a word types.</p>
  <p>The input to <italic>pyndl</italic> is agnostic to the actual
  domain as long as it is tokenized as Unicode character strings. Input
  sequences can consist of multiple tokens separated by underscores
  which is together with the tab-character the only special character in
  <italic>pyndl</italic>. While <italic>pyndl</italic> provides some
  basic preprocessing for grapheme tokenization, the preprocessing of
  ideograms, pictograms, logograms, and speech is possible using custom
  preprocessing. For example, word classification using tokenized speech
  audio recordings was investigated in Arnold et al.
  (<xref alt="2017" rid="ref-Arnold_2017" ref-type="bibr">2017</xref>).
  Inputs in this work consisted of around 50 tokens per time slice,
  where each token encoded the pitch, loudness, and variability into a
  string.</p>
  <p>The input format is based on previous implementations of NDL. In
  contrast to previous implementations, <italic>pyndl</italic> was
  open-source software from the beginning and developed with usability
  and maintainability in mind. The better maintainability of
  <italic>pyndl</italic> does not come at the cost of performance: The
  benchmark results in
  <xref alt="Figure 1" rid="figU003Abenchmark">Figure 1</xref>,
  described in detail in our package’s documentation, shows that
  <italic>pyndl</italic> is faster than <italic>ndl</italic> and
  <italic>ndl2</italic>. Memory is used efficiently by storing data
  records in compressed form and loading data points as they are needed
  during learning. <italic>pyndl</italic> provides the same core
  functionality as the previous R packages in Python. After
  installation, <italic>pyndl</italic> can be called from R or Julia
  scripts by convenient bridges, like any Python library. An example on
  how to use <italic>pyndl</italic> from R can be found in our
  documentation.</p>
  <fig>
    <caption><p>Execution wall time for different implementations of the
    Rescorla-Wagner learning rule for different number of learning
    events. The mean and standard-error (<inline-formula><alternatives>
    <tex-math><![CDATA[n]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>=10)
    for the wall time show that our implementation,
    <italic>pyndl</italic>, is the fastest for larger numbers of
    events.<styled-content id="figU003Abenchmark"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/benchmark_result.png" xlink:title="" />
  </fig>
  <p>The improved maintainability of <italic>pyndl</italic> also allows
  for an easier addition of new features. For example, the NDL learner
  was extended to a learner for continuous inputs as cues, outcomes or
  both. When both cues and outcomes are continuous, the Rescorla-Wagner
  learning rule changes to the Widrow-Hoff learning rule. This extension
  is added by keeping the API to the learner comparable to NDL and
  computationally exploiting the structure of the multi-hot encoded
  features in the symbolic representation of language.</p>
  <p><italic>pyndl</italic> is used by several research groups to
  analyse language data and is regularly used in scholarly work. These
  works use <italic>pyndl</italic> for models in a wide range of
  linguistic subfields and explicitly use the easy extensibility and
  UTF-8 support of <italic>pyndl</italic>. Tomaschek et al.
  (<xref alt="2019" rid="ref-TOMASCHEK_2019" ref-type="bibr">2019</xref>)
  and Baayen &amp; Smolka
  (<xref alt="2020" rid="ref-Baayen_2020" ref-type="bibr">2020</xref>)
  investigate morphological effects of the context of German and English
  languages, while Shafaei-Bajestan &amp; Baayen
  (<xref alt="2018" rid="ref-Shafaei_Bajestan_2018" ref-type="bibr">2018</xref>)
  and Sering et al.
  (<xref alt="2018" rid="ref-Sering_2018" ref-type="bibr">2018</xref>)
  show auditory comprehension based on simple acoustic features, and
  Romain et al.
  (<xref alt="2022" rid="ref-Romain_2022" ref-type="bibr">2022</xref>)
  model the learning of tenses. Milin et al.
  (<xref alt="2020" rid="ref-milin2020keeping" ref-type="bibr">2020</xref>)
  profited from <italic>pyndl</italic>’s UTF-8 support when using
  Cyrillic cues to show different language phenomena with the simple
  Widrow-Hoff learning rule as a special case of the Rescorla-Wagner.
  Tomaschek &amp; Duran
  (<xref alt="2019" rid="ref-TomaschekU003ADuranU003A2019" ref-type="bibr">2019</xref>)
  used <italic>pyndl</italic> to model the McGurk effect across
  different languages. Shafaei-Bajestan et al.
  (<xref alt="2021" rid="ref-Shafaei_Bajestan_2021" ref-type="bibr">2021</xref>)
  presented a linearized version of the Rescorla-Wagner rule that they
  could add to <italic>pyndl</italic> and compare with the classic
  version. Divjak et al.
  (<xref alt="2020" rid="ref-Divjak_2020" ref-type="bibr">2020</xref>)
  showed the benefits of learning language models over probabilistic and
  rule-based models.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors thank R. Harald Baayen for his support in creating and
  maintaining <italic>pyndl</italic> as a scientific software package in
  the Python ecosystem. Furthermore, the authors thank Lennart Schneider
  for his major contributions, as well as all
  <ext-link ext-link-type="uri" xlink:href="https://github.com/quantling/pyndl/graphs/contributors">other
  contributors on GitHub</ext-link>.</p>
  <p>This research was supported by an ERC advanced Grant (no. 742545)
  and by the Alexander von Humboldt Professorship awarded to R. Harald
  Baayen and by the University of Tübingen.</p>
  <p>The authors thank the International Max Planck Research School for
  Intelligent Systems (IMPRS-IS) for supporting David-Elias Künstle,
  funded by the Deutsche Forschungsgemeinschaft (DFG, German Research
  Foundation) under Germany’s Excellence Strategy – EXC number 2064/1 –
  Project number 390727645 and the High North Population Studies at UiT
  The Arctic University of Norway for supporting Marc Weitz.</p>
  <p>Finally, this paper and the associated package benefited from
  constructive and conscientious peer review. We would like to thank our
  three reviewers, Venktesh V, Jinhang Jiang, and especially Frankie
  Robertson, for their constructive and in-depth feedback and their
  suggestions on how to make the package more user-friendly.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-hoyer2017xarray">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hoyer</surname><given-names>S.</given-names></name>
        <name><surname>Hamman</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Xarray: N-D labeled arrays and datasets in Python</article-title>
      <source>Journal of Open Research Software</source>
      <publisher-name>Ubiquity Press</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <volume>5</volume>
      <issue>1</issue>
      <uri>
      https://doi.org/10.5334/jors.148
      </uri>
      <pub-id pub-id-type="doi">10.5334/jors.148</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-rescorla1972theory">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Rescorla</surname><given-names>Robert A</given-names></name>
        <name><surname>Wagner</surname><given-names>Allan R</given-names></name>
      </person-group>
      <article-title>A theory of pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement</article-title>
      <source>Classical conditioning: Current research and theory</source>
      <year iso-8601-date="1972">1972</year>
    </element-citation>
  </ref>
  <ref id="ref-widrow1960adaptive">
    <element-citation publication-type="report">
      <person-group person-group-type="author">
        <name><surname>Widrow</surname><given-names>Bernard</given-names></name>
        <name><surname>Hoff</surname><given-names>Marcian E</given-names></name>
      </person-group>
      <article-title>Adaptive switching circuits</article-title>
      <publisher-name>Stanford Univ Ca Stanford Electronics Labs</publisher-name>
      <year iso-8601-date="1960">1960</year>
      <pub-id pub-id-type="doi">10.21236/ad0241531</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Danks_2003">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Danks</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Equilibria of the rescorlawagner model</article-title>
      <source>Journal of Mathematical Psychology</source>
      <publisher-name>Elsevier BV</publisher-name>
      <year iso-8601-date="2003-04">2003</year><month>04</month>
      <volume>47</volume>
      <issue>2</issue>
      <uri>https://doi.org/10.1016%2Fs0022-2496%2802%2900016-0</uri>
      <pub-id pub-id-type="doi">10.1016/s0022-2496(02)00016-0</pub-id>
      <fpage>109</fpage>
      <lpage>121</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Baayen_2011">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Đurđević</surname><given-names>Dušica Filipović</given-names></name>
        <name><surname>Hendrix</surname><given-names>Peter</given-names></name>
        <name><surname>Marelli</surname><given-names>Marco</given-names></name>
      </person-group>
      <article-title>An amorphous model for morphological processing in visual comprehension based on naive discriminative learning.</article-title>
      <source>Psychological Review</source>
      <publisher-name>American Psychological Association (APA)</publisher-name>
      <year iso-8601-date="2011-07">2011</year><month>07</month>
      <volume>118</volume>
      <issue>3</issue>
      <uri>https://doi.org/10.1037%2Fa0023851</uri>
      <pub-id pub-id-type="doi">10.1037/a0023851</pub-id>
      <fpage>438</fpage>
      <lpage>481</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ndl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arppe</surname><given-names>Antti</given-names></name>
        <name><surname>Hendrix</surname><given-names>Peter</given-names></name>
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Baayen</surname><given-names>R Harald</given-names></name>
        <name><surname>Sering</surname><given-names>Tino</given-names></name>
        <name><surname>Shaoul</surname><given-names>Cyrus</given-names></name>
      </person-group>
      <article-title>Package ‘ndl’</article-title>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-ndl2">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shaoul</surname><given-names>Cyrus</given-names></name>
        <name><surname>Schilling</surname><given-names>N</given-names></name>
        <name><surname>Bitschnau</surname><given-names>S</given-names></name>
        <name><surname>Arppe</surname><given-names>Antti</given-names></name>
        <name><surname>Hendrix</surname><given-names>Peter</given-names></name>
        <name><surname>Baayen</surname><given-names>R Harald</given-names></name>
      </person-group>
      <article-title>Ndl2: Naı̈ve discriminative learning</article-title>
      <source>R package version</source>
      <year iso-8601-date="2014">2014</year>
      <volume>1</volume>
      <uri>https://github.com/quantling/ndl2</uri>
    </element-citation>
  </ref>
  <ref id="ref-Shafaei_Bajestan_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shafaei-Bajestan</surname><given-names>Elnaz</given-names></name>
        <name><surname>Moradipour-Tari</surname><given-names>Masoumeh</given-names></name>
        <name><surname>Uhrig</surname><given-names>Peter</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>LDL-AURIS: A computational model, grounded in error-driven learning, for the comprehension of single spoken words</article-title>
      <source>Language, Cognition and Neuroscience</source>
      <publisher-name>Informa UK Limited</publisher-name>
      <year iso-8601-date="2021-07">2021</year><month>07</month>
      <uri>https://doi.org/10.1080%2F23273798.2021.1954207</uri>
      <pub-id pub-id-type="doi">10.1080/23273798.2021.1954207</pub-id>
      <fpage>1</fpage>
      <lpage>28</lpage>
    </element-citation>
  </ref>
  <ref id="ref-TOMASCHEK_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tomaschek</surname><given-names>Fabian</given-names></name>
        <name><surname>Plag</surname><given-names>Ingo</given-names></name>
        <name><surname>Ernestus</surname><given-names>Mirjam</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>Phonetic effects of morphology and context: Modeling the duration of word-final s in english with naïve discriminative learning</article-title>
      <source>Journal of Linguistics</source>
      <publisher-name>Cambridge University Press (CUP)</publisher-name>
      <year iso-8601-date="2019-06">2019</year><month>06</month>
      <volume>57</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1017%2Fs0022226719000203</uri>
      <pub-id pub-id-type="doi">10.1017/s0022226719000203</pub-id>
      <fpage>123</fpage>
      <lpage>161</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Baayen_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
        <name><surname>Smolka</surname><given-names>Eva</given-names></name>
      </person-group>
      <article-title>Modeling morphological priming in german with naive discriminative learning</article-title>
      <source>Frontiers in Communication</source>
      <publisher-name>Frontiers Media SA</publisher-name>
      <year iso-8601-date="2020-04">2020</year><month>04</month>
      <volume>5</volume>
      <uri>https://doi.org/10.3389%2Ffcomm.2020.00017</uri>
      <pub-id pub-id-type="doi">10.3389/fcomm.2020.00017</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Shafaei_Bajestan_2018">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Shafaei-Bajestan</surname><given-names>Elnaz</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>Wide learning for auditory comprehension</article-title>
      <source>Interspeech 2018</source>
      <publisher-name>ISCA</publisher-name>
      <year iso-8601-date="2018-09">2018</year><month>09</month>
      <uri>https://doi.org/10.21437%2Finterspeech.2018-2420</uri>
      <pub-id pub-id-type="doi">10.21437/interspeech.2018-2420</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Sering_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sering</surname><given-names>Konstantin</given-names></name>
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>Language comprehension as a multi-label classification problem</article-title>
      <source>Statistica Neerlandica</source>
      <publisher-name>Wiley</publisher-name>
      <year iso-8601-date="2018-04">2018</year><month>04</month>
      <volume>72</volume>
      <issue>3</issue>
      <uri>https://doi.org/10.1111%2Fstan.12134</uri>
      <pub-id pub-id-type="doi">10.1111/stan.12134</pub-id>
      <fpage>339</fpage>
      <lpage>353</lpage>
    </element-citation>
  </ref>
  <ref id="ref-milin2020keeping">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Madabushi</surname><given-names>Harish Tayyar</given-names></name>
        <name><surname>Croucher</surname><given-names>Michael</given-names></name>
        <name><surname>Divjak</surname><given-names>Dagmar</given-names></name>
      </person-group>
      <article-title>Keeping it simple: Implementation and performance of the proto-principle of adaptation and learning in the language sciences</article-title>
      <source>arXiv preprint arXiv:2003.03813</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-Divjak_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Divjak</surname><given-names>Dagmar</given-names></name>
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Ez-zizi</surname><given-names>Adnane</given-names></name>
        <name><surname>Józefowski</surname><given-names>Jarosław</given-names></name>
        <name><surname>Adam</surname><given-names>Christian</given-names></name>
      </person-group>
      <article-title>What is learned from exposure: An error-driven approach to productivity in language</article-title>
      <source>Language, Cognition and Neuroscience</source>
      <publisher-name>Informa UK Limited</publisher-name>
      <year iso-8601-date="2020-09">2020</year><month>09</month>
      <volume>36</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1080%2F23273798.2020.1815813</uri>
      <pub-id pub-id-type="doi">10.1080/23273798.2020.1815813</pub-id>
      <fpage>60</fpage>
      <lpage>83</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Romain_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Romain</surname><given-names>Laurence</given-names></name>
        <name><surname>Ez-zizi</surname><given-names>Adnane</given-names></name>
        <name><surname>Milin</surname><given-names>Petar</given-names></name>
        <name><surname>Divjak</surname><given-names>Dagmar</given-names></name>
      </person-group>
      <article-title>What makes the past perfect and the future progressive? Experiential coordinates for a learnable, context-based model of tense and aspect</article-title>
      <source>Cognitive Linguistics</source>
      <publisher-name>Walter de Gruyter GmbH</publisher-name>
      <year iso-8601-date="2022-01">2022</year><month>01</month>
      <volume>0</volume>
      <issue>0</issue>
      <uri>https://doi.org/10.1515%2Fcog-2021-0006</uri>
      <pub-id pub-id-type="doi">10.1515/cog-2021-0006</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Baayen_2015">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
        <name><surname>Ramscar</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>5. Abstraction, storage and naive discriminative learning</article-title>
      <source>Handbook of cognitive linguistics</source>
      <publisher-name>DE GRUYTER</publisher-name>
      <year iso-8601-date="2015-12">2015</year><month>12</month>
      <uri>https://doi.org/10.1515%2F9783110292022-006</uri>
      <pub-id pub-id-type="doi">10.1515/9783110292022-006</pub-id>
      <fpage>100</fpage>
      <lpage>120</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Arnold_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Arnold</surname><given-names>Denis</given-names></name>
        <name><surname>Tomaschek</surname><given-names>Fabian</given-names></name>
        <name><surname>Sering</surname><given-names>Konstantin</given-names></name>
        <name><surname>Lopez</surname><given-names>Florence</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>Words from spontaneous conversational speech can be recognized with human-like accuracy by an error-driven learning algorithm that discriminates between meanings straight from smart acoustic features, bypassing the phoneme as recognition unit</article-title>
      <source>PLOS ONE</source>
      <person-group person-group-type="editor">
        <name><surname>Rijn</surname><given-names>Hedderik van</given-names></name>
      </person-group>
      <publisher-name>Public Library of Science (PLoS)</publisher-name>
      <year iso-8601-date="2017-04">2017</year><month>04</month>
      <volume>12</volume>
      <issue>4</issue>
      <uri>
      https://doi.org/10.1371%2Fjournal.pone.0174623
      </uri>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0174623</pub-id>
      <fpage>e0174623</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Rescorla1988PavlovianCI">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rescorla</surname><given-names>Robert A</given-names></name>
      </person-group>
      <article-title>Pavlovian conditioning: It’s not what you think it is.</article-title>
      <source>The American psychologist</source>
      <publisher-name>American Psychological Association</publisher-name>
      <year iso-8601-date="1988">1988</year>
      <volume>43</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1037/0003-066x.43.3.151</pub-id>
      <fpage>151</fpage>
      <lpage>60</lpage>
    </element-citation>
  </ref>
  <ref id="ref-TomaschekU003ADuranU003A2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tomaschek</surname><given-names>Fabian</given-names></name>
        <name><surname>Duran</surname><given-names>Daniel</given-names></name>
      </person-group>
      <article-title>Modelling multi-modal integration–the case of the McGurk effect</article-title>
      <publisher-name>PsyArXiv</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.31234/osf.io/prvzq</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-R_project">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <string-name>R Core Team</string-name>
      </person-group>
      <source>R: A language and environment for statistical computing</source>
      <publisher-name>R Foundation for Statistical Computing</publisher-name>
      <publisher-loc>Vienna, Austria</publisher-loc>
      <year iso-8601-date="2022">2022</year>
      <uri>https://www.R-project.org</uri>
    </element-citation>
  </ref>
  <ref id="ref-Shafaei_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shafaei-Bajestan</surname><given-names>Elnaz</given-names></name>
        <name><surname>Uhrig</surname><given-names>Peter</given-names></name>
        <name><surname>Baayen</surname><given-names>R. Harald</given-names></name>
      </person-group>
      <article-title>Making sense of spoken plurals</article-title>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
