<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5025</article-id>
<article-id pub-id-type="doi">10.21105/joss.05025</article-id>
<title-group>
<article-title>GNS: A generalizable Graph Neural Network-based simulator
for particulate and fluid modeling</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2144-5562</contrib-id>
<name>
<surname>Kumar</surname>
<given-names>Krishna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1601-3354</contrib-id>
<name>
<surname>Vantassel</surname>
<given-names>Joseph</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Assistant Professor, University of Texas at Austin, Texas,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Assistant Professor, Virginia Tech, Virginia,
USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Texas Advanced Computing Center, University of Texas at
Austin, Texas, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-10-16">
<day>16</day>
<month>10</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>88</issue>
<fpage>5025</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>machine learning</kwd>
<kwd>simulation</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Graph Network-based Simulator (GNS) is a framework for developing
  generalizable, efficient, and accurate machine learning (ML)-based
  surrogate models for particulate and fluid systems using Graph Neural
  Networks (GNNs). GNNs are the state-of-the-art geometric deep learning
  (GDL) that operates on graphs to represent rich relational information
  (<xref alt="Scarselli et al., 2008" rid="ref-scarselli2008graph" ref-type="bibr">Scarselli
  et al., 2008</xref>), which maps an input graph to an output graph
  with the same structure but potentially different node, edge, and
  global feature attributes. The graph network in GNS spans the physical
  domain with nodes representing an individual or a collection of
  particles, and the edges connecting the vertices representing the
  local interaction between particles or clusters of particles. The GNS
  computes the system dynamics via learned message passing.
  <xref alt="[fig:gns]" rid="figU003Agns">[fig:gns]</xref> shows an
  overview of how GNS learns to simulate n-body dynamics. The GNS has
  three components: (a) Encoder, which embeds particle information to a
  latent graph, the edges represent learned functions; (b) Processor,
  which allows data propagation and computes the nodal interactions
  across steps; and (c) Decoder, which extracts the relevant dynamics
  (e.g., particle acceleration) from the graph. The GNS learns the
  dynamics, such as momentum and energy exchange, through a form of
  messages passing
  (<xref alt="Gilmer et al., 2017" rid="ref-gilmer2017neural" ref-type="bibr">Gilmer
  et al., 2017</xref>), where latent information propagates between
  nodes via the graph edges. The GNS edge messages
  (<inline-formula><alternatives>
  <tex-math><![CDATA[e^\prime_k \leftarrow \phi^e(e_k, v_{r_k}, v_{s_k}, u)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mi>e</mml:mi><mml:mi>k</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:msubsup><mml:mo>‚Üê</mml:mo><mml:msup><mml:mi>œï</mml:mi><mml:mi>e</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>)
  are a learned linear combination of the interaction forces. The edge
  messages are aggregated at every node exploiting the principle of
  superposition <inline-formula><alternatives>
  <tex-math><![CDATA[\bar{e_i^\prime} \leftarrow \sum_{r_k = i} e_i^\prime]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mover><mml:msubsup><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:msubsup><mml:mo accent="true">‚Äæ</mml:mo></mml:mover><mml:mo>‚Üê</mml:mo><mml:msub><mml:mo>‚àë</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:msubsup></mml:mrow></mml:math></alternatives></inline-formula>.
  The node then encodes the connected edge features and its local
  features using a neural network: <inline-formula><alternatives>
  <tex-math><![CDATA[v_i^\prime \leftarrow \phi^v (\bar{e_i}, v_i, u)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>‚Ä≤</mml:mi></mml:msubsup><mml:mo>‚Üê</mml:mo><mml:msup><mml:mi>œï</mml:mi><mml:mi>v</mml:mi></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mover><mml:msub><mml:mi>e</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo accent="true">‚Äæ</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <fig>
    <caption><p>An overview of the graph network simulator
    (GNS).<styled-content id="figU003Agns"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figs/gnn.png" />
  </fig>
  <p>The GNS implementation uses semi-implicit Euler integration to
  update the state of the particles based on the nodes predicted
  accelerations. We introduce physics-inspired simple inductive biases,
  such as an inertial frame that allows learning algorithms to
  prioritize one solution over another, instead of learning to predict
  the inertial motion, the neural network learns to trivially predict a
  correction to the inertial trajectory, reducing learning time. We
  developed an open-source, PyTorch-based GNS that predicts the dynamics
  of fluid and particulate systems
  (<xref alt="Kumar &amp; Vantassel, 2022" rid="ref-Kumar_Graph_Network_Simulator_2022" ref-type="bibr">Kumar
  &amp; Vantassel, 2022</xref>). GNS trained on trajectory data is
  generalizable to predict particle kinematics in complex boundary
  conditions not seen during training.
  <xref alt="[fig:gns-mpm]" rid="figU003Agns-mpm">[fig:gns-mpm]</xref>
  shows the GNS prediction of granular flow around complex obstacles
  trained on 20 million steps with 40 trajectories on NVIDIA A100 GPUs.
  The trained model accurately predicts within 5% error of its
  associated material point method (MPM) simulation. The predictions are
  5,000x faster than traditional MPM simulations (2.5 hours for MPM
  simulations versus 20 s for GNS simulation of granular flow) and are
  widely used for solving optimization, control and inverse-type
  problems. In addition to surrogate modeling, GNS trained on flow
  problems is also used as an oracle to predict the dynamics of flows to
  identify critical regions of interest for in situ rendering and
  visualization
  (<xref alt="Kumar et al., 2022" rid="ref-kumar2022insitu" ref-type="bibr">Kumar
  et al., 2022</xref>). The GNS code is distributed under the
  open-source MIT license and is available on
  <ext-link ext-link-type="uri" xlink:href="https://github.com/geoelements/gns">GitHub
  Geoelements GNS</ext-link>.</p>
  <fig>
    <caption><p>GNS prediction of granular flow on ramps, compared
    against MPM
    simulation.<styled-content id="figU003Agns-mpm"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figs/gns-mpm.png" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Traditional numerical methods for solving differential equations
  are invaluable in scientific and engineering disciplines. However,
  such simulators are computationally expensive and intractable for
  solving large-scale and complex inverse problems, multiphysics, and
  multi-scale mechanics. Surrogate models trade off generality for
  accuracy in a narrow setting. Recent growth in data availability has
  spurred data-driven machine learning (ML) models that train directly
  from observed data
  (<xref alt="Prume et al., 2022" rid="ref-prume2022model" ref-type="bibr">Prume
  et al., 2022</xref>). ML models require significant training data to
  cover the large state space and complex dynamics. Instead of ignoring
  the vast amount of structured prior knowledge (physics), we can
  exploit such knowledge to construct physics-informed ML algorithms
  with limited training data. GNS uses static and inertial priors to
  learn the interactions between particles directly on graphs and can
  generalize with limited training data
  (<xref alt="Veliƒçkoviƒá et al., 2017" rid="ref-velivckovic2017graph" ref-type="bibr">Veliƒçkoviƒá
  et al., 2017</xref>;
  <xref alt="Wu et al., 2020" rid="ref-wu2020comprehensive" ref-type="bibr">Wu
  et al., 2020</xref>). Graph-based GNS offer powerful data
  representations of real-world applications, including particulate
  systems, material sciences, drug discovery, astrophysics, and
  engineering
  (<xref alt="Battaglia et al., 2018" rid="ref-battaglia2018relational" ref-type="bibr">Battaglia
  et al., 2018</xref>;
  <xref alt="Sanchez-Gonzalez et al., 2020" rid="ref-sanchez2020learning" ref-type="bibr">Sanchez-Gonzalez
  et al., 2020</xref>).</p>
</sec>
<sec id="state-of-the-art">
  <title>State of the art</title>
  <p>Numerical methods, such as particle-based approaches or continuum
  strategies like Material Point Method
  (<xref alt="Soga et al., 2016" rid="ref-soga2016" ref-type="bibr">Soga
  et al., 2016</xref>) and the Finite Element Method, serve as valuable
  tools for modeling a wide array of real-world engineering systems.
  Despite their versatility, these traditional numerical methods often
  prove computationally intensive, restricting them to a handful of
  simulations. With the growth of material sciences and the escalating
  complexity of engineering challenges, there is a pressing need to
  navigate expansive parametric spaces and solve complex optimization
  and inverse analysis. However, the computational bottleneck inherent
  in traditional methods thwarts our ability to achieve innovative
  data-driven discoveries. A surrogate model presents a solution to this
  hurdle. However, most current neural network-based surrogates operate
  as black box algorithms, lacking physics and underperforming when
  extrapolation beyond training regions is needed. Consequently, there
  is a need to develop generalizable surrogate models based on physics
  to bridge this gap effectively.</p>
  <p>Sanchez-Gonzalez et al.
  (<xref alt="2020" rid="ref-sanchez2020learning" ref-type="bibr">2020</xref>)
  developed a reference GNS implementation based on TensorFlow v1
  (<xref alt="Abadi et al., 2015" rid="ref-tensorflow2015whitepaper" ref-type="bibr">Abadi
  et al., 2015</xref>). Although the reference implementation runs both
  on CPU and GPU, it doesn‚Äôt achieve multi-GPU scaling. Furthermore, the
  dependence on TensorFlow v1 limits its ability to leverage features
  such as eager execution in TF v2. We develop a scalable and modular
  GNS using PyTorch using the Distributed Data Parallel model to run on
  multi-GPU systems.</p>
</sec>
<sec id="key-features">
  <title>Key features</title>
  <p>The Graph Network Simulator (GNS) uses PyTorch and PyTorch
  Geometric for constructing graphs and learned message passing. GNS is
  highly-scalable to 100,000 vertices and more than a million edges. The
  PyTorch GNS supports the following features:</p>
  <list list-type="bullet">
    <list-item>
      <p>CPU and GPU training</p>
    </list-item>
    <list-item>
      <p>Parallel training on multi-GPUs</p>
    </list-item>
    <list-item>
      <p>Multi-material interactions</p>
    </list-item>
    <list-item>
      <p>Complex boundary conditions</p>
    </list-item>
    <list-item>
      <p>Checkpoint restart</p>
    </list-item>
    <list-item>
      <p>VTK results</p>
    </list-item>
    <list-item>
      <p>Animation postprocessing</p>
    </list-item>
  </list>
</sec>
<sec id="gns-training-and-prediction">
  <title>GNS training and prediction</title>
  <p>GNS models are trained on 1000s of particle trajectories from MPM
  (for sands) and Smooth Particle Hydrodynamics (for water) for 20
  million steps. GNS predicts the rollout trajectories of particles,
  based on its training of MPM particle simulations. We employ Taichi
  MPM
  (<xref alt="Hu et al., 2018" rid="ref-hu2018mlsmpmcpic" ref-type="bibr">Hu
  et al., 2018</xref>) to compute the particle trajectories. The input
  to GNS includes the velocity context for five timesteps. GNS computes
  the acceleration between the five timesteps using the timestep
  <inline-formula><alternatives>
  <tex-math><![CDATA[\delta t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>Œ¥</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  GNS then rolls out the next states <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{X}_{i+5} \rightarrow, \ \ldots, \rightarrow \boldsymbol{X}_k]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>ùêó</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>‚Üí</mml:mo><mml:mo>,</mml:mo><mml:mspace width="0.222em"></mml:mspace><mml:mi>‚Ä¶</mml:mi><mml:mo>,</mml:mo><mml:mo>‚Üí</mml:mo><mml:msub><mml:mi>ùêó</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\boldsymbol{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  is the set of particle positions. We use the
  <monospace>.npz</monospace> format to store the training data, which
  includes a list of tuples of arbitrary length where each tuple
  corresponds to a differenet training trajectory and is of the form
  <monospace>(position, particle_type)</monospace>. The position is a
  3-D tensor of shape
  <monospace>(n_time_steps, n_particles, n_dimensions)</monospace> and
  particle_type is a 1-D tensor of shape
  <monospace>(n_particles)</monospace>.</p>
</sec>
<sec id="parallelization-and-scaling">
  <title>Parallelization and scaling</title>
  <p>The GNS is parallelized to run across multiple GPUs using the
  PyTorch Distributed Data Parallel (DDP) model. The DDP model spawns as
  many GNS models as the number of GPUs, distributing the dataset across
  all GPU nodes. Consider, our training dataset with 20 simulations,
  each with 206 time steps of positional data
  <inline-formula><alternatives>
  <tex-math><![CDATA[x_i]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
  which yields <inline-formula><alternatives>
  <tex-math><![CDATA[(206 - 6) \times 20 = 4000]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>206</mml:mn><mml:mo>‚àí</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>√ó</mml:mo><mml:mn>20</mml:mn><mml:mo>=</mml:mo><mml:mn>4000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  training trajectories. We subtract six position from the GNS training
  dataset as we utilize five previous velocities, computed from six
  positions, to predict the next position. The 4000 training tajectories
  are subsequently distributed equally to the four GPUs (1000 training
  trajectories/GPU). Assuming a batch size of 2, each GPU handles 500
  trajectories in a batch. The loss from the training trajectories are
  computed as difference between accelerations of GNS prediction and
  actual trajectories.</p>
  <p><disp-formula><alternatives>
  <tex-math><![CDATA[f(\theta) = \frac{1}{n}\sum_{i=1}^n ((\ddot{x}_t^i)_{GNS} - (\ddot{x}_t^i)_{actual})\,,]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munderover><mml:mo>‚àë</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:munderover><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mover><mml:mi>x</mml:mi><mml:mo accent="true">Ãà</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mi>G</mml:mi><mml:mi>N</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>‚àí</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:msubsup><mml:mover><mml:mi>x</mml:mi><mml:mo accent="true">Ãà</mml:mo></mml:mover><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msubsup><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="0.167em"></mml:mspace><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></p>
  <p>where <inline-formula><alternatives>
  <tex-math><![CDATA[n]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>n</mml:mi></mml:math></alternatives></inline-formula>
  is the number of particles (nodes) and <inline-formula><alternatives>
  <tex-math><![CDATA[\theta]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>Œ∏</mml:mi></mml:math></alternatives></inline-formula>
  is the learnable parameter in the GNS. In DDP, the gradient
  <inline-formula><alternatives>
  <tex-math><![CDATA[\nabla (f(\theta))]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>‚àá</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>Œ∏</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>
  is computed as the average gradient across all GPUs as shown in
  <xref alt="[fig:gns-ddp]" rid="figU003Agns-ddp">[fig:gns-ddp]</xref>.</p>
  <fig>
    <caption><p>Distributed data parallelization in
    GNS.<styled-content id="figU003Agns-ddp"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figs/gns-ddp.png" />
  </fig>
  <p>We tested the strong scaling of the GNS code on a single node of
  Lonestar 6 at the Texas Advanced Computing Center equipped with three
  NVIDIA A100 GPUs. We evaluated strong scaling for the
  <ext-link ext-link-type="uri" xlink:href="https://www.designsafe-ci.org/data/browser/public/designsafe.storage.published//PRJ-3702/WaterDropSample">WaterDropSample</ext-link>
  dataset for 6000 training steps using the recommended
  <monospace>nccl</monospace> DDP backend.
  <xref alt="[fig:gns-scaling]" rid="figU003Agns-scaling">[fig:gns-scaling]</xref>
  shows linear strong scaling performance.</p>
  <fig>
    <caption><p>GNS strong-scaling on up to three NVIDIA A100
    GPUs.<styled-content id="figU003Agns-scaling"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figs/gns-scaling.png" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge the support of National Science Foundation NSF OAC:
  2103937.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-battaglia2018relational">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Battaglia</surname><given-names>Peter W</given-names></name>
        <name><surname>Hamrick</surname><given-names>Jessica B</given-names></name>
        <name><surname>Bapst</surname><given-names>Victor</given-names></name>
        <name><surname>Sanchez-Gonzalez</surname><given-names>Alvaro</given-names></name>
        <name><surname>Zambaldi</surname><given-names>Vinicius</given-names></name>
        <name><surname>Malinowski</surname><given-names>Mateusz</given-names></name>
        <name><surname>Tacchetti</surname><given-names>Andrea</given-names></name>
        <name><surname>Raposo</surname><given-names>David</given-names></name>
        <name><surname>Santoro</surname><given-names>Adam</given-names></name>
        <name><surname>Faulkner</surname><given-names>Ryan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Relational inductive biases, deep learning, and graph networks</article-title>
      <source>arXiv preprint arXiv:1806.01261</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-Kumar_Graph_Network_Simulator_2022">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Kumar</surname><given-names>Krishna</given-names></name>
        <name><surname>Vantassel</surname><given-names>Joseph</given-names></name>
      </person-group>
      <article-title>Graph Network Simulator: v1.0.1</article-title>
      <year iso-8601-date="2022-06">2022</year><month>06</month>
      <uri>https://github.com/geoelements/gns</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.6658322</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-kumar2022insitu">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Kumar</surname><given-names>Krishna</given-names></name>
        <name><surname>Navratil</surname><given-names>Paul</given-names></name>
        <name><surname>Solis</surname><given-names>Andrew</given-names></name>
        <name><surname>Vantassel</surname><given-names>Joseph</given-names></name>
      </person-group>
      <article-title>Minority report: A graph-network oracle for large-scale in situ visualization</article-title>
      <publisher-name>IEEE</publisher-name>
      <publisher-loc>Oklahoma, USA</publisher-loc>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-sanchez2020learning">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Sanchez-Gonzalez</surname><given-names>Alvaro</given-names></name>
        <name><surname>Godwin</surname><given-names>Jonathan</given-names></name>
        <name><surname>Pfaff</surname><given-names>Tobias</given-names></name>
        <name><surname>Ying</surname><given-names>Rex</given-names></name>
        <name><surname>Leskovec</surname><given-names>Jure</given-names></name>
        <name><surname>Battaglia</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Learning to simulate complex physics with graph networks</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <uri>https://dl.acm.org/doi/10.5555/3524938.3525722</uri>
      <fpage>8459</fpage>
      <lpage>8468</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scarselli2008graph">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Scarselli</surname><given-names>Franco</given-names></name>
        <name><surname>Gori</surname><given-names>Marco</given-names></name>
        <name><surname>Tsoi</surname><given-names>Ah Chung</given-names></name>
        <name><surname>Hagenbuchner</surname><given-names>Markus</given-names></name>
        <name><surname>Monfardini</surname><given-names>Gabriele</given-names></name>
      </person-group>
      <article-title>The graph neural network model</article-title>
      <source>IEEE transactions on neural networks</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>20</volume>
      <issue>1</issue>
      <fpage>61</fpage>
      <lpage>80</lpage>
    </element-citation>
  </ref>
  <ref id="ref-gilmer2017neural">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Gilmer</surname><given-names>Justin</given-names></name>
        <name><surname>Schoenholz</surname><given-names>Samuel S</given-names></name>
        <name><surname>Riley</surname><given-names>Patrick F</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Dahl</surname><given-names>George E</given-names></name>
      </person-group>
      <article-title>Neural message passing for quantum chemistry</article-title>
      <source>International conference on machine learning</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <fpage>1263</fpage>
      <lpage>1272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wu2020comprehensive">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Zonghan</given-names></name>
        <name><surname>Pan</surname><given-names>Shirui</given-names></name>
        <name><surname>Chen</surname><given-names>Fengwen</given-names></name>
        <name><surname>Long</surname><given-names>Guodong</given-names></name>
        <name><surname>Zhang</surname><given-names>Chengqi</given-names></name>
        <name><surname>Philip</surname><given-names>S Yu</given-names></name>
      </person-group>
      <article-title>A comprehensive survey on graph neural networks</article-title>
      <source>IEEE Transactions on Neural Networks and Learning Systems</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-velivckovic2017graph">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Veliƒçkoviƒá</surname><given-names>Petar</given-names></name>
        <name><surname>Cucurull</surname><given-names>Guillem</given-names></name>
        <name><surname>Casanova</surname><given-names>Arantxa</given-names></name>
        <name><surname>Romero</surname><given-names>Adriana</given-names></name>
        <name><surname>Lio</surname><given-names>Pietro</given-names></name>
        <name><surname>Bengio</surname><given-names>Yoshua</given-names></name>
      </person-group>
      <article-title>Graph attention networks</article-title>
      <source>arXiv preprint arXiv:1710.10903</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-prume2022model">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Prume</surname><given-names>Erik</given-names></name>
        <name><surname>Reese</surname><given-names>Stefanie</given-names></name>
        <name><surname>Ortiz</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>Model-free data-driven inference in computational mechanics</article-title>
      <source>arXiv preprint arXiv:2207.06419</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.1016/j.cma.2022.115704</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow2015whitepaper">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>Mart√≠n</given-names></name>
        <name><surname>Agarwal</surname><given-names>Ashish</given-names></name>
        <name><surname>Barham</surname><given-names>Paul</given-names></name>
        <name><surname>Brevdo</surname><given-names>Eugene</given-names></name>
        <name><surname>Chen</surname><given-names>Zhifeng</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg S.</given-names></name>
        <name><surname>Davis</surname><given-names>Andy</given-names></name>
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Ghemawat</surname><given-names>Sanjay</given-names></name>
        <name><surname>Goodfellow</surname><given-names>Ian</given-names></name>
        <name><surname>Harp</surname><given-names>Andrew</given-names></name>
        <name><surname>Irving</surname><given-names>Geoffrey</given-names></name>
        <name><surname>Isard</surname><given-names>Michael</given-names></name>
        <name><surname>Jia</surname><given-names>Yangqing</given-names></name>
        <name><surname>Jozefowicz</surname><given-names>Rafal</given-names></name>
        <name><surname>Kaiser</surname><given-names>Lukasz</given-names></name>
        <name><surname>Kudlur</surname><given-names>Manjunath</given-names></name>
        <name><surname>Levenberg</surname><given-names>Josh</given-names></name>
        <name><surname>Man√©</surname><given-names>Dandelion</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Moore</surname><given-names>Sherry</given-names></name>
        <name><surname>Murray</surname><given-names>Derek</given-names></name>
        <name><surname>Olah</surname><given-names>Chris</given-names></name>
        <name><surname>Schuster</surname><given-names>Mike</given-names></name>
        <name><surname>Shlens</surname><given-names>Jonathon</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Sutskever</surname><given-names>Ilya</given-names></name>
        <name><surname>Talwar</surname><given-names>Kunal</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Vanhoucke</surname><given-names>Vincent</given-names></name>
        <name><surname>Vasudevan</surname><given-names>Vijay</given-names></name>
        <name><surname>Vi√©gas</surname><given-names>Fernanda</given-names></name>
        <name><surname>Vinyals</surname><given-names>Oriol</given-names></name>
        <name><surname>Warden</surname><given-names>Pete</given-names></name>
        <name><surname>Wattenberg</surname><given-names>Martin</given-names></name>
        <name><surname>Wicke</surname><given-names>Martin</given-names></name>
        <name><surname>Yu</surname><given-names>Yuan</given-names></name>
        <name><surname>Zheng</surname><given-names>Xiaoqiang</given-names></name>
      </person-group>
      <article-title>TensorFlow: Large-scale machine learning on heterogeneous systems</article-title>
      <year iso-8601-date="2015">2015</year>
      <uri>https://www.tensorflow.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-soga2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Soga</surname><given-names>K.</given-names></name>
        <name><surname>Alonso</surname><given-names>E.</given-names></name>
        <name><surname>Yerro</surname><given-names>A.</given-names></name>
        <name><surname>Kumar</surname><given-names>K.</given-names></name>
        <name><surname>Bandara</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>Trends in large-deformation analysis of landslide mass movements with particular emphasis on the material point method</article-title>
      <source>G√©otechnique</source>
      <year iso-8601-date="2016">2016</year>
      <volume>66</volume>
      <issue>3</issue>
      <issn>0016-8505</issn>
      <pub-id pub-id-type="doi">10.1680/jgeot.15.LM.005</pub-id>
      <fpage>248</fpage>
      <lpage>273</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hu2018mlsmpmcpic">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hu</surname><given-names>Yuanming</given-names></name>
        <name><surname>Fang</surname><given-names>Yu</given-names></name>
        <name><surname>Ge</surname><given-names>Ziheng</given-names></name>
        <name><surname>Qu</surname><given-names>Ziyin</given-names></name>
        <name><surname>Zhu</surname><given-names>Yixin</given-names></name>
        <name><surname>Pradhana</surname><given-names>Andre</given-names></name>
        <name><surname>Jiang</surname><given-names>Chenfanfu</given-names></name>
      </person-group>
      <article-title>A moving least squares material point method with displacement discontinuity and two-way rigid body coupling</article-title>
      <source>ACM Transactions on Graphics (TOG)</source>
      <publisher-name>ACM</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>37</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1145/3197517.3201293</pub-id>
      <fpage>150</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
