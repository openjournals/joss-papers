<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7584</article-id>
<article-id pub-id-type="doi">10.21105/joss.07584</article-id>
<title-group>
<article-title>OPTIMUS: A Multidimensional Global Optimization
Package</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<name>
<surname>Tsoulos</surname>
<given-names>Ioannis G.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Charilogis</surname>
<given-names>Vasileios</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Kyrou</surname>
<given-names>Glykeria</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Stavrou</surname>
<given-names>Vasileios N.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>Tzallas</surname>
<given-names>Alexandros</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Informatics and Telecommunications,
University of Ioannina, Greece</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Division of Physical Sciences, Hellenic Naval Academy,
Greece</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-10-16">
<day>16</day>
<month>10</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>108</issue>
<fpage>7584</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Global optimization</kwd>
<kwd>Evolutionary methods</kwd>
<kwd>Stopping rules</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The location of global minima is applied in a variety of problems
  derived from physics
  (<xref alt="Duan et al., 1992" rid="ref-duan1992effective_1992" ref-type="bibr">Duan
  et al., 1992</xref>), chemistry
  (<xref alt="Liwo et al., 1999" rid="ref-liwo1999protein_1999" ref-type="bibr">Liwo
  et al., 1999</xref>), medicine
  (<xref alt="Lee, 2007" rid="ref-lee2007large_2007" ref-type="bibr">Lee,
  2007</xref>) etc. Also, global optimization can be used in complex
  problems, such as the training of neural networks
  (<xref alt="Chiroma et al., 2017" rid="ref-chiroma2017neural_2017" ref-type="bibr">Chiroma
  et al., 2017</xref>). Methods used in the global optimization process
  can be divided into stochastic and deterministic and a comparison
  between methods of these categories can be found in the work of
  Sergeyev et
  al.(<xref alt="Sergeyev et al., 2018" rid="ref-sergeyev2018efficiency_2018" ref-type="bibr">Sergeyev
  et al., 2018</xref>). This article proposes a software that
  incorporates a variety of stochastic methods to handle the
  optimization of multidimensional functions.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>This paper presents the <monospace>GlobalOptimus</monospace>
  software, written in ANSI C++, which allows researchers wanting to
  solve optimization problems in their domain to formulate their
  optimization problems and apply different global optimization methods
  to identify the global minimum of the problem. The main features of
  the proposed software are: a) Coding of the objective problem in a
  high level language such as ANSI C++ b) Incorporation of many global
  optimization techniques to solve the objective problem c)
  Parameterization of global optimization methods using user-defined
  parameters d) Usage of a GUI application to control the optimization
  strategy. Some of the frequently used optimization tools are MATLAB
  Optimization Toolbox, SciPy optimization module and Gurobi. MATLAB and
  SciPy are usually based on high-level languages, which can limit
  performance when computational problems are required. GlobalOptimus is
  an algorithmic optimization tool written in C++, which combines high
  performance with ease of parameterization for solving optimization
  problems in various domains. Unlike other tools, it stands out for its
  open architecture and adaptability, allowing full customization and
  integration of software from other platforms without restrictions. In
  addition, GlobalOptimus supports multiple optimization algorithms,
  such as genetic algorithms and differential evolution, giving
  flexibility in formulating the optimal method for each problem.
  Although both MATLAB and SciPy offer support for such algorithms,
  GlobalOptimus provides one extensible approach. Finally, Gurobi is an
  extremely powerful tool for mathematical optimization, but it requires
  a commercial license. Instead, GlobalOptimus is open source, which
  makes it more accessible to researchers and developers who wish to
  adapt it to their needs.</p>
</sec>
<sec id="usage-of-the-software">
  <title>Usage of the Software</title>
  <p>The software is compiled using the QT library, available from
  <ext-link ext-link-type="uri" xlink:href="https://qt.io">Qt main
  url</ext-link> The researcher should code their objective function and
  a number of other mandatory functions in the C++ programming language,
  specifying the dimension of the objective function and the bounds of
  the function domain. Subsequently, they can select a global
  optimization method to apply to the problem from a wide range of
  available methods.</p>
  <p>In addition, the user can create his own objective function by
  implementing the methods of category
  <monospace>UserProblem</monospace>, the methods of which are presented
  in the following diagram.</p>
  <fig>
    <caption><p>The diagram of the user defined problem
    UserProblem</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="userproblem.png" />
  </fig>
  <sec id="implemented-global-optimization-methods">
    <title>Implemented global optimization methods</title>
    <p>In the proposed software, each implemented global optimization
    method has a set of parameters that can determine the overall
    optimization path and the effectiveness of the method. Some of the
    global optimization methods are as follows:</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="50%" />
          <col width="50%" />
        </colgroup>
        <thead>
          <tr>
            <th>METHOD</th>
            <th>DESCRIPTION</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><bold>Differential Evolution</bold></td>
            <td>An evolutionary algorithm used in many areas
            (<xref alt="Pant et al., 2020" rid="ref-pant2020differential_2020" ref-type="bibr">Pant
            et al., 2020</xref>)</td>
          </tr>
          <tr>
            <td><bold>Parallel Differential Evolution</bold></td>
            <td>A parallel version of the Differential Evolution method
            (<xref alt="Charilogis &amp; Tsoulos, 2023" rid="ref-charilogis2023parallel_2023" ref-type="bibr">Charilogis
            &amp; Tsoulos, 2023</xref>).</td>
          </tr>
          <tr>
            <td><bold>Double precision genetic algorithm</bold></td>
            <td>An Improved version of the Genetic Algorithm
            (<xref alt="Tsoulos, 2008" rid="ref-tsoulos2008modifications_2008" ref-type="bibr">Tsoulos,
            2008</xref>).</td>
          </tr>
          <tr>
            <td><bold>Improved Particle Swarm Optimization</bold></td>
            <td>Suggested as an improvement of the PSO method
            (<xref alt="Charilogis &amp; Tsoulos, 2022" rid="ref-charilogis2022toward_2022" ref-type="bibr">Charilogis
            &amp; Tsoulos, 2022</xref>).</td>
          </tr>
          <tr>
            <td><bold>Multistart</bold></td>
            <td>Initiates local searches from different start points
            (<xref alt="Martı́ et al., 2010" rid="ref-marti2010advanced_2010" ref-type="bibr">Martı́
            et al., 2010</xref>).</td>
          </tr>
          <tr>
            <td><bold>NeuralMinimizer</bold></td>
            <td>Utilizes Radial Basis Functions (RBF) networks to
            increase the speed of the Multistart method
            (<xref alt="Tsoulos et al., 2023" rid="ref-tsoulos2023neuralminimizer_2023" ref-type="bibr">Tsoulos
            et al., 2023</xref>).</td>
          </tr>
          <tr>
            <td><bold>Parallel Particle Swarm optimizer</bold></td>
            <td>A novell parallel PSO variant, published recently
            (<xref alt="Charilogis et al., 2023" rid="ref-charilogis2023improved_2023" ref-type="bibr">Charilogis
            et al., 2023</xref>).</td>
          </tr>
          <tr>
            <td><bold>Simulated annealing optimizer</bold></td>
            <td>included in the software under the name Simman
            (<xref alt="Kirkpatrick et al., 1983" rid="ref-kirkpatrick1983optimization_1983" ref-type="bibr">Kirkpatrick
            et al., 1983</xref>).</td>
          </tr>
          <tr>
            <td><bold>The optimal foraging algorithm (OFA)</bold></td>
            <td>motivated by animal behavioral ecology included the
            software named Ofa
            (<xref alt="Kyrou et al., 2024a" rid="ref-kyrou2024eofa_2024" ref-type="bibr">Kyrou
            et al., 2024a</xref>).</td>
          </tr>
          <tr>
            <td><bold>Bio-inspired metaheuristic algorithm Giant
            Armadillo Optimization (GAO)</bold></td>
            <td>This process mimics the natural behavior of the giant
            armadillo
            (<xref alt="Kyrou et al., 2024b" rid="ref-kyrou2024improving_2024" ref-type="bibr">Kyrou
            et al., 2024b</xref>).</td>
          </tr>
          <tr>
            <td><bold>The Gray Wolf Optimizer (GWO)</bold></td>
            <td>This process mimics hunting mechanism of gray wolves in
            nature
            (<xref alt="Li et al., 2021" rid="ref-li2021improved_2021" ref-type="bibr">Li
            et al., 2021</xref>).</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="implemented-local-optimization-methods">
    <title>Implemented local optimization methods</title>
    <p>The parameter <monospace>−−opt_localsearch</monospace> is used to
    select the used local optimization procedure. The implemented local
    optimization methods are the following:</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="50%" />
          <col width="50%" />
        </colgroup>
        <thead>
          <tr>
            <th>METHOD</th>
            <th>DESCRIPTION</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><bold>The bfgs method</bold></td>
            <td>The Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm
            was implemented
            (<xref alt="Powell, 1989" rid="ref-powell1989tolerant_1989" ref-type="bibr">Powell,
            1989</xref>).</td>
          </tr>
          <tr>
            <td><bold>The lbfgs method</bold></td>
            <td>The limited memory BFGS method is implemented as an
            approximation of the BFGS method using a limited amount of
            computer memory
            (<xref alt="Liu &amp; Nocedal, 1989" rid="ref-liu1989limited_1989" ref-type="bibr">Liu
            &amp; Nocedal, 1989</xref>).</td>
          </tr>
          <tr>
            <td><bold>The Gradient descent method</bold></td>
            <td>This method is denoted as gradient in the software and
            implements the Gradient Descent local optimization procedure
            (<xref alt="Amari, 1993" rid="ref-amari1993backpropagation_1993" ref-type="bibr">Amari,
            1993</xref>).</td>
          </tr>
          <tr>
            <td><bold>The Nelder Mead method</bold></td>
            <td>The Nelder Mead simplex procedure for local optimization
            is also included in the software
            (<xref alt="Olsson &amp; Nelson, 1975" rid="ref-olsson1975nelder_1975" ref-type="bibr">Olsson
            &amp; Nelson, 1975</xref>).</td>
          </tr>
          <tr>
            <td><bold>The adam method</bold></td>
            <td>The adam optimizer
            (<xref alt="Kingma, 2014" rid="ref-kingma2014adam_2014" ref-type="bibr">Kingma,
            2014</xref>).</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </sec>
  <sec id="implementing-a-user-defined-optimization-method">
    <title>Implementing a user-defined optimization method</title>
    <p>The software can be extended by implementing custom optimization
    techniques by extending the optimization method class
    <monospace>UserMethod</monospace> and implementing the provided
    functions according to the requirements of the method.</p>
    <table-wrap>
      <table>
        <colgroup>
          <col width="50%" />
          <col width="50%" />
        </colgroup>
        <thead>
          <tr>
            <th>Method</th>
            <th>DESCRIPTION</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><bold>init()</bold></td>
            <td>This function is called every time the optimization
            method starts</td>
          </tr>
          <tr>
            <td><bold>step()</bold></td>
            <td>This function implements the actual step of the
            optimization method.</td>
          </tr>
          <tr>
            <td><bold>terminated()</bold></td>
            <td>This function is used as the termination step of the
            optimization method.</td>
          </tr>
          <tr>
            <td><bold>done()</bold></td>
            <td>This function will be called when the optimization
            method terminates.</td>
          </tr>
          <tr>
            <td><bold>~UserMethod()</bold></td>
            <td>This is the destructor of the optimization method.</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
    <p>A flowchart of any used optimization process provided below:</p>
    <fig>
      <caption><p>The diagram of the optimization process</p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="usermethod.png" />
    </fig>
    <p>An example run for the Rastrigin function and the Genetic
    algorithm is outlined in
    <xref alt="[FIG:rastrigin]" rid="FIGU003Arastrigin">[FIG:rastrigin]</xref>.</p>
    <fig>
      <caption><p>An example run for the Rastrigin function
      <styled-content id="FIGU003Arastrigin"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="png" xlink:href="rastrigin.png" />
    </fig>
  </sec>
</sec>
<sec id="conclusions">
  <title>Conclusions</title>
  <p>In this work, an environment for executing global optimization
  problems was presented, where the user can implement their
  optimization problem using a set of predefined functions and then has
  the possibility to choose among several global optimization methods to
  solve the defined problem. In addition, it is given the possibility to
  choose to use a local optimization method to enhance the reliability
  of the produced results. This programming environment is freely
  available and easy to extend to accommodate more global optimization
  techniques.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-pant2020differential_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pant</surname><given-names>Millie</given-names></name>
        <name><surname>Zaheer</surname><given-names>Hira</given-names></name>
        <name><surname>Garcia-Hernandez</surname><given-names>Laura</given-names></name>
        <name><surname>Abraham</surname><given-names>Ajith</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Differential evolution: A review of more than two decades of research</article-title>
      <source>Engineering Applications of Artificial Intelligence</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>90</volume>
      <pub-id pub-id-type="doi">10.1016/j.engappai.2020.103479</pub-id>
      <fpage>103479</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-charilogis2023parallel_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Charilogis</surname><given-names>Vasileios</given-names></name>
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
      </person-group>
      <article-title>A parallel implementation of the differential evolution method</article-title>
      <source>Analytics</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>2</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.3390/analytics2010002</pub-id>
      <fpage>17</fpage>
      <lpage>30</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tsoulos2008modifications_2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
      </person-group>
      <article-title>Modifications of real code genetic algorithm for global optimization</article-title>
      <source>Applied Mathematics and Computation</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>203</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1016/j.amc.2008.05.005</pub-id>
      <fpage>598</fpage>
      <lpage>607</lpage>
    </element-citation>
  </ref>
  <ref id="ref-charilogis2022toward_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Charilogis</surname><given-names>Vasileios</given-names></name>
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
      </person-group>
      <article-title>Toward an ideal particle swarm optimizer for multidimensional functions</article-title>
      <source>Information</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>13</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.3390/info13050217</pub-id>
      <fpage>217</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tsoulos2023neuralminimizer_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
        <name><surname>Tzallas</surname><given-names>Alexandros</given-names></name>
        <name><surname>Karvounis</surname><given-names>Evangelos</given-names></name>
        <name><surname>Tsalikakis</surname><given-names>Dimitrios</given-names></name>
      </person-group>
      <article-title>NeuralMinimizer: A novel method for global optimization</article-title>
      <source>Information</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>14</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.3390/info14020066</pub-id>
      <fpage>66</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-charilogis2023improved_2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Charilogis</surname><given-names>Vasileios</given-names></name>
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
        <name><surname>Tzallas</surname><given-names>Alexandros</given-names></name>
      </person-group>
      <article-title>An improved parallel particle swarm optimization</article-title>
      <source>SN Computer Science</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>4</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.1007/s42979-023-02227-9</pub-id>
      <fpage>766</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-kirkpatrick1983optimization_1983">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kirkpatrick</surname><given-names>Scott</given-names></name>
        <name><surname>Gelatt Jr</surname><given-names>C Daniel</given-names></name>
        <name><surname>Vecchi</surname><given-names>Mario P</given-names></name>
      </person-group>
      <article-title>Optimization by simulated annealing</article-title>
      <source>science</source>
      <publisher-name>American association for the advancement of science</publisher-name>
      <year iso-8601-date="1983">1983</year>
      <volume>220</volume>
      <issue>4598</issue>
      <pub-id pub-id-type="doi">10.1126/science.220.4598.671</pub-id>
      <fpage>671</fpage>
      <lpage>680</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kyrou2024eofa_2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kyrou</surname><given-names>Glykeria</given-names></name>
        <name><surname>Charilogis</surname><given-names>Vasileios</given-names></name>
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
      </person-group>
      <article-title>EOFA: An extended version of the optimal foraging algorithm for global optimization problems</article-title>
      <source>Computation</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>12</volume>
      <issue>8</issue>
      <pub-id pub-id-type="doi">10.3390/computation12080158</pub-id>
      <fpage>158</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-powell1989tolerant_1989">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Powell</surname><given-names>MJD</given-names></name>
      </person-group>
      <article-title>A tolerant algorithm for linearly constrained optimization calculations</article-title>
      <source>Mathematical Programming</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="1989">1989</year>
      <volume>45</volume>
      <pub-id pub-id-type="doi">10.1007/bf01589118</pub-id>
      <fpage>547</fpage>
      <lpage>566</lpage>
    </element-citation>
  </ref>
  <ref id="ref-amari1993backpropagation_1993">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Amari</surname><given-names>Shun-ichi</given-names></name>
      </person-group>
      <article-title>Backpropagation and stochastic gradient descent method</article-title>
      <source>Neurocomputing</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="1993">1993</year>
      <volume>5</volume>
      <issue>4-5</issue>
      <pub-id pub-id-type="doi">10.1016/0925-2312(93)90006-o</pub-id>
      <fpage>185</fpage>
      <lpage>196</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kingma2014adam_2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kingma</surname><given-names>Diederik P</given-names></name>
      </person-group>
      <article-title>Adam: A method for stochastic optimization</article-title>
      <source>arXiv preprint arXiv:1412.6980</source>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-olsson1975nelder_1975">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Olsson</surname><given-names>Donald M</given-names></name>
        <name><surname>Nelson</surname><given-names>Lloyd S</given-names></name>
      </person-group>
      <article-title>The nelder-mead simplex procedure for function minimization</article-title>
      <source>Technometrics</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="1975">1975</year>
      <volume>17</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.2307/1267998</pub-id>
      <fpage>45</fpage>
      <lpage>51</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liu1989limited_1989">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Dong C</given-names></name>
        <name><surname>Nocedal</surname><given-names>Jorge</given-names></name>
      </person-group>
      <article-title>On the limited memory BFGS method for large scale optimization</article-title>
      <source>Mathematical programming</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="1989">1989</year>
      <volume>45</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1007/bf01589116</pub-id>
      <fpage>503</fpage>
      <lpage>528</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lee2007large_2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lee</surname><given-names>Eva K</given-names></name>
      </person-group>
      <article-title>Large-scale optimization-based classification models in medicine and biology</article-title>
      <source>Annals of biomedical engineering</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>35</volume>
      <pub-id pub-id-type="doi">10.1007/s10439-007-9317-7</pub-id>
      <fpage>1095</fpage>
      <lpage>1109</lpage>
    </element-citation>
  </ref>
  <ref id="ref-chiroma2017neural_2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chiroma</surname><given-names>Haruna</given-names></name>
        <name><surname>Noor</surname><given-names>Ahmad Shukri Mohd</given-names></name>
        <name><surname>Abdulkareem</surname><given-names>Sameem</given-names></name>
        <name><surname>Abubakar</surname><given-names>Adamu I</given-names></name>
        <name><surname>Hermawan</surname><given-names>Arief</given-names></name>
        <name><surname>Qin</surname><given-names>Hongwu</given-names></name>
        <name><surname>Hamza</surname><given-names>Mukhtar Fatihu</given-names></name>
        <name><surname>Herawan</surname><given-names>Tutut</given-names></name>
      </person-group>
      <article-title>Neural networks optimization through genetic algorithm searches: A review</article-title>
      <source>Appl. Math. Inf. Sci</source>
      <year iso-8601-date="2017">2017</year>
      <volume>11</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.18576/amis/110602</pub-id>
      <fpage>1543</fpage>
      <lpage>1564</lpage>
    </element-citation>
  </ref>
  <ref id="ref-liwo1999protein_1999">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liwo</surname><given-names>Adam</given-names></name>
        <name><surname>Lee</surname><given-names>Jooyoung</given-names></name>
        <name><surname>Ripoll</surname><given-names>Daniel R</given-names></name>
        <name><surname>Pillardy</surname><given-names>Jaroslaw</given-names></name>
        <name><surname>Scheraga</surname><given-names>Harold A</given-names></name>
      </person-group>
      <article-title>Protein structure prediction by global optimization of a potential energy function</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <publisher-name>National Acad Sciences</publisher-name>
      <year iso-8601-date="1999">1999</year>
      <volume>96</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1073/pnas.96.10.5482</pub-id>
      <fpage>5482</fpage>
      <lpage>5485</lpage>
    </element-citation>
  </ref>
  <ref id="ref-duan1992effective_1992">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Duan</surname><given-names>Qingyun</given-names></name>
        <name><surname>Sorooshian</surname><given-names>Soroosh</given-names></name>
        <name><surname>Gupta</surname><given-names>Vijai</given-names></name>
      </person-group>
      <article-title>Effective and efficient global optimization for conceptual rainfall-runoff models</article-title>
      <source>Water resources research</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1992">1992</year>
      <volume>28</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1029/91wr02985</pub-id>
      <fpage>1015</fpage>
      <lpage>1031</lpage>
    </element-citation>
  </ref>
  <ref id="ref-li2021improved_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Yu</given-names></name>
        <name><surname>Lin</surname><given-names>Xiaoxiao</given-names></name>
        <name><surname>Liu</surname><given-names>Jingsen</given-names></name>
      </person-group>
      <article-title>An improved gray wolf optimization algorithm to solve engineering problems</article-title>
      <source>Sustainability</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>13</volume>
      <issue>6</issue>
      <pub-id pub-id-type="doi">10.3390/su13063208</pub-id>
      <fpage>3208</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-marti2010advanced_2010">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Martı́</surname><given-names>Rafael</given-names></name>
        <name><surname>Moreno-Vega</surname><given-names>J Marcos</given-names></name>
        <name><surname>Duarte</surname><given-names>Abraham</given-names></name>
      </person-group>
      <article-title>Advanced multi-start methods</article-title>
      <source>Handbook of metaheuristics</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.1007/978-1-4419-1665-5_9</pub-id>
      <fpage>265</fpage>
      <lpage>281</lpage>
    </element-citation>
  </ref>
  <ref id="ref-kyrou2024improving_2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kyrou</surname><given-names>Glykeria</given-names></name>
        <name><surname>Charilogis</surname><given-names>Vasileios</given-names></name>
        <name><surname>Tsoulos</surname><given-names>Ioannis G</given-names></name>
      </person-group>
      <article-title>Improving the giant-armadillo optimization method</article-title>
      <source>Analytics</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2024">2024</year>
      <volume>3</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.20944/preprints202404.1784.v1</pub-id>
      <fpage>225</fpage>
      <lpage>240</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sergeyev2018efficiency_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sergeyev</surname><given-names>Ya D</given-names></name>
        <name><surname>Kvasov</surname><given-names>DE</given-names></name>
        <name><surname>Mukhametzhanov</surname><given-names>MS</given-names></name>
      </person-group>
      <article-title>On the efficiency of nature-inspired metaheuristics in expensive global optimization with limited budget</article-title>
      <source>Scientific reports</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>8</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-017-18940-4</pub-id>
      <fpage>453</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
