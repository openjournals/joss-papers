<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6880</article-id>
<article-id pub-id-type="doi">10.21105/joss.06880</article-id>
<title-group>
<article-title>ChainoPy: A Python Library for Discrete Time Markov Chain
based stochastic analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Chinubhai</surname>
<given-names>Aadya A.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>School of Engineering and Applied Sciences (SEAS),
Ahmedabad University, Ahmedabad, Gujarat, India</institution>
</institution-wrap>
</aff>
</contrib-group>
<volume>9</volume>
<issue>100</issue>
<fpage>6880</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Markov Chains</kwd>
<kwd>Markov Chain Neural Networks</kwd>
<kwd>Stochastic Analysis</kwd>
<kwd>High Performance Computing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summmary">
  <title>Summmary</title>
  <p>Modeling time series data, such as stock prices and text sequences,
  is effectively achieved using Markov Chains. ChainoPy facilitates the
  modeling of time series data with Markov Chains and Markov Switching
  Models, optimizing for computational efficiency in terms of speed and
  memory usage. Additionally, ChainoPy enables the integration of
  probabilistic models like Markov Chains with Neural Networks,
  traditionally considered deterministic, through the
  MarkovChainNeuralNetwork class. This hybrid approach leverages the
  strengths of both probabilistic and neural network methodologies.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>There are limitations in current Markov Chain packages like PyDTMC
  (<xref alt="PyDTMC, n.d." rid="ref-pydtmc" ref-type="bibr"><italic>PyDTMC</italic>,
  n.d.</xref>), simple-markov
  (<xref alt="Simple-Markov, n.d." rid="ref-simple-markov" ref-type="bibr"><italic>Simple-Markov</italic>,
  n.d.</xref>), mchmm
  (<xref alt="Mchmm, n.d." rid="ref-mchmm" ref-type="bibr"><italic>Mchmm</italic>,
  n.d.</xref>) that rely solely on NumPy
  (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
  et al., 2020</xref>) and Python for implementation. Markov Chains
  often require iterative convergence-based algorithms
  (<xref alt="Rosenthal, 1995" rid="ref-rosenthal1995convergence" ref-type="bibr">Rosenthal,
  1995</xref>), where Python’s dynamic typing, Global Interpreter Lock
  (GIL), and garbage collection can hinder potential performance
  improvements like parallelism. To address these issues, we enhance our
  library with extensions like Cython for efficient algorithm
  implementation. Additionally, we introduce a Markov Chain Neural
  Network
  (<xref alt="Awiszus &amp; Rosenhahn, 2018" rid="ref-awiszus2018markov" ref-type="bibr">Awiszus
  &amp; Rosenhahn, 2018</xref>) that simulates given Markov Chains while
  preserving statistical properties from the training data. This
  approach eliminates the need for post-processing steps such as
  sampling from the outcome distribution while giving neural networks
  stochastic properties rather than deterministic behavior. Finally, we
  implement the famous Markov Switching Models
  (<xref alt="Hamilton, 2010" rid="ref-hamilton2010regime" ref-type="bibr">Hamilton,
  2010</xref>) which are one of the fundamental and widely used models
  in applications such as Stock Market price prediction. ChainoPy
  enables new workflows through its advanced algorithms, such as Markov
  Chain Neural Networks and Markov Switching Models, which are not
  available in PyDTMC. These capabilities, combined with significant
  performance improvements in both fast and slow functions, provide
  added value for complex stochastic analysis tasks.</p>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <p>We implement three public classes
  <monospace>MarkovChain</monospace>,
  <monospace>MarkovChainNeuralNetwork</monospace> and
  <monospace>MarkovSwitchingModel</monospace> that contain core
  functionalities of the package. Performance intensive functions for
  the <monospace>MarkovChain</monospace> class are implemented in the
  <monospace>_backend</monospace> directory where a custom Cython
  (<xref alt="Behnel et al., 2010" rid="ref-behnel2010cython" ref-type="bibr">Behnel
  et al., 2010</xref>) backend is implemented circumventing drawbacks of
  Python like the GIL, dynamic typing etc. The
  <monospace>MarkovChain</monospace> class implements various
  functionalities for discrete-time Markov chains. It provides methods
  for fitting the transition matrix from data, simulating the chain,
  calculating properties. It also supports visualization for Markov
  chains.</p>
  <p>We do the following key optimizations:</p>
  <list list-type="bullet">
    <list-item>
      <p>Efficient matrix power: If the matrix is diagonalizable, an
      eigenvalue decomposition based matrix power is performed.</p>
    </list-item>
    <list-item>
      <p>Parallel Execution: Some functions are parallelized.</p>
    </list-item>
    <list-item>
      <p><monospace>__slots__</monospace> usage:
      <monospace>__slots__</monospace> is used instead of
      <monospace>__dict__</monospace> for storing object attributes,
      reducing memory overhead.</p>
    </list-item>
    <list-item>
      <p>Caching decorator: Class methods are decorated with caching to
      avoid recomputation of unnecessary results.</p>
    </list-item>
    <list-item>
      <p>Direct LAPACK use: LAPACK function <monospace>dgeev</monospace>
      is directly used to calculate stationary-distribution via SciPy’s
      (<xref alt="Virtanen et al., 2020" rid="ref-virtanen2020scipy" ref-type="bibr">Virtanen
      et al., 2020</xref>) <monospace>cython_lapack</monospace> API
      instead of additional numpy overhead.</p>
    </list-item>
    <list-item>
      <p>Utility functions for visualization: Utility functions are
      implemented for visualizing the Markov chain.</p>
    </list-item>
    <list-item>
      <p>Sparse storage of transition matrix: The model is stored as a
      JSON object, and if 40% or more elements of the transition matrix
      are near zero, it is stored in a sparse format.</p>
    </list-item>
  </list>
  <p>The <monospace>MarkovChainNeuralNetwork</monospace> implementation
  defines a neural network model, using PyTorch
  (<xref alt="Ansel et al., 2024" rid="ref-ansel2024pytorch" ref-type="bibr">Ansel
  et al., 2024</xref>) for simulating Markov chain behavior. It takes a
  Markov chain object and the number of layers as input, with each layer
  being a linear layer. The model’s forward method computes the output
  probabilities for the next state. The model is trained using
  stochastic gradient descent (SGD) with a learning rate scheduler.
  Finally, the model’s performance is evaluated using the KL divergence
  between the original Markov chain’s transition probabilities and those
  estimated from the simulated walks.</p>
  <p>API of the library:</p>
  <preformat>- chainopy.MarkovChain(transition-matrix: ndarray, states: list)
        
        Public Methods
        -------------- 
        
        - fit()
        - simulate()
        - predict()
        - adjacency_matrix()
        - nstep_distribution()
        - is_ergodic()
        - is_symmetric()
        - stationary_dist()
        - is_absorbing()
        - is_aperiodic()
        - period()
        - is_irreducible()
        - is_transient(state)
        - is_recurrent(state)
        - fundamental_matrix()
        - absorption_probabilities()
        - expected_time_to_absorption()
        - expected_number_of_visits()
        - expected_hitting_time(state)
        - visualize_transition_matrix()
        - visualize_chain()
        - save_model()
        - load_model()
        - marginal_dist()
        - fit_from_file()

- chainopy.MarkovChainNeuralNetwork(chainopy.MarkovChain, num_layers)

        Public Methods
        --------------

        - train_model()
        - get_weights()
        - simulate_random_walk()

- chainopy.MarkovSwitchingModel()

        Public Methods
        --------------

        - fit()
        - predict()
        - evaluate()

- chainopy.divergance_analysis(MarkovChain, MarkovChainNeuralNetwork)</preformat>
</sec>
<sec id="documentation-testing-and-benchmarking">
  <title>Documentation, Testing and Benchmarking</title>
  <p>For Documentation we use Sphinx. For Testing and Benchmarking we
  use the Pytest and PyDTMC
  (<xref alt="PyDTMC, n.d." rid="ref-pydtmc" ref-type="bibr"><italic>PyDTMC</italic>,
  n.d.</xref>) packages.</p>
  <p>The results are as follows:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>is_absorbing</monospace> Methods</p>
    </list-item>
  </list>
  <table-wrap>
    <table>
      <colgroup>
        <col width="22%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
      </colgroup>
      <thead>
        <tr>
          <th>Transition-Matrix Size</th>
          <th>10</th>
          <th></th>
          <th>50</th>
          <th></th>
          <th>100</th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td></td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
        </tr>
        <tr>
          <td>Function</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>1. is_absorbing (ChainoPy)</td>
          <td>97.3ns</td>
          <td>2.46ns</td>
          <td>91.8ns</td>
          <td>0.329ns</td>
          <td>98ns</td>
          <td>0.4ns</td>
        </tr>
        <tr>
          <td>1. is_absorbing (PyDTMC)</td>
          <td>386ns</td>
          <td>5.79ns</td>
          <td>402ns</td>
          <td>2.01ns</td>
          <td>417ns</td>
          <td>3ns</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <list list-type="bullet">
    <list-item>
      <p><monospace>stationary_dist</monospace> vs
      <monospace>pi</monospace> Methods</p>
    </list-item>
  </list>
  <table-wrap>
    <table>
      <colgroup>
        <col width="22%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
      </colgroup>
      <thead>
        <tr>
          <th>Transition-Matrix Size</th>
          <th>10</th>
          <th></th>
          <th>50</th>
          <th></th>
          <th>100</th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td></td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
        </tr>
        <tr>
          <td>Function</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>1. stationary_dist (ChainoPy)</td>
          <td>1.47us</td>
          <td>1.36us</td>
          <td>93.4ns</td>
          <td>5.26ns</td>
          <td>96.6ns</td>
          <td>3.9ns</td>
        </tr>
        <tr>
          <td>1. pi (PyDTMC)</td>
          <td>137us</td>
          <td>12.9us</td>
          <td>395ns</td>
          <td>15.4ns</td>
          <td>398ns</td>
          <td>10.5ns</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <list list-type="bullet">
    <list-item>
      <p><monospace>fit</monospace> vs
      <monospace>fit_sequence</monospace> Method:</p>
    </list-item>
  </list>
  <table-wrap>
    <table>
      <colgroup>
        <col width="22%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
        <col width="12%" />
        <col width="14%" />
      </colgroup>
      <thead>
        <tr>
          <th>Number of Words</th>
          <th>10</th>
          <th></th>
          <th>50</th>
          <th></th>
          <th>100</th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td></td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
          <td>Mean</td>
          <td>St. dev</td>
        </tr>
        <tr>
          <td>Function</td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
        </tr>
        <tr>
          <td>1. fit (ChainoPy)</td>
          <td>116 µs</td>
          <td>5.28 µs</td>
          <td>266 µs</td>
          <td>15 µs</td>
          <td>496 µs</td>
          <td>47.3 µs</td>
        </tr>
        <tr>
          <td>1. fit_sequence (PyDTMC)</td>
          <td>14 ms</td>
          <td>1.74 ms</td>
          <td>14.4 ms</td>
          <td>1.17 ms</td>
          <td>17.3 ms</td>
          <td>2.18 ms</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <list list-type="bullet">
    <list-item>
      <p><monospace>simulate</monospace> Method</p>
    </list-item>
  </list>
  <table-wrap>
    <table>
      <colgroup>
        <col width="25%" />
        <col width="9%" />
        <col width="16%" />
        <col width="19%" />
        <col width="14%" />
        <col width="17%" />
      </colgroup>
      <thead>
        <tr>
          <th>Transition-Matrix Size</th>
          <th>N-Steps</th>
          <th>ChainoPy Mean</th>
          <th>ChainoPy St. dev</th>
          <th>PyDTMC Mean</th>
          <th>PyDTMC St. dev</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>10</td>
          <td>1000</td>
          <td>22.8 ms</td>
          <td>2.32 ms</td>
          <td>28.2 ms</td>
          <td>933 µs</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>86.8 ms</td>
          <td>2.76 ms</td>
          <td>155 ms</td>
          <td>5.25 ms</td>
        </tr>
        <tr>
          <td>50</td>
          <td>1000</td>
          <td>17.6 ms</td>
          <td>1.2 ms</td>
          <td>29.9 ms</td>
          <td>1.09 ms</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>84.5 ms</td>
          <td>4.84 ms</td>
          <td>161 ms</td>
          <td>7.62 ms</td>
        </tr>
        <tr>
          <td>100</td>
          <td>1000</td>
          <td>21.6 ms</td>
          <td>901 µs</td>
          <td>37.4 ms</td>
          <td>3.99 ms</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>110 ms</td>
          <td>11.3 ms</td>
          <td>162 ms</td>
          <td>5.75 ms</td>
        </tr>
        <tr>
          <td>500</td>
          <td>1000</td>
          <td>24 ms</td>
          <td>3.73 ms</td>
          <td>39.6 ms</td>
          <td>6.07 ms</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>112 ms</td>
          <td>6.63 ms</td>
          <td>178 ms</td>
          <td>26.5 ms</td>
        </tr>
        <tr>
          <td>1000</td>
          <td>1000</td>
          <td>26.1 ms</td>
          <td>620 µs</td>
          <td>46.1 ms</td>
          <td>6.47 ms</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>136 ms</td>
          <td>2.49 ms</td>
          <td>188 ms</td>
          <td>2.43 ms</td>
        </tr>
        <tr>
          <td>2500</td>
          <td>1000</td>
          <td>42 ms</td>
          <td>3.77 ms</td>
          <td>59.6 ms</td>
          <td>2.29 ms</td>
        </tr>
        <tr>
          <td></td>
          <td>5000</td>
          <td>209 ms</td>
          <td>16.4 ms</td>
          <td>285 ms</td>
          <td>27.6ms</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <p>Apart from this, we test the
  <monospace>MarkovChainNeuralNetworks</monospace> by training them and
  comparing random walks between the original
  <monospace>MarkovChain</monospace> object and those generated by
  <monospace>MarkovChainNeuralNetworks</monospace> through a
  Histogram.</p>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>In conclusion, ChainoPy offers a Python library for discrete-time
  Markov Chains and includes features for Markov Chain Neural Networks,
  providing a useful tool for researchers and practitioners in
  stochastic analysis with efficient performance.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-awiszus2018markov">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Awiszus</surname><given-names>Maren</given-names></name>
        <name><surname>Rosenhahn</surname><given-names>Bodo</given-names></name>
      </person-group>
      <article-title>Markov chain neural networks</article-title>
      <source>Proceedings of the IEEE conference on computer vision and pattern recognition workshops</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.1109/CVPRW.2018.00293</pub-id>
      <fpage>2180</fpage>
      <lpage>2187</lpage>
    </element-citation>
  </ref>
  <ref id="ref-pydtmc">
    <element-citation>
      <article-title>PyDTMC</article-title>
      <publisher-name>https://github.com/TommasoBelluzzo/PyDTMC</publisher-name>
      <uri>https://github.com/TommasoBelluzzo/PyDTMC</uri>
    </element-citation>
  </ref>
  <ref id="ref-simple-markov">
    <element-citation>
      <article-title>Simple-markov</article-title>
      <publisher-name>https://github.com/Mandragorian/simple-markov</publisher-name>
      <uri>https://github.com/Mandragorian/simple-markov</uri>
    </element-citation>
  </ref>
  <ref id="ref-mchmm">
    <element-citation>
      <article-title>Mchmm</article-title>
      <publisher-name>https://github.com/maximtrp/mchmm</publisher-name>
      <uri>https://github.com/maximtrp/mchmm</uri>
    </element-citation>
  </ref>
  <ref id="ref-behnel2010cython">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Behnel</surname><given-names>Stefan</given-names></name>
        <name><surname>Bradshaw</surname><given-names>Robert</given-names></name>
        <name><surname>Citro</surname><given-names>Craig</given-names></name>
        <name><surname>Dalcin</surname><given-names>Lisandro</given-names></name>
        <name><surname>Seljebotn</surname><given-names>Dag Sverre</given-names></name>
        <name><surname>Smith</surname><given-names>Kurt</given-names></name>
      </person-group>
      <article-title>Cython: The best of both worlds</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <volume>13</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2010.118</pub-id>
      <fpage>31</fpage>
      <lpage>39</lpage>
    </element-citation>
  </ref>
  <ref id="ref-harris2020array">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>Charles R</given-names></name>
        <name><surname>Millman</surname><given-names>K Jarrod</given-names></name>
        <name><surname>Van Der Walt</surname><given-names>Stéfan J</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Wieser</surname><given-names>Eric</given-names></name>
        <name><surname>Taylor</surname><given-names>Julian</given-names></name>
        <name><surname>Berg</surname><given-names>Sebastian</given-names></name>
        <name><surname>Smith</surname><given-names>Nathaniel J</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-virtanen2020scipy">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>Pauli</given-names></name>
        <name><surname>Gommers</surname><given-names>Ralf</given-names></name>
        <name><surname>Oliphant</surname><given-names>Travis E</given-names></name>
        <name><surname>Haberland</surname><given-names>Matt</given-names></name>
        <name><surname>Reddy</surname><given-names>Tyler</given-names></name>
        <name><surname>Cournapeau</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>Peterson</surname><given-names>Pearu</given-names></name>
        <name><surname>Weckesser</surname><given-names>Warren</given-names></name>
        <name><surname>Bright</surname><given-names>Jonathan</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in python</article-title>
      <source>Nature methods</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ansel2024pytorch">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Ansel</surname><given-names>Jason</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>He</surname><given-names>Horace</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Jain</surname><given-names>Animesh</given-names></name>
        <name><surname>Voznesensky</surname><given-names>Michael</given-names></name>
        <name><surname>Bao</surname><given-names>Bin</given-names></name>
        <name><surname>Bell</surname><given-names>Peter</given-names></name>
        <name><surname>Berard</surname><given-names>David</given-names></name>
        <name><surname>Burovski</surname><given-names>Evgeni</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch 2: Faster machine learning through dynamic python bytecode transformation and graph compilation</article-title>
      <source>Proceedings of the 29th ACM international conference on architectural support for programming languages and operating systems, volume 2</source>
      <year iso-8601-date="2024">2024</year>
      <pub-id pub-id-type="doi">10.1145/3620665.3640366</pub-id>
      <fpage>929</fpage>
      <lpage>947</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rosenthal1995convergence">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rosenthal</surname><given-names>Jeffrey S</given-names></name>
      </person-group>
      <article-title>Convergence rates for markov chains</article-title>
      <source>Siam Review</source>
      <publisher-name>SIAM</publisher-name>
      <year iso-8601-date="1995">1995</year>
      <volume>37</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1137/1037083</pub-id>
      <fpage>387</fpage>
      <lpage>405</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hamilton2010regime">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Hamilton</surname><given-names>James D</given-names></name>
      </person-group>
      <article-title>Regime switching models</article-title>
      <source>Macroeconometrics and time series analysis</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.1057/9780230280830_23</pub-id>
      <fpage>202</fpage>
      <lpage>209</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
