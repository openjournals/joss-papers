<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20230413T083110-cdd672df576f31c4e603037483321ba8cc050658</doi_batch_id>
    <timestamp>20230413083110</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>04</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>84</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Crowsetta: A Python tool to work with any format for
annotating animal vocalizations and bioacoustics data.</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>David</given_name>
            <surname>Nicholson</surname>
            <ORCID>https://orcid.org/0000-0002-4261-4719</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>04</month>
          <day>13</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5338</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05338</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.7781587</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5338</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05338</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05338</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05338.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="wirthlin_modular_2019">
            <article_title>A Modular Approach to Vocal Learning:
Disentangling the Diversity of a Complex Behavioral
Trait</article_title>
            <author>Wirthlin</author>
            <journal_title>Neuron</journal_title>
            <issue>1</issue>
            <volume>104</volume>
            <doi>10.1016/j.neuron.2019.09.036</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Wirthlin, M., Chang, E. F.,
Knörnschild, M., Krubitzer, L. A., Mello, C. V., Miller, C. T.,
Pfenning, A. R., Vernes, S. C., Tchernichovski, O., &amp; Yartsev, M. M.
(2019). A Modular Approach to Vocal Learning: Disentangling the
Diversity of a Complex Behavioral Trait. Neuron, 104(1), 87–99.
https://doi.org/10.1016/j.neuron.2019.09.036</unstructured_citation>
          </citation>
          <citation key="sainburg_toward_2021">
            <article_title>Toward a Computational Neuroethology of Vocal
Communication: From Bioacoustics to Neurophysiology, Emerging Tools and
Future Directions</article_title>
            <author>Sainburg</author>
            <journal_title>Frontiers in Behavioral
Neuroscience</journal_title>
            <volume>15</volume>
            <doi>10.3389/fnbeh.2021.811737</doi>
            <issn>1662-5153</issn>
            <cYear>2021</cYear>
            <unstructured_citation>Sainburg, T., &amp; Gentner, T. Q.
(2021). Toward a Computational Neuroethology of Vocal Communication:
From Bioacoustics to Neurophysiology, Emerging Tools and Future
Directions. Frontiers in Behavioral Neuroscience, 15, 811737.
https://doi.org/10.3389/fnbeh.2021.811737</unstructured_citation>
          </citation>
          <citation key="stowell_computational_2022">
            <article_title>Computational bioacoustics with deep
learning: A review and roadmap</article_title>
            <author>Stowell</author>
            <cYear>2022</cYear>
            <unstructured_citation>Stowell, D. (2022). Computational
bioacoustics with deep learning: A review and roadmap.
46.</unstructured_citation>
          </citation>
          <citation key="hauser_faculty_2002">
            <article_title>The Faculty of Language: What Is It, Who Has
It, and How Did It Evolve?</article_title>
            <author>Hauser</author>
            <journal_title>Science</journal_title>
            <issue>5598</issue>
            <volume>298</volume>
            <doi>10.1126/science.298.5598.1569</doi>
            <issn>0036-8075</issn>
            <cYear>2002</cYear>
            <unstructured_citation>Hauser, M. D., Chomsky, N., &amp;
Fitch, W. T. (2002). The Faculty of Language: What Is It, Who Has It,
and How Did It Evolve? Science, 298(5598), 1569–1579.
https://doi.org/10.1126/science.298.5598.1569</unstructured_citation>
          </citation>
          <citation key="fukuzawa_computational_2022">
            <article_title>Computational methods for a generalised
acoustics analysis workflow: A thesis presented in partial fulfilment of
the requirements for the degree of Master of Science in Computer Science
at Massey University, Auckland, New Zealand</article_title>
            <author>Fukuzawa</author>
            <cYear>2022</cYear>
            <unstructured_citation>Fukuzawa, Y. (2022). Computational
methods for a generalised acoustics analysis workflow: A thesis
presented in partial fulfilment of the requirements for the degree of
Master of Science in Computer Science at Massey University, Auckland,
New Zealand [{PhD} {Thesis}]. Massey University.</unstructured_citation>
          </citation>
          <citation key="goffinet_low-dimensional_2021">
            <article_title>Low-dimensional learned feature spaces
quantify individual and group differences in vocal
repertoires</article_title>
            <author>Goffinet</author>
            <journal_title>eLife</journal_title>
            <volume>10</volume>
            <doi>10.7554/eLife.67855</doi>
            <issn>2050-084X</issn>
            <cYear>2021</cYear>
            <unstructured_citation>Goffinet, J., Brudner, S., Mooney,
R., &amp; Pearson, J. (2021). Low-dimensional learned feature spaces
quantify individual and group differences in vocal repertoires. eLife,
10, e67855. https://doi.org/10.7554/eLife.67855</unstructured_citation>
          </citation>
          <citation key="paul_boersma_praat_2021">
            <article_title>Praat: Doing phonetics by
computer</article_title>
            <author>Paul Boersma</author>
            <doi>10.1097/aud.0b013e31821473f7</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Paul Boersma, &amp; David Weenink.
(2021). Praat: Doing phonetics by computer.
https://doi.org/10.1097/aud.0b013e31821473f7</unstructured_citation>
          </citation>
          <citation key="audacity_team_audacity_2019">
            <article_title>Audacity</article_title>
            <author>Audacity Team</author>
            <cYear>2019</cYear>
            <unstructured_citation>Audacity Team. (2019). Audacity.
https://www.audacityteam.org/</unstructured_citation>
          </citation>
          <citation key="program_raven_2016">
            <article_title>Raven Lite: Interactive Sound Analysis
Software (Version 2.0)</article_title>
            <author>Program</author>
            <cYear>2016</cYear>
            <unstructured_citation>Program, B. R. (2016). Raven Lite:
Interactive Sound Analysis Software (Version 2.0). The Cornell Lab of
Ornithology Ithaca, NY.</unstructured_citation>
          </citation>
          <citation key="charif_raven_2006">
            <article_title>Raven Lite 1.0 user’s guide</article_title>
            <author>Charif</author>
            <journal_title>Cornell Laboratory of Ornithology, Ithaca,
NY</journal_title>
            <cYear>2006</cYear>
            <unstructured_citation>Charif, R., Ponirakis, D., &amp;
Krein, T. (2006). Raven Lite 1.0 user’s guide. Cornell Laboratory of
Ornithology, Ithaca, NY.</unstructured_citation>
          </citation>
          <citation key="mcgregor_shared_2022">
            <article_title>Shared mechanisms of auditory and
non-auditory vocal learning in the songbird brain</article_title>
            <author>McGregor</author>
            <journal_title>eLife</journal_title>
            <volume>11</volume>
            <doi>10.7554/eLife.75691</doi>
            <issn>2050-084X</issn>
            <cYear>2022</cYear>
            <unstructured_citation>McGregor, J. N., Grassler, A. L.,
Jaffe, P. I., Jacob, A. L., Brainard, M. S., &amp; Sober, S. J. (2022).
Shared mechanisms of auditory and non-auditory vocal learning in the
songbird brain. eLife, 11, e75691.
https://doi.org/10.7554/eLife.75691</unstructured_citation>
          </citation>
          <citation key="provost_impacts_2022">
            <article_title>The impacts of fine-tuning, phylogenetic
distance, and sample size on big-data bioacoustics</article_title>
            <author>Provost</author>
            <journal_title>PLOS ONE</journal_title>
            <issue>12</issue>
            <volume>17</volume>
            <doi>10.1371/journal.pone.0278522</doi>
            <issn>1932-6203</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Provost, K. L., Yang, J., &amp;
Carstens, B. C. (2022). The impacts of fine-tuning, phylogenetic
distance, and sample size on big-data bioacoustics. PLOS ONE, 17(12),
e0278522.
https://doi.org/10.1371/journal.pone.0278522</unstructured_citation>
          </citation>
          <citation key="cohen_automated_2022">
            <article_title>Automated annotation of birdsong with a
neural network that segments spectrograms</article_title>
            <author>Cohen</author>
            <journal_title>Elife</journal_title>
            <volume>11</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Cohen, Y., Nicholson, D. A.,
Sanchioni, A., Mallaber, E. K., Skidanova, V., &amp; Gardner, T. J.
(2022). Automated annotation of birdsong with a neural network that
segments spectrograms. Elife, 11, e63853.</unstructured_citation>
          </citation>
          <citation key="cohen_tweetynet_2023">
            <article_title>Tweetynet</article_title>
            <author>Cohen</author>
            <doi>10.5281/zenodo.7627197</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Cohen, Y., &amp; Nicholson, D.
(2023). Tweetynet. Zenodo.
https://doi.org/10.5281/zenodo.7627197</unstructured_citation>
          </citation>
          <citation key="nicholson_vak_2022">
            <article_title>Vak</article_title>
            <author>Nicholson</author>
            <doi>10.5281/zenodo.6808839</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Nicholson, D., &amp; Cohen, Y.
(2022). Vak. Zenodo.
https://doi.org/10.5281/zenodo.6808839</unstructured_citation>
          </citation>
          <citation key="mcfee_pump_nodate">
            <article_title>PUMP UP THE JAMS: V0.2 AND
BEYOND</article_title>
            <author>McFee</author>
            <unstructured_citation>McFee, B., Humphrey, E. J., Nieto,
O., Salamon, J., Bittner, R., Forsyth, J., &amp; Bello, J. P. (n.d.).
PUMP UP THE JAMS: V0.2 AND BEYOND. 8.</unstructured_citation>
          </citation>
          <citation key="humphrey_jams_2014">
            <article_title>JAMS: A JSON ANNOTATED MUSIC SPECIFICATION
FOR REPRODUCIBLE MIR RESEARCH</article_title>
            <author>Humphrey</author>
            <cYear>2014</cYear>
            <unstructured_citation>Humphrey, E. J., Salamon, J., Nieto,
O., Forsyth, J., Bittner, R. M., &amp; Bello, J. P. (2014). JAMS: A JSON
ANNOTATED MUSIC SPECIFICATION FOR REPRODUCIBLE MIR RESEARCH.
6.</unstructured_citation>
          </citation>
          <citation key="roch_tethys_nodate">
            <article_title>Tethys: A workbench and database for passive
acoustic metadata</article_title>
            <author>Roch</author>
            <unstructured_citation>Roch, M. A., Baumann-Pickering, S.,
Batchelor, H., Širovi, A., Berchok, C. L., Cholewiak, D., Oleson, E. M.,
&amp; Soldevilla, M. S. (n.d.). Tethys: A workbench and database for
passive acoustic metadata. 5.</unstructured_citation>
          </citation>
          <citation key="dragly_experimental_2018">
            <article_title>Experimental Directory Structure (Exdir): An
Alternative to HDF5 Without Introducing a New File
Format</article_title>
            <author>Dragly</author>
            <journal_title>Frontiers in Neuroinformatics</journal_title>
            <volume>12</volume>
            <doi>10.3389/fninf.2018.00016</doi>
            <issn>1662-5196</issn>
            <cYear>2018</cYear>
            <unstructured_citation>Dragly, S.-A., Hobbi Mobarhan, M.,
Lepperød, M. E., Tennøe, S., Fyhn, M., Hafting, T., &amp;
Malthe-Sørenssen, A. (2018). Experimental Directory Structure (Exdir):
An Alternative to HDF5 Without Introducing a New File Format. Frontiers
in Neuroinformatics, 12.
https://doi.org/10.3389/fninf.2018.00016</unstructured_citation>
          </citation>
          <citation key="cohen_recent_2022">
            <article_title>Recent Advances at the Interface of
Neuroscience and Artificial Neural Networks</article_title>
            <author>Cohen</author>
            <journal_title>Journal of Neuroscience</journal_title>
            <issue>45</issue>
            <volume>42</volume>
            <doi>10.1523/JNEUROSCI.1503-22.2022</doi>
            <issn>0270-6474</issn>
            <cYear>2022</cYear>
            <unstructured_citation>Cohen, Y., Engel, T. A., Langdon, C.,
Lindsay, G. W., Ott, T., Peters, M. A. K., Shine, J. M.,
Breton-Provencher, V., &amp; Ramaswamy, S. (2022). Recent Advances at
the Interface of Neuroscience and Artificial Neural Networks. Journal of
Neuroscience, 42(45), 8514–8523.
https://doi.org/10.1523/JNEUROSCI.1503-22.2022</unstructured_citation>
          </citation>
          <citation key="baskauf_tdwgac_2022">
            <article_title>Tdwg/ac: Audubon Core standard 2022-02-23
version</article_title>
            <author>Baskauf</author>
            <doi>10.5281/zenodo.6590205</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Baskauf, S., Desmet, P., Klazenga,
N., Blum, S., Baker, E., Morris, B., Webbink, K., danstowell, Döring,
M., &amp; Junior, M. (2022). Tdwg/ac: Audubon Core standard 2022-02-23
version. Zenodo.
https://doi.org/10.5281/zenodo.6590205</unstructured_citation>
          </citation>
          <citation key="recalde_pykanto_nodate">
            <article_title>Pykanto: A python library to accelerate
research on wild bird song</article_title>
            <author>Recalde</author>
            <unstructured_citation>Recalde, N. M. (n.d.). Pykanto: A
python library to accelerate research on wild bird
song.</unstructured_citation>
          </citation>
          <citation key="wilkinson_fair_2016">
            <article_title>The FAIR Guiding Principles for scientific
data management and stewardship</article_title>
            <author>Wilkinson</author>
            <journal_title>Scientific Data</journal_title>
            <issue>1</issue>
            <volume>3</volume>
            <doi>10.1038/sdata.2016.18</doi>
            <issn>2052-4463</issn>
            <cYear>2016</cYear>
            <unstructured_citation>Wilkinson, M. D., Dumontier, M.,
Aalbersberg, Ij. J., Appleton, G., Axton, M., Baak, A., Blomberg, N.,
Boiten, J.-W., Silva Santos, L. B. da, Bourne, P. E., Bouwman, J.,
Brookes, A. J., Clark, T., Crosas, M., Dillo, I., Dumon, O., Edmunds,
S., Evelo, C. T., Finkers, R., … Mons, B. (2016). The FAIR Guiding
Principles for scientific data management and stewardship. Scientific
Data, 3(1), 160018.
https://doi.org/10.1038/sdata.2016.18</unstructured_citation>
          </citation>
          <citation key="coffey_deepsqueak_2019">
            <article_title>DeepSqueak: A deep learning-based system for
detection and analysis of ultrasonic vocalizations</article_title>
            <author>Coffey</author>
            <journal_title>Neuropsychopharmacology</journal_title>
            <issue>5</issue>
            <volume>44</volume>
            <doi>10.1038/s41386-018-0303-6</doi>
            <issn>0893-133X</issn>
            <cYear>2019</cYear>
            <unstructured_citation>Coffey, K. R., Marx, R. E., &amp;
Neumaier, J. F. (2019). DeepSqueak: A deep learning-based system for
detection and analysis of ultrasonic vocalizations.
Neuropsychopharmacology, 44(5), 859–868.
https://doi.org/10.1038/s41386-018-0303-6</unstructured_citation>
          </citation>
          <citation key="sainburg_finding_2020">
            <article_title>Finding, visualizing, and quantifying latent
structure across diverse animal vocal repertoires</article_title>
            <author>Sainburg</author>
            <journal_title>PLOS Computational Biology</journal_title>
            <issue>10</issue>
            <volume>16</volume>
            <doi>10.1371/journal.pcbi.1008228</doi>
            <issn>1553-7358</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Sainburg, T., Thielk, M., &amp;
Gentner, T. Q. (2020). Finding, visualizing, and quantifying latent
structure across diverse animal vocal repertoires. PLOS Computational
Biology, 16(10), e1008228.
https://doi.org/10.1371/journal.pcbi.1008228</unstructured_citation>
          </citation>
          <citation key="steinfath_fast_2021">
            <article_title>Fast and accurate annotation of acoustic
signals with deep neural networks</article_title>
            <author>Steinfath</author>
            <journal_title>eLife</journal_title>
            <volume>10</volume>
            <doi>10.7554/eLife.68837</doi>
            <issn>2050-084X</issn>
            <cYear>2021</cYear>
            <unstructured_citation>Steinfath, E., Palacios-Muñoz, A.,
Rottschäfer, J. R., Yuezak, D., &amp; Clemens, J. (2021). Fast and
accurate annotation of acoustic signals with deep neural networks.
eLife, 10, e68837.
https://doi.org/10.7554/eLife.68837</unstructured_citation>
          </citation>
          <citation key="araya-salas_rraven_2020">
            <article_title>Rraven: Connecting R and Raven bioacoustic
software. R package version 1.0.9.</article_title>
            <author>Araya-Salas</author>
            <cYear>2020</cYear>
            <unstructured_citation>Araya-Salas, M. (2020). Rraven:
Connecting R and Raven bioacoustic software. R package version
1.0.9.</unstructured_citation>
          </citation>
          <citation key="jadoul_introducing_2018-1">
            <article_title>Introducing Parselmouth: A Python interface
to Praat</article_title>
            <author>Jadoul</author>
            <journal_title>Journal of Phonetics</journal_title>
            <volume>71</volume>
            <doi>10.1016/j.wocn.2018.07.001</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Jadoul, Y., Thompson, B., &amp; Boer,
B. de. (2018). Introducing Parselmouth: A Python interface to Praat.
Journal of Phonetics, 71, 1–15.
https://doi.org/10.1016/j.wocn.2018.07.001</unstructured_citation>
          </citation>
          <citation key="haupert_scikit-maadscikit-maad_2022">
            <article_title>Scikit-maad/scikit-maad: Stable Release :
v1.3.12</article_title>
            <author>HAUPERT</author>
            <doi>10.5281/zenodo.7324324</doi>
            <cYear>2022</cYear>
            <unstructured_citation>HAUPERT, S., Ulloa, J. S., Gil, J. F.
L., scikit-maad, &amp; Suarez, G. A. P. (2022). Scikit-maad/scikit-maad:
Stable Release : v1.3.12. Zenodo.
https://doi.org/10.5281/zenodo.7324324</unstructured_citation>
          </citation>
          <citation key="buschmeier_textgridtools_nodate">
            <article_title>TEXTGRIDTOOLS: A TEXTGRID PROCESSING AND
ANALYSIS TOOLKIT FOR PYTHON</article_title>
            <author>Buschmeier</author>
            <unstructured_citation>Buschmeier, H., &amp; Włodarczak, M.
(n.d.). TEXTGRIDTOOLS: A TEXTGRID PROCESSING AND ANALYSIS TOOLKIT FOR
PYTHON.</unstructured_citation>
          </citation>
          <citation key="berman_measuring_2018">
            <article_title>Measuring behavior across
scales</article_title>
            <author>Berman</author>
            <journal_title>BMC Biology</journal_title>
            <issue>1</issue>
            <volume>16</volume>
            <doi>10.1186/s12915-018-0494-7</doi>
            <issn>1741-7007</issn>
            <cYear>2018</cYear>
            <unstructured_citation>Berman, G. J. (2018). Measuring
behavior across scales. BMC Biology, 16(1), 23.
https://doi.org/10.1186/s12915-018-0494-7</unstructured_citation>
          </citation>
          <citation key="pereira_quantifying_2020">
            <article_title>Quantifying behavior to understand the
brain</article_title>
            <author>Pereira</author>
            <journal_title>Nature Neuroscience</journal_title>
            <issue>12</issue>
            <volume>23</volume>
            <doi>10.1038/s41593-020-00734-z</doi>
            <issn>1546-1726</issn>
            <cYear>2020</cYear>
            <unstructured_citation>Pereira, T. D., Shaevitz, J. W.,
&amp; Murthy, M. (2020). Quantifying behavior to understand the brain.
Nature Neuroscience, 23(12), 1537–1549.
https://doi.org/10.1038/s41593-020-00734-z</unstructured_citation>
          </citation>
          <citation key="recalde_pykanto_2023">
            <article_title>Pykanto: A python library to accelerate
research on wild bird song</article_title>
            <author>Recalde</author>
            <doi>10.48550/arXiv.2302.10340</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Recalde, N. M. (2023). Pykanto: A
python library to accelerate research on wild bird song. arXiv.
https://doi.org/10.48550/arXiv.2302.10340</unstructured_citation>
          </citation>
          <citation key="reback2020pandas">
            <article_title>Pandas-dev/pandas: pandas</article_title>
            <author>team</author>
            <doi>10.5281/zenodo.3509134</doi>
            <cYear>2020</cYear>
            <unstructured_citation>team, T. pandas development. (2020).
Pandas-dev/pandas: pandas (latest). Zenodo.
https://doi.org/10.5281/zenodo.3509134</unstructured_citation>
          </citation>
          <citation key="mckinney-proc-scipy-2010">
            <article_title>Data Structures for Statistical Computing in
Python</article_title>
            <author>McKinney</author>
            <journal_title>Proceedings of the 9th Python in Science
Conference</journal_title>
            <doi>10.25080/Majora-92bf1922-00a</doi>
            <cYear>2010</cYear>
            <unstructured_citation>McKinney, Wes. (2010). Data
Structures for Statistical Computing in Python. In Stéfan van der Walt
&amp; Jarrod Millman (Eds.), Proceedings of the 9th Python in Science
Conference (pp. 56–61).
https://doi.org/10.25080/Majora-92bf1922-00a</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
