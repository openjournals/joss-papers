<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5338</article-id>
<article-id pub-id-type="doi">10.21105/joss.05338</article-id>
<title-group>
<article-title>Crowsetta: A Python tool to work with any format for
annotating animal vocalizations and bioacoustics data.</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4261-4719</contrib-id>
<name>
<surname>Nicholson</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Independent Research, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-03-07">
<day>7</day>
<month>3</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>84</issue>
<fpage>5338</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>animal acoustic communication</kwd>
<kwd>bioacoustics</kwd>
<kwd>Python</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Studying how animals communicate with sound allows researchers to
  answer a wide range of questions, from “What species of birds live in
  this area?”, to “How do mice mothers protect their young?”. Some
  animals learn their vocalizations, and by studying the vocal behavior
  of these animals we can investigate questions like “How did speech
  evolve?”. Answering these questions require computational methods and
  big team science across many disciplines, such as ecology, ethology,
  bioacoustics, psychology, neuroscience, linguistics, and genomics.</p>
  <p>Analyses of animal acoustic communication often require annotating
  the sounds animals make. To annotate animal sounds, researchers
  typically use graphical user interfaces (GUIs), that enable them to
  annotate audio and/or spectrograms. Such annotations usually include
  the times when sound events start and stop, and labels that assign
  each sound to some set of classes chosen by the annotator. GUI
  applications save the annotations in many different file formats. This
  Python package, crowsetta, can parse the most widely used formats, and
  it provides software abstractions that make it easy to extend the
  library to parse new formats. In this way, crowsetta allows
  researchers to work with data annotated in a wider variety of formats.
  Additionally, crowsetta helps users convert annotations to simple file
  formats, such as csv files, that do not require detailed knowledge of
  the annotation format itself. This facilitates loading the annotations
  with widely used libraries for data analysis (e.g., Pandas in Python),
  and also promotes sharing data. Overall, crowsetta supports the
  interdisciplinary collaboration required for the study of animal
  acoustic communication.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Studying how animals communicate with sound allows researchers to
  answer a wide range of questions. For example: “What is the language
  faculty, and how did it evolve?”
  (<xref alt="Hauser et al., 2002" rid="ref-hauser_faculty_2002" ref-type="bibr">Hauser
  et al., 2002</xref>), and “What is the basis of learned vocalizations
  in animals?”
  (<xref alt="Wirthlin et al., 2019" rid="ref-wirthlin_modular_2019" ref-type="bibr">Wirthlin
  et al., 2019</xref>). Answering these questions will require big team
  science and true interdisciplinary collaborations
  (<xref alt="Hauser et al., 2002" rid="ref-hauser_faculty_2002" ref-type="bibr">Hauser
  et al., 2002</xref>;
  <xref alt="Wirthlin et al., 2019" rid="ref-wirthlin_modular_2019" ref-type="bibr">Wirthlin
  et al., 2019</xref>). The methods used to answer these questions are
  becoming ever more computational and data driven. As one example of
  such methods, deep learning models have recently proliferated in the
  field of bioacoustics
  (<xref alt="Stowell, 2022" rid="ref-stowell_computational_2022" ref-type="bibr">Stowell,
  2022</xref>) and adjacent fields like animal behavior
  (<xref alt="Berman, 2018" rid="ref-berman_measuring_2018" ref-type="bibr">Berman,
  2018</xref>;
  <xref alt="Pereira et al., 2020" rid="ref-pereira_quantifying_2020" ref-type="bibr">Pereira
  et al., 2020</xref>), specifically studies of vocal behavior
  (<xref alt="Sainburg &amp; Gentner, 2021" rid="ref-sainburg_toward_2021" ref-type="bibr">Sainburg
  &amp; Gentner, 2021</xref>). These models have also become common in
  the field of neuroscience
  (<xref alt="Cohen, Engel, et al., 2022" rid="ref-cohen_recent_2022" ref-type="bibr">Cohen,
  Engel, et al., 2022</xref>), and more specifically in studies of the
  neural bases of vocal communication
  (<xref alt="Coffey et al., 2019" rid="ref-coffey_deepsqueak_2019" ref-type="bibr">Coffey
  et al., 2019</xref>;
  <xref alt="Cohen, Nicholson, et al., 2022" rid="ref-cohen_automated_2022" ref-type="bibr">Cohen,
  Nicholson, et al., 2022</xref>;
  <xref alt="Goffinet et al., 2021" rid="ref-goffinet_low-dimensional_2021" ref-type="bibr">Goffinet
  et al., 2021</xref>;
  <xref alt="Sainburg et al., 2020" rid="ref-sainburg_finding_2020" ref-type="bibr">Sainburg
  et al., 2020</xref>;
  <xref alt="Steinfath et al., 2021" rid="ref-steinfath_fast_2021" ref-type="bibr">Steinfath
  et al., 2021</xref>). The performance of these data-driven deep
  learning models depends crucially on annotated data. High-quality
  annotations from humans are needed to train supervised learning models
  to predict annotations
  (<xref alt="Coffey et al., 2019" rid="ref-coffey_deepsqueak_2019" ref-type="bibr">Coffey
  et al., 2019</xref>;
  <xref alt="Cohen, Nicholson, et al., 2022" rid="ref-cohen_automated_2022" ref-type="bibr">Cohen,
  Nicholson, et al., 2022</xref>;
  <xref alt="Steinfath et al., 2021" rid="ref-steinfath_fast_2021" ref-type="bibr">Steinfath
  et al., 2021</xref>), or to provide inputs to unsupervised models that
  perform dimensionality reduction and compute measures of similarity
  (<xref alt="Goffinet et al., 2021" rid="ref-goffinet_low-dimensional_2021" ref-type="bibr">Goffinet
  et al., 2021</xref>;
  <xref alt="Sainburg et al., 2020" rid="ref-sainburg_finding_2020" ref-type="bibr">Sainburg
  et al., 2020</xref>). In spite of this clear-cut need for high
  quality, easily accessible human annotations of animal acoustic
  communication data, it is surprisingly difficult to work with these
  annotations in code. Ironically, researchers studying animal acoustic
  communication face challenges when communicating with each other.</p>
  <p>This barrier arises in part because of the lack of a generalized
  schema for annotating audio datasets, which in turn results in a
  proliferation of annotation formats. A standardized format for
  annotations would greatly facilitate interoperability, one of the
  guiding FAIR principles
  (<xref alt="Wilkinson et al., 2016" rid="ref-wilkinson_fair_2016" ref-type="bibr">Wilkinson
  et al., 2016</xref>) There are some existing schema for bioacoustics
  datasets that include annotations
  (<xref alt="Baskauf et al., 2022" rid="ref-baskauf_tdwgac_2022" ref-type="bibr">Baskauf
  et al., 2022</xref>;
  <xref alt="Fukuzawa, 2022" rid="ref-fukuzawa_computational_2022" ref-type="bibr">Fukuzawa,
  2022</xref>;
  <xref alt="Recalde, n.d." rid="ref-recalde_pykanto_nodate" ref-type="bibr">Recalde,
  n.d.</xref>,
  <xref alt="2023" rid="ref-recalde_pykanto_2023" ref-type="bibr">2023</xref>;
  <xref alt="Roch et al., n.d." rid="ref-roch_tethys_nodate" ref-type="bibr">Roch
  et al., n.d.</xref>) but there has been no broad effort at
  standardization. Likewise, in animal behavior and neuroscience,
  specifically in areas that study acoustic communication, there have
  been proposals for datasets structures, file formats, and databases
  schema, e.g. Dragly et al.
  (<xref alt="2018" rid="ref-dragly_experimental_2018" ref-type="bibr">2018</xref>),
  that would by necessity include annotations. But again there has been
  little formalization across research groups and again, no wider effort
  to make these interoperable with audio annotations more generally. The
  most direct work on standardization that I am aware of is the Json
  Annotated Music Specification (JAMS)
  (<xref alt="Humphrey et al., 2014" rid="ref-humphrey_jams_2014" ref-type="bibr">Humphrey
  et al., 2014</xref>;
  <xref alt="McFee et al., n.d." rid="ref-mcfee_pump_nodate" ref-type="bibr">McFee
  et al., n.d.</xref>), a standard for annotation in music information
  retrieval research. One implementation of this standard has been
  provided in a Python library of the same name
  (<xref alt="Humphrey et al., 2014" rid="ref-humphrey_jams_2014" ref-type="bibr">Humphrey
  et al., 2014</xref>). These issues were discussed at the first Audio
  Across Domains (AudioXd) conference
  (<ext-link ext-link-type="uri" xlink:href="https://kitzeslab.github.io/audioxd/">https://kitzeslab.github.io/audioxd/</ext-link>),
  and a generalizable schema for audio annotation is being proposed (in
  preparation). In the meantime, the lack of standardization creates a
  clear need for software tools that provide a sort of interoperability
  layer.</p>
  <p>Currently, there is little tooling available that makes it easy to
  convert between annotation formats, and to share annotations in widely
  used simple formats such as a csv file. Many tools have been developed
  to work with specific formats, e.g., Praat Textgrid
  (<xref alt="Buschmeier &amp; Włodarczak, n.d." rid="ref-buschmeier_textgridtools_nodate" ref-type="bibr">Buschmeier
  &amp; Włodarczak, n.d.</xref>;
  <xref alt="Jadoul et al., 2018" rid="ref-jadoul_introducing_2018-1" ref-type="bibr">Jadoul
  et al., 2018</xref>) or Raven selection tables saved as text files (in
  Python
  (<xref alt="HAUPERT et al., 2022" rid="ref-haupert_scikit-maadscikit-maad_2022" ref-type="bibr">HAUPERT
  et al., 2022</xref>), and in R
  (<xref alt="Araya-Salas, 2020" rid="ref-araya-salas_rraven_2020" ref-type="bibr">Araya-Salas,
  2020</xref>)), but to the best of my knowledge there are no tools that
  focus specifically on interoperability of formats. This lack of
  tooling stands in contrast to the clear-cut need for researchers to be
  able to collaborate across disciplines when working with annotated
  audio datasets.</p>
  <p>Crowsetta addresses the clear need for a tool that allows for
  interoperability between the many existing annotation formats, and
  that makes it possible to flexibly access annotations within Python
  for development of downstream applications. Crowsetta also meets the
  needs of researchers to easily share annotations and to use them
  within Python for imperative code, e.g., scientist-coder scripts used
  to fit statistical models or analyze behavior. To address these needs
  all these needs, the package has built-in support for many widely used
  formats such as Audacity
  (<xref alt="Audacity Team, 2019" rid="ref-audacity_team_audacity_2019" ref-type="bibr">Audacity
  Team, 2019</xref>) label tracks, Praat
  (<xref alt="Paul Boersma &amp; David Weenink, 2021" rid="ref-paul_boersma_praat_2021" ref-type="bibr">Paul
  Boersma &amp; David Weenink, 2021</xref>) TextGrid files, and Raven
  (<xref alt="Charif et al., 2006" rid="ref-charif_raven_2006" ref-type="bibr">Charif
  et al., 2006</xref>;
  <xref alt="Program, 2016" rid="ref-program_raven_2016" ref-type="bibr">Program,
  2016</xref>) selection tables exported to text files. The design of
  crowsetta also focuses on interoperability. It allows researchers to
  convert annotations loaded into built-in formats to more generic
  formats: for example, a generic “sequence-like” format that can
  represent annotated sequences of speech and animal vocalizations. This
  generic format can be saved as a csv file, making data easier to share
  and easier to work with through widely-used data analysis libraries
  like Pandas
  (<xref alt="McKinney, 2010" rid="ref-mckinney-proc-scipy-2010" ref-type="bibr">McKinney,
  2010</xref>;
  <xref alt="team, 2020" rid="ref-reback2020pandas" ref-type="bibr">team,
  2020</xref>), In this way crowsetta minimizes the need for specialized
  knowledge of tool-specific formats. In sum, the package provides a
  Pythonic way to work with annotation formats for animal vocalizations
  and bioacoustics data.</p>
  <p>Originally, crowsetta was developed for use with vak
  (<xref alt="Nicholson &amp; Cohen, 2022" rid="ref-nicholson_vak_2022" ref-type="bibr">Nicholson
  &amp; Cohen, 2022</xref>), a neural network framework for researchers
  studying animal acoustic communication. Crowsetta made it possible to
  work with several annotation formats when using vak to benchmark a
  neural network architecture that automates annotation of birdsong,
  TweetyNet
  (<xref alt="Cohen, Nicholson, et al., 2022" rid="ref-cohen_automated_2022" ref-type="bibr">Cohen,
  Nicholson, et al., 2022</xref>;
  <xref alt="Cohen &amp; Nicholson, 2023" rid="ref-cohen_tweetynet_2023" ref-type="bibr">Cohen
  &amp; Nicholson, 2023</xref>). Since then, crowsetta has been used in
  tandem with vak by several research groups in neuroscience
  (<xref alt="Goffinet et al., 2021" rid="ref-goffinet_low-dimensional_2021" ref-type="bibr">Goffinet
  et al., 2021</xref>;
  <xref alt="McGregor et al., 2022" rid="ref-mcgregor_shared_2022" ref-type="bibr">McGregor
  et al., 2022</xref>) and bioacoustics
  (<xref alt="Provost et al., 2022" rid="ref-provost_impacts_2022" ref-type="bibr">Provost
  et al., 2022</xref>).</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>I would like to acknowledge support from Yarden Cohen for
  development of crowsetta as part of the VocalPy ecosystem. I would
  also like to acknowledge contributions to crowsetta during the
  pyOpenSci review, made by the two reviewers, Tessa Rhinehart and
  Sylain Haupert, as well as expert advice from Yannick Jadoul on
  support for TextGrid, with guidance of Chia Marmo as the editor.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-wirthlin_modular_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wirthlin</surname><given-names>Morgan</given-names></name>
        <name><surname>Chang</surname><given-names>Edward F.</given-names></name>
        <name><surname>Knörnschild</surname><given-names>Mirjam</given-names></name>
        <name><surname>Krubitzer</surname><given-names>Leah A.</given-names></name>
        <name><surname>Mello</surname><given-names>Claudio V.</given-names></name>
        <name><surname>Miller</surname><given-names>Cory T.</given-names></name>
        <name><surname>Pfenning</surname><given-names>Andreas R.</given-names></name>
        <name><surname>Vernes</surname><given-names>Sonja C.</given-names></name>
        <name><surname>Tchernichovski</surname><given-names>Ofer</given-names></name>
        <name><surname>Yartsev</surname><given-names>Michael M.</given-names></name>
      </person-group>
      <article-title>A Modular Approach to Vocal Learning: Disentangling the Diversity of a Complex Behavioral Trait</article-title>
      <source>Neuron</source>
      <year iso-8601-date="2019-10">2019</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2021-05-22">2021</year><month>05</month><day>22</day></date-in-citation>
      <volume>104</volume>
      <issue>1</issue>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S0896627319308396</uri>
      <pub-id pub-id-type="doi">10.1016/j.neuron.2019.09.036</pub-id>
      <fpage>87</fpage>
      <lpage>99</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sainburg_toward_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sainburg</surname><given-names>Tim</given-names></name>
        <name><surname>Gentner</surname><given-names>Timothy Q.</given-names></name>
      </person-group>
      <article-title>Toward a Computational Neuroethology of Vocal Communication: From Bioacoustics to Neurophysiology, Emerging Tools and Future Directions</article-title>
      <source>Frontiers in Behavioral Neuroscience</source>
      <year iso-8601-date="2021-12">2021</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-01-22">2022</year><month>01</month><day>22</day></date-in-citation>
      <volume>15</volume>
      <issn>1662-5153</issn>
      <uri>https://www.frontiersin.org/articles/10.3389/fnbeh.2021.811737/full</uri>
      <pub-id pub-id-type="doi">10.3389/fnbeh.2021.811737</pub-id>
      <fpage>811737</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-stowell_computational_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Stowell</surname><given-names>Dan</given-names></name>
      </person-group>
      <article-title>Computational bioacoustics with deep learning: A review and roadmap</article-title>
      <year iso-8601-date="2022">2022</year>
      <fpage>46</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-hauser_faculty_2002">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hauser</surname><given-names>Marc D.</given-names></name>
        <name><surname>Chomsky</surname><given-names>Noam</given-names></name>
        <name><surname>Fitch</surname><given-names>W. Tecumseh</given-names></name>
      </person-group>
      <article-title>The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?</article-title>
      <source>Science</source>
      <year iso-8601-date="2002-11">2002</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-07-06">2022</year><month>07</month><day>06</day></date-in-citation>
      <volume>298</volume>
      <issue>5598</issue>
      <issn>0036-8075</issn>
      <uri>https://www.science.org/doi/10.1126/science.298.5598.1569</uri>
      <pub-id pub-id-type="doi">10.1126/science.298.5598.1569</pub-id>
      <fpage>1569</fpage>
      <lpage>1579</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fukuzawa_computational_2022">
    <element-citation publication-type="thesis">
      <person-group person-group-type="author">
        <name><surname>Fukuzawa</surname><given-names>Yukio</given-names></name>
      </person-group>
      <article-title>Computational methods for a generalised acoustics analysis workflow: A thesis presented in partial fulfilment of the requirements for the degree of Master of Science in Computer Science at Massey University, Auckland, New Zealand</article-title>
      <publisher-name>Massey University</publisher-name>
      <year iso-8601-date="2022">2022</year>
    </element-citation>
  </ref>
  <ref id="ref-goffinet_low-dimensional_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goffinet</surname><given-names>Jack</given-names></name>
        <name><surname>Brudner</surname><given-names>Samuel</given-names></name>
        <name><surname>Mooney</surname><given-names>Richard</given-names></name>
        <name><surname>Pearson</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>Low-dimensional learned feature spaces quantify individual and group differences in vocal repertoires</article-title>
      <source>eLife</source>
      <year iso-8601-date="2021-05">2021</year><month>05</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2022-08-09">2022</year><month>08</month><day>09</day></date-in-citation>
      <volume>10</volume>
      <issn>2050-084X</issn>
      <uri>https://elifesciences.org/articles/67855</uri>
      <pub-id pub-id-type="doi">10.7554/eLife.67855</pub-id>
      <fpage>e67855</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-paul_boersma_praat_2021">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Paul Boersma</string-name>
        <string-name>David Weenink</string-name>
      </person-group>
      <article-title>Praat: Doing phonetics by computer</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>http://www.praat.org/</uri>
      <pub-id pub-id-type="doi">10.1097/aud.0b013e31821473f7</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-audacity_team_audacity_2019">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Audacity Team</string-name>
      </person-group>
      <article-title>Audacity</article-title>
      <year iso-8601-date="2019">2019</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2020-02-22">2020</year><month>02</month><day>22</day></date-in-citation>
      <uri>https://www.audacityteam.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-program_raven_2016">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Program</surname><given-names>Bioacoustics Research</given-names></name>
      </person-group>
      <article-title>Raven Lite: Interactive Sound Analysis Software (Version 2.0)</article-title>
      <publisher-name>The Cornell Lab of Ornithology Ithaca, NY</publisher-name>
      <year iso-8601-date="2016">2016</year>
    </element-citation>
  </ref>
  <ref id="ref-charif_raven_2006">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Charif</surname><given-names>RA</given-names></name>
        <name><surname>Ponirakis</surname><given-names>DW</given-names></name>
        <name><surname>Krein</surname><given-names>TP</given-names></name>
      </person-group>
      <article-title>Raven Lite 1.0 user’s guide</article-title>
      <source>Cornell Laboratory of Ornithology, Ithaca, NY</source>
      <year iso-8601-date="2006">2006</year>
    </element-citation>
  </ref>
  <ref id="ref-mcgregor_shared_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McGregor</surname><given-names>James N</given-names></name>
        <name><surname>Grassler</surname><given-names>Abigail L</given-names></name>
        <name><surname>Jaffe</surname><given-names>Paul I</given-names></name>
        <name><surname>Jacob</surname><given-names>Amanda Louise</given-names></name>
        <name><surname>Brainard</surname><given-names>Michael S</given-names></name>
        <name><surname>Sober</surname><given-names>Samuel J</given-names></name>
      </person-group>
      <article-title>Shared mechanisms of auditory and non-auditory vocal learning in the songbird brain</article-title>
      <source>eLife</source>
      <person-group person-group-type="editor">
        <name><surname>Goldberg</surname><given-names>Jesse H</given-names></name>
        <name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name>
        <name><surname>Giret</surname><given-names>Nicolas</given-names></name>
      </person-group>
      <year iso-8601-date="2022-09">2022</year><month>09</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>11</volume>
      <issn>2050-084X</issn>
      <uri>https://doi.org/10.7554/eLife.75691</uri>
      <pub-id pub-id-type="doi">10.7554/eLife.75691</pub-id>
      <fpage>e75691</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-provost_impacts_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Provost</surname><given-names>Kaiya L.</given-names></name>
        <name><surname>Yang</surname><given-names>Jiaying</given-names></name>
        <name><surname>Carstens</surname><given-names>Bryan C.</given-names></name>
      </person-group>
      <article-title>The impacts of fine-tuning, phylogenetic distance, and sample size on big-data bioacoustics</article-title>
      <source>PLOS ONE</source>
      <year iso-8601-date="2022-12">2022</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>17</volume>
      <issue>12</issue>
      <issn>1932-6203</issn>
      <uri>https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0278522</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pone.0278522</pub-id>
      <fpage>e0278522</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-cohen_automated_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cohen</surname><given-names>Yarden</given-names></name>
        <name><surname>Nicholson</surname><given-names>David Aaron</given-names></name>
        <name><surname>Sanchioni</surname><given-names>Alexa</given-names></name>
        <name><surname>Mallaber</surname><given-names>Emily K</given-names></name>
        <name><surname>Skidanova</surname><given-names>Viktoriya</given-names></name>
        <name><surname>Gardner</surname><given-names>Timothy J</given-names></name>
      </person-group>
      <article-title>Automated annotation of birdsong with a neural network that segments spectrograms</article-title>
      <source>Elife</source>
      <year iso-8601-date="2022">2022</year>
      <volume>11</volume>
      <fpage>e63853</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-cohen_tweetynet_2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Cohen</surname><given-names>Yarden</given-names></name>
        <name><surname>Nicholson</surname><given-names>David</given-names></name>
      </person-group>
      <article-title>Tweetynet</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2023-02">2023</year><month>02</month>
      <uri>https://doi.org/10.5281/zenodo.7627197</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.7627197</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-nicholson_vak_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Nicholson</surname><given-names>David</given-names></name>
        <name><surname>Cohen</surname><given-names>Yarden</given-names></name>
      </person-group>
      <article-title>Vak</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-03">2022</year><month>03</month>
      <uri>https://doi.org/10.5281/zenodo.6808839</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.6808839</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mcfee_pump_nodate">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>McFee</surname><given-names>Brian</given-names></name>
        <name><surname>Humphrey</surname><given-names>Eric J</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
        <name><surname>Salamon</surname><given-names>Justin</given-names></name>
        <name><surname>Bittner</surname><given-names>Rachel</given-names></name>
        <name><surname>Forsyth</surname><given-names>Jon</given-names></name>
        <name><surname>Bello</surname><given-names>Juan P</given-names></name>
      </person-group>
      <article-title>PUMP UP THE JAMS: V0.2 AND BEYOND</article-title>
      <fpage>8</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-humphrey_jams_2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Humphrey</surname><given-names>Eric J</given-names></name>
        <name><surname>Salamon</surname><given-names>Justin</given-names></name>
        <name><surname>Nieto</surname><given-names>Oriol</given-names></name>
        <name><surname>Forsyth</surname><given-names>Jon</given-names></name>
        <name><surname>Bittner</surname><given-names>Rachel M</given-names></name>
        <name><surname>Bello</surname><given-names>Juan P</given-names></name>
      </person-group>
      <article-title>JAMS: A JSON ANNOTATED MUSIC SPECIFICATION FOR REPRODUCIBLE MIR RESEARCH</article-title>
      <year iso-8601-date="2014">2014</year>
      <fpage>6</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-roch_tethys_nodate">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roch</surname><given-names>Marie A</given-names></name>
        <name><surname>Baumann-Pickering</surname><given-names>Simone</given-names></name>
        <name><surname>Batchelor</surname><given-names>Heidi</given-names></name>
        <name><surname>Širovi</surname><given-names>Ana</given-names></name>
        <name><surname>Berchok</surname><given-names>Catherine L</given-names></name>
        <name><surname>Cholewiak</surname><given-names>Danielle</given-names></name>
        <name><surname>Oleson</surname><given-names>Erin M</given-names></name>
        <name><surname>Soldevilla</surname><given-names>Melissa S</given-names></name>
      </person-group>
      <article-title>Tethys: A workbench and database for passive acoustic metadata</article-title>
      <fpage>5</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-dragly_experimental_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dragly</surname><given-names>Svenn-Arne</given-names></name>
        <name><surname>Hobbi Mobarhan</surname><given-names>Milad</given-names></name>
        <name><surname>Lepperød</surname><given-names>Mikkel E.</given-names></name>
        <name><surname>Tennøe</surname><given-names>Simen</given-names></name>
        <name><surname>Fyhn</surname><given-names>Marianne</given-names></name>
        <name><surname>Hafting</surname><given-names>Torkel</given-names></name>
        <name><surname>Malthe-Sørenssen</surname><given-names>Anders</given-names></name>
      </person-group>
      <article-title>Experimental Directory Structure (Exdir): An Alternative to HDF5 Without Introducing a New File Format</article-title>
      <source>Frontiers in Neuroinformatics</source>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>12</volume>
      <issn>1662-5196</issn>
      <uri>https://www.frontiersin.org/articles/10.3389/fninf.2018.00016</uri>
      <pub-id pub-id-type="doi">10.3389/fninf.2018.00016</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-cohen_recent_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cohen</surname><given-names>Yarden</given-names></name>
        <name><surname>Engel</surname><given-names>Tatiana A.</given-names></name>
        <name><surname>Langdon</surname><given-names>Christopher</given-names></name>
        <name><surname>Lindsay</surname><given-names>Grace W.</given-names></name>
        <name><surname>Ott</surname><given-names>Torben</given-names></name>
        <name><surname>Peters</surname><given-names>Megan A. K.</given-names></name>
        <name><surname>Shine</surname><given-names>James M.</given-names></name>
        <name><surname>Breton-Provencher</surname><given-names>Vincent</given-names></name>
        <name><surname>Ramaswamy</surname><given-names>Srikanth</given-names></name>
      </person-group>
      <article-title>Recent Advances at the Interface of Neuroscience and Artificial Neural Networks</article-title>
      <source>Journal of Neuroscience</source>
      <year iso-8601-date="2022-11">2022</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>42</volume>
      <issue>45</issue>
      <issn>0270-6474</issn>
      <uri>https://www.jneurosci.org/content/42/45/8514</uri>
      <pub-id pub-id-type="doi">10.1523/JNEUROSCI.1503-22.2022</pub-id>
      <pub-id pub-id-type="pmid">36351830</pub-id>
      <fpage>8514</fpage>
      <lpage>8523</lpage>
    </element-citation>
  </ref>
  <ref id="ref-baskauf_tdwgac_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Baskauf</surname><given-names>Steve</given-names></name>
        <name><surname>Desmet</surname><given-names>Peter</given-names></name>
        <name><surname>Klazenga</surname><given-names>Niels</given-names></name>
        <name><surname>Blum</surname><given-names>Stan</given-names></name>
        <name><surname>Baker</surname><given-names>Ed</given-names></name>
        <name><surname>Morris</surname><given-names>Bob</given-names></name>
        <name><surname>Webbink</surname><given-names>Kate</given-names></name>
        <string-name>danstowell</string-name>
        <name><surname>Döring</surname><given-names>Markus</given-names></name>
        <name><surname>Junior</surname><given-names>Meshack</given-names></name>
      </person-group>
      <article-title>Tdwg/ac: Audubon Core standard 2022-02-23 version</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-05">2022</year><month>05</month>
      <uri>https://doi.org/10.5281/zenodo.6590205</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.6590205</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-recalde_pykanto_nodate">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Recalde</surname><given-names>Nilo Merino</given-names></name>
      </person-group>
      <article-title>Pykanto: A python library to accelerate research on wild bird song</article-title>
    </element-citation>
  </ref>
  <ref id="ref-wilkinson_fair_2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wilkinson</surname><given-names>Mark D.</given-names></name>
        <name><surname>Dumontier</surname><given-names>Michel</given-names></name>
        <name><surname>Aalbersberg</surname><given-names>IJsbrand Jan</given-names></name>
        <name><surname>Appleton</surname><given-names>Gabrielle</given-names></name>
        <name><surname>Axton</surname><given-names>Myles</given-names></name>
        <name><surname>Baak</surname><given-names>Arie</given-names></name>
        <name><surname>Blomberg</surname><given-names>Niklas</given-names></name>
        <name><surname>Boiten</surname><given-names>Jan-Willem</given-names></name>
        <name><surname>Silva Santos</surname><given-names>Luiz Bonino da</given-names></name>
        <name><surname>Bourne</surname><given-names>Philip E.</given-names></name>
        <name><surname>Bouwman</surname><given-names>Jildau</given-names></name>
        <name><surname>Brookes</surname><given-names>Anthony J.</given-names></name>
        <name><surname>Clark</surname><given-names>Tim</given-names></name>
        <name><surname>Crosas</surname><given-names>Mercè</given-names></name>
        <name><surname>Dillo</surname><given-names>Ingrid</given-names></name>
        <name><surname>Dumon</surname><given-names>Olivier</given-names></name>
        <name><surname>Edmunds</surname><given-names>Scott</given-names></name>
        <name><surname>Evelo</surname><given-names>Chris T.</given-names></name>
        <name><surname>Finkers</surname><given-names>Richard</given-names></name>
        <name><surname>Gonzalez-Beltran</surname><given-names>Alejandra</given-names></name>
        <name><surname>Gray</surname><given-names>Alasdair J. G.</given-names></name>
        <name><surname>Groth</surname><given-names>Paul</given-names></name>
        <name><surname>Goble</surname><given-names>Carole</given-names></name>
        <name><surname>Grethe</surname><given-names>Jeffrey S.</given-names></name>
        <name><surname>Heringa</surname><given-names>Jaap</given-names></name>
        <name><surname>Hoen</surname><given-names>Peter A. C ’t</given-names></name>
        <name><surname>Hooft</surname><given-names>Rob</given-names></name>
        <name><surname>Kuhn</surname><given-names>Tobias</given-names></name>
        <name><surname>Kok</surname><given-names>Ruben</given-names></name>
        <name><surname>Kok</surname><given-names>Joost</given-names></name>
        <name><surname>Lusher</surname><given-names>Scott J.</given-names></name>
        <name><surname>Martone</surname><given-names>Maryann E.</given-names></name>
        <name><surname>Mons</surname><given-names>Albert</given-names></name>
        <name><surname>Packer</surname><given-names>Abel L.</given-names></name>
        <name><surname>Persson</surname><given-names>Bengt</given-names></name>
        <name><surname>Rocca-Serra</surname><given-names>Philippe</given-names></name>
        <name><surname>Roos</surname><given-names>Marco</given-names></name>
        <name><surname>Schaik</surname><given-names>Rene van</given-names></name>
        <name><surname>Sansone</surname><given-names>Susanna-Assunta</given-names></name>
        <name><surname>Schultes</surname><given-names>Erik</given-names></name>
        <name><surname>Sengstag</surname><given-names>Thierry</given-names></name>
        <name><surname>Slater</surname><given-names>Ted</given-names></name>
        <name><surname>Strawn</surname><given-names>George</given-names></name>
        <name><surname>Swertz</surname><given-names>Morris A.</given-names></name>
        <name><surname>Thompson</surname><given-names>Mark</given-names></name>
        <name><surname>Lei</surname><given-names>Johan van der</given-names></name>
        <name><surname>Mulligen</surname><given-names>Erik van</given-names></name>
        <name><surname>Velterop</surname><given-names>Jan</given-names></name>
        <name><surname>Waagmeester</surname><given-names>Andra</given-names></name>
        <name><surname>Wittenburg</surname><given-names>Peter</given-names></name>
        <name><surname>Wolstencroft</surname><given-names>Katherine</given-names></name>
        <name><surname>Zhao</surname><given-names>Jun</given-names></name>
        <name><surname>Mons</surname><given-names>Barend</given-names></name>
      </person-group>
      <article-title>The FAIR Guiding Principles for scientific data management and stewardship</article-title>
      <source>Scientific Data</source>
      <year iso-8601-date="2016-03">2016</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>3</volume>
      <issue>1</issue>
      <issn>2052-4463</issn>
      <uri>https://www.nature.com/articles/sdata201618</uri>
      <pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id>
      <fpage>160018</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-coffey_deepsqueak_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Coffey</surname><given-names>Kevin R.</given-names></name>
        <name><surname>Marx</surname><given-names>Ruby E.</given-names></name>
        <name><surname>Neumaier</surname><given-names>John F.</given-names></name>
      </person-group>
      <article-title>DeepSqueak: A deep learning-based system for detection and analysis of ultrasonic vocalizations</article-title>
      <source>Neuropsychopharmacology</source>
      <year iso-8601-date="2019-04">2019</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>44</volume>
      <issue>5</issue>
      <issn>0893-133X</issn>
      <uri>https://www.nature.com/articles/s41386-018-0303-6</uri>
      <pub-id pub-id-type="doi">10.1038/s41386-018-0303-6</pub-id>
      <fpage>859</fpage>
      <lpage>868</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sainburg_finding_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sainburg</surname><given-names>Tim</given-names></name>
        <name><surname>Thielk</surname><given-names>Marvin</given-names></name>
        <name><surname>Gentner</surname><given-names>Timothy Q.</given-names></name>
      </person-group>
      <article-title>Finding, visualizing, and quantifying latent structure across diverse animal vocal repertoires</article-title>
      <source>PLOS Computational Biology</source>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>16</volume>
      <issue>10</issue>
      <issn>1553-7358</issn>
      <uri>https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008228</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008228</pub-id>
      <fpage>e1008228</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-steinfath_fast_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Steinfath</surname><given-names>Elsa</given-names></name>
        <name><surname>Palacios-Muñoz</surname><given-names>Adrian</given-names></name>
        <name><surname>Rottschäfer</surname><given-names>Julian R</given-names></name>
        <name><surname>Yuezak</surname><given-names>Deniz</given-names></name>
        <name><surname>Clemens</surname><given-names>Jan</given-names></name>
      </person-group>
      <article-title>Fast and accurate annotation of acoustic signals with deep neural networks</article-title>
      <source>eLife</source>
      <person-group person-group-type="editor">
        <name><surname>Calabrese</surname><given-names>Ronald L</given-names></name>
        <name><surname>Egnor</surname><given-names>SE Roian</given-names></name>
        <name><surname>Troyer</surname><given-names>Todd</given-names></name>
      </person-group>
      <year iso-8601-date="2021-11">2021</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>10</volume>
      <issn>2050-084X</issn>
      <uri>https://doi.org/10.7554/eLife.68837</uri>
      <pub-id pub-id-type="doi">10.7554/eLife.68837</pub-id>
      <fpage>e68837</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-araya-salas_rraven_2020">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Araya-Salas</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Rraven: Connecting R and Raven bioacoustic software. R package version 1.0.9.</article-title>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-jadoul_introducing_2018-1">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jadoul</surname><given-names>Yannick</given-names></name>
        <name><surname>Thompson</surname><given-names>Bill</given-names></name>
        <name><surname>Boer</surname><given-names>Bart de</given-names></name>
      </person-group>
      <article-title>Introducing Parselmouth: A Python interface to Praat</article-title>
      <source>Journal of Phonetics</source>
      <year iso-8601-date="2018-11">2018</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-03-09">2023</year><month>03</month><day>09</day></date-in-citation>
      <volume>71</volume>
      <uri>https://linkinghub.elsevier.com/retrieve/pii/S0095447017301389</uri>
      <pub-id pub-id-type="doi">10.1016/j.wocn.2018.07.001</pub-id>
      <fpage>1</fpage>
      <lpage>15</lpage>
    </element-citation>
  </ref>
  <ref id="ref-haupert_scikit-maadscikit-maad_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>HAUPERT</surname><given-names>Sylvain</given-names></name>
        <name><surname>Ulloa</surname><given-names>Juan Sebastian</given-names></name>
        <name><surname>Gil</surname><given-names>Juan Felipe Latorre</given-names></name>
        <string-name>scikit-maad</string-name>
        <name><surname>Suarez</surname><given-names>Gabriel Alejandro Perilla</given-names></name>
      </person-group>
      <article-title>Scikit-maad/scikit-maad: Stable Release : v1.3.12</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-11">2022</year><month>11</month>
      <uri>https://doi.org/10.5281/zenodo.7324324</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.7324324</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-buschmeier_textgridtools_nodate">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Buschmeier</surname><given-names>Hendrik</given-names></name>
        <name><surname>Włodarczak</surname><given-names>Marcin</given-names></name>
      </person-group>
      <article-title>TEXTGRIDTOOLS: A TEXTGRID PROCESSING AND ANALYSIS TOOLKIT FOR PYTHON</article-title>
    </element-citation>
  </ref>
  <ref id="ref-berman_measuring_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Berman</surname><given-names>Gordon J.</given-names></name>
      </person-group>
      <article-title>Measuring behavior across scales</article-title>
      <source>BMC Biology</source>
      <year iso-8601-date="2018-02">2018</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-04-04">2023</year><month>04</month><day>04</day></date-in-citation>
      <volume>16</volume>
      <issue>1</issue>
      <issn>1741-7007</issn>
      <uri>https://doi.org/10.1186/s12915-018-0494-7</uri>
      <pub-id pub-id-type="doi">10.1186/s12915-018-0494-7</pub-id>
      <fpage>23</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-pereira_quantifying_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pereira</surname><given-names>Talmo D.</given-names></name>
        <name><surname>Shaevitz</surname><given-names>Joshua W.</given-names></name>
        <name><surname>Murthy</surname><given-names>Mala</given-names></name>
      </person-group>
      <article-title>Quantifying behavior to understand the brain</article-title>
      <source>Nature Neuroscience</source>
      <year iso-8601-date="2020-12">2020</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-04-04">2023</year><month>04</month><day>04</day></date-in-citation>
      <volume>23</volume>
      <issue>12</issue>
      <issn>1546-1726</issn>
      <uri>https://www.nature.com/articles/s41593-020-00734-z</uri>
      <pub-id pub-id-type="doi">10.1038/s41593-020-00734-z</pub-id>
      <fpage>1537</fpage>
      <lpage>1549</lpage>
    </element-citation>
  </ref>
  <ref id="ref-recalde_pykanto_2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Recalde</surname><given-names>Nilo Merino</given-names></name>
      </person-group>
      <article-title>Pykanto: A python library to accelerate research on wild bird song</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2023-02">2023</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2023-04-04">2023</year><month>04</month><day>04</day></date-in-citation>
      <uri>http://arxiv.org/abs/2302.10340</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2302.10340</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-reback2020pandas">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>team</surname><given-names>The pandas development</given-names></name>
      </person-group>
      <article-title>Pandas-dev/pandas: pandas</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2020-02">2020</year><month>02</month>
      <uri>https://doi.org/10.5281/zenodo.3509134</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.3509134</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mckinney-proc-scipy-2010">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>McKinney</surname></name>
      </person-group>
      <article-title>Data Structures for Statistical Computing in Python</article-title>
      <source>Proceedings of the 9th Python in Science Conference</source>
      <person-group person-group-type="editor">
        <name><surname>Walt</surname></name>
        <name><surname>Millman</surname></name>
      </person-group>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.25080/Majora-92bf1922-00a</pub-id>
      <fpage>56 </fpage>
      <lpage> 61</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
