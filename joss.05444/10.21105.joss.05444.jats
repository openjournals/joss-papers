<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5444</article-id>
<article-id pub-id-type="doi">10.21105/joss.05444</article-id>
<title-group>
<article-title>shmem4py: OpenSHMEM for Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5662-2082</contrib-id>
<name>
<surname>Rogowski</surname>
<given-names>Marcin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-8086-0155</contrib-id>
<name>
<surname>Dalcin</surname>
<given-names>Lisandro</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3181-8190</contrib-id>
<name>
<surname>Hammond</surname>
<given-names>Jeff R.</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4052-7224</contrib-id>
<name>
<surname>Keyes</surname>
<given-names>David E.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>King Abdullah University of Science and Technology, Saudi
Arabia</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>NVIDIA Helsinki Oy, Finland</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-04-13">
<day>13</day>
<month>4</month>
<year>2023</year>
</pub-date>
<volume>8</volume>
<issue>87</issue>
<fpage>5444</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PGAS</kwd>
<kwd>OpenSHMEM</kwd>
<kwd>shared memory</kwd>
<kwd>High Performance Computing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>shmem4py</monospace> brings the Partitioned Global
  Address Space (PGAS) programming model to Python by exposing the
  functionality of the OpenSHMEM Application Programming Interface (API)
  specification. The feature set includes one-sided communication,
  shared memory access, atomic memory operations, and collective
  operations. The Python implementation of
  <monospace>shmem4py</monospace> emphasizes using NumPy arrays,
  providing convenient access to the symmetric memory allocations
  central to OpenSHMEM’s programming model. Thanks to Python’s
  versatility and OpenSHMEM implementations’ focus on performance,
  <monospace>shmem4py</monospace> offers a seamless experience on a
  variety of hardware, from laptops to supercomputers, and for a wide
  range of applications and users. <monospace>shmem4py</monospace> API
  grounds in OpenSHMEM 1.5 specification; however, it also supports
  legacy 1.4 implementations.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>Python applications can be scaled to multiple processes and compute
  nodes in various ways. When working on a single node, the
  <monospace>multiprocessing</monospace> or
  <monospace>concurrent.futures</monospace> packages from the Python
  standard library offer solutions for task-based parallelism. As we
  expand beyond a single node, more advanced frameworks like Dask
  (<xref alt="Rocklin, 2015" rid="ref-dask" ref-type="bibr">Rocklin,
  2015</xref>), Ray
  (<xref alt="Moritz et al., 2018" rid="ref-ray" ref-type="bibr">Moritz
  et al., 2018</xref>) or <monospace>mpi4py.futures</monospace>
  (<xref alt="Rogowski et al., 2023" rid="ref-mpi4py.futures" ref-type="bibr">Rogowski
  et al., 2023</xref>) are commonly used. Typically, these high-level
  frameworks handle interprocess communication transparently to the
  user.</p>
  <p>More challenging applications often require specialized
  communication patterns. In those cases, Python applications can
  leverage communication frameworks originally designed for
  high-performance computing. One such example is
  <monospace>mpi4py</monospace>
  (<xref alt="Dalcin &amp; Fang, 2021" rid="ref-mpi4py" ref-type="bibr">Dalcin
  &amp; Fang, 2021</xref>), which offers MPI bindings for Python.
  <monospace>shmem4py</monospace> adopts a similar approach, providing
  Python bindings to OpenSHMEM
  (<xref alt="Chapman et al., 2010" rid="ref-introducing-shmem" ref-type="bibr">Chapman
  et al., 2010</xref>) implementations with a Python-centric and
  high-level API built on top of a low-level CFFI
  (<xref alt="Rigo &amp; Fijalkowski, 2022" rid="ref-cffi" ref-type="bibr">Rigo
  &amp; Fijalkowski, 2022</xref>) module. This way,
  <monospace>shmem4py</monospace> is accessible to a diverse audience
  while offering a proven programming model with reliable
  performance.</p>
  <p><monospace>shmem4py</monospace> complements
  <monospace>mpi4py</monospace> in the same way that OpenSHMEM
  complements MPI. MPI is extremely popular in high-performance
  computing because of its combination of a rich feature set and
  generality. OpenSHMEM is based on a different philosophy, which
  provides a smaller set of features that better match high-performance
  computing system hardware capabilities. For example, OpenSHMEM
  one-sided communication and atomic operations are aligned with
  networking capabilities such as remote direct memory access (RDMA)
  such that no intervention is required on the remote side
  (<xref alt="Jose et al., 2014" rid="ref-mvapich-openshmem-perf" ref-type="bibr">Jose
  et al., 2014</xref>). In contrast, MPI’s one-sided communication
  functionality includes many more features, some of which are known to
  make implementations based strictly on RDMA more difficult
  (<xref alt="Hammond et al., 2014" rid="ref-oshmpi" ref-type="bibr">Hammond
  et al., 2014</xref>;
  <xref alt="Si et al., 2015" rid="ref-casper" ref-type="bibr">Si et
  al., 2015</xref>). Another aspect of the tradeoff between generality
  and close-to-hardware features is observed in the context of
  specialized processors, such as GPUs. It has been shown that
  OpenSHMEM-like APIs can be implemented natively on GPUs, e.g., NVSHMEM
  (<xref alt="Hsu et al., 2020" rid="ref-nvshmem" ref-type="bibr">Hsu et
  al., 2020</xref>;
  <xref alt="Potluri et al., 2017" rid="ref-nvshmem-ib" ref-type="bibr">Potluri
  et al., 2017</xref>). On the other hand, implementing MPI send and
  receive operations in the same context poses significant challenges.
  In fact, <monospace>shmem4py</monospace> will provide the backbone for
  future extensions supporting inter-GPU communication.</p>
  <p>We envision two distinct groups of users that may be interested in
  <monospace>shmem4py</monospace>. The first, and likely most numerous,
  group includes Python programmers who lack the expertise or the time
  to write low-level code in C and have applications well-suited for the
  PGAS paradigm. The second group comprises high-performance computing
  professionals familiar with OpenSHMEM who want to prototype or port
  parts of their applications to Python. Both groups of users can
  benefit greatly from the ease of development in Python,
  <monospace>shmem4py</monospace>’s convenience functions for
  manipulating NumPy arrays in symmetric memory, and all of OpenSHMEM
  features. We also expect that current users of
  <monospace>mpi4py</monospace> may want to try
  <monospace>shmem4py</monospace> as a complementary communication model
  for the same reasons that OpenSHMEM is used alongside MPI.</p>
</sec>
<sec id="supported-openshmem-implementations">
  <title>Supported OpenSHMEM Implementations</title>
  <p>The <monospace>shmem4py</monospace> package supports, and is tested
  with, all major implementations of the OpenSHMEM specification:</p>
  <list list-type="bullet">
    <list-item>
      <p>Cray OpenSHMEMX</p>
    </list-item>
    <list-item>
      <p>Open MPI OpenSHMEM</p>
    </list-item>
    <list-item>
      <p>Open Source Software Solutions (OSSS) OpenSHMEM</p>
    </list-item>
    <list-item>
      <p>OSHMPI</p>
    </list-item>
    <list-item>
      <p>Sandia OpenSHMEM</p>
    </list-item>
  </list>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors thank the KAUST Supercomputing Laboratory for their
  computing resources.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-introducing-shmem">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Chapman</surname><given-names>Barbara</given-names></name>
        <name><surname>Curtis</surname><given-names>Tony</given-names></name>
        <name><surname>Pophale</surname><given-names>Swaroop</given-names></name>
        <name><surname>Poole</surname><given-names>Stephen</given-names></name>
        <name><surname>Kuehn</surname><given-names>Jeff</given-names></name>
        <name><surname>Koelbel</surname><given-names>Chuck</given-names></name>
        <name><surname>Smith</surname><given-names>Lauren</given-names></name>
      </person-group>
      <article-title>Introducing OpenSHMEM: SHMEM for the PGAS community</article-title>
      <source>Proceedings of the fourth conference on partitioned global address space programming model</source>
      <year iso-8601-date="2010">2010</year>
      <pub-id pub-id-type="doi">10.1145/2020373.2020375</pub-id>
      <fpage>1</fpage>
      <lpage>3</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dask">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rocklin</surname><given-names>Matthew</given-names></name>
      </person-group>
      <article-title>Dask: Parallel computation with blocked algorithms and task scheduling</article-title>
      <source>Proceedings of the 14th Python in Science Conference</source>
      <publisher-name>Citeseer</publisher-name>
      <year iso-8601-date="2015">2015</year>
      <volume>130</volume>
      <pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-013</pub-id>
      <fpage>136</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ray">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
        <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
        <name><surname>Wang</surname><given-names>Stephanie</given-names></name>
        <name><surname>Tumanov</surname><given-names>Alexey</given-names></name>
        <name><surname>Liaw</surname><given-names>Richard</given-names></name>
        <name><surname>Liang</surname><given-names>Eric</given-names></name>
        <name><surname>Elibol</surname><given-names>Melih</given-names></name>
        <name><surname>Yang</surname><given-names>Zongheng</given-names></name>
        <name><surname>Paul</surname><given-names>William</given-names></name>
        <name><surname>Jordan</surname><given-names>Michael I.</given-names></name>
        <name><surname>Stoica</surname><given-names>Ion</given-names></name>
      </person-group>
      <article-title>Ray: A distributed framework for emerging AI applications</article-title>
      <source>13th USENIX symposium on operating systems design and implementation (OSDI 18)</source>
      <publisher-name>USENIX Association</publisher-name>
      <publisher-loc>Carlsbad, CA</publisher-loc>
      <year iso-8601-date="2018-10">2018</year><month>10</month>
      <isbn>978-1-939133-08-3</isbn>
      <uri>https://www.usenix.org/conference/osdi18/presentation/moritz</uri>
      <fpage>561</fpage>
      <lpage>577</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mpi4py">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dalcin</surname><given-names>Lisandro</given-names></name>
        <name><surname>Fang</surname><given-names>Yao-Lung L</given-names></name>
      </person-group>
      <article-title>mpi4py: Status update after 12 years of development</article-title>
      <source>Computing in Science &amp; Engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>23</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1109/MCSE.2021.3083216</pub-id>
      <fpage>47</fpage>
      <lpage>54</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mpi4py.futures">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rogowski</surname><given-names>Marcin</given-names></name>
        <name><surname>Aseeri</surname><given-names>Samar</given-names></name>
        <name><surname>Keyes</surname><given-names>David</given-names></name>
        <name><surname>Dalcin</surname><given-names>Lisandro</given-names></name>
      </person-group>
      <article-title>mpi4py.futures: MPI-based asynchronous task execution for Python</article-title>
      <source>IEEE Transactions on Parallel and Distributed Systems</source>
      <year iso-8601-date="2023">2023</year>
      <volume>34</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1109/TPDS.2022.3225481</pub-id>
      <fpage>611</fpage>
      <lpage>622</lpage>
    </element-citation>
  </ref>
  <ref id="ref-casper">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Si</surname><given-names>Min</given-names></name>
        <name><surname>Peña</surname><given-names>Antonio J.</given-names></name>
        <name><surname>Hammond</surname><given-names>Jeff</given-names></name>
        <name><surname>Balaji</surname><given-names>Pavan</given-names></name>
        <name><surname>Takagi</surname><given-names>Masamichi</given-names></name>
        <name><surname>Ishikawa</surname><given-names>Yutaka</given-names></name>
      </person-group>
      <article-title>Casper: An asynchronous progress model for MPI RMA on many-core architectures</article-title>
      <source>2015 IEEE international parallel and distributed processing symposium</source>
      <year iso-8601-date="2015">2015</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/IPDPS.2015.35</pub-id>
      <fpage>665</fpage>
      <lpage>676</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nvshmem-ib">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Potluri</surname><given-names>Sreeram</given-names></name>
        <name><surname>Goswami</surname><given-names>Anshuman</given-names></name>
        <name><surname>Rossetti</surname><given-names>Davide</given-names></name>
        <name><surname>Newburn</surname><given-names>C. J.</given-names></name>
        <name><surname>Venkata</surname><given-names>Manjunath Gorentla</given-names></name>
        <name><surname>Imam</surname><given-names>Neena</given-names></name>
      </person-group>
      <article-title>GPU-centric communication on NVIDIA GPU clusters with InfiniBand: A case study with OpenSHMEM</article-title>
      <source>2017 IEEE 24th international conference on high performance computing (HiPC)</source>
      <year iso-8601-date="2017">2017</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/HiPC.2017.00037</pub-id>
      <fpage>253</fpage>
      <lpage>262</lpage>
    </element-citation>
  </ref>
  <ref id="ref-nvshmem">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hsu</surname><given-names>Chung-Hsing</given-names></name>
        <name><surname>Imam</surname><given-names>Neena</given-names></name>
        <name><surname>Langer</surname><given-names>Akhil</given-names></name>
        <name><surname>Potluri</surname><given-names>Sreeram</given-names></name>
        <name><surname>Newburn</surname><given-names>Chris J.</given-names></name>
      </person-group>
      <article-title>An initial assessment of NVSHMEM for high performance computing</article-title>
      <source>2020 IEEE international parallel and distributed processing symposium workshops (IPDPSW)</source>
      <year iso-8601-date="2020">2020</year>
      <volume></volume>
      <pub-id pub-id-type="doi">10.1109/IPDPSW50202.2020.00104</pub-id>
      <fpage>1</fpage>
      <lpage>10</lpage>
    </element-citation>
  </ref>
  <ref id="ref-oshmpi">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hammond</surname><given-names>Jeff R</given-names></name>
        <name><surname>Ghosh</surname><given-names>Sayan</given-names></name>
        <name><surname>Chapman</surname><given-names>Barbara M</given-names></name>
      </person-group>
      <article-title>Implementing OpenSHMEM using MPI-3 one-sided communication</article-title>
      <source>OpenSHMEM and related technologies. Experiences, implementations, and tools: First workshop, OpenSHMEM 2014, annapolis, MD, USA, march 4-6, 2014. Proceedings 1</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.1007/978-3-319-05215-1</pub-id>
      <fpage>44</fpage>
      <lpage>58</lpage>
    </element-citation>
  </ref>
  <ref id="ref-mvapich-openshmem-perf">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Jose</surname><given-names>Jithin</given-names></name>
        <name><surname>Zhang</surname><given-names>Jie</given-names></name>
        <name><surname>Venkatesh</surname><given-names>Akshay</given-names></name>
        <name><surname>Potluri</surname><given-names>Sreeram</given-names></name>
        <name><surname>Panda</surname><given-names>Dhabaleswar K</given-names></name>
      </person-group>
      <article-title>A comprehensive performance evaluation of OpenSHMEM libraries on InfiniBand clusters</article-title>
      <source>OpenSHMEM and related technologies. Experiences, implementations, and tools: First workshop, OpenSHMEM 2014, annapolis, MD, USA, march 4-6, 2014. Proceedings 1</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <pub-id pub-id-type="doi">10.1007/978-3-319-05215-1_2</pub-id>
      <fpage>14</fpage>
      <lpage>28</lpage>
    </element-citation>
  </ref>
  <ref id="ref-cffi">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Rigo</surname><given-names>Armin</given-names></name>
        <name><surname>Fijalkowski</surname><given-names>Maciej</given-names></name>
      </person-group>
      <article-title>CFFI: C Foreign Function Interface for Python</article-title>
      <source>Read the Docs</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://cffi.readthedocs.io/</uri>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
