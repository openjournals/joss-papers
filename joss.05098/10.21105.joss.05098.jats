<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5098</article-id>
<article-id pub-id-type="doi">10.21105/joss.05098</article-id>
<title-group>
<article-title>Brainchop: In-browser MRI volumetric segmentation and
rendering</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5365-242X</contrib-id>
<name>
<surname>Masoud</surname>
<given-names>Mohamed</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1816-2011</contrib-id>
<name>
<surname>Hu</surname>
<given-names>Farfalla</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0040-0365</contrib-id>
<name>
<surname>Plis</surname>
<given-names>Sergey</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Tri-institutional Center for Translational Research in
Neuroimaging and Data Science (TReNDS), Georgia State University,
Georgia Institute of Technology, Emory University, Atlanta, United
States of America</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Computer Science, Georgia State University,
Atlanta, United States of America</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-12-09">
<day>9</day>
<month>12</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>83</issue>
<fpage>5098</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>visualization</kwd>
<kwd>Web machine learning</kwd>
<kwd>Magnetic resonance imaging</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><ext-link ext-link-type="uri" xlink:href="https://github.com/neuroneural/brainchop">Brainchop</ext-link>
  is a web-based tool that allows scientists and clinicians to perform
  volumetric analysis of structural Magnetic Resonance Imaging (MRI)
  using pre-trained deep learning models, without the need for technical
  expertise or setup of AI solutions. It is the first browser-based
  front-end MRI segmentation tool that can process full brain volumes in
  a single pass, using the lightweight and reliable Meshnet model
  (<xref alt="Fedorov et al., 2017" rid="ref-FedorovU003A2017" ref-type="bibr">Fedorov
  et al., 2017</xref>) modified to incorporate volumetric dilated
  convolutions
  (<xref alt="Yu &amp; Koltun, 2016" rid="ref-YuU003A2016" ref-type="bibr">Yu
  &amp; Koltun, 2016</xref>) for increased accuracy and modest
  computational requirements. In addition to offering extensibility
  through an open-source framework, Brainchop also addresses privacy
  concerns by performing client-side processing and leveraging hardware
  acceleration across different brands and architectures.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Accurate segmentation of brain tissue from MRI volumes is a crucial
  step in various brain imaging analysis pipelines, with applications
  ranging from surgical planning and measurement of brain changes to
  visualization of anatomical structures. However, few professionals
  possess both domain expertise and machine learning skills, as well as
  the necessary computational infrastructure, to take advantage of the
  benefits of artificial intelligence (AI)-assisted neuroimaging. This
  limitation is particularly pronounced in rural areas and developing
  countries. To address these challenges, we developed brainchop, an
  in-browser machine learning platform for volumetric neuroimaging that
  offers high accessibility, scalability, low latency, ease of use, lack
  of installation requirements, and cross-platform operation, while also
  preserving end-user data privacy. Unlike traditional web applications,
  brainchop operates within the browser environment to process the
  end-users data locally on their machines without the need for
  server-side processing and the associated privacy and portability
  issues. Currently available tools for 3D Neuroimaging such as
  Freesurfer
  (<xref alt="Dale et al., 1999" rid="ref-FSU003A1999" ref-type="bibr">Dale
  et al., 1999</xref>) and SPM
  (<xref alt="Friston et al., 1994" rid="ref-FristonU003A1994" ref-type="bibr">Friston
  et al., 1994</xref>) are not web-based tools hence they lack the
  aforementioned features that the browser can bring to Neuroimaging
  applications. For many clinicians, setting up neuroimaging pipelines
  is a technological barrier, and providing those pipelines through the
  browser will help democratize these computational approaches.</p>
</sec>
<sec id="pipeline">
  <title>Pipeline</title>
  <p>In order to deploy the PyTorch MeshNet model in the browser, there
  is a need to convert it first to tensorflow.js
  (<xref alt="Smilkov et al., 2019" rid="ref-tensorflow-js" ref-type="bibr">Smilkov
  et al., 2019</xref>). Brainchop has a pre-processing pipeline,
  full-volume and sub-volumes inference options, 3D input/output
  rendering, and post-processing capability as illustrated in
  <xref alt="[fig:Brainchop-Pipleline]" rid="figU003ABrainchop-Pipleline">[fig:Brainchop-Pipleline]</xref>
  for the tool high-level architecture.</p>
  <fig>
    <caption><p>Brainchop high-level
    architecture.<styled-content id="figU003ABrainchop-Pipleline"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/BrainchopPipleline.png" />
  </fig>
</sec>
<sec id="preprocessing">
  <title>Preprocessing</title>
  <p>Brainchop is designed to support T1 weighted MRI volume
  segmentation. The input is read in Nifti format
  (<xref alt="“NIfTI Reader,” 2017" rid="ref-NIfTI-Reader" ref-type="bibr">“NIfTI
  Reader,” 2017</xref>). T1 image needs to be in shape 256x256x256,
  scaled, and resampled to 1 mm isotropic voxels as a preprocessing step
  for proper results. This preprocessing can be made in brainchop by
  using mri_convert.js which uses Pyodide
  (<xref alt="Droettboom et al., 2022" rid="ref-pyodide_2022" ref-type="bibr">Droettboom
  et al., 2022</xref>) to deploy the conform function used by FastSurfer
  (<xref alt="Henschel et al., 2020" rid="ref-HenschelU003A2020" ref-type="bibr">Henschel
  et al., 2020</xref>) for reshaping, scaling, and re-sampling MRI T1
  raw image data as shown in
  <xref alt="[fig:Convert-Enhance-Pipeline]" rid="figU003AConvert-Enhance-Pipeline">[fig:Convert-Enhance-Pipeline]</xref>
  (a).</p>
  <fig>
    <caption><p>Brainchop preprocessing pipeline (a) Conform operation
    (b) MRI enhancement
    operations.<styled-content id="figU003AConvert-Enhance-Pipeline"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/ConvertAndEnhancePipeline.png" />
  </fig>
  <p>The rest of the preprocessing pipeline is to remove input noisy
  voxels and enhance input volume intensities to improve the
  segmentation accuracy
  <xref alt="[fig:Convert-Enhance-Pipeline]" rid="figU003AConvert-Enhance-Pipeline">[fig:Convert-Enhance-Pipeline]</xref>
  (b). In addition, brainchop also supports MRI tissue cropping to speed
  up the inference process and lower memory use.</p>
</sec>
<sec id="inference-model">
  <title>Inference Model</title>
  <p>The advantage of MeshNet small size is due to its simple
  architecture and using dilated convolution in which a typical model
  for the segmentation task can be constructed with few layers as shown
  in
  <xref alt="[fig:MeshNet-Arch]" rid="figU003AMeshNet-Arch">[fig:MeshNet-Arch]</xref>.</p>
  <fig>
    <caption><p>MeshNet
    architecture.<styled-content id="figU003AMeshNet-Arch"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/MeshNetArch.png" />
  </fig>
  <p>While the MeshNet model has fewer parameters compared to the
  classical segmentation model U-Net, it also can achieve a competitive
  DICE score as shown in
  <xref alt="[tab:Table-1]" rid="tabU003ATable-1">[tab:Table-1]</xref>.</p>
  <boxed-text id="tabU003ATable-1">
    <table-wrap>
      <caption>
        <p>Segmentation models performance.</p>
      </caption>
      <table>
        <thead>
          <tr>
            <th align="left"><bold>Model</bold></th>
            <th align="center"><bold>Inference Speed</bold></th>
            <th align="center"><bold>Model Size</bold></th>
            <th align="center"><bold>Macro DICE</bold></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">MeshNet GMWM</td>
            <td align="center">116 subvolumes/sec</td>
            <td align="center">.89 mb</td>
            <td align="center">0.96</td>
          </tr>
          <tr>
            <td align="left">U-Net GMWM</td>
            <td align="center">13 subvolumes/sec</td>
            <td align="center">288 mb</td>
            <td align="center">0.96</td>
          </tr>
          <tr>
            <td align="left">MeshNet GMWM (full brain model)</td>
            <td align="center">0.001 sec/volume</td>
            <td align="center">0.022 mb</td>
            <td align="center">0.96</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </boxed-text>
</sec>
<sec id="results">
  <title>Results</title>
  <p>Multiple pre-trained models are available with brainchop for
  full-volume and sub-volumes inference including brain masking, gray
  matter white matter (GMWM) segmentation models, in addition to brain
  atlas models for 50 cortical regions and 104 cortical and subcortical
  structures as shown in
  <xref alt="[fig:Gallery-1]" rid="figU003AGallery-1">[fig:Gallery-1]</xref>.</p>
  <fig>
    <caption><p>Brainchop
    outputs.<styled-content id="figU003AGallery-1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/Gallery.png" />
  </fig>
  <p>Normally 3D noisy regions can result from the inference process due
  to possible bias, variance and irreducible error (e.g. noise with
  data). To remove these noisy volumes we designed a 3D connected
  components algorithm to filter out those noisy regions.</p>
  <p>Papaya
  (<xref alt="Papaya dev. team, 2019" rid="ref-Papaya" ref-type="bibr">Papaya
  dev. team, 2019</xref>) viewer is used to visualize the input and
  output images, and a composite operation is also provided to
  subjectively verify the output image accuracy compared to the
  input.</p>
  <p>Also, brainchop supports 3D real-time rendering of the input and
  output volume by using Three.js
  (<xref alt="Cabello et al., 2010" rid="ref-threejs" ref-type="bibr">Cabello
  et al., 2010</xref>) with the capability of Region of Interest (ROI)
  selection as shown in
  <xref alt="[fig:rendering]" rid="figU003Arendering">[fig:rendering]</xref>.</p>
  <fig>
    <caption><p>Brainchop rendering segmentation output in
    3D.<styled-content id="figU003Arendering"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/Output.png" />
  </fig>
</sec>
<sec id="code-availability">
  <title>Code availability</title>
  <p>The source code is publicly accessible in a GitHub repository
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroneural/brainchop">https://github.com/neuroneural/brainchop</ext-link>)
  with detailed step-by-step
  <ext-link ext-link-type="uri" xlink:href="https://github.com/neuroneural/brainchop/wiki">documentation</ext-link>.
  Built-in models also are provided with the brainchop
  <ext-link ext-link-type="uri" xlink:href="https://neuroneural.github.io/brainchop/">live
  demo</ext-link>.</p>
</sec>
<sec id="author-contributions">
  <title>Author contributions</title>
  <p>We describe contributions to this paper using the CRediT taxonomy
  (<xref alt="Brand et al., 2015" rid="ref-credit" ref-type="bibr">Brand
  et al., 2015</xref>). Writing – Original Draft: M.M., F.H., and S.P.;
  Writing – Review &amp; Editing: M.M., and S.P.; Conceptualization and
  methodology: M.M., and S.P.; Software and data curation: M.M., F.H.,
  and S.P.; Validation: M.M., and S.P.; Resources: S.P.; Visualization:
  M.M., and F.H.; Supervision: S.P.; Project Administration: M.M., and
  S.P.; Funding Acquisition: S.P.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>The authors would like to thank Kevin Wang and Alex Fedorov for
  discussions and pre-trained Meshnet models.</p>
  <p>This work was funded by the NIH grant RF1MH121885. Additional
  support from NIH R01MH123610, R01EB006841 and NSF 2112455.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-FedorovU003A2017">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fedorov</surname><given-names>A.</given-names></name>
        <name><surname>Johnson</surname><given-names>J.</given-names></name>
        <name><surname>Damaraju</surname><given-names>E.</given-names></name>
        <name><surname>Ozerin</surname><given-names>A.</given-names></name>
        <name><surname>Calhoun</surname><given-names>V.</given-names></name>
        <name><surname>Plis</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>End-to-end learning of brain tissue segmentation from imperfect labeling</article-title>
      <source>IEEE International Joint Conference on Neural Networks (IJCNN)</source>
      <year iso-8601-date="2017-05">2017</year><month>05</month>
      <pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966333</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-YuU003A2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Yu</surname><given-names>F.</given-names></name>
        <name><surname>Koltun</surname><given-names>V.</given-names></name>
      </person-group>
      <article-title>Multi-scale context aggregation by dilated convolutions</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2016-04">2016</year><month>04</month>
      <pub-id pub-id-type="doi">10.48550/arXiv.1511.07122</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tensorflow-js">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Smilkov</surname><given-names>D.</given-names></name>
        <name><surname>Thorat</surname><given-names>N.</given-names></name>
        <name><surname>Assogba</surname><given-names>Y.</given-names></name>
        <name><surname>Nicholson</surname><given-names>C.</given-names></name>
        <name><surname>Kreeger</surname><given-names>N.</given-names></name>
        <name><surname>Yu</surname><given-names>P.</given-names></name>
        <name><surname>Cai</surname><given-names>S.</given-names></name>
        <name><surname>Nielsen</surname><given-names>E.</given-names></name>
        <name><surname>Soegel</surname><given-names>D.</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>TensorFlow.js: Machine learning for the web and beyond</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1901.05350</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-NIfTI-Reader">
    <element-citation>
      <article-title>NIfTI reader</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2017">2017</year>
      <uri>https://github.com/rii-mango/NIFTI-Reader-JS</uri>
    </element-citation>
  </ref>
  <ref id="ref-pyodide_2022">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Droettboom</surname><given-names>M.</given-names></name>
        <name><surname>Chatham</surname><given-names>H.</given-names></name>
        <name><surname>Yurchak</surname><given-names>R.</given-names></name>
        <name><surname>Chua</surname><given-names>D.</given-names></name>
        <name><surname>Choi</surname><given-names>G.</given-names></name>
        <name><surname>Schreiner</surname><given-names>H.</given-names></name>
        <name><surname>Abramowitz</surname><given-names>M.</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pyodide</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2022-04">2022</year><month>04</month>
      <uri>https://doi.org/10.5281/zenodo.6430433</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.6430433</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-HenschelU003A2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Henschel</surname><given-names>L.</given-names></name>
        <name><surname>Conjeti</surname><given-names>S.</given-names></name>
        <name><surname>Estrada</surname><given-names>S.</given-names></name>
        <name><surname>Diers</surname><given-names>K.</given-names></name>
        <name><surname>Fischl</surname><given-names>B.</given-names></name>
        <name><surname>Reuter</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>FastSurfer - a fast and accurate deep learning based neuroimaging pipeline</article-title>
      <source>Neuroimage</source>
      <year iso-8601-date="2020-10">2020</year><month>10</month>
      <pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117012</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Papaya">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>Papaya dev. team</string-name>
      </person-group>
      <article-title>Papaya</article-title>
      <publisher-name>GitHub Pages</publisher-name>
      <year iso-8601-date="2019-03">2019</year><month>03</month>
      <uri>http://rii-mango.github.io/Papaya</uri>
    </element-citation>
  </ref>
  <ref id="ref-threejs">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Cabello</surname><given-names>R.</given-names></name>
        <name><surname>Qualia</surname><given-names>A.</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Three.js</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2010">2010</year>
      <uri>https://github.com/mrdoob/three.js</uri>
    </element-citation>
  </ref>
  <ref id="ref-credit">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brand</surname><given-names>Amy</given-names></name>
        <name><surname>Allen</surname><given-names>Liz</given-names></name>
        <name><surname>Altman</surname><given-names>Micah</given-names></name>
        <name><surname>Hlava</surname><given-names>Marjorie</given-names></name>
        <name><surname>Scott</surname><given-names>Jo</given-names></name>
      </person-group>
      <article-title>Beyond authorship: Attribution, contribution, collaboration, and credit</article-title>
      <source>Learned Publishing</source>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <issue>2</issue>
      <uri>https://onlinelibrary.wiley.com/doi/abs/10.1087/20150211</uri>
      <pub-id pub-id-type="doi">10.1087/20150211</pub-id>
      <fpage>151</fpage>
      <lpage>155</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FristonU003A1994">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Friston</surname><given-names>KJ.</given-names></name>
        <name><surname>Holmes</surname><given-names>AP.</given-names></name>
        <name><surname>Worsley</surname><given-names>KJ.</given-names></name>
        <name><surname>Poline</surname><given-names>JB.</given-names></name>
        <name><surname>Frith</surname><given-names>CD.</given-names></name>
        <name><surname>Frackowiak</surname><given-names>RSJ.</given-names></name>
      </person-group>
      <article-title>Statistical parametric maps in functional imaging: A general linear approach</article-title>
      <source>Human Brain Mapping</source>
      <year iso-8601-date="1994">1994</year>
      <volume>2</volume>
      <issue>4</issue>
      <uri>https://onlinelibrary.wiley.com/doi/10.1002/hbm.460020402</uri>
      <pub-id pub-id-type="doi">10.1002/hbm.460020402</pub-id>
      <fpage>189</fpage>
      <lpage>210</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FSU003A1999">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dale</surname><given-names>AM.</given-names></name>
        <name><surname>Fischl</surname><given-names>B.</given-names></name>
        <name><surname>Sereno</surname><given-names>MI.</given-names></name>
      </person-group>
      <article-title>Cortical surface-based analysis: I. Segmentation and surface reconstruction</article-title>
      <source>Neuroimage</source>
      <year iso-8601-date="1999">1999</year>
      <volume>9</volume>
      <issue>2</issue>
      <issn>1053-8119</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S1053811998903950</uri>
      <pub-id pub-id-type="doi">10.1006/nimg.1998.0395</pub-id>
      <fpage>179</fpage>
      <lpage>194</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
