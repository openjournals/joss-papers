<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>f75f76e59c3f7d773d0af0794c80d518</doi_batch_id>
    <timestamp>20211218175809</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>12</month>
          <year>2021</year>
        </publication_date>
        <journal_volume>
          <volume>6</volume>
        </journal_volume>
        <issue>68</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Phonemizer: Text to Phones Transcription for Multiple Languages in Python</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Mathieu</given_name>
            <surname>Bernard</surname>
            <ORCID>http://orcid.org/0000-0001-7586-7133</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Hadrien</given_name>
            <surname>Titeux</surname>
            <ORCID>http://orcid.org/0000-0002-8511-1644</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>12</month>
          <day>18</day>
          <year>2021</year>
        </publication_date>
        <pages>
          <first_page>3958</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.03958</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.5281/zenodo.5791097”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/3958</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.03958</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.03958</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.03958.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <doi>10.3758/s13428-019-01223-3</doi>
          </citation>
          <citation key="ref2">
            <unstructured_citation>Deep learning for Text to Speech, Mozilla, 2021, GitHub, GitHub repository, https://github.com/mozilla/TTS</unstructured_citation>
          </citation>
          <citation key="ref3">
            <unstructured_citation>Non-autoregressive Transformer based neural network for Text-to-Speech, Ideas Engineering, 2021, GitHub, GitHub repository, https://github.com/as-ideas/TransformerTTS</unstructured_citation>
          </citation>
          <citation key="ref4">
            <unstructured_citation>The Festival Speech Synthesis System, Black, Alan W. and Clark, Rob and Richmond, Korin and Yamagishi, Junichi and Oura, Keiichiro and King, Simon, 2014, 2.4, CSTR, University of Edinburgh, https://www.cstr.ed.ac.uk/projects/festival</unstructured_citation>
          </citation>
          <citation key="ref5">
            <doi>10.5281/zenodo.3549784</doi>
          </citation>
          <citation key="ref6">
            <unstructured_citation>Dunn, Reece H. and Vitolins, Valdis, eSpeak NG speech synthetizer, 2019, 1.50, GitHub, GitHub repository, https://github.com/espeak-ng/espeak-ng</unstructured_citation>
          </citation>
          <citation key="ref7">
            <unstructured_citation>Tits, Noé and Vitolins, Valdis, MBROLA, 3.3, 2019, GitHub, GitHub repostory, https://github.com/numediart/MBROLA</unstructured_citation>
          </citation>
          <citation key="ref8">
            <doi>10.1162/opmi_a_00022</doi>
          </citation>
          <citation key="ref9">
            <doi>10.1109/asru.2017.8268953</doi>
          </citation>
          <citation key="ref10">
            <unstructured_citation>The role of prosody and speech register in word segmentation: A computational modelling perspective, Ludusan, Bogdan and Mazuka, Reiko and Bernard, Mathieu and Cristia, Alejandrina and Dupoux, Emmanuel, Proceedings of the Association for Computational Linguistics, 178–183, 2017</unstructured_citation>
          </citation>
          <citation key="ref11">
            <unstructured_citation>Povey, Daniel and Ghoshal, Arnab and Boulianne, Gilles and Burget, Lukas and Glembek, Ondrej and Goel, Nagendra and Hannemann, Mirko and Motlicek, Petr and Qian, Yanmin and Schwarz, Petr and Silovsky, Jan and Stemmer, Georg and Vesely, Karel, The Kaldi Speech Recognition Toolkit, IEEE 2011 Workshop on Automatic Speech Recognition and Understanding, 2011, IEEE Signal Processing Society</unstructured_citation>
          </citation>
          <citation key="ref12">
            <doi>10.21437/Interspeech.2018-1456</doi>
          </citation>
          <citation key="ref13">
            <doi>10.5281/zenodo.9846</doi>
          </citation>
          <citation key="ref14">
            <doi>10.21437/interspeech.2017-1386</doi>
          </citation>
          <citation key="ref15">
            <unstructured_citation>Using forced alignment for sociophonetic research on a minority language, Barth, Danielle and Grama, James and Gonzalez, Simon and Travis, Catherine, University of Pennsylvania Working Papers in Linguistics, 25, 2, 2, 2020</unstructured_citation>
          </citation>
          <citation key="ref16">
            <unstructured_citation>Forced alignment for understudied language varieties: Testing Prosodylab-Aligner with Tongan data, Johnson, Lisa M and Di Paolo, Marianna and Bell, Adrian, 2018, University of Hawaii Press</unstructured_citation>
          </citation>
          <citation key="ref17">
            <doi>10.1371/journal.pone.0237702</doi>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
