<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6533</article-id>
<article-id pub-id-type="doi">10.21105/joss.06533</article-id>
<title-group>
<article-title>IKPLS: Improved Kernel Partial Least Squares and Fast
Cross-Validation Algorithms for Python with CPU and GPU Implementations
Using NumPy and JAX</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7906-4589</contrib-id>
<name>
<surname>Engstr√∏m</surname>
<given-names>Ole-Christian Galbo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9784-7504</contrib-id>
<name>
<surname>Dreier</surname>
<given-names>Erik Schou</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8695-1450</contrib-id>
<name>
<surname>Jespersen</surname>
<given-names>Birthe M√∏ller</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3713-0960</contrib-id>
<name>
<surname>Pedersen</surname>
<given-names>Kim Steenstrup</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>FOSS Analytical A/S, Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Computer Science (DIKU), University of
Copenhagen, Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Department of Food Science (UCPH FOOD), University of
Copenhagen, Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>UCL University College, Denmark</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Natural History Museum of Denmark (NHMD), University of
Copenhagen, Denmark</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-06-22">
<day>22</day>
<month>6</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6533</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PLS</kwd>
<kwd>latent variables</kwd>
<kwd>multivariate statistics</kwd>
<kwd>cross-validation</kwd>
<kwd>deep learning</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The <monospace>ikpls</monospace> software package provides fast and
  efficient tools for PLS (Partial Least Squares) modeling. This package
  is designed to help researchers and practitioners handle PLS modeling
  faster than previously possible - particularly on large datasets. The
  PLS implementations in <monospace>ikpls</monospace> use the fast IKPLS
  (Improved Kernel PLS) algorithms
  (<xref alt="Dayal &amp; MacGregor, 1997" rid="ref-dayal1997improved" ref-type="bibr">Dayal
  &amp; MacGregor, 1997</xref>), providing a substantial speedup
  compared to scikit-learn‚Äôs
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>) PLS implementation, which is based on NIPALS
  (Nonlinear Iterative Partial Least Squares)
  (<xref alt="H. Wold, 1966" rid="ref-wold1966estimation" ref-type="bibr">H.
  Wold, 1966</xref>). The <monospace>ikpls</monospace> package also
  offers an implementation of IKPLS combined with the fast
  cross-validation algorithm by O.-C. G. Engstr√∏m
  (<xref alt="2024" rid="ref-engstr√∏m2024shortcutting" ref-type="bibr">2024</xref>),
  significantly accelerating cross-validation of PLS models - especially
  when using a large number of cross-validation splits.</p>
  <p><monospace>ikpls</monospace> offers NumPy-based CPU and JAX-based
  CPU/GPU/TPU implementations. The JAX implementations are also
  differentiable, allowing seamless integration with deep learning
  techniques. This versatility enables users to handle diverse data
  dimensions efficiently.</p>
  <p>In conclusion, <monospace>ikpls</monospace> empowers researchers
  and practitioners in machine learning, chemometrics, and related
  fields with efficient, scalable, and end-to-end differentiable tools
  for PLS modeling, facilitating optimal component selection and
  preprocessing decisions by offering implementations of</p>
  <list list-type="order">
    <list-item>
      <p>both variants of IKPLS for CPUs;</p>
    </list-item>
    <list-item>
      <p>both variants of IKPLS for GPUs, both of which are end-to-end
      differentiable, allowing integration with deep learning
      models;</p>
    </list-item>
    <list-item>
      <p>IKPLS combined with a cross-validation algorithm that yields a
      substantial speedup compared to the classical cross-validation
      algorithm.</p>
    </list-item>
  </list>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>PLS
  (<xref alt="H. Wold, 1966" rid="ref-wold1966estimation" ref-type="bibr">H.
  Wold, 1966</xref>) is a standard method in machine learning and
  chemometrics. PLS can be used as a regression model, PLS-R (PLS
  regression)
  (<xref alt="S. Wold et al., 1983" rid="ref-wold1983food" ref-type="bibr">S.
  Wold et al., 1983</xref>,
  <xref alt="2001" rid="ref-wold2001pls" ref-type="bibr">2001</xref>),
  or a classification model, PLS-DA (PLS discriminant analysis)
  (<xref alt="Barker &amp; Rayens, 2003" rid="ref-barker2003partial" ref-type="bibr">Barker
  &amp; Rayens, 2003</xref>). PLS takes as input a matrix
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  with dimension <inline-formula><alternatives>
  <tex-math><![CDATA[(N, K)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  of predictor variables and a matrix <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>
  with dimension <inline-formula><alternatives>
  <tex-math><![CDATA[(N, M)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
  of response variables. PLS decomposes <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>
  into <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>
  latent variables (also called components), which are linear
  combinations of the original <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>.
  Choosing the optimal number of components,
  <inline-formula><alternatives>
  <tex-math><![CDATA[A]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>,
  depends on the input data and varies from task to task. Additionally,
  selecting the optimal preprocessing method is challenging to assess
  before model validation S√∏rensen et al.
  (<xref alt="2021" rid="ref-sorensen2021nir" ref-type="bibr">2021</xref>)
  but is required for achieving optimal performance
  (<xref alt="Du et al., 2022" rid="ref-du2022quantitative" ref-type="bibr">Du
  et al., 2022</xref>). The optimal number of components and the optimal
  preprocessing method are typically chosen by cross-validation, which
  may be very computationally expensive. The implementations of the fast
  cross-validation algorithm
  (<xref alt="O.-C. G. Engstr√∏m, 2024" rid="ref-engstr√∏m2024shortcutting" ref-type="bibr">O.-C.
  G. Engstr√∏m, 2024</xref>) will significantly reduce the computational
  cost of cross-validation.</p>
  <p>This work introduces the Python software package,
  <monospace>ikpls</monospace>, with novel, fast implementations of
  IKPLS Algorithm #1 and Algorithm #2 by Dayal &amp; MacGregor
  (<xref alt="1997" rid="ref-dayal1997improved" ref-type="bibr">1997</xref>),
  which have previously been compared with other PLS algorithms and
  shown to be fast
  (<xref alt="Alin, 2009" rid="ref-alin2009comparison" ref-type="bibr">Alin,
  2009</xref>) and numerically stable
  (<xref alt="Andersson, 2009" rid="ref-andersson2009comparison" ref-type="bibr">Andersson,
  2009</xref>). The implementations introduced in this work use NumPy
  (<xref alt="Harris et al., 2020" rid="ref-harris2020array" ref-type="bibr">Harris
  et al., 2020</xref>) and JAX
  (<xref alt="Bradbury et al., 2018" rid="ref-jax2018github" ref-type="bibr">Bradbury
  et al., 2018</xref>). The NumPy implementations can be executed on
  CPUs, and the JAX implementations can be executed on CPUs, GPUs, and
  TPUs. The JAX implementations are also end-to-end differentiable,
  allowing integration into deep learning methods. This work compares
  the execution time of the implementations on input data of varying
  dimensions. It reveals that choosing the implementation that best fits
  the data will yield orders of magnitude faster execution than the
  common NIPALS
  (<xref alt="H. Wold, 1966" rid="ref-wold1966estimation" ref-type="bibr">H.
  Wold, 1966</xref>) implementation of PLS, which is the one implemented
  by scikit-learn
  (<xref alt="Pedregosa et al., 2011" rid="ref-scikit-learn" ref-type="bibr">Pedregosa
  et al., 2011</xref>), an extensive machine learning library for
  Python. With the implementations introduced in this work, choosing the
  optimal number of components and the optimal preprocessing becomes
  much more feasible than previously. Indeed, derivatives of this work
  have previously been applied to do this precisely
  (<xref alt="O.-C. G. Engstr√∏m et al., 2023a" rid="ref-engstrom2023improving" ref-type="bibr">O.-C.
  G. Engstr√∏m et al., 2023a</xref>,
  <xref alt="2023b" rid="ref-engstrom2023analyzing" ref-type="bibr">2023b</xref>).</p>
  <p>Other implementations of other PLS algorithms with NumPy and
  scikit-learn exist, even for more specialized tasks such as multiblock
  PLS
  (<xref alt="Baum &amp; Vermue, 2019" rid="ref-baum2019multiblock" ref-type="bibr">Baum
  &amp; Vermue, 2019</xref>). These implementations, however, are not as
  fast as IKPLS
  (<xref alt="Alin, 2009" rid="ref-alin2009comparison" ref-type="bibr">Alin,
  2009</xref>). Implementations of IKPLS exist in R and MATLAB. To the
  best of the authors‚Äô knowledge, however, there are no Python
  implementations of IKPLS that simultaneously correctly handle all
  possible dimensions of <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  and <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>.
  To the best of the authors‚Äô knowledge, no other PLS algorithms exist
  in JAX, nor do implementations of IKPLS in other frameworks with
  automatic differentiation.</p>
</sec>
<sec id="implementations">
  <title>Implementations</title>
  <p>IKPLS
  (<xref alt="Dayal &amp; MacGregor, 1997" rid="ref-dayal1997improved" ref-type="bibr">Dayal
  &amp; MacGregor, 1997</xref>) comes in two variants: Algorithm #1 and
  Algorithm #2. The implementations compute internal matrices
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{W}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêñ</mml:mi></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  weights) of dimension (K, A), <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{P}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêè</mml:mi></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  loadings) of dimension (K, A), <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Q}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêê</mml:mi></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{Y}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêò</mml:mi></mml:math></alternatives></inline-formula>
  loadings) of dimension (M, A), <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{R}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêë</mml:mi></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  rotations) of dimension (K, A) and a tensor
  <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{B}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêÅ</mml:mi></mml:math></alternatives></inline-formula>
  (regression coefficients) of dimension (A, K, M). Algorithm #1 also
  computes <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{T}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêì</mml:mi></mml:math></alternatives></inline-formula>
  (<inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùêó</mml:mi></mml:math></alternatives></inline-formula>
  scores) of dimension <inline-formula><alternatives>
  <tex-math><![CDATA[(N, A)]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <p>The <monospace>ikpls</monospace> package has been rigorously tested
  for equivalence against scikit-learn‚Äôs NIPALS using NIR spectra data
  from Dreier et al.
  (<xref alt="2022" rid="ref-dreier2022hyperspectral" ref-type="bibr">2022</xref>)
  and scikit-learn‚Äôs PLS test-suite.
  <ext-link ext-link-type="uri" xlink:href="https://github.com/Sm00thix/IKPLS/blob/main/examples/">Examples</ext-link>
  are provided for core functionalities, demonstrating fitting,
  predicting, cross-validating on CPU and GPU, and gradient propagation
  through PLS fitting.</p>
  <sec id="numpy">
    <title>NumPy</title>
    <p><monospace>ikpls</monospace> includes a Python class implementing
    both NumPy-based CPU IKPLS algorithms. It subclasses scikit-learn‚Äôs
    <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html"><monospace>BaseEstimator</monospace></ext-link>,
    facilitating integration with functions like
    <ext-link ext-link-type="uri" xlink:href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html"><monospace>cross_validate</monospace></ext-link>.
    Another class with IKPLS and fast cross-validation
    (<xref alt="O.-C. G. Engstr√∏m, 2024" rid="ref-engstr√∏m2024shortcutting" ref-type="bibr">O.-C.
    G. Engstr√∏m, 2024</xref>) is available.</p>
  </sec>
  <sec id="jax">
    <title>JAX</title>
    <p>For GPU/TPU acceleration, ikpls provides Python classes for each
    IKPLS algorithm using JAX. JAX combines Autograd
    (<xref alt="Maclaurin et al., 2015" rid="ref-maclaurin2015autograd" ref-type="bibr">Maclaurin
    et al., 2015</xref>) with
    <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/xla">XLA
    (Accelerated Linear Algebra)</ext-link> for high-performance
    computation on various hardware. Automatic differentiation in
    forward and backward modes enables seamless integration with deep
    learning techniques, supporting user-defined metric functions.</p>
  </sec>
</sec>
<sec id="benchmarks">
  <title>Benchmarks</title>
  <p>Benchmarks compare ikpls implementations with scikit-learn‚Äôs NIPALS
  across varying data dimensions and component numbers. Single fits and
  leave-one-out cross-validation (LOOCV) scenarios are explored. To
  estimate execution time in a realistic scenario, the reported
  execution times for LOOCV include calibration of the PLS models and
  computation of the root mean squared error on the validation sample
  for all components from 1 to A.</p>
  <p>The benchmarks use randomly generated data with fixed seeds for
  consistency. Default parameters are <inline-formula><alternatives>
  <tex-math><![CDATA[N=10,000]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[K=500]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[A=30]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>,
  testing both single-target (PLS1) and multi-target (PLS2)
  scenarios.</p>
  <p>The results in
  <xref alt="[fig:timings]" rid="figU003Atimings">[fig:timings]</xref>
  suggest CPU IKPLS for single fits, with a preference for IKPLS #2 if
  <inline-formula><alternatives>
  <tex-math><![CDATA[N \gg K]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>‚â´</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>.
  GPU usage is advised for larger datasets. In cross-validation, IKPLS
  options consistently outperform scikit-learn‚Äôs NIPALS, with CPU IKPLS
  #2 (fast cross-validation) excelling, especially for large datasets.
  GPU IKPLS #1 is optimal in specific cases, considering preprocessing
  constraints. Fast cross-validation delivers significant speedup, more
  pronounced for IKPLS #2, especially when dealing with a larger number
  of target variables (<inline-formula><alternatives>
  <tex-math><![CDATA[M]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>M</mml:mi></mml:math></alternatives></inline-formula>)
  (<xref alt="O.-C. G. Engstr√∏m, 2024" rid="ref-engstr√∏m2024shortcutting" ref-type="bibr">O.-C.
  G. Engstr√∏m, 2024</xref>).</p>
  <p>In an attempt to give guidelines for algorithm choice for the most
  common use cases, we report the execution time of the implementations
  with varying values for each of the parameters above. Specifically, we
  define a list of values for each parameter to take while the rest of
  the parameters maintain their default settings. We use</p>
  <p><inline-formula><alternatives>
  <tex-math><![CDATA[N \in [10^1, 10^2, 10^3, 10^4, 10^5, 10^6]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>N</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>1</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[K \in [30, 50, 10^2, 5\cdot 10^2, 10^3, 5\cdot 10^3, 10^4]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>K</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>‚ãÖ</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  <inline-formula><alternatives>
  <tex-math><![CDATA[A \in [10, 20, 30, 50, 100, 200, 500]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>A</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and <inline-formula><alternatives>
  <tex-math><![CDATA[M \in [1, 10]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>M</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>.</p>
  <p>All the experiments are executed on the hardware shown in
  <xref alt="Table¬†1" rid="tabU003Ahardware">Table¬†1</xref> on a machine
  running Ubuntu 22.04 Jammy Jellyfish.</p>
  <table-wrap>
    <caption>
      <p>Hardware used in the execution time experiments.
      <styled-content id="tabU003Ahardware"></styled-content></p>
    </caption>
    <table>
      <thead>
        <tr>
          <th>Component</th>
          <th>Name</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Motherboard</td>
          <td>ASUS PRO WS X570-ACE</td>
        </tr>
        <tr>
          <td>CPU</td>
          <td>AMD Ryzen 9 5950X</td>
        </tr>
        <tr>
          <td>CPU Cooler</td>
          <td>NZXT Kraken X73</td>
        </tr>
        <tr>
          <td>GPU</td>
          <td>NVIDIA GeForce RTX3090 Ti, CUDA 11.8</td>
        </tr>
        <tr>
          <td>RAM</td>
          <td>4x32GB, DDR4, 3.2GHz, C16</td>
        </tr>
      </tbody>
    </table>
  </table-wrap>
  <fig>
    <caption><p>Results of our timing experiments. We vary
    <inline-formula><alternatives>
    <tex-math><![CDATA[N]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>N</mml:mi></mml:math></alternatives></inline-formula>,
    <inline-formula><alternatives>
    <tex-math><![CDATA[K]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>K</mml:mi></mml:math></alternatives></inline-formula>,
    and <inline-formula><alternatives>
    <tex-math><![CDATA[A]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>A</mml:mi></mml:math></alternatives></inline-formula>
    in the first, second, and third columns. The first two rows are
    PLS1. The last two rows are PLS2. The first and third rows are
    single-fit. The second and fourth rows are leave-one-out
    cross-validation, computing the mean squared error and best number
    of components for each validation split. A circle indicates that the
    experiment was run until the end, and the time reported is exact. A
    square means that the experiment was run until the time per
    iteration had stabilized and used to forecast the time usage if the
    experiment was run to
    completion.<styled-content id="figU003Atimings"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="timings.png" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work is part of an industrial Ph.D.¬†project receiving funding
  from FOSS Analytical A/S and The Innovation Fund Denmark. Grant
  Number: 1044-00108B.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-wold1966estimation">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wold</surname><given-names>H.</given-names></name>
      </person-group>
      <article-title>Estimation of principal components and related models by iterative least squares</article-title>
      <source>Multivariate analysis</source>
      <publisher-name>Academic Press</publisher-name>
      <year iso-8601-date="1966">1966</year>
      <fpage>391</fpage>
      <lpage>420</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wold1983food">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wold</surname><given-names>S.</given-names></name>
        <name><surname>Albano</surname><given-names>C.</given-names></name>
        <name><surname>Dunn</surname><given-names>W. J.</given-names></name>
        <name><surname>Esbensen</surname><given-names>K.</given-names></name>
        <name><surname>Hellberg</surname><given-names>S.</given-names></name>
        <name><surname>Johansson</surname><given-names>E.</given-names></name>
        <name><surname>Sj√∂str√∂m</surname><given-names>M.</given-names></name>
        <name><surname>Martens</surname><given-names>H.</given-names></name>
        <name><surname>Russwurm</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Food research and data analysis</article-title>
      <source>London: H. Martens and H. Russwurn Jr</source>
      <year iso-8601-date="1983">1983</year>
    </element-citation>
  </ref>
  <ref id="ref-wold2001pls">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wold</surname><given-names>S.</given-names></name>
        <name><surname>Sj√∂str√∂m</surname><given-names>M.</given-names></name>
        <name><surname>Eriksson</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>PLS-regression: A basic tool of chemometrics</article-title>
      <source>Chemometrics and intelligent laboratory systems</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2001">2001</year>
      <volume>58</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1016/s0169-7439(01)00155-1</pub-id>
      <fpage>109</fpage>
      <lpage>130</lpage>
    </element-citation>
  </ref>
  <ref id="ref-barker2003partial">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Barker</surname><given-names>M.</given-names></name>
        <name><surname>Rayens</surname><given-names>W.</given-names></name>
      </person-group>
      <article-title>Partial least squares for discrimination</article-title>
      <source>Journal of Chemometrics: A Journal of the Chemometrics Society</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2003">2003</year>
      <volume>17</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1002/cem.785</pub-id>
      <fpage>166</fpage>
      <lpage>173</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dayal1997improved">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dayal</surname><given-names>B. S.</given-names></name>
        <name><surname>MacGregor</surname><given-names>J. F.</given-names></name>
      </person-group>
      <article-title>Improved PLS algorithms</article-title>
      <source>Journal of Chemometrics: A Journal of the Chemometrics Society</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="1997">1997</year>
      <volume>11</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1002/(SICI)1099-128X(199701)11:1&lt;73::AID-CEM435&gt;3.0.CO;2-%23</pub-id>
      <fpage>73</fpage>
      <lpage>85</lpage>
    </element-citation>
  </ref>
  <ref id="ref-scikit-learn">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>Varoquaux</surname><given-names>G.</given-names></name>
        <name><surname>Gramfort</surname><given-names>A.</given-names></name>
        <name><surname>Michel</surname><given-names>V.</given-names></name>
        <name><surname>Thirion</surname><given-names>B.</given-names></name>
        <name><surname>Grisel</surname><given-names>O.</given-names></name>
        <name><surname>Blondel</surname><given-names>M.</given-names></name>
        <name><surname>Prettenhofer</surname><given-names>P.</given-names></name>
        <name><surname>Weiss</surname><given-names>R.</given-names></name>
        <name><surname>Dubourg</surname><given-names>V.</given-names></name>
        <name><surname>Vanderplas</surname><given-names>J.</given-names></name>
        <name><surname>Passos</surname><given-names>A.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Brucher</surname><given-names>M.</given-names></name>
        <name><surname>Perrot</surname><given-names>M.</given-names></name>
        <name><surname>Duchesnay</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>Scikit-learn: Machine learning in Python</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2011">2011</year>
      <volume>12</volume>
      <fpage>2825</fpage>
      <lpage>2830</lpage>
    </element-citation>
  </ref>
  <ref id="ref-rinnan2009review">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rinnan</surname><given-names>√Ö.</given-names></name>
        <name><surname>Berg</surname><given-names>F. van den</given-names></name>
        <name><surname>Engelsen</surname><given-names>S. B.</given-names></name>
      </person-group>
      <article-title>Review of the most common pre-processing techniques for near-infrared spectra</article-title>
      <source>TrAC Trends in Analytical Chemistry</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>28</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1016/j.trac.2009.07.007</pub-id>
      <fpage>1201</fpage>
      <lpage>1222</lpage>
    </element-citation>
  </ref>
  <ref id="ref-sorensen2021nir">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>S√∏rensen</surname><given-names>K. M.</given-names></name>
        <name><surname>Berg</surname><given-names>F. van den</given-names></name>
        <name><surname>Engelsen</surname><given-names>S. B.</given-names></name>
      </person-group>
      <article-title>NIR data exploration and regression by chemometrics‚Äîa primer</article-title>
      <source>Near-Infrared Spectroscopy: Theory, Spectral Analysis, Instrumentation, and Applications</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/978-981-15-8648-4_7</pub-id>
      <fpage>127</fpage>
      <lpage>189</lpage>
    </element-citation>
  </ref>
  <ref id="ref-du2022quantitative">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Du</surname><given-names>Z.</given-names></name>
        <name><surname>Tian</surname><given-names>W.</given-names></name>
        <name><surname>Tilley</surname><given-names>M.</given-names></name>
        <name><surname>Wang</surname><given-names>D.</given-names></name>
        <name><surname>Zhang</surname><given-names>G.</given-names></name>
        <name><surname>Li</surname><given-names>Y.</given-names></name>
      </person-group>
      <article-title>Quantitative assessment of wheat quality using near-infrared spectroscopy: A comprehensive review</article-title>
      <source>Comprehensive Reviews in Food Science and Food Safety</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>21</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1111/1541-4337.12958</pub-id>
      <fpage>2956</fpage>
      <lpage>3009</lpage>
    </element-citation>
  </ref>
  <ref id="ref-alin2009comparison">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Alin</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Comparison of PLS algorithms when number of objects is much larger than number of variables</article-title>
      <source>Statistical papers</source>
      <publisher-name>Springer Nature BV</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>50</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.1007/s00362-009-0251-7</pub-id>
      <fpage>711</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-andersson2009comparison">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Andersson</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>A comparison of nine PLS1 algorithms</article-title>
      <source>Journal of Chemometrics: A Journal of the Chemometrics Society</source>
      <publisher-name>Wiley Online Library</publisher-name>
      <year iso-8601-date="2009">2009</year>
      <volume>23</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1002/cem.1248</pub-id>
      <fpage>518</fpage>
      <lpage>529</lpage>
    </element-citation>
  </ref>
  <ref id="ref-engstrom2023improving">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Engstr√∏m</surname><given-names>O.-C. G.</given-names></name>
        <name><surname>Dreier</surname><given-names>E. S.</given-names></name>
        <name><surname>Jespersen</surname><given-names>B. M.</given-names></name>
        <name><surname>Pedersen</surname><given-names>K. S.</given-names></name>
      </person-group>
      <article-title>Improving deep learning on hyperspectral images of grain by incorporating domain knowledge from chemometrics</article-title>
      <source>Proceedings of the IEEE/CVF international conference on computer vision</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.1109/iccvw60793.2023.00055</pub-id>
      <fpage>485</fpage>
      <lpage>494</lpage>
    </element-citation>
  </ref>
  <ref id="ref-engstrom2023analyzing">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Engstr√∏m</surname><given-names>O.-C. G.</given-names></name>
        <name><surname>Dreier</surname><given-names>E. S.</given-names></name>
        <name><surname>Jespersen</surname><given-names>B. M.</given-names></name>
        <name><surname>Pedersen</surname><given-names>K. S.</given-names></name>
      </person-group>
      <article-title>Analyzing near-infrared hyperspectral imaging for protein content regression and grain variety classification using bulk references and varying grain-to-background ratios</article-title>
      <source>arXiv preprint arXiv:2311.04042</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2311.04042</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-harris2020array">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>C. R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. J.</given-names></name>
        <name><surname>Walt</surname><given-names>S. J. van der</given-names></name>
        <name><surname>Gommers</surname><given-names>R.</given-names></name>
        <name><surname>Virtanen</surname><given-names>P.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Wieser</surname><given-names>E.</given-names></name>
        <name><surname>Taylor</surname><given-names>J.</given-names></name>
        <name><surname>Berg</surname><given-names>S.</given-names></name>
        <name><surname>Smith</surname><given-names>N. J.</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <issue>7825</issue>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-jax2018github">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Bradbury</surname><given-names>J.</given-names></name>
        <name><surname>Frostig</surname><given-names>R.</given-names></name>
        <name><surname>Hawkins</surname><given-names>P.</given-names></name>
        <name><surname>Johnson</surname><given-names>M. J.</given-names></name>
        <name><surname>Leary</surname><given-names>C.</given-names></name>
        <name><surname>Maclaurin</surname><given-names>D.</given-names></name>
        <name><surname>Necula</surname><given-names>G.</given-names></name>
        <name><surname>Paszke</surname><given-names>A.</given-names></name>
        <name><surname>VanderPlas</surname><given-names>J.</given-names></name>
        <name><surname>Wanderman-Milne</surname><given-names>S.</given-names></name>
        <name><surname>Zhang</surname><given-names>Q.</given-names></name>
      </person-group>
      <article-title>JAX: Composable transformations of Python+NumPy programs</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>http://github.com/google/jax</uri>
    </element-citation>
  </ref>
  <ref id="ref-maclaurin2015autograd">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Maclaurin</surname><given-names>D.</given-names></name>
        <name><surname>Duvenaud</surname><given-names>D.</given-names></name>
        <name><surname>Adams</surname><given-names>R. P.</given-names></name>
      </person-group>
      <article-title>Autograd: Effortless gradients in numpy</article-title>
      <source>ICML 2015 AutoML workshop</source>
      <year iso-8601-date="2015">2015</year>
      <volume>238</volume>
      <fpage>5</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-dreier2022hyperspectral">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dreier</surname><given-names>E. S.</given-names></name>
        <name><surname>S√∏rensen</surname><given-names>K. M.</given-names></name>
        <name><surname>Lund-Hansen</surname><given-names>T.</given-names></name>
        <name><surname>Jespersen</surname><given-names>B. M.</given-names></name>
        <name><surname>Pedersen</surname><given-names>K. S.</given-names></name>
      </person-group>
      <article-title>Hyperspectral imaging for classification of bulk grain samples with deep convolutional neural networks</article-title>
      <source>Journal of Near Infrared Spectroscopy</source>
      <publisher-name>SAGE Publications Sage UK: London, England</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>30</volume>
      <issue>3</issue>
      <pub-id pub-id-type="doi">10.1177/09670335221078356</pub-id>
      <fpage>107</fpage>
      <lpage>121</lpage>
    </element-citation>
  </ref>
  <ref id="ref-baum2019multiblock">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Baum</surname><given-names>A.</given-names></name>
        <name><surname>Vermue</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>Multiblock PLS: Block dependent prediction modeling for python</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2019">2019</year>
      <volume>4</volume>
      <issue>34</issue>
      <pub-id pub-id-type="doi">10.21105/joss.01190</pub-id>
      <fpage>1190</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-engstr√∏m2024shortcutting">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Engstr√∏m</surname><given-names>Ole-Christian Galbo</given-names></name>
      </person-group>
      <article-title>Shortcutting cross-validation: Efficiently deriving column-wise centered and scaled training set \mathbf{X}^\mathbf{T}\mathbf{X} and \mathbf{X}^\mathbf{T}\mathbf{Y} without full recomputation of matrix products or statistical moments</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://arxiv.org/abs/2401.13185</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2401.13185</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
