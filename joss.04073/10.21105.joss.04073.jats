<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4073</article-id>
<article-id pub-id-type="doi">10.21105/joss.04073</article-id>
<title-group>
<article-title>Elephas: Distributed Deep Learning with Keras &amp;
Spark</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Pumperla</surname>
<given-names>Max</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cahall</surname>
<given-names>Daniel</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>IU Internationale Hochschule, Erfurt, Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Anyscale Inc, San Francisco, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Independent researcher, Philadelphia, USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-11-18">
<day>18</day>
<month>11</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>80</issue>
<fpage>4073</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Distributed Computing</kwd>
<kwd>Deep Learning</kwd>
<kwd>Keras</kwd>
<kwd>Tensorflow</kwd>
<kwd>Apache Spark</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Elephas is an extension of
  <ext-link ext-link-type="uri" xlink:href="https://keras.io/">Keras</ext-link>,
  which allows you to run distributed deep learning models at scale with
  <ext-link ext-link-type="uri" xlink:href="http://spark.apache.org/">Apache
  Spark</ext-link>. It was built to allow researchers and developers to
  distribute their deep learning experiments as easily as possible on a
  Spark computer cluster. With elephas, researchers can currently run
  data-parallel training of deep learning models with distribution modes
  as suggested in
  (<xref alt="Dean et al., 2012" rid="ref-NIPS2012_6aca9700" ref-type="bibr">Dean
  et al., 2012</xref>),
  (<xref alt="Recht et al., 2011" rid="ref-NIPS2011_218a0aef" ref-type="bibr">Recht
  et al., 2011</xref>) and
  (<xref alt="Noel &amp; Osindero, 2014" rid="ref-Noel2014DogwildD" ref-type="bibr">Noel
  &amp; Osindero, 2014</xref>). Additionally, elephas supports
  distributed training of ensemble models. Until version 2.1., elephas
  also supported distributed hyper-parameter optimization of Keras
  models.</p>
  <p>Elephas keeps the simplicity and high usability of Keras, thereby
  allowing for fast prototyping of distributed models. When ready,
  researchers can then scale out their experiments on massive data sets.
  Elephas comes with
  <ext-link ext-link-type="uri" xlink:href="https://danielenricocahall.github.io/elephas/">full
  API documentation</ext-link> and
  <ext-link ext-link-type="uri" xlink:href="https://github.com/danielenricocahall/elephas/blob/master/examples/Spark_ML_Pipeline.ipynb">examples</ext-link>
  to get you started. Initiated in late 2015, elephas has been actively
  maintained since then and has reached maturity for distributed deep
  learning on Spark.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Modern deep learning solutions require ever more data and
  computation power. A study by
  <ext-link ext-link-type="uri" xlink:href="https://openai.com/blog/ai-and-compute">OpenAI</ext-link>
  suggests that the number of operations needed for AI systems, by now
  measured in <italic>petaflops</italic>, shows exponential growth and
  has in fact been doubling every 3.4 months since 2012. This vastly
  outweighs the computational gains to expect from single machines, even
  according to the most optimistic version of Moore’s law. There’s a
  clear need for solutions that can scale to compute clusters. While
  some large companies have such large deep learning models that they
  have to resort to distributing the models themselves
  (model-parallelism), for most researchers and the majority of
  companies, compute time and data volume are the predominant
  bottlenecks.</p>
  <p>Apache Spark has established itself as one of the most popular
  platforms for distributed computing. However, its native machine
  learning (ML) capabilities are limited by design. Spark excels when
  transforming massive datasets and applying built-in ML algorithms with
  <ext-link ext-link-type="uri" xlink:href="http://spark.apache.org/mllib/">Spark
  MLlib</ext-link>, but does not support implementation of custom
  algorithms with deep learning frameworks such as
  <ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">Google’s
  TensorFlow</ext-link> and specifically its convenient Keras API.</p>
  <p>Elephas was the first open-source framework to support distributed
  training with Keras on Spark. It was followed by libraries such as
  Yahoo’s
  <ext-link ext-link-type="uri" xlink:href="https://github.com/yahoo/TensorFlowOnSpark">TensorFlowOnSpark</ext-link>,
  which does not follow Keras API design principles, later on. In recent
  years, other popular distributed deep learning frameworks have
  emerged, such as the powerful Horovod
  (<xref alt="Sergeev &amp; Balso, 2018" rid="ref-sergeev2018horovod" ref-type="bibr">Sergeev
  &amp; Balso, 2018</xref>), which initially did not have Spark support.
  BigDL
  (<xref alt="Dai et al., 2019" rid="ref-SOCC2019_BIGDL" ref-type="bibr">Dai
  et al., 2019</xref>) is another such framework worth mentioning,
  especially in conjunction with
  <ext-link ext-link-type="uri" xlink:href="https://github.com/intel-analytics/analytics-zoo">Intel’s
  Analytics Zoo</ext-link>. Elephas is still in active use and has been
  leveraged by
  <ext-link ext-link-type="uri" xlink:href="https://pypistats.org/packages/elephas">millions
  of users</ext-link> in the Python deep learning community.</p>
</sec>
<sec id="design-and-api">
  <title>Design and API</title>
  <p>Elephas has a tight integration with many of Spark’s core
  abstraction layers. Apart from the basic integration for Spark’s
  resilient distributed datasets (RDDs), elephas works with MLlib
  models, with Spark ML estimators, can train ensemble models and run
  distributed inference.</p>
  <sec id="basic-spark-training">
    <title>Basic Spark training</title>
    <p>Elephas’ core abstraction to bring Keras models to Spark is
    called <monospace>SparkModel</monospace>. To use it, you define a
    Keras model, load your data, define your
    <monospace>SparkModel</monospace> with the training mode and update
    frequency of your choice, and then fit your model on distributed
    data:</p>
    <code language="python">from elephas.spark_model import SparkModel
from elephas.utils.rdd_utils import to_simple_rdd
from pyspark import SparkContext, SparkConf
from tensorflow.keras.models import Sequential

# Define Spark context
conf = SparkConf().setAppName('Elephas_App')\
                  .setMaster('local[8]')
sc = SparkContext(conf=conf)

# Define Keras model
model = Sequential()  
model.add(...)
model.compile(...)

# Load training data
x_train, y_train = ... 

# Convert to Spark RDD
rdd = to_simple_rdd(sc, x_train, y_train)

# Define Elephas model
spark_model = SparkModel(model, frequency='epoch', mode='asynchronous')

# Run distributed training
spark_model.fit(rdd, ...)</code>
    <p>Afterwards your training job can be submitted to any Spark
    cluster like this:</p>
    <code language="bash">spark-submit --driver-memory 1G ./your_script.py</code>
  </sec>
  <sec id="distributed-inference">
    <title>Distributed Inference</title>
    <p>When your model <monospace>spark_model</monospace> has finished
    training, you can easily run distributed inference on your test data
    or compute evaluation metrics on it as follows.</p>
    <code language="python"># Load test data
x_test, y_test = ...

# Perform  distributed inference
predictions = spark_model.predict(x_test)

# Run distributed evaluation/scoring
evaluation = spark_model.evaluate(x_test, y_test)</code>
  </sec>
  <sec id="spark-mllib-and-ml-integrations">
    <title>Spark MLlib and ML integrations</title>
    <p>To leverage Spark’s <monospace>LabeledPoint</monospace> RDD to
    encode features and labels for a prediction task, you can use helper
    functions provided by elephas and then train a so-called
    <monospace>SparkMLlibModel</monospace> on your data for your Keras
    <monospace>model</monospace>.</p>
    <code language="python">from elephas.utils.rdd_utils import to_labeled_point
from elephas.spark_model import SparkMLlibModel

# Create a LabeledPoint RDD
lp_rdd = to_labeled_point(sc, x_train, y_train, categorical=True)

# Define and train a SparkMLlib model
spark_model = SparkMLlibModel(model, frequency='batch', mode='hogwild')
spark_model.train(lp_rdd, ...)</code>
    <p>Likewise, to create a Spark ML <monospace>Estimator</monospace>
    to fit it on a Spark <monospace>DataFrame</monospace>, you can use
    elephas’ s <monospace>SparkEstimator</monospace>.</p>
    <code language="python">from elephas.ml.adapter import to_data_frame
from elephas.ml_model import ElephasEstimator

# Create a Spark DataFrame
df = to_data_frame(sc, x_train, y_train, categorical=True)

# Define and fit an Elephas Estimator
estimator = ElephasEstimator(model, ...)
fitted_model = estimator.fit(df)</code>
    <p>To summarize, elephas provides you with training and evaluation
    support for custom Keras models for many practical scenarios and
    data structures supported on a Spark cluster.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We would like to thank all the open-source contributors that helped
  making <monospace>elephas</monospace> what it is today.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-NIPS2012_6aca9700">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dean</surname><given-names>Jeffrey</given-names></name>
        <name><surname>Corrado</surname><given-names>Greg</given-names></name>
        <name><surname>Monga</surname><given-names>Rajat</given-names></name>
        <name><surname>Chen</surname><given-names>Kai</given-names></name>
        <name><surname>Devin</surname><given-names>Matthieu</given-names></name>
        <name><surname>Mao</surname><given-names>Mark</given-names></name>
        <name><surname>Ranzato</surname><given-names>Marcaurelio</given-names></name>
        <name><surname>Senior</surname><given-names>Andrew</given-names></name>
        <name><surname>Tucker</surname><given-names>Paul</given-names></name>
        <name><surname>Yang</surname><given-names>Ke</given-names></name>
        <name><surname>Le</surname><given-names>Quoc</given-names></name>
        <name><surname>Ng</surname><given-names>Andrew</given-names></name>
      </person-group>
      <article-title>Large scale distributed deep networks</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Pereira</surname><given-names>F.</given-names></name>
        <name><surname>Burges</surname><given-names>C. J. C.</given-names></name>
        <name><surname>Bottou</surname><given-names>L.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <volume>25</volume>
      <uri>https://proceedings.neurips.cc/paper/2012/file/6aca97005c68f1206823815f66102863-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-NIPS2011_218a0aef">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Recht</surname><given-names>Benjamin</given-names></name>
        <name><surname>Re</surname><given-names>Christopher</given-names></name>
        <name><surname>Wright</surname><given-names>Stephen</given-names></name>
        <name><surname>Niu</surname><given-names>Feng</given-names></name>
      </person-group>
      <article-title>Hogwild!: A lock-free approach to parallelizing stochastic gradient descent</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Shawe-Taylor</surname><given-names>J.</given-names></name>
        <name><surname>Zemel</surname><given-names>R.</given-names></name>
        <name><surname>Bartlett</surname><given-names>P.</given-names></name>
        <name><surname>Pereira</surname><given-names>F.</given-names></name>
        <name><surname>Weinberger</surname><given-names>K. Q.</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2011">2011</year>
      <volume>24</volume>
      <uri>https://proceedings.neurips.cc/paper/2011/file/218a0aefd1d1a4be65601cc6ddc1520e-Paper.pdf</uri>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Noel2014DogwildD">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Noel</surname><given-names>Cyprien</given-names></name>
        <name><surname>Osindero</surname><given-names>Simon</given-names></name>
      </person-group>
      <article-title>Dogwild! – distributed hogwild for CPU &amp; GPU</article-title>
      <year iso-8601-date="2014">2014</year>
    </element-citation>
  </ref>
  <ref id="ref-sergeev2018horovod">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sergeev</surname><given-names>Alexander</given-names></name>
        <name><surname>Balso</surname><given-names>Mike Del</given-names></name>
      </person-group>
      <article-title>Horovod: Fast and easy distributed deep learning in TensorFlow</article-title>
      <source>arXiv preprint arXiv:1802.05799</source>
      <year iso-8601-date="2018">2018</year>
    </element-citation>
  </ref>
  <ref id="ref-SOCC2019_BIGDL">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Dai</surname><given-names>Jason (Jinquan)</given-names></name>
        <name><surname>Wang</surname><given-names>Yiheng</given-names></name>
        <name><surname>Qiu</surname><given-names>Xin</given-names></name>
        <name><surname>Ding</surname><given-names>Ding</given-names></name>
        <name><surname>Zhang</surname><given-names>Yao</given-names></name>
        <name><surname>Wang</surname><given-names>Yanzhang</given-names></name>
        <name><surname>Jia</surname><given-names>Xianyan</given-names></name>
        <name><surname>Zhang</surname><given-names>Li (Cherry)</given-names></name>
        <name><surname>Wan</surname><given-names>Yan</given-names></name>
        <name><surname>Li</surname><given-names>Zhichao</given-names></name>
        <name><surname>Wang</surname><given-names>Jiao</given-names></name>
        <name><surname>Huang</surname><given-names>Shengsheng</given-names></name>
        <name><surname>Wu</surname><given-names>Zhongyuan</given-names></name>
        <name><surname>Wang</surname><given-names>Yang</given-names></name>
        <name><surname>Yang</surname><given-names>Yuhao</given-names></name>
        <name><surname>She</surname><given-names>Bowen</given-names></name>
        <name><surname>Shi</surname><given-names>Dongjie</given-names></name>
        <name><surname>Lu</surname><given-names>Qi</given-names></name>
        <name><surname>Huang</surname><given-names>Kai</given-names></name>
        <name><surname>Song</surname><given-names>Guoqiong</given-names></name>
      </person-group>
      <article-title>BigDL: A distributed deep learning framework for big data</article-title>
      <source>Proceedings of the ACM symposium on cloud computing</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://arxiv.org/pdf/1804.05839.pdf</uri>
      <pub-id pub-id-type="doi">10.1145/3357223.3362707</pub-id>
      <fpage>50</fpage>
      <lpage>60</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
