<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7101</article-id>
<article-id pub-id-type="doi">10.21105/joss.07101</article-id>
<title-group>
<article-title>Fundus Image Toolbox: A Python package for fundus image
processing</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-7286-0017</contrib-id>
<name>
<surname>Gervelmeyer</surname>
<given-names>Julius</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1500-8673</contrib-id>
<name>
<surname>Müller</surname>
<given-names>Sarah</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7517-0395</contrib-id>
<name>
<surname>Huang</surname>
<given-names>Ziwei</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-0199-4727</contrib-id>
<name>
<surname>Berens</surname>
<given-names>Philipp</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Hertie Institute for AI in Brain Health, University of
Tübingen, Tübingen, Germany</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-02-03">
<day>3</day>
<month>2</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>108</issue>
<fpage>7101</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>fundus image</kwd>
<kwd>retina</kwd>
<kwd>registration</kwd>
<kwd>optic disc</kwd>
<kwd>fovea</kwd>
<kwd>vessel segmentation</kwd>
<kwd>quality prediction</kwd>
<kwd>circle crop</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The Fundus Image Toolbox is an open source Python suite of tools
  for working with retinal fundus images. It includes quality
  prediction, fovea and optic disc center localization, blood vessel
  segmentation, image registration, and fundus cropping functions. It
  also provides a collection of useful utilities for image manipulation
  and image-based PyTorch models. The toolbox is designed to be flexible
  and easy to use, thus helping to speed up research workflows. It is
  available as a PyPI package and at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/berenslab/fundus_image_toolbox">https://github.com/berenslab/fundus_image_toolbox</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>In ophthalmic research, retinal fundus images are often used as a
  resource for studying various eye diseases such as diabetic
  retinopathy, glaucoma and age-related macular degeneration.
  Consequently, there is a large amount of research on machine learning
  for fundus image analysis. However, many of the works do not publish
  their source code, and very few of them provide ready-to-use open
  source preprocessing tools to the community.</p>
  <p>The Fundus Image Toolbox was developed to address this need within
  the medical image analysis community. It offers a comprehensive set of
  tools for automated processing of retinal fundus images, covering a
  wide range of tasks (see Tools). The methods accept paths to images,
  standard image types (e.g., images loaded with Pillow, OpenCV, or
  Matplotlib) and batches thereof and where possible, image batches are
  efficiently processed as such. This allows the tools to be seamlessly
  combined into a processing pipeline. The quality prediction and
  localization models have been developed by the authors and allow for
  both prediction and retraining, while the other main functionalities
  are based on state-of-the-art methods from the literature that are
  applicable for inference. By providing an interface for these tasks,
  the toolbox facilitates the development of new algorithms and models
  in the field of fundus image analysis. AutoMorph is the closest
  related work
  (<xref alt="Zhou et al., 2022" rid="ref-zhou2022" ref-type="bibr">Zhou
  et al., 2022</xref>), which provides a distinct and smaller set of
  tools for fundus image processing.</p>
</sec>
<sec id="tools">
  <title>Tools</title>
  <p>The main functionalities of the Fundus Image Toolbox are:</p>
  <list list-type="bullet">
    <list-item>
      <p>Quality prediction
      (<xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>a).
      We trained an ensemble of ResNets and EfficientNets on the
      combined DeepDRiD and DrimDB datasets
      (<xref alt="R. Liu et al., 2022" rid="ref-deepdrid" ref-type="bibr">R.
      Liu et al., 2022</xref>;
      <xref alt="Sevik et al., 2014" rid="ref-drimdb" ref-type="bibr">Sevik
      et al., 2014</xref>) to predict the gradeability of fundus images.
      The datasets are publicly available and comprise images of retinas
      with diabetic retinopathy, healthy retinas and outliers such as
      outer eye images. The model ensemble achieved an accuracy of 0.78
      and an area under the receiver operating characteristic curve
      (AUROC) of 0.84 on a DeepDRiD test split, surpassing the previous
      best model evaluated on DeepDRiD
      (<xref alt="Tummala et al., 2023" rid="ref-tummala2023" ref-type="bibr">Tummala
      et al., 2023</xref>). Further, on a DrimDB test split, the
      accuracy and AUROC of our model were 1.0 and 1.0,
      respectively.</p>
    </list-item>
    <list-item>
      <p>Fovea and optic disc localization
      (<xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>b).
      The center coordinates of the fovea and optic disc can be
      predicted using a multitask EfficientNet model. We trained the
      model on the combined ADAM, REFUGE and IDRID datasets which
      include images from eyes with age-related macular degeneration,
      glaucoma, diabetic retinopathy and healthy retinas
      (<xref alt="Fang et al., 2022" rid="ref-adam" ref-type="bibr">Fang
      et al., 2022</xref>;
      <xref alt="Orlando et al., 2020" rid="ref-refuge" ref-type="bibr">Orlando
      et al., 2020</xref>;
      <xref alt="Porwal et al., 2020" rid="ref-idrid" ref-type="bibr">Porwal
      et al., 2020</xref>). All datasets are publicly available. On our
      test split, the model achieved an average distance to the fovea
      and optic disc targets of 0.88 % of the image size. This
      corresponds to a mean distance of 3.08 pixels in the 350 x 350
      pixel images used for training and testing.</p>
    </list-item>
    <list-item>
      <p>Vessel segmentation
      (<xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>c).
      The segmentation method produces a mask of blood vessels in a
      fundus image using an ensemble of FR-UNets. The ensemble achieved
      an average Dice score of 0.887 on the test split of the FIVES
      dataset
      (<xref alt="Köhler et al., 2024" rid="ref-koehler2024" ref-type="bibr">Köhler
      et al., 2024</xref>). FIVES includes images with age-related
      macular degeneration, glaucoma, diabetic retinopathy and healthy
      retinas
      (<xref alt="Jin et al., 2022" rid="ref-fives" ref-type="bibr">Jin
      et al., 2022</xref>).</p>
    </list-item>
    <list-item>
      <p>Registration
      (<xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>d).
      Two fundus images of the same eye can be aligned using SuperRetina
      (<xref alt="J. Liu et al., 2022" rid="ref-liu2022" ref-type="bibr">J.
      Liu et al., 2022</xref>). The deep learning based model detects
      key points on the vessel trees of the two images and matches them.
      This results in a registered version of the second image that is
      aligned with the first. SuperRetina produced registrations of at
      least acceptable quality in 98.5 % of the cases on the test split
      of the FIRE dataset
      (<xref alt="Hernandez-Matas et al., 2017" rid="ref-fire" ref-type="bibr">Hernandez-Matas
      et al., 2017</xref>).</p>
    </list-item>
    <list-item>
      <p>Circle cropping. The OpenCV-based implementation fastly crops
      the circular background from a fundus image. The circle is further
      resized to touch the edges of the image, which centers the fundus
      (<xref alt="Fu et al., 2019" rid="ref-fu2019" ref-type="bibr">Fu
      et al., 2019</xref>).</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Examples for main functionalities of the Fundus Image
    Toolbox. (a.) Fovea and optic disc localization. (b.) Quality
    prediction. (c.) Vessel segmentation. (d.)
    Registration.<styled-content id="figU003Aexample"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="fig3.pdf" />
  </fig>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We thank Murat Seçkin Ayhan for inspiring the development of the
  quality prediction model. This project was supported by the Hertie
  Foundation. Julius Gervelmeyer received funding through the Else
  Kröner Medical Scientist Kolleg “ClinbrAIn: Artificial Intelligence
  for Clinical Brain Research”. The authors thank the International Max
  Planck Research School for Intelligent Systems (IMPRS-IS) for
  supporting Sarah Müller.</p>
</sec>
<sec id="author-contributions">
  <title>Author contributions</title>
  <p>Julius Gervelmeyer: Conceptualization, Software, Writing. Sarah
  Müller: Software. Ziwei Huang: Software: Code review &amp; PyPI.
  Philipp Berens: Supervision, Writing: Review &amp; Editing.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-liu2022">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Jiazhen</given-names></name>
        <name><surname>Li</surname><given-names>Xirong</given-names></name>
        <name><surname>Wei</surname><given-names>Qijie</given-names></name>
        <name><surname>Xu</surname><given-names>Jie</given-names></name>
        <name><surname>Ding</surname><given-names>Dayong</given-names></name>
      </person-group>
      <article-title>Semi-supervised keypoint detector and descriptor for retinal image matching</article-title>
      <source>Computer vision – ECCV 2022</source>
      <publisher-name>Springer Nature Switzerland</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <isbn>978-3-031-19803-8</isbn>
      <uri>https://link.springer.com/chapter/10.1007/978-3-031-19803-8_35</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-031-19803-8_35</pub-id>
      <fpage>593</fpage>
      <lpage>609</lpage>
    </element-citation>
  </ref>
  <ref id="ref-koehler2024">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Köhler</surname><given-names>Patrick</given-names></name>
        <name><surname>Fadugba</surname><given-names>Jeremiah</given-names></name>
        <name><surname>Berens</surname><given-names>Philipp</given-names></name>
        <name><surname>Koch</surname><given-names>Lisa M.</given-names></name>
      </person-group>
      <article-title>Efficiently correcting patch-based segmentation errors to control image-level performance</article-title>
      <source>Accepted at medical imaging with deep learning</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://openreview.net/forum?id=DDHRGHfwji</uri>
    </element-citation>
  </ref>
  <ref id="ref-fu2019">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Fu</surname><given-names>Huazhu</given-names></name>
        <name><surname>Wang</surname><given-names>Boyang</given-names></name>
        <name><surname>Shen</surname><given-names>Jianbing</given-names></name>
        <name><surname>Cui</surname><given-names>Shanshan</given-names></name>
        <name><surname>Xu</surname><given-names>Yanwu</given-names></name>
        <name><surname>Liu</surname><given-names>Jiang</given-names></name>
        <name><surname>Shao</surname><given-names>Ling</given-names></name>
      </person-group>
      <article-title>Evaluation of retinal image quality assessment networks in different color-spaces</article-title>
      <source>Medical image computing and computer assisted intervention – MICCAI 2019</source>
      <publisher-name>Springer International Publishing</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <isbn>9783030322397</isbn>
      <issn>1611-3349</issn>
      <uri>http://dx.doi.org/10.1007/978-3-030-32239-7_6</uri>
      <pub-id pub-id-type="doi">10.1007/978-3-030-32239-7_6</pub-id>
      <fpage>48</fpage>
      <lpage>56</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zhou2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhou</surname><given-names>Yukun</given-names></name>
        <name><surname>Wagner</surname><given-names>Siegfried K</given-names></name>
        <name><surname>Chia</surname><given-names>Mark A</given-names></name>
        <name><surname>Zhao</surname><given-names>An</given-names></name>
        <name><surname>Xu</surname><given-names>Moucheng</given-names></name>
        <name><surname>Struyven</surname><given-names>Robbert</given-names></name>
        <name><surname>Alexander</surname><given-names>Daniel C</given-names></name>
        <name><surname>Keane</surname><given-names>Pearse A</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>AutoMorph: Automated retinal vascular morphology quantification via a deep learning pipeline</article-title>
      <source>Translational vision science &amp; technology</source>
      <publisher-name>The Association for Research in Vision; Ophthalmology</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>11</volume>
      <issue>7</issue>
      <uri>https://tvst.arvojournals.org/article.aspx?articleid=2783477</uri>
      <pub-id pub-id-type="doi">10.1167/tvst.11.7.12</pub-id>
      <fpage>12</fpage>
      <lpage>12</lpage>
    </element-citation>
  </ref>
  <ref id="ref-adam">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Fang</surname><given-names>Huihui</given-names></name>
        <name><surname>Li</surname><given-names>Fei</given-names></name>
        <name><surname>Fu</surname><given-names>Huazhu</given-names></name>
        <name><surname>Sun</surname><given-names>Xu</given-names></name>
        <name><surname>Cao</surname><given-names>Xingxing</given-names></name>
        <name><surname>Lin</surname><given-names>Fengbin</given-names></name>
        <name><surname>Son</surname><given-names>Jaemin</given-names></name>
        <name><surname>Kim</surname><given-names>Sunho</given-names></name>
        <name><surname>Quellec</surname><given-names>Gwenole</given-names></name>
        <name><surname>Matta</surname><given-names>Sarah</given-names></name>
        <name><surname>Shankaranarayana</surname><given-names>Sharath M.</given-names></name>
        <name><surname>Chen</surname><given-names>Yi-Ting</given-names></name>
        <name><surname>Wang</surname><given-names>Chuen-Heng</given-names></name>
        <name><surname>Shah</surname><given-names>Nisarg A.</given-names></name>
        <name><surname>Lee</surname><given-names>Chia-Yen</given-names></name>
        <name><surname>Hsu</surname><given-names>Chih-Chung</given-names></name>
        <name><surname>Xie</surname><given-names>Hai</given-names></name>
        <name><surname>Lei</surname><given-names>Baiying</given-names></name>
        <name><surname>Baid</surname><given-names>Ujjwal</given-names></name>
        <name><surname>Innani</surname><given-names>Shubham</given-names></name>
        <name><surname>Dang</surname><given-names>Kang</given-names></name>
        <name><surname>Shi</surname><given-names>Wenxiu</given-names></name>
        <name><surname>Kamble</surname><given-names>Ravi</given-names></name>
        <name><surname>Singhal</surname><given-names>Nitin</given-names></name>
        <name><surname>Wang</surname><given-names>Ching-Wei</given-names></name>
        <name><surname>Lo</surname><given-names>Shih-Chang</given-names></name>
        <name><surname>Orlando</surname><given-names>Jose Ignacio</given-names></name>
        <name><surname>Bogunovic</surname><given-names>Hrvoje</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiulan</given-names></name>
        <name><surname>Xu</surname><given-names>Yanwu</given-names></name>
      </person-group>
      <article-title>ADAM challenge: Detecting age-related macular degeneration from fundus images</article-title>
      <source>IEEE Transactions on Medical Imaging</source>
      <publisher-name>Institute of Electrical; Electronics Engineers (IEEE)</publisher-name>
      <year iso-8601-date="2022-10">2022</year><month>10</month>
      <volume>41</volume>
      <issue>10</issue>
      <issn>1558-254X</issn>
      <uri>https://dx.doi.org/10.1109/TMI.2022.3172773</uri>
      <pub-id pub-id-type="doi">10.1109/tmi.2022.3172773</pub-id>
      <fpage>2828</fpage>
      <lpage>2847</lpage>
    </element-citation>
  </ref>
  <ref id="ref-refuge">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Orlando</surname><given-names>José Ignacio</given-names></name>
        <name><surname>Fu</surname><given-names>Huazhu</given-names></name>
        <name><surname>Barbosa Breda</surname><given-names>João</given-names></name>
        <name><surname>Keer</surname><given-names>Karel van</given-names></name>
        <name><surname>Bathula</surname><given-names>Deepti R.</given-names></name>
        <name><surname>Diaz-Pinto</surname><given-names>Andrés</given-names></name>
        <name><surname>Fang</surname><given-names>Ruogu</given-names></name>
        <name><surname>Heng</surname><given-names>Pheng-Ann</given-names></name>
        <name><surname>Kim</surname><given-names>Jeyoung</given-names></name>
        <name><surname>Lee</surname><given-names>JoonHo</given-names></name>
        <name><surname>Lee</surname><given-names>Joonseok</given-names></name>
        <name><surname>Li</surname><given-names>Xiaoxiao</given-names></name>
        <name><surname>Liu</surname><given-names>Peng</given-names></name>
        <name><surname>Lu</surname><given-names>Shuai</given-names></name>
        <name><surname>Murugesan</surname><given-names>Balamurali</given-names></name>
        <name><surname>Naranjo</surname><given-names>Valery</given-names></name>
        <name><surname>Phaye</surname><given-names>Sai Samarth R.</given-names></name>
        <name><surname>Shankaranarayana</surname><given-names>Sharath M.</given-names></name>
        <name><surname>Sikka</surname><given-names>Apoorva</given-names></name>
        <name><surname>Son</surname><given-names>Jaemin</given-names></name>
        <name><surname>Hengel</surname><given-names>Anton van den</given-names></name>
        <name><surname>Wang</surname><given-names>Shujun</given-names></name>
        <name><surname>Wu</surname><given-names>Junyan</given-names></name>
        <name><surname>Wu</surname><given-names>Zifeng</given-names></name>
        <name><surname>Xu</surname><given-names>Guanghui</given-names></name>
        <name><surname>Xu</surname><given-names>Yongli</given-names></name>
        <name><surname>Yin</surname><given-names>Pengshuai</given-names></name>
        <name><surname>Li</surname><given-names>Fei</given-names></name>
        <name><surname>Zhang</surname><given-names>Xiulan</given-names></name>
        <name><surname>Xu</surname><given-names>Yanwu</given-names></name>
        <name><surname>Bogunović</surname><given-names>Hrvoje</given-names></name>
      </person-group>
      <article-title>REFUGE challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs</article-title>
      <source>Medical Image Analysis</source>
      <publisher-name>Elsevier BV</publisher-name>
      <year iso-8601-date="2020-01">2020</year><month>01</month>
      <volume>59</volume>
      <issn>1361-8415</issn>
      <uri>https://dx.doi.org/10.1016/j.media.2019.101570</uri>
      <pub-id pub-id-type="doi">10.1016/j.media.2019.101570</pub-id>
      <fpage>101570</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-idrid">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Porwal</surname><given-names>Prasanna</given-names></name>
        <name><surname>Pachade</surname><given-names>Samiksha</given-names></name>
        <name><surname>Kokare</surname><given-names>Manesh</given-names></name>
        <name><surname>Deshmukh</surname><given-names>Girish</given-names></name>
        <name><surname>Son</surname><given-names>Jaemin</given-names></name>
        <name><surname>Bae</surname><given-names>Woong</given-names></name>
        <name><surname>Liu</surname><given-names>Lihong</given-names></name>
        <name><surname>Wang</surname><given-names>Jianzong</given-names></name>
        <name><surname>Liu</surname><given-names>Xinhui</given-names></name>
        <name><surname>Gao</surname><given-names>Liangxin</given-names></name>
        <name><surname>Wu</surname><given-names>TianBo</given-names></name>
        <name><surname>Xiao</surname><given-names>Jing</given-names></name>
        <name><surname>Wang</surname><given-names>Fengyan</given-names></name>
        <name><surname>Yin</surname><given-names>Baocai</given-names></name>
        <name><surname>Wang</surname><given-names>Yunzhi</given-names></name>
        <name><surname>Danala</surname><given-names>Gopichandh</given-names></name>
        <name><surname>He</surname><given-names>Linsheng</given-names></name>
        <name><surname>Choi</surname><given-names>Yoon Ho</given-names></name>
        <name><surname>Lee</surname><given-names>Yeong Chan</given-names></name>
        <name><surname>Jung</surname><given-names>Sang-Hyuk</given-names></name>
        <name><surname>Li</surname><given-names>Zhongyu</given-names></name>
        <name><surname>Sui</surname><given-names>Xiaodan</given-names></name>
        <name><surname>Wu</surname><given-names>Junyan</given-names></name>
        <name><surname>Li</surname><given-names>Xiaolong</given-names></name>
        <name><surname>Zhou</surname><given-names>Ting</given-names></name>
        <name><surname>Toth</surname><given-names>Janos</given-names></name>
        <name><surname>Baran</surname><given-names>Agnes</given-names></name>
        <name><surname>Kori</surname><given-names>Avinash</given-names></name>
        <name><surname>Chennamsetty</surname><given-names>Sai Saketh</given-names></name>
        <name><surname>Safwan</surname><given-names>Mohammed</given-names></name>
        <name><surname>Alex</surname><given-names>Varghese</given-names></name>
        <name><surname>Lyu</surname><given-names>Xingzheng</given-names></name>
        <name><surname>Cheng</surname><given-names>Li</given-names></name>
        <name><surname>Chu</surname><given-names>Qinhao</given-names></name>
        <name><surname>Li</surname><given-names>Pengcheng</given-names></name>
        <name><surname>Ji</surname><given-names>Xin</given-names></name>
        <name><surname>Zhang</surname><given-names>Sanyuan</given-names></name>
        <name><surname>Shen</surname><given-names>Yaxin</given-names></name>
        <name><surname>Dai</surname><given-names>Ling</given-names></name>
        <name><surname>Saha</surname><given-names>Oindrila</given-names></name>
        <name><surname>Sathish</surname><given-names>Rachana</given-names></name>
        <name><surname>Melo</surname><given-names>Tânia</given-names></name>
        <name><surname>Araújo</surname><given-names>Teresa</given-names></name>
        <name><surname>Harangi</surname><given-names>Balazs</given-names></name>
        <name><surname>Sheng</surname><given-names>Bin</given-names></name>
        <name><surname>Fang</surname><given-names>Ruogu</given-names></name>
        <name><surname>Sheet</surname><given-names>Debdoot</given-names></name>
        <name><surname>Hajdu</surname><given-names>Andras</given-names></name>
        <name><surname>Zheng</surname><given-names>Yuanjie</given-names></name>
        <name><surname>Mendonça</surname><given-names>Ana Maria</given-names></name>
        <name><surname>Zhang</surname><given-names>Shaoting</given-names></name>
        <name><surname>Campilho</surname><given-names>Aurélio</given-names></name>
        <name><surname>Zheng</surname><given-names>Bin</given-names></name>
        <name><surname>Shen</surname><given-names>Dinggang</given-names></name>
        <name><surname>Giancardo</surname><given-names>Luca</given-names></name>
        <name><surname>Quellec</surname><given-names>Gwenolé</given-names></name>
        <name><surname>Mériaudeau</surname><given-names>Fabrice</given-names></name>
      </person-group>
      <article-title>IDRiD: Diabetic retinopathy – segmentation and grading challenge</article-title>
      <source>Medical Image Analysis</source>
      <year iso-8601-date="2020">2020</year>
      <volume>59</volume>
      <issn>1361-8415</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S1361841519301033</uri>
      <pub-id pub-id-type="doi">10.1016/j.media.2019.101561</pub-id>
      <fpage>101561</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-deepdrid">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Liu</surname><given-names>Ruhan</given-names></name>
        <name><surname>Wang</surname><given-names>Xiangning</given-names></name>
        <name><surname>Wu</surname><given-names>Qiang</given-names></name>
        <name><surname>Dai</surname><given-names>Ling</given-names></name>
        <name><surname>Fang</surname><given-names>Xi</given-names></name>
        <name><surname>Yan</surname><given-names>Tao</given-names></name>
        <name><surname>Son</surname><given-names>Jaemin</given-names></name>
        <name><surname>Tang</surname><given-names>Shiqi</given-names></name>
        <name><surname>Li</surname><given-names>Jiang</given-names></name>
        <name><surname>Gao</surname><given-names>Zijian</given-names></name>
        <name><surname>Galdran</surname><given-names>Adrian</given-names></name>
        <name><surname>Poorneshwaran</surname><given-names>J. M.</given-names></name>
        <name><surname>Liu</surname><given-names>Hao</given-names></name>
        <name><surname>Wang</surname><given-names>Jie</given-names></name>
        <name><surname>Chen</surname><given-names>Yerui</given-names></name>
        <name><surname>Porwal</surname><given-names>Prasanna</given-names></name>
        <name><surname>Wei Tan</surname><given-names>Gavin Siew</given-names></name>
        <name><surname>Yang</surname><given-names>Xiaokang</given-names></name>
        <name><surname>Dai</surname><given-names>Chao</given-names></name>
        <name><surname>Song</surname><given-names>Haitao</given-names></name>
        <name><surname>Chen</surname><given-names>Mingang</given-names></name>
        <name><surname>Li</surname><given-names>Huating</given-names></name>
        <name><surname>Jia</surname><given-names>Weiping</given-names></name>
        <name><surname>Shen</surname><given-names>Dinggang</given-names></name>
        <name><surname>Sheng</surname><given-names>Bin</given-names></name>
        <name><surname>Zhang</surname><given-names>Ping</given-names></name>
      </person-group>
      <article-title>DeepDRiD: Diabetic retinopathy—grading and image quality estimation challenge</article-title>
      <source>Patterns</source>
      <year iso-8601-date="2022">2022</year>
      <volume>3</volume>
      <issue>6</issue>
      <issn>2666-3899</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S2666389922001040</uri>
      <pub-id pub-id-type="doi">10.1016/j.patter.2022.100512</pub-id>
      <fpage>100512</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-drimdb">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sevik</surname><given-names>Ugur</given-names></name>
        <name><surname>Kose</surname><given-names>Cemal</given-names></name>
        <name><surname>Berber</surname><given-names>Tolga</given-names></name>
        <name><surname>Erdol</surname><given-names>Hidayet</given-names></name>
      </person-group>
      <article-title>Identification of suitable fundus images using automated quality assessment methods</article-title>
      <source>Journal of Biomedical Optics</source>
      <publisher-name>SPIE</publisher-name>
      <year iso-8601-date="2014">2014</year>
      <volume>19</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.1117/1.JBO.19.4.046006</uri>
      <pub-id pub-id-type="doi">10.1117/1.JBO.19.4.046006</pub-id>
      <fpage>046006</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-fives">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jin</surname><given-names>Kai</given-names></name>
        <name><surname>Huang</surname><given-names>Xingru</given-names></name>
        <name><surname>Zhou</surname><given-names>Jingxing</given-names></name>
        <name><surname>Li</surname><given-names>Yunxiang</given-names></name>
        <name><surname>Yan</surname><given-names>Yan</given-names></name>
        <name><surname>Sun</surname><given-names>Yibao</given-names></name>
        <name><surname>Zhang</surname><given-names>Qianni</given-names></name>
        <name><surname>Wang</surname><given-names>Yaqi</given-names></name>
        <name><surname>Ye</surname><given-names>Juan</given-names></name>
      </person-group>
      <article-title>Fives: A fundus image dataset for artificial intelligence based vessel segmentation</article-title>
      <source>Scientific data</source>
      <publisher-name>Nature Publishing Group UK London</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>9</volume>
      <issue>1</issue>
      <uri>https://doi.org/10.1038/s41597-022-01564-3</uri>
      <pub-id pub-id-type="doi">10.1038/s41597-022-01564-3</pub-id>
      <fpage>475</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-fire">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hernandez-Matas</surname><given-names>Carlos</given-names></name>
        <name><surname>Zabulis</surname><given-names>Xenophon</given-names></name>
        <name><surname>Triantafyllou</surname><given-names>Areti</given-names></name>
        <name><surname>Anyfanti</surname><given-names>Panagiota</given-names></name>
        <name><surname>Douma</surname><given-names>Stella</given-names></name>
        <name><surname>Argyros</surname><given-names>Antonis A</given-names></name>
      </person-group>
      <article-title>FIRE: Fundus image registration dataset</article-title>
      <source>Modeling and Artificial Intelligence in Ophthalmology</source>
      <year iso-8601-date="2017">2017</year>
      <volume>1</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.35119/maio.v1i4.42</uri>
      <pub-id pub-id-type="doi">10.35119/maio.v1i4.42</pub-id>
      <fpage>16</fpage>
      <lpage>28</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tummala2023">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tummala</surname><given-names>Sudhakar</given-names></name>
        <name><surname>Thadikemalla</surname><given-names>Venkata Sainath Gupta</given-names></name>
        <name><surname>Kadry</surname><given-names>Seifedine</given-names></name>
        <name><surname>Sharaf</surname><given-names>Mohamed</given-names></name>
        <name><surname>Rauf</surname><given-names>Hafiz Tayyab</given-names></name>
      </person-group>
      <article-title>EfficientNetV2 based ensemble model for quality estimation of diabetic retinopathy images from DeepDRiD</article-title>
      <source>Diagnostics</source>
      <publisher-name>MDPI</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <volume>13</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.3390/diagnostics13040622</uri>
      <pub-id pub-id-type="doi">10.3390/diagnostics13040622</pub-id>
      <fpage>622</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
