<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">3829</article-id>
<article-id pub-id-type="doi">10.21105/joss.03829</article-id>
<title-group>
<article-title>HARDy: Handling Arbitrary Recognition of Data in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5815-3371</contrib-id>
<name>
<surname>Politi</surname>
<given-names>Maria</given-names>
</name>
<email>politim@uw.edu</email>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-9582-0372</contrib-id>
<name>
<surname>Moeez</surname>
<given-names>Abdul</given-names>
</name>
<email>amoeez@uw.edu</email>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-5371-7035</contrib-id>
<name>
<surname>Beck</surname>
<given-names>David</given-names>
</name>
<email>dacb@uw.edu</email>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Adler</surname>
<given-names>Stuart</given-names>
</name>
<email>stuadler@uw.edu</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-7104-9061</contrib-id>
<name>
<surname>Pozzo</surname>
<given-names>Lilo</given-names>
</name>
<email>dpozzo@uw.edu</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Washington, Department of Chemical
Engineering, Seattle, WA, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>University of Washington, Department of Materials Science
and Engineering, Seattle, WA, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>eScience Institute, University of Washington, Seattle, WA,
USA</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email>politim@uw.edu</email></corresp>
</author-notes>
<volume>7</volume>
<issue>71</issue>
<fpage>3829</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Feature Engineering</kwd>
<kwd>Kernel methods</kwd>
<kwd>Machine Learning</kwd>
<kwd>Python</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><monospace>HARDy</monospace> is a Python-based package that helps
evaluate differences in data through feature engineering coupled with
kernel methods. The package provides an extension to machine learning by
adding layers of feature transformation and representation. The workflow
of the package is as follows:</p>
<list list-type="bullet">
  <list-item>
    <p><italic>Configuration</italic>: Sets attribute for user-defined
    transformations, machine learning hyperparameters or hyperparameter
    space</p>
  </list-item>
  <list-item>
    <p><italic>Handling</italic>: Imports pre-labelled data from
    <monospace>.csv</monospace> files and loads into the catalogue.
    Later the data will be split into training and testing sets</p>
  </list-item>
  <list-item>
    <p><italic>Arbitrage</italic>: Applies user defined numerical and
    visual transformations to all the data loaded.</p>
  </list-item>
  <list-item>
    <p><italic>Recognition</italic>: Machine Learning module that
    applies user defined hyperparameter search space for training and
    evaluation of model</p>
  </list-item>
  <list-item>
    <p><italic>Data-Reporting</italic>: Imports result of machine
    learning models and reports it into dataframes and plots</p>
  </list-item>
</list>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>High Throughput Experimentation (HTE) and High Throughput Testing
  (HTT) have exponentially increased the volume of experimental data
  available to scientists. One of the major bottlenecks in their
  implementation is the data analysis. The need for autonomous binning
  and classification has seen an increase in the employment of machine
  learning approaches in discovery of catalysts, energy materials and
  process parameters for design of experiment
  (<xref alt="Becker et al., 2019" rid="ref-becker2019low" ref-type="bibr">Becker
  et al., 2019</xref>;
  <xref alt="Williams et al., 2019" rid="ref-williams2019enabling" ref-type="bibr">Williams
  et al., 2019</xref>). However, these solutions rely on specific sets
  of hyperparameters for their machine learning models to achieve the
  desired purpose. Furthermore, numerical data from experimental
  characterization of materials carries diversity in both features and
  magnitude. These features are traditionally extracted using
  deterministic models based on empirical relationships between
  variables of the process under investigation. As an example, X-ray
  diffraction (XRD) data is easier to characterize in linear form as
  compared to small angle X-ray scattering data, which requires
  transformation of axis to log-log scale.</p>
  <p>One of the most widely applied strategy to enhance the performance
  of machine learning model is Combined Automatic Machine Learning
  (AutoML) for CASH (Combined Alogrithm Selection and Hyperparameter
  Optimization)
  (<xref alt="Hutter et al., 2019" rid="ref-hutter2019automated" ref-type="bibr">Hutter
  et al., 2019</xref>). However, these packages are only limited to
  hyper-parameter tuning and data features remain untouched. To improve
  the effectiveness of machine learning models, some of the popular
  feature engineering strategies used for simple numerical data include
  binning, binarization, normalization, Box-Cox Transformations and
  Quantile Sketch Array (QSA)
  (<xref alt="Nargesian et al., 2017" rid="ref-nargesian2017learning" ref-type="bibr">Nargesian
  et al., 2017</xref>;
  <xref alt="Zheng &amp; Casari, 2018" rid="ref-zheng2018feature" ref-type="bibr">Zheng
  &amp; Casari, 2018</xref>). Moreover, Deep Feature Synthesis has also
  shown promising results. Here features are generated from relational
  databases by performing multi-layer mathematical transformation
  operations
  (<xref alt="Kanter &amp; Veeramachaneni, 2015" rid="ref-kanter2015deep" ref-type="bibr">Kanter
  &amp; Veeramachaneni, 2015</xref>).</p>
  <p><monospace>HARDy</monospace> presents an infrastructure which aids
  in the identification of the best combination of numerical and visual
  transformations to improve data classification through Convolutional
  Neural Networks (CNN). <monospace>HARDy</monospace> exploits the
  difference between human-readable images of experimental data (i.e.,
  Cartesian representation) and computer-readable plots, which maximizes
  the data density presented to an algorithm and reduce superfluous
  information. <monospace>HARDy</monospace> uses configuration files,
  fed to the open-source package <monospace>KerasTuner</monospace>,
  removing the need for the user to manually generate unique parameters
  combinations for each neural network model to be investigated.</p>
</sec>
<sec id="description-and-use-case">
  <title>Description and Use Case</title>
  <p>The Python-based package <monospace>HARDy</monospace> is a
  modularly structured package which classifies data using 2D
  convolutional neural networks. A schematic for the package can be
  found in figure 1.</p>
  <fig>
    <caption><p>Workflow schematics for HARDy. The data files, left-most
    column, are subject to numerical and visual transformations,
    according to the rules outlined in the user defined configuration
    files. The images are then fed into either a CNN or a tuner, for
    which the hyperparameter space is controlled through another
    configuration file. Finally, each transformation produces a report
    comprising of the best trained model file, the log of training
    session and the model validation result.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/./images/HARDy_diagram.png" xlink:title="" />
  </fig>
  <p>The package was tested on a set of simulated Small Angle Scattering
  (SAS) data to be classified into four different particle models:
  spherical, ellipsoidal, cylindrical and core-shell spherical. A total
  of ten thousand files were generated for each model. The data was
  generated using . The geometrical and physical parameters used to
  obtain each spectrum were taken from a published work discussing a
  similar classification task
  (<xref alt="Archibald et al., 2020" rid="ref-ArchibaldRichardK2020Caas" ref-type="bibr">Archibald
  et al., 2020</xref>). The name of each SAS model was used as label for
  the data, allowing for further validation of the test set results.
  These models were selected as they present similar parameters and data
  features, which at times make it challenging to distinguish between
  them.</p>
  <fig>
    <caption><p>Summary table of few transformations visualized using
    cartesian coordinate and RGB representation along with the
    respective fittings. The left-most column shows the transformations
    applied to the data: the scattering vector <italic>q</italic> and
    scattering intensity <italic>I(q)</italic>. The final model
    accuracies are also provided. The original data label corresponds to
    the icon on the left-most graph, whereas the icon under each fit
    correspond to the model prediction.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/./images/transformation_run_example.png" xlink:title="" />
  </fig>
  <p>First, the pre-labelled data was loaded. A subset of the files,
  three thousand files in total, was identified as the testing set. All
  the ML models initialized in the same code run were validated using
  the same testing set. A user-provided list of transformations,
  inputted through a configuration file, was then applied to the data.
  Different trials can be specified, so that multiple sets of
  transformations can be investigated. Both Cartesian and RGB plots
  representations were compared. The latter visualization option was
  obtained by encoding the data into the pixel values of each channel
  composing a color image, for a total of six-channels available (i.e.,
  3 RGB channels in horizontal/vertical orthogonal directions).</p>
  <p>The data was then fed into a convolutional neural network, whose
  hyperparameters and structure were defined using another configuration
  file. Alternatively, it is also possible to train multiple classifiers
  for a single transformation trial through the use of a tuner, by
  instead providing a hyperparameter space and a search method. The
  classification results, as well as the best performing trained model
  were saved for each transformation run. The package also allows to
  visually compare, through parallel coordinates plots (see
  documentation), the performance of each transformation. Figure 2 shows
  a summary of few runs comparing the two visualization strategies and
  their best performing model accuracies.</p>
  <p>Comprehensive results for all transformations tested are available
  in the documentation. It can be noticed that data representation using
  Cartesian coordinate plots yielded a higher number of instances in
  which the accuracy of the trained machine learning model was ~25%.
  This value corresponds to machine learning model’s inability to
  recognize differences in a four-class classification task. On the
  other hand, the RGB plots show, on average, higher accuracy for the
  same combinations of numerical transformations. To further validate
  the results, mathematical fitting was performed on a test set using
  the SASmodels package. The fitting was based on probabilities
  determined by the ML model for each label. In scenarios where the
  output probability was below 70%, the data was also fitted using the
  second highest possible SAS model.</p>
  <fig>
    <caption><p>Model fitting examples for various validation files used
    under the following transformation: <italic>log(q)</italic> &amp;
    <italic>derivative I(q)</italic>. The model was trained using RGB
    images and yielded the highest classification accuracy of 90.1%. The
    bottom labels used correspond to the predicted model that was
    assigned to the data by the model, whereas the top label correspond
    to the original model used to generate the data.</p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/./images/fitting_example.png" xlink:title="" />
  </fig>
  <p>The average chi-square parameter of the fitted data was determined
  to be 7.5. Approximately 11 % of the data had a probability lower than
  70%. In all cases,as seen in figure 3, if the neural network was not
  able to correctly label the data with the highest probability label,
  the second highest probability label was the correct one.</p>
  <p>In conclusion, <monospace>HARDy</monospace> can significantly
  improve data classification so that automatic data fitting and
  modeling can be executed without human intervention and without
  compromising on reliability. We also note that data representation for
  computer-classification tasks may not follow human intuition and/or
  standard conventions. <monospace>HARDy</monospace> serves a key role
  in the optimization of visual data representations for CNN
  classification tasks. Finally, the flexibility of
  <monospace>HARDy</monospace> allows for deployment of the task on a
  supercomputing cluster system, possibly removing the limitations given
  by the high computational power required to run these ML algorithms.
  All configuration files and scripts used to run the example presented
  in this paper can be found in the package documentation.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This project was supported by: National Science Foundation through
  NSF-CBET grant no. 1917340, the Data Intensive Research Enabling Clean
  Technology (DIRECT) National Science Foundation (NSF) National
  Research Traineeship (DGE-1633216), the State of Washington through
  the University of Washington (UW) Clean Energy Institute and the UW
  eScience Institute. This work was facilitated through the use of
  advanced computational, storage, and networking infrastructure
  provided by the Hyak supercomputer system and funded by the STF at the
  University of Washington.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-williams2019enabling">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Williams</surname><given-names>Travis</given-names></name>
          <name><surname>McCullough</surname><given-names>Katherine</given-names></name>
          <name><surname>Lauterbach</surname><given-names>Jochen A</given-names></name>
        </person-group>
        <article-title>Enabling catalyst discovery through machine learning and high-throughput experimentation</article-title>
        <source>Chemistry of Materials</source>
        <publisher-name>ACS Publications</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>32</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1021/acs.chemmater.9b03043.s001</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-becker2019low">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Becker</surname><given-names>Pascal</given-names></name>
          <name><surname>Márquez</surname><given-names>José A</given-names></name>
          <name><surname>Just</surname><given-names>Justus</given-names></name>
          <name><surname>Al-Ashouri</surname><given-names>Amran</given-names></name>
          <name><surname>Hages</surname><given-names>Charles</given-names></name>
          <name><surname>Hempel</surname><given-names>Hannes</given-names></name>
          <name><surname>Jošt</surname><given-names>Marko</given-names></name>
          <name><surname>Albrecht</surname><given-names>Steve</given-names></name>
          <name><surname>Frahm</surname><given-names>Ronald</given-names></name>
          <name><surname>Unold</surname><given-names>Thomas</given-names></name>
        </person-group>
        <article-title>Low temperature synthesis of stable \gamma-CsPbI3 perovskite layers for solar cells obtained by high throughput experimentation</article-title>
        <source>Advanced Energy Materials</source>
        <publisher-name>Wiley Online Library</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>9</volume>
        <issue>22</issue>
        <pub-id pub-id-type="doi">10.1002/aenm.201900555</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hutter2019automated">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Hutter</surname><given-names>Frank</given-names></name>
          <name><surname>Kotthoff</surname><given-names>Lars</given-names></name>
          <name><surname>Vanschoren</surname><given-names>Joaquin</given-names></name>
        </person-group>
        <source>Automated machine learning: Methods, systems, challenges</source>
        <publisher-name>Springer Nature</publisher-name>
        <year iso-8601-date="2019">2019</year>
      </element-citation>
    </ref>
    <ref id="ref-zheng2018feature">
      <element-citation publication-type="book">
        <person-group person-group-type="author">
          <name><surname>Zheng</surname><given-names>Alice</given-names></name>
          <name><surname>Casari</surname><given-names>Amanda</given-names></name>
        </person-group>
        <source>Feature engineering for machine learning: Principles and techniques for data scientists</source>
        <publisher-name>O’Reilly Media, Inc.</publisher-name>
        <year iso-8601-date="2018">2018</year>
      </element-citation>
    </ref>
    <ref id="ref-kanter2015deep">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Kanter</surname><given-names>James Max</given-names></name>
          <name><surname>Veeramachaneni</surname><given-names>Kalyan</given-names></name>
        </person-group>
        <article-title>Deep feature synthesis: Towards automating data science endeavors</article-title>
        <source>2015 IEEE international conference on data science and advanced analytics (DSAA)</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2015">2015</year>
        <pub-id pub-id-type="doi">10.1109/dsaa.2015.7344858</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-nargesian2017learning">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Nargesian</surname><given-names>Fatemeh</given-names></name>
          <name><surname>Samulowitz</surname><given-names>Horst</given-names></name>
          <name><surname>Khurana</surname><given-names>Udayan</given-names></name>
          <name><surname>Khalil</surname><given-names>Elias B.</given-names></name>
          <name><surname>Turaga</surname><given-names>Deepak</given-names></name>
        </person-group>
        <article-title>Learning feature engineering for classification</article-title>
        <source>Proceedings of the twenty-sixth international joint conference on artificial intelligence</source>
        <publisher-name>International Joint Conferences on Artificial Intelligence Organization</publisher-name>
        <year iso-8601-date="2017-08">2017</year><month>08</month>
        <pub-id pub-id-type="doi">10.24963/ijcai.2017/352</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-ArchibaldRichardK2020Caas">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Archibald</surname><given-names>Richard K</given-names></name>
          <name><surname>Doucet</surname><given-names>Mathieu</given-names></name>
          <name><surname>Johnston</surname><given-names>Travis</given-names></name>
          <name><surname>Young</surname><given-names>Steven R</given-names></name>
          <name><surname>Yang</surname><given-names>Erika</given-names></name>
          <name><surname>Heller</surname><given-names>William T</given-names></name>
        </person-group>
        <article-title>Classifying and analyzing small‐angle scattering data using weighted k nearest neighbors machine learning techniques</article-title>
        <source>Journal of Applied Crystallography</source>
        <publisher-name>Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States); International Union of Crystallography</publisher-name>
        <publisher-loc>5 Abbey Square, Chester, Cheshire CH1 2HU, England</publisher-loc>
        <year iso-8601-date="2020">2020</year>
        <volume>53</volume>
        <issue>2</issue>
        <issn>1600-5767</issn>
        <pub-id pub-id-type="doi">10.1107/s1600576720000552</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
