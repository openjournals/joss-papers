<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9688</article-id>
<article-id pub-id-type="doi">10.21105/joss.09688</article-id>
<title-group>
<article-title>pyfive: A pure-Python HDF5 reader</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9262-7860</contrib-id>
<name>
<surname>Lawrence</surname>
<given-names>Bryan N.</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-8437-2068</contrib-id>
<name>
<surname>Cimadevilla</surname>
<given-names>Ezequiel</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-2258-9402</contrib-id>
<name>
<surname>Nolf</surname>
<given-names>Wout De</given-names>
</name>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5312-4950</contrib-id>
<name>
<surname>Hassell</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Helmus</surname>
<given-names>Jonathan</given-names>
</name>
<xref ref-type="aff" rid="aff-5"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Hodel</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="aff-8"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-6105-8789</contrib-id>
<name>
<surname>Maranville</surname>
<given-names>Brian</given-names>
</name>
<xref ref-type="aff" rid="aff-6"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6599-1034</contrib-id>
<name>
<surname>Mühlbauer</surname>
<given-names>Kai</given-names>
</name>
<xref ref-type="aff" rid="aff-7"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-9729-6578</contrib-id>
<name>
<surname>Predoi</surname>
<given-names>Valeriu</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>National Center for Atmospheric Science (NCAS), United
Kingdom.</institution>
<institution-id institution-id-type="ROR">01wwwe276</institution-id>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Department of Meteorology, University of Reading, Reading,
United Kingdom.</institution>
<institution-id institution-id-type="ROR">05v62cm79</institution-id>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Instituto de Física de Cantabria (IFCA), CSIC-Universidad
de Cantabria, Santander, Spain.</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>European Synchrotron Radiation Facility (ESRF), Grenoble,
France.</institution>
</institution-wrap>
</aff>
<aff id="aff-5">
<institution-wrap>
<institution>Astral Software Inc., USA.</institution>
</institution-wrap>
</aff>
<aff id="aff-6">
<institution-wrap>
<institution>NIST Center for Neutron Research, USA.</institution>
</institution-wrap>
</aff>
<aff id="aff-7">
<institution-wrap>
<institution>Institute of Geosciences, Meteorology Section, University
of Bonn, Germany.</institution>
</institution-wrap>
</aff>
<aff id="aff-8">
<institution-wrap>
<institution>Independent Researcher, USA.</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-12-14">
<day>14</day>
<month>12</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>118</issue>
<fpage>9688</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Atmospheric Science</kwd>
<kwd>Physics</kwd>
<kwd>Climate Model Data</kwd>
<kwd>Engineering</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>pyfive</monospace> is an open-source and thread-safe
  pure Python package for reading data stored in HDF5. While it is not a
  complete implementation of all the specifications and capabilities of
  HDF5, it includes all the core functionality necessary to read gridded
  datasets, whether stored contiguously or with chunks (with or without
  standard compression options). All data access is fully lazy as the
  data is only read from storage when the numpy data arrays are
  manipulated. Originally developed some years ago, the package has
  recently been expanded to support lazy data access, and to add missing
  features necessary for handling all the HDF5-based environmental data
  known to the authors. It is now a realistic option for production data
  access in environmental science and more broadly across other domains.
  The API is based on that of <monospace>h5py</monospace>
  (<ext-link ext-link-type="uri" xlink:href="https://github.com/h5py/h5py">https://github.com/h5py/h5py</ext-link>,
  a Python shimmy over the HDF5 C-library which itself is not
  thread-safe), with some API extensions to help optimise remote access.
  With these extensions, coupled with thread safety, many of the
  limitations precluding the efficient use of HDF5 (and netCDF4) on
  cloud storage have been removed.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>HDF5<xref ref-type="fn" rid="fn1">1</xref>
  (<xref alt="Folk et al., 2011" rid="ref-FolEA11" ref-type="bibr">Folk
  et al., 2011</xref>) is arguably the most important data format in
  physical science. It is of particular importance in the environmental
  sciences that rely on the
  netCDF4<xref ref-type="fn" rid="fn2">2</xref>
  (<xref alt="Rew et al., 2006" rid="ref-Rew06" ref-type="bibr">Rew et
  al., 2006</xref>) data format, which itself uses the HDF data format
  underneath. From satellite missions to climate models and radar
  systems, the default binary format has been HDF5 for decades. While
  newer data formats are starting to get mindshare, there are petabytes,
  if not exabytes, of existing HDF5, and there remain many good use
  cases for creating new data in the HDF5 format today. However, despite
  its historical importance, there are few libraries available for
  reading HDF5 file data that do not depend on the official HDF5 library
  maintained by the HDF Group. In particular, apart from
  <monospace>pyfive</monospace>, there are no Python HDF5 libraries that
  address the data access needs of environmental science. While the HDF5
  C library is reliable and performant, and battle-tested over decades,
  there are some caveats to depending upon it. Firstly, it is not
  thread-safe. Secondly, the underlying code is large and complex, and
  should anything happen to the financial stability of the HDF Group, it
  is not obvious it could be maintained. Finally, the code complexity
  also means that it is not suitable for developing bespoke code for
  data recovery in the case of partially corrupt data. From a long-term
  curation perspective these last two constraints present a major
  concern.</p>
  <p>Reliance on a complex codebase controlled by a single private
  company presents significant challenges for long-term data access.
  Addressing these challenges requires well-documented data formats, the
  use of only those documented features, and the existence of publicly
  available code that can be sustainably maintained. The HDF Group have
  provided good documentation for the HDF5 format, but while there are
  communities of developers beyond those of the HDF Group, recent events
  suggest that given most of those developers and their existing funding
  are based in the USA, some spreading of risk would be desirable. To
  that end, a pure Python code covering the core HDF5 features of
  interest to the target scientific community, which is relatively small
  and maintained by an international constituency, provides some
  assurance that the community can maintain HDF5 access for the
  foreseeable future. A pure Python code also makes it easier to develop
  scripts that can work around data and metadata corruption should they
  occur, and has the additional advantage of being able to be deployed
  in resource or operating-system constrained environments (such as on
  mobile).</p>
</sec>
<sec id="current-status-of-pyfive">
  <title>Current Status of pyfive</title>
  <p>The original implementation of <monospace>pyfive</monospace> (by
  JH), which included all the low-level functionality to deal with the
  internals of an HDF5 file, was developed with POSIX access in mind.
  The recent upgrades were developed with the use cases of performant
  remote access to curated data as the primary motivation - including
  full support for lazy loading only the relevant parts of chunked
  datasets as they are needed.</p>
  <p>Thread safety has become a concern given the wide use of
  Dask<xref ref-type="fn" rid="fn3">3</xref> in Python-based analysis
  workflows, and this, coupled with a lack of user knowledge about how
  to efficiently use HDF5, has led to a community perception that HDF5
  is not fit for remote access (especially on cloud storage).
  <monospace>pyfive</monospace> addresses thread safety by bypassing the
  underlying HDF5 C library. It addresses some of the issues with remote
  access by supporting the determination of whether or not a given file
  is cloud-optimised, and by optimising access to internal file metadata
  (in particular, the chunk indexes).</p>
  <p>To improve internal metadata access, <monospace>pyfive</monospace>
  supports several levels of laziness for instantating chunked datasets
  (variables). The default method preloads internal indices to make
  parallellism more efficient, but a completely lazy option without
  index loading is also possible. Neither method loads data until it is
  requested.</p>
  <p>To be fully cloud-optimised, files needs sensible chunking, and
  variables need contiguous indices. Chunking information has always
  been easy to determine. <monospace>pyfive</monospace> now also
  provides simple methods to expose information about internal file
  layout - both in API extensions, and via a new
  <monospace>p5dump</monospace> utility packaged with the
  <monospace>pyfive</monospace>
  library<xref ref-type="fn" rid="fn4">4</xref>. Either method allows
  one to determine whether the key internal “b-tree” indices are
  contiguous in storage, and to determine the parameters necessary to
  rewrite the data with contiguous indices. While
  <monospace>pyfive</monospace> itself cannot rewrite files to address
  chunking or layout, tools such as the HDF5
  <ext-link ext-link-type="uri" xlink:href="https://support.hdfgroup.org/documentation/hdf5/latest/_h5_t_o_o_l__r_p__u_g.html">repack</ext-link>
  utility can do this very efficiently
  (<xref alt="Hassell &amp; Cimadevilla Alvarez, 2025" rid="ref-HasCim25" ref-type="bibr">Hassell
  &amp; Cimadevilla Alvarez, 2025</xref>).</p>
  <p>With the use of <monospace>pyfive</monospace>, suitably repacked
  and rechunked HDF5 data can now be considered “cloud-optimised”,
  insofar as with lazy loading, improved index handling, and
  thread-safety, there are no “format-induced” constraints on
  performance during remote access.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>Most of the recent developments outlined have been supported by the
  UK Met Office and UKRI via 1) UK Excalibur Exascale programme
  (ExcaliWork), 2) the UKRI Digital Research Infrastructure programme
  (WacaSoft), and 3) the national capability funding of the UK National
  Center for Atmospheric Science (NCAS). Ongoing maintenance of
  <monospace>pyfive</monospace> is expected to continue with NCAS
  national capability funding.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-FolEA11">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Folk</surname><given-names>Mike</given-names></name>
        <name><surname>Heber</surname><given-names>Gerd</given-names></name>
        <name><surname>Koziol</surname><given-names>Quincey</given-names></name>
        <name><surname>Pourmal</surname><given-names>Elena</given-names></name>
        <name><surname>Robinson</surname><given-names>Dana</given-names></name>
      </person-group>
      <article-title>An overview of the HDF5 technology suite and its applications</article-title>
      <source>Proceedings of the EDBT/ICDT 2011 workshop on array databases</source>
      <publisher-name>ACM</publisher-name>
      <publisher-loc>Upsala</publisher-loc>
      <year iso-8601-date="2011-03">2011</year><month>03</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2018-05-23">2018</year><month>05</month><day>23</day></date-in-citation>
      <isbn>978-1-4503-0614-0</isbn>
      <pub-id pub-id-type="doi">10.1145/1966895.1966900</pub-id>
      <fpage>36</fpage>
      <lpage>47</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HasCim25">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hassell</surname><given-names>David</given-names></name>
        <name><surname>Cimadevilla Alvarez</surname><given-names>Ezequiel</given-names></name>
      </person-group>
      <article-title>Cmip7repack: Repack CMIP7 netCDF-4 datasets</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2025-11">2025</year><month>11</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-11-07">2025</year><month>11</month><day>07</day></date-in-citation>
      <pub-id pub-id-type="doi">10.5281/zenodo.17550920</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Rew06">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rew</surname><given-names>Russ</given-names></name>
        <name><surname>Hartnett</surname><given-names>Ed</given-names></name>
        <name><surname>Caron</surname><given-names>John</given-names></name>
      </person-group>
      <article-title>NetCDF-4: Software implementing an enhanced data model for the geosciences</article-title>
      <source>22nd inernational conference on interactive information processing systems for meteorology, oceanography and hydrology</source>
      <publisher-name>AMS</publisher-name>
      <publisher-loc>Atlanta</publisher-loc>
      <year iso-8601-date="2006">2006</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2016-05-10">2016</year><month>05</month><day>10</day></date-in-citation>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>https://www.hdfgroup.org/solutions/hdf5/</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>https://www.unidata.ucar.edu/software/netcdf</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>https://www.dask.org/</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>https://pyfive.readthedocs.io/</p>
  </fn>
</fn-group>
</back>
</article>
