<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8497</article-id>
<article-id pub-id-type="doi">10.21105/joss.08497</article-id>
<title-group>
<article-title>fairmetrics: An R package for group fairness
evaluation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-2206-0177</contrib-id>
<name>
<surname>Smith</surname>
<given-names>Benjamin</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0915-1473</contrib-id>
<name>
<surname>Gao</surname>
<given-names>Jianhui</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-0265-033X</contrib-id>
<name>
<surname>Chou</surname>
<given-names>Benson</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5360-5869</contrib-id>
<name>
<surname>Gronsbell</surname>
<given-names>Jessica</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Department of Statistical Sciences, University of
Toronto</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-06-10">
<day>10</day>
<month>6</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>113</issue>
<fpage>8497</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>R</kwd>
<kwd>Fairness</kwd>
<kwd>Machine Learning</kwd>
<kwd>Software</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Fairness is a growing area of machine learning (ML) that focuses on
  ensuring that models do not produce systematically biased outcomes
  across groups defined by protected attributes, such as race, gender,
  or age. The <monospace>fairmetrics</monospace> R package provides a
  user-friendly framework for rigorously evaluating group-based fairness
  criteria, including independence (e.g., statistical parity),
  separation (e.g., equalized odds), and sufficiency (e.g., predictive
  parity) for binary protected attributes. The package provides both
  point and interval estimates for a variety of commonly used criteria.
  <monospace>fairmetrics</monospace> also includes an example dataset
  derived from the Medical Information Mart for Intensive Care, version
  II (MIMIC-II) database
  (<xref alt="Goldberger et al., 2000" rid="ref-goldberger2000physiobank" ref-type="bibr">Goldberger
  et al., 2000</xref>;
  <xref alt="J. Raffa, 2016" rid="ref-raffa2016clinical" ref-type="bibr">J.
  Raffa, 2016</xref>;
  <xref alt="J. D. Raffa et al., 2016" rid="ref-raffa2016data" ref-type="bibr">J.
  D. Raffa et al., 2016</xref>) to demonstrate its use.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>ML models are increasingly used in high-stakes domains such as
  criminal justice, healthcare, finance, employment, and education
  (<xref alt="Gao et al., 2024" rid="ref-Gao_Chou_McCaw_Thurston_Varghese_Hong_Gronsbell_2024" ref-type="bibr">Gao
  et al., 2024</xref>;
  <xref alt="Mattu, 2016" rid="ref-mattuMachineBias" ref-type="bibr">Mattu,
  2016</xref>;
  <xref alt="Mehrabi et al., 2021" rid="ref-mehrabi_survey_21" ref-type="bibr">Mehrabi
  et al., 2021</xref>). Existing fairness evaluation software report
  point estimates and/or visualizations, without any measures of
  uncertainty. This limits users’ ability to determine whether observed
  disparities are statistically significant.
  <monospace>fairmetrics</monospace> addresses this limitation by
  including confidence intervals (CIs) for both difference and ratio
  based fairness metrics to enable more robust and statistically
  grounded fairness assessments.</p>
</sec>
<sec id="fairness-criteria">
  <title>Fairness Criteria</title>
  <p><monospace>fairmetrics</monospace> is designed to evaluate fairness
  of binary classification models across binary protected attributes.
  The package supports the evaluation of metrics belonging to three
  major group fairness criteria:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Independence:</bold> Statistical Parity (compares the
      overall rate of positive predictions between groups).</p>
    </list-item>
    <list-item>
      <p><bold>Separation:</bold> Equal Opportunity (compares false
      negative rates between groups), Predictive Equality (compares
      false positive rates between groups), Balance for Positive Class
      (compares the average predicted probabilities among individuals
      whose true outcome is positive across groups), and Balance for
      Negative Class (compares the average predicted probabilities among
      individuals whose true outcome is negative across groups).</p>
    </list-item>
    <list-item>
      <p><bold>Sufficiency:</bold> Positive Predictive Parity (compares
      the positive predictive values across groups), Negative Predictive
      Parity (compares the negative predictive values across
      groups).</p>
    </list-item>
  </list>
  <p>The package also includes additional metrics, such as the Brier
  Score Parity (compares the Brier score across groups), Accuracy Parity
  (compares the overall accuracy across groups), and Treatment Equality
  (compares the ratio of false negatives to false positives across
  groups).</p>
</sec>
<sec id="evaluating-fairness-criteria">
  <title>Evaluating Fairness Criteria</title>
  <p>The input required to evaluate model fairness with the
  <monospace>fairmetrics</monospace> package is a
  <monospace>data.frame</monospace> or <monospace>tibble</monospace>
  containing the model’s predicted probabilities, the true outcomes, and
  the protected attribute.
  <xref alt="Figure  [workflow]" rid="workflow">Figure
   <xref alt="[workflow]" rid="workflow">[workflow]</xref></xref> shows
  the workflow for using <monospace>fairmetrics</monospace>.</p>
  <fig>
    <caption><p>Workflow for using <monospace>fairmetrics</monospace> to
    evaluate model fairness across multiple criteria.
    <styled-content id="workflow"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="fairmetrics-workflow.png" />
  </fig>
  <p>A simple example of how to use the
  <monospace>fairmetrics</monospace> package is illustrated below. The
  example makes use of the <monospace>mimic_preprocessed</monospace>
  dataset, a pre-processed version of the Indwelling Arterial Catheter
  (IAC) Clinical dataset, from the MIMIC-II clinical database
  (<xref alt="Goldberger et al., 2000" rid="ref-goldberger2000physiobank" ref-type="bibr">Goldberger
  et al., 2000</xref>;
  <xref alt="J. Raffa, 2016" rid="ref-raffa2016clinical" ref-type="bibr">J.
  Raffa, 2016</xref>;
  <xref alt="J. D. Raffa et al., 2016" rid="ref-raffa2016data" ref-type="bibr">J.
  D. Raffa et al., 2016</xref>).</p>
  <p>While the choice of fairness metric used is context dependent, we
  show all criteria available with the
  <monospace>get_fairness_metrics()</monospace> function for
  illustrative purposes. In this example, we evaluate the model’s
  fairness with respect to the binary protected attribute
  <monospace>gender</monospace>. The model is trained on a subset of the
  data and the predictions are made and evaluated on a test set. A
  statistically significant difference across groups at a given level of
  significance is indicated when the CI for a difference-based metric
  does not include zero or when the interval for a ratio-based metric
  does not include one.</p>
  <code language="r script">library(fairmetrics)
# Setting alpha=0.05 for 95% CIs
get_fairness_metrics(
 data = test_data,
 outcome = &quot;day_28_flg&quot;,
 group = &quot;gender&quot;,
 probs = &quot;pred&quot;,
 cutoff = 0.41, 
 alpha = 0.05
)

           Fairness Assessment                                 Metric
1          Statistical Parity                Positive Prediction Rate
2           Equal Opportunity                     False Negative Rate
3         Predictive Equality                     False Positive Rate
4  Balance for Positive Class           Avg. Predicted Positive Prob.
5  Balance for Negative Class           Avg. Predicted Negative Prob.
6  Positive Predictive Parity               Positive Predictive Value
7  Negative Predictive Parity               Negative Predictive Value
8          Brier Score Parity                             Brier Score
9     Overall Accuracy Parity                                Accuracy
10         Treatment Equality (False Negative)/(False Positive) Ratio

   GroupFemale GroupMale Difference    95% Diff CI Ratio 95% Ratio CI
1         0.17      0.08       0.09   [0.05, 0.13]  2.12 [1.49, 3.04]
2         0.38      0.62      -0.24 [-0.39, -0.09]  0.61 [0.44, 0.86]
3         0.08      0.03       0.05   [0.02, 0.08]  2.67  [1.4, 5.08]
4         0.46      0.37       0.09   [0.04, 0.14]  1.24 [1.09, 1.42]
5         0.15      0.10       0.05   [0.03, 0.07]  1.50 [1.29, 1.74]
6         0.62      0.66      -0.04  [-0.21, 0.13]  0.94 [0.72, 1.22]
7         0.92      0.90       0.02  [-0.02, 0.06]  1.02 [0.98, 1.07]
8         0.09      0.08       0.01  [-0.01, 0.03]  1.12 [0.89, 1.43]
9         0.87      0.88      -0.01  [-0.05, 0.03]  0.99 [0.94, 1.04]
10        1.03      3.24      -2.21 [-4.38, -0.04]  0.32 [0.15, 0.68]</code>
  <p>Users can also compute individual metrics using functions like
  <monospace>eval_eq_opp()</monospace> to test specific fairness
  conditions. Full usage examples are provided in the package
  documentation.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>Other R packages similar to <monospace>fairmetrics</monospace>
  include <monospace>fairness</monospace>
  (<xref alt="Kozodoi &amp; V. Varga, 2021" rid="ref-fairness_package" ref-type="bibr">Kozodoi
  &amp; V. Varga, 2021</xref>), <monospace>fairmodels</monospace>
  (<xref alt="Wiśniewski &amp; Biecek, 2022" rid="ref-wisniewski2022fairmodels" ref-type="bibr">Wiśniewski
  &amp; Biecek, 2022</xref>) and <monospace>mlr3fairness</monospace>
  (<xref alt="Pfisterer et al., 2024" rid="ref-mlr3fairness_package" ref-type="bibr">Pfisterer
  et al., 2024</xref>). <monospace>fairmetrics</monospace> differs from
  these packages in two ways. The first difference is that
  <monospace>fairmetrics</monospace> calculates ratio and
  difference-based group fairness metrics and their corresponding CIs,
  allowing for more meaningful inferences about the fairness criteria.
  The second difference is that <monospace>fairmetrics</monospace> does
  not possess any external dependencies and has a lower memory
  footprint.
  <xref alt="Table [tab:memory_dep_usage]" rid="tabU003Amemory_dep_usage">Table <xref alt="[tab:memory_dep_usage]" rid="tabU003Amemory_dep_usage">[tab:memory_dep_usage]</xref></xref>
  shows the comparison of memory used and dependencies required when
  loading each library.</p>
  <boxed-text id="tabU003Amemory_dep_usage">
    <table-wrap>
      <caption>
        <p>Memory usage (in MB) and dependencies of ‘fairmetrics‘ vs
        similar packages.</p>
      </caption>
      <table>
        <thead>
          <tr>
            <th align="left"><bold>Package</bold></th>
            <th align="right"><bold>Memory (MB)</bold></th>
            <th align="right"><bold>Dependencies</bold></th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td align="left">fairmodels</td>
            <td align="right">17.02</td>
            <td align="right">29</td>
          </tr>
          <tr>
            <td align="left">fairness</td>
            <td align="right">117.61</td>
            <td align="right">141</td>
          </tr>
          <tr>
            <td align="left">mlr3fairness</td>
            <td align="right">58.11</td>
            <td align="right">45</td>
          </tr>
          <tr>
            <td align="left">fairmetrics</td>
            <td align="right">0.05</td>
            <td align="right">0</td>
          </tr>
        </tbody>
      </table>
    </table-wrap>
  </boxed-text>
  <p>For Python users, the <monospace>fairlearn</monospace> library
  (<xref alt="Weerts et al., 2023" rid="ref-fairlearn_paper" ref-type="bibr">Weerts
  et al., 2023</xref>) provides additional fairness metrics and
  algorithms. The <monospace>fairmetrics</monospace> package is designed
  for seamless integration with R workflows, making it a more convenient
  choice for R users.</p>
</sec>
<sec id="licensing-and-availability">
  <title>Licensing and Availability</title>
  <p>The <monospace>fairmetrics</monospace> package is under the MIT
  license. It is available on CRAN and can be installed by using
  <monospace>install.packages(&quot;fairmetrics&quot;)</monospace>. Full
  documentation and its examples are available at:
  https://jianhuig.github.io/fairmetrics/articles/fairmetrics.html.
  Source code and issue tracking are hosted on GitHub:
  https://github.com/jianhuig/fairmetrics/.</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-mattuMachineBias">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Mattu</surname><given-names>Lauren Kirchner</given-names><suffix>Jeff Larson</suffix></name>
      </person-group>
      <article-title>Machine Bias</article-title>
      <source>ProPublica</source>
      <publisher-name>https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-05-29">2025</year><month>05</month><day>29</day></date-in-citation>
    </element-citation>
  </ref>
  <ref id="ref-mehrabi_survey_21">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mehrabi</surname><given-names>Ninareh</given-names></name>
        <name><surname>Morstatter</surname><given-names>Fred</given-names></name>
        <name><surname>Saxena</surname><given-names>Nripsuta</given-names></name>
        <name><surname>Lerman</surname><given-names>Kristina</given-names></name>
        <name><surname>Galstyan</surname><given-names>Aram</given-names></name>
      </person-group>
      <article-title>A survey on bias and fairness in machine learning</article-title>
      <source>ACM Comput. Surv.</source>
      <publisher-name>Association for Computing Machinery</publisher-name>
      <publisher-loc>New York, NY, USA</publisher-loc>
      <year iso-8601-date="2021-07">2021</year><month>07</month>
      <volume>54</volume>
      <issue>6</issue>
      <issn>0360-0300</issn>
      <uri>https://doi.org/10.1145/3457607</uri>
      <pub-id pub-id-type="doi">10.1145/3457607</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Gao_Chou_McCaw_Thurston_Varghese_Hong_Gronsbell_2024">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gao</surname><given-names>Jianhui</given-names></name>
        <name><surname>Chou</surname><given-names>Benson</given-names></name>
        <name><surname>McCaw</surname><given-names>Zachary R.</given-names></name>
        <name><surname>Thurston</surname><given-names>Hilary</given-names></name>
        <name><surname>Varghese</surname><given-names>Paul</given-names></name>
        <name><surname>Hong</surname><given-names>Chuan</given-names></name>
        <name><surname>Gronsbell</surname><given-names>Jessica</given-names></name>
      </person-group>
      <article-title>What is fair? Defining fairness in machine learning for health</article-title>
      <source>arXiv.org</source>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <uri>https://arxiv.org/abs/2406.09307</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2406.09307</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-fairness_package">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Kozodoi</surname><given-names>Nikita</given-names></name>
        <name><surname>V. Varga</surname><given-names>Tibor</given-names></name>
      </person-group>
      <source>fairness: Algorithmic fairness metrics</source>
      <year iso-8601-date="2021">2021</year>
      <uri>https://CRAN.R-project.org/package=fairness</uri>
      <pub-id pub-id-type="doi">10.32614/cran.package.fairness</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-raffa2016clinical">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Raffa</surname><given-names>Jesse</given-names></name>
      </person-group>
      <article-title>Clinical data from the MIMIC-II database for a case study on indwelling arterial catheters (version 1.0)</article-title>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.13026/C2NC7F</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-raffa2016data">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Raffa</surname><given-names>Jesse D.</given-names></name>
        <name><surname>Ghassemi</surname><given-names>Mohammad</given-names></name>
        <name><surname>Naumann</surname><given-names>Tristan</given-names></name>
        <name><surname>Feng</surname><given-names>Mengling</given-names></name>
        <name><surname>Hsu</surname><given-names>Daniel J.</given-names></name>
      </person-group>
      <article-title>Data analysis</article-title>
      <source>Secondary analysis of electronic health records</source>
      <publisher-name>Springer, Cham</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1007/978-3-319-43742-2_9</pub-id>
      <fpage>109</fpage>
      <lpage>122</lpage>
    </element-citation>
  </ref>
  <ref id="ref-goldberger2000physiobank">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Goldberger</surname><given-names>Ary L.</given-names></name>
        <name><surname>Amaral</surname><given-names>Luis A. N.</given-names></name>
        <name><surname>Glass</surname><given-names>Leon</given-names></name>
        <name><surname>Hausdorff</surname><given-names>Jeffrey M.</given-names></name>
        <name><surname>Ivanov</surname><given-names>Plamen Ch.</given-names></name>
        <name><surname>Mark</surname><given-names>Roger G.</given-names></name>
        <name><surname>Mietus</surname><given-names>Joseph E.</given-names></name>
        <name><surname>Moody</surname><given-names>George B.</given-names></name>
        <name><surname>Peng</surname><given-names>Chung-Kang</given-names></name>
        <name><surname>Stanley</surname><given-names>H. Eugene</given-names></name>
      </person-group>
      <article-title>PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals</article-title>
      <source>Circulation [Online]</source>
      <year iso-8601-date="2000">2000</year>
      <volume>101</volume>
      <issue>23</issue>
      <uri>https://doi.org/10.1161/01.CIR.101.23.e215</uri>
      <pub-id pub-id-type="doi">10.1161/01.CIR.101.23.e215</pub-id>
      <fpage>e215</fpage>
      <lpage>e220</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wisniewski2022fairmodels">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wiśniewski</surname><given-names>Jakub</given-names></name>
        <name><surname>Biecek</surname><given-names>Przemysław</given-names></name>
      </person-group>
      <article-title>fairmodels: A flexible tool for bias detection, visualization, and mitigation in binary classification models</article-title>
      <source>The R Journal</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>1</issue>
      <uri>https://rj.urbanek.nz/articles/RJ-2022-019/</uri>
      <pub-id pub-id-type="doi">10.32614/RJ-2022-019</pub-id>
      <fpage>227</fpage>
      <lpage>243</lpage>
    </element-citation>
  </ref>
  <ref id="ref-fairlearn_paper">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Weerts</surname><given-names>Hilde</given-names></name>
        <name><surname>Dudík</surname><given-names>Miroslav</given-names></name>
        <name><surname>Edgar</surname><given-names>Richard</given-names></name>
        <name><surname>Jalali</surname><given-names>Adrin</given-names></name>
        <name><surname>Lutz</surname><given-names>Roman</given-names></name>
        <name><surname>Madaio</surname><given-names>Michael</given-names></name>
      </person-group>
      <article-title>FairLearn: Assessing and improving fairness of AI systems</article-title>
      <source>arXiv.org</source>
      <year iso-8601-date="2023-03">2023</year><month>03</month>
      <uri>https://arxiv.org/abs/2303.16626</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2303.16626</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-mlr3fairness_package">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Pfisterer</surname><given-names>Florian</given-names></name>
        <name><surname>Siyi</surname><given-names>Wei</given-names></name>
        <name><surname>Lang</surname><given-names>Michel</given-names></name>
      </person-group>
      <source>mlr3fairness: Fairness auditing and debiasing for ’mlr3’</source>
      <year iso-8601-date="2024">2024</year>
      <uri>https://mlr3fairness.mlr-org.com</uri>
      <pub-id pub-id-type="doi">10.32614/cran.package.mlr3fairness</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
