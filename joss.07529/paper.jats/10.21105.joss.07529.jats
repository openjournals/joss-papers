<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">7529</article-id>
<article-id pub-id-type="doi">10.21105/joss.07529</article-id>
<title-group>
<article-title>Generating Visualizations Conversationally using Guided
Autocomplete and LLMs</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-6515-1671</contrib-id>
<name>
<surname>Smith</surname>
<given-names>Andrew K</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-5622-8683</contrib-id>
<name>
<surname>Neuhaus</surname>
<given-names>Isaac</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Informatics &amp; Predictive Sciences, Knowledge Science
Research, Computational Genomics, Bristol Myers Squibb 3551
Lawrenceville Rd, Lawrence Township, NJ 08648</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-09-30">
<day>30</day>
<month>9</month>
<year>2024</year>
</pub-date>
<volume>10</volume>
<issue>114</issue>
<fpage>7529</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Artificial Intelligence (AI)</kwd>
<kwd>Natural Language Processing (NLP)</kwd>
<kwd>Large Language Model (LLM)</kwd>
<kwd>Generative AI</kwd>
<kwd>Visualization</kwd>
<kwd>Data Analytics</kwd>
<kwd>Exploratory Data Analysis (EDA)</kwd>
<kwd>Bioinformatics</kwd>
<kwd>CanvasXpress</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Powerful data visualization packages like CanvasXpress offer rich
  features for exploring datasets but often have a high learning curve,
  requiring detailed web development skills. We present a system that
  combines guided autocomplete and Large Language Models (LLMs) to
  simplify the use of such packages, focusing specifically on
  CanvasXpress. Users upload tabular data or start from any CanvasXpress
  visualization, then describe their desired visualization in plain
  English—either by following guided suggestions or typing free-form
  text. Guided autocomplete ensures perfect configuration accuracy,
  while LLMs serve as a robust backup for flexible, free-form input. The
  system can also generate large-scale synthetic training data for use
  as few-shot examples or for fine-tuning. We evaluate accuracy of LLM
  CanvasXpress configuration generation using thousands of synthetic
  examples with Retrieval Augmented Generation (RAG), achieving over 97%
  exact match accuracy. Results show that accuracy increases with more
  few-shot examples but decreases with prompt complexity, highlighting
  the challenge of generating complex visualizations from natural
  language.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of Need</title>
  <p>CanvasXpress
  (<xref alt="Neuhaus, n.d." rid="ref-neuhaus_canvasxpress_nodate" ref-type="bibr">Neuhaus,
  n.d.</xref>) is a JavaScript library for interactive data
  visualizations using HTML5 canvas technology. It supports a wide range
  of chart types and allows extensive customization through JSON
  configuration objects. However, the need for coding skills creates
  barriers for domain scientists in genomics, proteomics, and clinical
  studies, who may have deep field knowledge but limited programming
  experience. This presents a research challenge: can we enable
  visualization creation through natural language, lowering the
  technical barrier and accelerating scientific discovery?</p>
  <p>Our system leverages guided autocomplete and LLMs for
  conversational visualization generation. CanvasXpress is particularly
  suitable for LLM-based generation due to its well-structured JSON
  schema, declarative approach, sensible defaults, and scientific domain
  focus. Unlike procedural libraries that require complex code
  generation, CanvasXpress’s declarative configurations are natural
  targets for LLMs. While other libraries like Plotly also use JSON,
  CanvasXpress’s schema and domain focus make it especially well-suited
  for this approach.</p>
  <p>To simplify visualization generation, users upload tabular data
  (CSV/TSV) and describe the desired graph in plain English, such as “I
  want a line graph comparing the quarterly sales growth of Product A
  and Product B.” The system guides users with autocomplete suggestions
  or allows free-form input, making visualization accessible to
  non-programmers.</p>
</sec>
<sec id="implementation">
  <title>Implementation</title>
  <sec id="guided-autocomplete">
    <title>Guided Autocomplete</title>
    <p>Guided autocomplete helps users build prompts incrementally by
    suggesting valid completions at each step. For example, typing
    “Create” offers options like “a bar graph” or “a line chart.” This
    approach, inspired by menu-based natural language systems
    (<xref alt="Tennant, 1984" rid="ref-tennant_menu-based_1984" ref-type="bibr">Tennant,
    1984</xref>), ensures perfect accuracy when followed, as the system
    can deterministically generate the correct CanvasXpress JSON
    configuration. If users deviate from suggestions, the LLM serves as
    a backup, interpreting free-form input to generate the
    configuration. The guided autocomplete system, called “Copilot,” can
    also generate synthetic prompt/configuration pairs at scale, which
    are used as few-shot examples for LLM prompting or as training data
    for fine-tuning (we also use in a train/test split for our accuracy
    testing).</p>
  </sec>
  <sec id="prompt-engineering-and-retrieval-augmented-generation-rag">
    <title>Prompt Engineering and Retrieval Augmented Generation
    (RAG)</title>
    <p>Prompt engineering is critical for effective LLM use in
    visualization tasks. We employ few-shot prompting
    (<xref alt="Brown et al., 2020" rid="ref-brown_language_2020" ref-type="bibr">Brown
    et al., 2020</xref>;
    <xref alt="Schulhoff, 2024" rid="ref-schulhoff_showing_2024" ref-type="bibr">Schulhoff,
    2024</xref>), where the LLM is provided with several example user
    queries and their correct outputs. All few-shot examples are
    generated automatically by the guided autocomplete system, not
    manually curated, enabling the creation of thousands of diverse,
    high-quality examples efficiently. Users can also enrich the system
    with their own few-shot examples if desired.</p>
    <p>Due to LLM context window limits, we provide schema information
    for the ~150 most-used CanvasXpress fields (out of ~1500 total),
    which suffices for most use cases. Retrieval Augmented Generation
    (RAG)
    (<xref alt="Gao et al., 2023" rid="ref-gao_retrieval-augmented_2023" ref-type="bibr">Gao
    et al., 2023</xref>) uses vector databases to efficiently select the
    most relevant few-shot examples for each user query. We create
    1024-dimensional semantic vectors of all few-shot English
    descriptions using the BGE-M3 embedding model
    (<xref alt="Chen et al., 2024" rid="ref-chen_bge_2024" ref-type="bibr">Chen
    et al., 2024</xref>), storing them in a PyMilvus/Milvus
    (<xref alt="Guo et al., 2022" rid="ref-guo_manu_2022" ref-type="bibr">Guo
    et al., 2022</xref>;
    <xref alt="Wang et al., 2021" rid="ref-wang_milvus_2021" ref-type="bibr">Wang
    et al., 2021</xref>) vector database. User queries are vectorized
    and searched against the vector database to retrieve the 25 most
    semantically similar examples.</p>
  </sec>
</sec>
<sec id="assessing-accuracy">
  <title>Assessing Accuracy</title>
  <p>We evaluated two factors: the number of few-shot examples (100–2500
  in increments of 100) and prompt complexity (maximum 4, 6, or 10
  sentences). For each synthetic prompt, we generated three alternative
  phrasings using GPT-4o to ensure diversity of expression. Accuracy
  metrics include exact match percentage and a JSON similarity score,
  which recursively compares structure and values of generated and
  reference configurations.</p>
  <p>Few-shot examples were systematically generated to cover common
  chart types, different data structures, and customization options. We
  generated up to 2,500 examples for evaluation across complexity
  levels. Results show that accuracy improves with more few-shot
  examples and then plateaus, with the 4-sentence dataset showing a
  slight decrease around 1800 examples. Accuracy is excellent (&gt;97%)
  for all datasets but decreases with complexity—4-sentence prompts
  outperform 6-sentence, which outperform 10-sentence prompts. This
  suggests that longer, more complex descriptions pose greater
  challenges for LLMs.</p>
  <p>Future work includes fine-tuning LLMs using synthetic data
  (thousands of examples easily generated by the Copilot). We have
  already developed a basic agentic version of the system using AWS
  Bedrock, and we plan to further enhance and expand this as future work
  (see the “User Interface” section below for details).</p>
  <p>While the system achieves high accuracy for most use cases, there
  are current limitations. Due to LLM context window constraints, only
  the 150 most-used CanvasXpress fields are supported, and very complex,
  multi-step visualization requests may still challenge current LLMs. In
  future work, we plan to expand schema coverage, explore
  chain-of-thought prompting, and implement divide-and-conquer
  strategies for handling more complex tasks.</p>
</sec>
<sec id="research-impact-and-applications">
  <title>Research Impact and Applications</title>
  <p>Our system addresses research challenges by: (1) reducing time from
  analysis to visualization, enabling faster hypothesis generation and
  testing; (2) allowing researchers to explore more visualization
  options without technical barriers; (3) eliminating the need for
  specialized programming support; and (4) accelerating research in
  data-intensive domains. For example, genomics researchers can generate
  complex heatmaps by describing “show me a heatmap of gene expression
  levels across tissue types, clustered by similarity” rather than
  learning JSON syntax.</p>
</sec>
<sec id="related-work">
  <title>Related Work</title>
  <p>While RAG and few-shot learning are established in machine
  learning, their application to scientific visualization generation is
  novel. Standard RAG focuses on text generation, whereas we generate
  structured JSON configurations. Our approach uses guided autocomplete
  for automatic synthetic example generation, avoiding manual curation
  typical in few-shot applications.</p>
  <p>Most visualization libraries present challenges for LLM generation:
  D3.js requires DOM manipulation, ggplot2 uses sequential function
  calls, and Matplotlib relies on procedural commands. These require
  syntactically correct executable code rather than declarative
  configurations. CanvasXpress’s JSON-based approach creates natural LLM
  targets with graceful parameter handling and meaningful defaults.</p>
</sec>
<sec id="user-interface">
  <title>User Interface</title>
  <p>We integrated the Copilot and LLM generation UI into every
  CanvasXpress visualization, making it the first standalone JavaScript
  library leveraging client-side AI. JSONP calls ensure fast access to
  canvasxpress.org servers, sending only prompts, parameters, and
  headers to minimize load. Users can also implement their own services.
  The integrated UI allows describing new visualizations through guided
  Copilot or free-form text, with results replacing current
  visualizations or adding new ones.</p>
  <p>The system is implemented as a professional Python package with
  modular architecture, featuring comprehensive automated testing with
  over 85 unit and integration tests validating both real API
  functionality and graceful fallback to mock responses when external
  services are unavailable.</p>
  <p>We have also developed an initial agentic version of our system
  using AWS Bedrock, which lays the groundwork for capabilities beyond
  simple visualization generation. While still in its early stages, this
  agent-based implementation is designed to eventually support
  autonomous interaction with multiple data sources, execution of
  complex multi-step analysis workflows, and integration with other AI
  agents for comprehensive data analysis and visualization pipelines. As
  the system matures, the agentic architecture is intended to enable
  more sophisticated use cases, such as automatically suggesting
  appropriate visualization types based on data characteristics,
  performing data preprocessing and validation, and generating multiple
  complementary visualizations to support comprehensive data
  exploration.</p>
  <fig>
    <caption><p>Accuracy results for different datasets showing accuracy
    as number of few shots
    increases.<styled-content id="figU003Atable"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="four_six_ten_results.png" />
  </fig>
  <fig>
    <caption><p>Current web UI for CanvasXpress LLM generation working
    as a chatbot for continual visualization description and
    updates.<styled-content id="figU003AUI"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="CX_chat_ui.png" />
  </fig>
  <fig>
    <caption><p>Overview of how the Copilot/guided autocomplete
    works.<styled-content id="figU003Aguided_autocomplete"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="GuidedAutocompleteFig.png" />
  </fig>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-schulhoff_showing_2024">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Schulhoff</surname><given-names>Sander</given-names></name>
      </person-group>
      <article-title>Showing Examples</article-title>
      <source>Showing Examples</source>
      <year iso-8601-date="2024-07">2024</year><month>07</month>
      <uri>https://learnprompting.org/docs/basics/few_shot</uri>
    </element-citation>
  </ref>
  <ref id="ref-neuhaus_canvasxpress_nodate">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Neuhaus</surname><given-names>Isaac</given-names></name>
      </person-group>
      <article-title>CanvasXpress: A JavaScript Library for Data Analytics with Full Audit Trail Capabilities</article-title>
      <uri>https://www.canvasxpress.org/</uri>
    </element-citation>
  </ref>
  <ref id="ref-gao_retrieval-augmented_2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Gao</surname><given-names>Yunfan</given-names></name>
        <name><surname>Xiong</surname><given-names>Yun</given-names></name>
        <name><surname>Gao</surname><given-names>Xinyu</given-names></name>
        <name><surname>Jia</surname><given-names>Kangxiang</given-names></name>
        <name><surname>Pan</surname><given-names>Jinliu</given-names></name>
        <name><surname>Bi</surname><given-names>Yuxi</given-names></name>
        <name><surname>Dai</surname><given-names>Yi</given-names></name>
        <name><surname>Sun</surname><given-names>Jiawei</given-names></name>
        <name><surname>Wang</surname><given-names>Meng</given-names></name>
        <name><surname>Wang</surname><given-names>Haofen</given-names></name>
      </person-group>
      <article-title>Retrieval-Augmented Generation for Large Language Models: A Survey</article-title>
      <source>arXiv.org</source>
      <year iso-8601-date="2023-12">2023</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-08-21">2024</year><month>08</month><day>21</day></date-in-citation>
      <uri>https://arxiv.org/abs/2312.10997v5</uri>
    </element-citation>
  </ref>
  <ref id="ref-chen_bge_2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Chen</surname><given-names>Jianlv</given-names></name>
        <name><surname>Xiao</surname><given-names>Shitao</given-names></name>
        <name><surname>Zhang</surname><given-names>Peitian</given-names></name>
        <name><surname>Luo</surname><given-names>Kun</given-names></name>
        <name><surname>Lian</surname><given-names>Defu</given-names></name>
        <name><surname>Liu</surname><given-names>Zheng</given-names></name>
      </person-group>
      <article-title>BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2024-08-21">2024</year><month>08</month><day>21</day></date-in-citation>
      <uri>http://arxiv.org/abs/2402.03216</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2402.03216</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-tennant_menu-based_1984">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Tennant</surname><given-names>Harry</given-names></name>
      </person-group>
      <article-title>Menu-based natural language understanding</article-title>
      <publisher-name>IEEE Computer Society</publisher-name>
      <year iso-8601-date="1984-12">1984</year><month>12</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-01-08">2025</year><month>01</month><day>08</day></date-in-citation>
      <uri>https://www.computer.org/csdl/proceedings-article/afips/1984/50910629/12OmNwcUjYE</uri>
      <pub-id pub-id-type="doi">10.1109/AFIPS.1984.52</pub-id>
      <fpage>629</fpage>
      <lpage>629</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wang_milvus_2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Wang</surname><given-names>Jianguo</given-names></name>
        <name><surname>Yi</surname><given-names>Xiaomeng</given-names></name>
        <name><surname>Guo</surname><given-names>Rentong</given-names></name>
        <name><surname>Jin</surname><given-names>Hai</given-names></name>
        <name><surname>Xu</surname><given-names>Peng</given-names></name>
        <name><surname>Li</surname><given-names>Shengjun</given-names></name>
        <name><surname>Wang</surname><given-names>Xiangyu</given-names></name>
        <name><surname>Guo</surname><given-names>Xiangzhou</given-names></name>
        <name><surname>Li</surname><given-names>Chengming</given-names></name>
        <name><surname>Xu</surname><given-names>Xiaohai</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Milvus: A Purpose-Built Vector Data Management System</article-title>
      <source>Proceedings of the 2021 international conference on management of data</source>
      <year iso-8601-date="2021">2021</year>
      <fpage>2614</fpage>
      <lpage>2627</lpage>
    </element-citation>
  </ref>
  <ref id="ref-guo_manu_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Guo</surname><given-names>Rentong</given-names></name>
        <name><surname>Luan</surname><given-names>Xiaofan</given-names></name>
        <name><surname>Xiang</surname><given-names>Long</given-names></name>
        <name><surname>Yan</surname><given-names>Xiao</given-names></name>
        <name><surname>Yi</surname><given-names>Xiaomeng</given-names></name>
        <name><surname>Luo</surname><given-names>Jigao</given-names></name>
        <name><surname>Cheng</surname><given-names>Qianya</given-names></name>
        <name><surname>Xu</surname><given-names>Weizhi</given-names></name>
        <name><surname>Luo</surname><given-names>Jiarui</given-names></name>
        <name><surname>Liu</surname><given-names>Frank</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Manu: A cloud native vector database management system</article-title>
      <source>Proceedings of the VLDB Endowment</source>
      <publisher-name>VLDB Endowment</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>15</volume>
      <issue>12</issue>
      <fpage>3548</fpage>
      <lpage>3561</lpage>
    </element-citation>
  </ref>
  <ref id="ref-brown_language_2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Brown</surname><given-names>Tom</given-names></name>
        <name><surname>Mann</surname><given-names>Benjamin</given-names></name>
        <name><surname>Ryder</surname><given-names>Nick</given-names></name>
        <name><surname>Subbiah</surname><given-names>Melanie</given-names></name>
        <name><surname>Kaplan</surname><given-names>Jared D</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
        <name><surname>Neelakantan</surname><given-names>Arvind</given-names></name>
        <name><surname>Shyam</surname><given-names>Pranav</given-names></name>
        <name><surname>Saxe</surname><given-names>Girish</given-names></name>
        <name><surname>Bosma</surname><given-names>Amanda</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Language Models are Few-Shot Learners</article-title>
      <source>Advances in neural information processing systems</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <uri>https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html</uri>
      <fpage>1877</fpage>
      <lpage>1901</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
