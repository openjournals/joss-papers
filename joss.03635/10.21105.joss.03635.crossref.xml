<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>0d47797656f5cfc18f7af0fbfa7cb219</doi_batch_id>
    <timestamp>20210903203144</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>09</month>
          <year>2021</year>
        </publication_date>
        <journal_volume>
          <volume>6</volume>
        </journal_volume>
        <issue>65</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>AstronomicAL: an interactive dashboard for visualisation, integration and classification of data with Active Learning</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Grant</given_name>
            <surname>Stevens</surname>
            <ORCID>http://orcid.org/0000-0002-8885-4443</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Sotiria</given_name>
            <surname>Fotopoulou</surname>
            <ORCID>http://orcid.org/0000-0002-9686-254X</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Malcolm</given_name>
            <surname>Bremer</surname>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Oliver</given_name>
            <surname>Ray</surname>
          </person_name>
        </contributors>
        <publication_date>
          <month>09</month>
          <day>03</day>
          <year>2021</year>
        </publication_date>
        <pages>
          <first_page>3635</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.03635</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.5281/zenodo.5396671”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/3635</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.03635</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.03635</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.03635.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <unstructured_citation>Bokeh: Python library for interactive visualization, Bokeh Development Team, 2018, https://bokeh.pydata.org/en/latest/</unstructured_citation>
          </citation>
          <citation key="ref2">
            <doi>10.5281/zenodo.4581995</doi>
          </citation>
          <citation key="ref3">
            <doi>10.5281/zenodo.3987379</doi>
          </citation>
          <citation key="ref4">
            <doi>10.5281/zenodo.4573728</doi>
          </citation>
          <citation key="ref5">
            <unstructured_citation>modAL: A modular active learning framework for Python, Danka, Tivadar and Horvath, Peter, 2018, https://github.com/modAL-python/modAL, available on arXiv at https://arxiv.org/abs/1805.00979</unstructured_citation>
          </citation>
          <citation key="ref6">
            <unstructured_citation>Scikit-learn: Machine Learning in Python, Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E., Journal of Machine Learning Research, 12, 2825–2830, 2011</unstructured_citation>
          </citation>
          <citation key="ref7">
            <doi>10.2200/S00429ED1V01Y201207AIM018</doi>
          </citation>
          <citation key="ref8">
            <doi>10.1051/0004-6361/201730763</doi>
          </citation>
          <citation key="ref9">
            <doi>10.1086/176166</doi>
          </citation>
          <citation key="ref10">
            <doi>10.3847/1538-4365/ab929e</doi>
          </citation>
          <citation key="ref11">
            <unstructured_citation>PyTorch: An Imperative Style, High-Performance Deep Learning Library, Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith, Advances in Neural Information Processing Systems 32, Wallach, H. and Larochelle, H. and Beygelzimer, A. and d’ Alché-Buc, F. and Fox, E. and Garnett, R., 8024–8035, 2019, Curran Associates, Inc., http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</unstructured_citation>
          </citation>
          <citation key="ref12">
            <doi>10.5281/zenodo.4724125</doi>
          </citation>
          <citation key="ref13">
            <unstructured_citation>How well does active learning actually work? Time-based evaluation of cost-reduction strategies for language documentation., Baldridge, Jason and Palmer, Alexis, Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, 296–305, 2009</unstructured_citation>
          </citation>
          <citation key="ref14">
            <doi>10.3115/1613715.1613855</doi>
          </citation>
          <citation key="ref15">
            <unstructured_citation>Deep bayesian active learning with image data, Gal, Yarin and Islam, Riashat and Ghahramani, Zoubin, International Conference on Machine Learning, 1183–1192, 2017, PMLR</unstructured_citation>
          </citation>
          <citation key="ref16">
            <doi>10.1093/mnras/stz2816</doi>
          </citation>
          <citation key="ref17">
            <doi>10.1038/s41587-020-0521-4</doi>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
