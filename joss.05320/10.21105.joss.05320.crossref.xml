<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20231009T120458-bc80038991f97e11b8b3d6fb8bd48380eee160b5</doi_batch_id>
    <timestamp>20231009120458</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>10</month>
          <year>2023</year>
        </publication_date>
        <journal_volume>
          <volume>8</volume>
        </journal_volume>
        <issue>90</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>BayesO: A Bayesian optimization framework in
Python</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Jungtaek</given_name>
            <surname>Kim</surname>
            <ORCID>https://orcid.org/0000-0002-1905-1399</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Seungjin</given_name>
            <surname>Choi</surname>
            <ORCID>https://orcid.org/0000-0002-7873-4616</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>10</month>
          <day>09</day>
          <year>2023</year>
        </publication_date>
        <pages>
          <first_page>5320</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.05320</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.8419023</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/5320</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.05320</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.05320</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.05320.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="BrochuE2010arxiv">
            <article_title>A tutorial on Bayesian optimization of
expensive cost functions, with application to active user modeling and
hierarchical reinforcement learning</article_title>
            <author>Brochu</author>
            <journal_title>arXiv preprint
arXiv:1012.2599</journal_title>
            <cYear>2010</cYear>
            <unstructured_citation>Brochu, E., Cora, V. M., &amp; de
Freitas, N. (2010). A tutorial on Bayesian optimization of expensive
cost functions, with application to active user modeling and
hierarchical reinforcement learning. arXiv Preprint
arXiv:1012.2599.</unstructured_citation>
          </citation>
          <citation key="ShahriariB2016procieee">
            <article_title>Taking the human out of the loop: A review of
Bayesian optimization</article_title>
            <author>Shahriari</author>
            <journal_title>Proceedings of the IEEE</journal_title>
            <issue>1</issue>
            <volume>104</volume>
            <doi>10.1109/JPROC.2015.2494218</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Shahriari, B., Swersky, K., Wang, Z.,
Adams, R. P., &amp; de Freitas, N. (2016). Taking the human out of the
loop: A review of Bayesian optimization. Proceedings of the IEEE,
104(1), 148–175.
https://doi.org/10.1109/JPROC.2015.2494218</unstructured_citation>
          </citation>
          <citation key="GarnettR2023book">
            <volume_title>Bayesian Optimization</volume_title>
            <author>Garnett</author>
            <cYear>2023</cYear>
            <unstructured_citation>Garnett, R. (2023). Bayesian
Optimization. Cambridge University Press.</unstructured_citation>
          </citation>
          <citation key="SnoekJ2012neurips">
            <article_title>Practical Bayesian optimization of machine
learning algorithms</article_title>
            <author>Snoek</author>
            <journal_title>Advances in neural information processing
systems (NeurIPS)</journal_title>
            <volume>25</volume>
            <cYear>2012</cYear>
            <unstructured_citation>Snoek, J., Larochelle, H., &amp;
Adams, R. P. (2012). Practical Bayesian optimization of machine learning
algorithms. Advances in Neural Information Processing Systems (NeurIPS),
25, 2951–2959.</unstructured_citation>
          </citation>
          <citation key="FeurerM2015neurips">
            <article_title>Efficient and robust automated machine
learning</article_title>
            <author>Feurer</author>
            <journal_title>Advances in neural information processing
systems (NeurIPS)</journal_title>
            <volume>28</volume>
            <cYear>2015</cYear>
            <unstructured_citation>Feurer, M., Klein, A., Eggensperger,
K., Springenberg, J. T., Blum, M., &amp; Hutter, F. (2015). Efficient
and robust automated machine learning. Advances in Neural Information
Processing Systems (NeurIPS), 28, 2962–2970.</unstructured_citation>
          </citation>
          <citation key="FeurerM2022jmlr">
            <article_title>Auto-Sklearn 2.0: Hands-free AutoML via
meta-learning</article_title>
            <author>Feurer</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <issue>261</issue>
            <volume>23</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Feurer, M., Eggensperger, K.,
Falkner, S., Lindauer, M., &amp; Hutter, F. (2022). Auto-Sklearn 2.0:
Hands-free AutoML via meta-learning. Journal of Machine Learning
Research, 23(261), 1–61.</unstructured_citation>
          </citation>
          <citation key="KandasamyK2018neurips">
            <article_title>Neural architecture search with Bayesian
optimisation and optimal transport</article_title>
            <author>Kandasamy</author>
            <journal_title>Advances in neural information processing
systems (NeurIPS)</journal_title>
            <volume>31</volume>
            <cYear>2018</cYear>
            <unstructured_citation>Kandasamy, K., Neiswanger, W.,
Schneider, J., Póczos, B., &amp; Xing, E. P. (2018). Neural architecture
search with Bayesian optimisation and optimal transport. Advances in
Neural Information Processing Systems (NeurIPS), 31,
2016–2025.</unstructured_citation>
          </citation>
          <citation key="FrazierP2016ismdd">
            <article_title>Bayesian optimization for materials
design</article_title>
            <author>Frazier</author>
            <journal_title>Information science for materials discovery
and design</journal_title>
            <doi>10.1007/978-3-319-23871-5_3</doi>
            <cYear>2016</cYear>
            <unstructured_citation>Frazier, P. I., &amp; Wang, J.
(2016). Bayesian optimization for materials design. In Information
science for materials discovery and design (pp. 45–75). Springer.
https://doi.org/10.1007/978-3-319-23871-5_3</unstructured_citation>
          </citation>
          <citation key="DurisJ2020phrl">
            <article_title>Bayesian optimization of a free-electron
laser</article_title>
            <author>Duris</author>
            <journal_title>Physical Review Letters</journal_title>
            <issue>12</issue>
            <volume>124</volume>
            <doi>10.1103/PhysRevLett.124.124801</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Duris, J., Kennedy, D., Hanuka, A.,
Shtalenkova, J., Edelen, A., Baxevanis, P., Egger, A., Cope, T.,
McIntire, M., Ermon, S., &amp; Ratner, D. (2020). Bayesian optimization
of a free-electron laser. Physical Review Letters, 124(12), 124801.
https://doi.org/10.1103/PhysRevLett.124.124801</unstructured_citation>
          </citation>
          <citation key="KorovinaK2020aistats">
            <article_title>ChemBO: Bayesian optimization of small
organic molecules with synthesizable recommendations</article_title>
            <author>Korovina</author>
            <journal_title>Proceedings of the international conference
on artificial intelligence and statistics (AISTATS)</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Korovina, K., Xu, S., Kandasamy, K.,
Neiswanger, W., Póczos, B., Schneider, J., &amp; Xing, E. P. (2020).
ChemBO: Bayesian optimization of small organic molecules with
synthesizable recommendations. Proceedings of the International
Conference on Artificial Intelligence and Statistics (AISTATS),
3393–3403.</unstructured_citation>
          </citation>
          <citation key="KimJ2020ml4eng">
            <article_title>Combinatorial 3D shape generation via
sequential assembly</article_title>
            <author>Kim</author>
            <journal_title>Neural information processing systems
workshop on machine learning for engineering modeling, simulation, and
design (ML4Eng)</journal_title>
            <cYear>2020</cYear>
            <unstructured_citation>Kim, J., Chung, H., Lee, J., Cho, M.,
&amp; Park, J. (2020). Combinatorial 3D shape generation via sequential
assembly. Neural Information Processing Systems Workshop on Machine
Learning for Engineering Modeling, Simulation, and Design
(ML4Eng).</unstructured_citation>
          </citation>
          <citation key="ShieldsBJ2021nature">
            <article_title>Bayesian reaction optimization as a tool for
chemical synthesis</article_title>
            <author>Shields</author>
            <journal_title>Nature</journal_title>
            <volume>590</volume>
            <doi>10.1038/s41586-021-03213-y</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Shields, B. J., Stevens, J., Li, J.,
Parasram, M., Damani, F., Alvarado, J. I. M., Janey, J. M., Adams, R.
P., &amp; Doyle, A. G. (2021). Bayesian reaction optimization as a tool
for chemical synthesis. Nature, 590, 89–96.
https://doi.org/10.1038/s41586-021-03213-y</unstructured_citation>
          </citation>
          <citation key="KimJ2021ml">
            <article_title>Bayesian optimization with approximate set
kernels</article_title>
            <author>Kim</author>
            <journal_title>Machine Learning</journal_title>
            <issue>5</issue>
            <volume>110</volume>
            <doi>10.1007/s10994-021-05949-0</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Kim, J., McCourt, M., You, T., Kim,
S., &amp; Choi, S. (2021). Bayesian optimization with approximate set
kernels. Machine Learning, 110(5), 857–879.
https://doi.org/10.1007/s10994-021-05949-0</unstructured_citation>
          </citation>
          <citation key="RasmussenCE2006book">
            <volume_title>Gaussian processes for machine
learning</volume_title>
            <author>Rasmussen</author>
            <cYear>2006</cYear>
            <unstructured_citation>Rasmussen, C. E., &amp; Williams, C.
K. I. (2006). Gaussian processes for machine learning. MIT
Press.</unstructured_citation>
          </citation>
          <citation key="ShahA2014aistats">
            <article_title>Student-t processes as alternatives to
Gaussian processes</article_title>
            <author>Shah</author>
            <journal_title>Proceedings of the international conference
on artificial intelligence and statistics (AISTATS)</journal_title>
            <cYear>2014</cYear>
            <unstructured_citation>Shah, A., Wilson, A. G., &amp;
Ghahramani, Z. (2014). Student-t processes as alternatives to Gaussian
processes. Proceedings of the International Conference on Artificial
Intelligence and Statistics (AISTATS), 877–885.</unstructured_citation>
          </citation>
          <citation key="HarrisCR2020nature">
            <article_title>Array programming with NumPy</article_title>
            <author>Harris</author>
            <journal_title>Nature</journal_title>
            <volume>585</volume>
            <doi>10.1038/s41586-020-2649-2</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Harris, C. R., Millman, K. J., van
der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E.,
Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van
Kerkwijk, M. H., Brett, M., Haldane, A., del Río, J. F., Wiebe, M.,
Peterson, P., … Oliphant, T. E. (2020). Array programming with NumPy.
Nature, 585, 357–362.
https://doi.org/10.1038/s41586-020-2649-2</unstructured_citation>
          </citation>
          <citation key="VirtanenP2020nm">
            <article_title>SciPy 1.0: Fundamental algorithms for
scientific computing in Python</article_title>
            <author>Virtanen</author>
            <journal_title>Nature Methods</journal_title>
            <volume>17</volume>
            <doi>10.1038/s41592-019-0686-2</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Virtanen, P., Gommers, R., Oliphant,
T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson,
P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson,
J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R.,
Larson, E., … SciPy 1.0 Contributors. (2020). SciPy 1.0: Fundamental
algorithms for scientific computing in Python. Nature Methods, 17,
261–272.
https://doi.org/10.1038/s41592-019-0686-2</unstructured_citation>
          </citation>
          <citation key="HansenN2019software">
            <article_title>CMA-ES/pycma on GitHub</article_title>
            <author>Hansen</author>
            <cYear>2019</cYear>
            <unstructured_citation>Hansen, N., Akimoto, Y., &amp;
Baudis, P. (2019). CMA-ES/pycma on GitHub. GitHub.
https://github.com/CMA-ES/pycma</unstructured_citation>
          </citation>
          <citation key="BreimanL2001ml">
            <article_title>Random forests</article_title>
            <author>Breiman</author>
            <journal_title>Machine Learning</journal_title>
            <issue>1</issue>
            <volume>45</volume>
            <doi>10.1023/A:1010933404324</doi>
            <cYear>2001</cYear>
            <unstructured_citation>Breiman, L. (2001). Random forests.
Machine Learning, 45(1), 5–32.
https://doi.org/10.1023/A:1010933404324</unstructured_citation>
          </citation>
          <citation key="HutterF2014ai">
            <article_title>Algorithm runtime prediction: Methods &amp;
evaluation</article_title>
            <author>Hutter</author>
            <journal_title>Artificial Intelligence</journal_title>
            <volume>206</volume>
            <doi>10.1016/j.artint.2013.10.003</doi>
            <cYear>2014</cYear>
            <unstructured_citation>Hutter, F., Xu, L., Hoos, H. H.,
&amp; Leyton-Brown, K. (2014). Algorithm runtime prediction: Methods
&amp; evaluation. Artificial Intelligence, 206, 79–111.
https://doi.org/10.1016/j.artint.2013.10.003</unstructured_citation>
          </citation>
          <citation key="JonesDR1998jgo">
            <article_title>Efficient global optimization of expensive
black-box functions</article_title>
            <author>Jones</author>
            <journal_title>Journal of Global
Optimization</journal_title>
            <volume>13</volume>
            <doi>10.1023/A:1008306431147</doi>
            <cYear>1998</cYear>
            <unstructured_citation>Jones, D. R., Schonlau, M., &amp;
Welch, W. J. (1998). Efficient global optimization of expensive
black-box functions. Journal of Global Optimization, 13, 455–492.
https://doi.org/10.1023/A:1008306431147</unstructured_citation>
          </citation>
          <citation key="ThompsonWR1933biometrika">
            <article_title>On the likelihood that one unknown
probability exceeds another in view of the evidence of two
samples</article_title>
            <author>Thompson</author>
            <journal_title>Biometrika</journal_title>
            <issue>3/4</issue>
            <volume>25</volume>
            <doi>10.1093/biomet/25.3-4.285</doi>
            <cYear>1933</cYear>
            <unstructured_citation>Thompson, W. R. (1933). On the
likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4), 285–294.
https://doi.org/10.1093/biomet/25.3-4.285</unstructured_citation>
          </citation>
          <citation key="HuangD2006jgo">
            <article_title>Global optimization of stochastic black-box
systems via sequential kriging meta-models</article_title>
            <author>Huang</author>
            <journal_title>Journal of Global
Optimization</journal_title>
            <volume>34</volume>
            <doi>10.1007/s10898-005-2454-3</doi>
            <cYear>2006</cYear>
            <unstructured_citation>Huang, D., Allen, T. T., Notz, W. I.,
&amp; Zheng, N. (2006). Global optimization of stochastic black-box
systems via sequential kriging meta-models. Journal of Global
Optimization, 34, 441–466.
https://doi.org/10.1007/s10898-005-2454-3</unstructured_citation>
          </citation>
          <citation key="SrinivasN2010icml">
            <article_title>Gaussian process optimization in the bandit
setting: No regret and experimental design</article_title>
            <author>Srinivas</author>
            <journal_title>Proceedings of the international conference
on machine learning (ICML)</journal_title>
            <cYear>2010</cYear>
            <unstructured_citation>Srinivas, N., Krause, A., Kakade, S.,
&amp; Seeger, M. (2010). Gaussian process optimization in the bandit
setting: No regret and experimental design. Proceedings of the
International Conference on Machine Learning (ICML),
1015–1022.</unstructured_citation>
          </citation>
          <citation key="ByrdRH1995siamjsc">
            <article_title>A limited memory algorithm for bound
constrained optimization</article_title>
            <author>Byrd</author>
            <journal_title>SIAM Journal on Scientific
Computing</journal_title>
            <issue>5</issue>
            <volume>16</volume>
            <doi>10.1137/0916069</doi>
            <cYear>1995</cYear>
            <unstructured_citation>Byrd, R. H., Lu, P., Nocedal, J.,
&amp; Zhu, C. (1995). A limited memory algorithm for bound constrained
optimization. SIAM Journal on Scientific Computing, 16(5), 1190–1208.
https://doi.org/10.1137/0916069</unstructured_citation>
          </citation>
          <citation key="HansenN1997eufit">
            <article_title>Convergence properties of evolution
strategies with the derandomized covariance matrix adaptation: The
(\mu/\mu_\mathrm{I}, \lambda)-CMA-ES</article_title>
            <author>Hansen</author>
            <journal_title>Proceedings of the european congress on
intelligent techniques and soft computing (EUFIT)</journal_title>
            <cYear>1997</cYear>
            <unstructured_citation>Hansen, N., &amp; Ostermeier, A.
(1997). Convergence properties of evolution strategies with the
derandomized covariance matrix adaptation: The (\mu/\mu_\mathrm{I},
\lambda)-CMA-ES. Proceedings of the European Congress on Intelligent
Techniques and Soft Computing (EUFIT), 650–654.</unstructured_citation>
          </citation>
          <citation key="JonesDR1993jota">
            <article_title>Lipschitzian optimization without the
Lipschitz constant</article_title>
            <author>Jones</author>
            <journal_title>Journal of Optimization Theory and
Applications</journal_title>
            <issue>1</issue>
            <volume>79</volume>
            <doi>10.1007/BF00941892</doi>
            <cYear>1993</cYear>
            <unstructured_citation>Jones, D. R., Perttunen, C. D., &amp;
Stuckman, B. E. (1993). Lipschitzian optimization without the Lipschitz
constant. Journal of Optimization Theory and Applications, 79(1),
157–181. https://doi.org/10.1007/BF00941892</unstructured_citation>
          </citation>
          <citation key="RoyPT2023joss">
            <article_title>Quasi-Monte Carlo methods in
Python</article_title>
            <author>Roy</author>
            <journal_title>Journal of Open Source
Software</journal_title>
            <issue>84</issue>
            <volume>8</volume>
            <doi>10.21105/joss.05309</doi>
            <cYear>2023</cYear>
            <unstructured_citation>Roy, P. T., Owen, A. B., Balandat,
M., &amp; Haberland, M. (2023). Quasi-Monte Carlo methods in Python.
Journal of Open Source Software, 8(84), 5309.
https://doi.org/10.21105/joss.05309</unstructured_citation>
          </citation>
          <citation key="tqdm2016software">
            <article_title>tqdm</article_title>
            <author>The tqdm authors</author>
            <cYear>2016</cYear>
            <unstructured_citation>The tqdm authors. (2016). tqdm.
GitHub. https://github.com/tqdm/tqdm</unstructured_citation>
          </citation>
          <citation key="gpy2012software">
            <article_title>GPy: A Gaussian process framework in
Python</article_title>
            <author>GPy</author>
            <cYear>2012</cYear>
            <unstructured_citation>GPy. (2012). GPy: A Gaussian process
framework in Python. GitHub.
https://github.com/SheffieldML/GPy</unstructured_citation>
          </citation>
          <citation key="gpyopt2016software">
            <article_title>GPyOpt: A Bayesian optimization framework in
Python</article_title>
            <author>The GPyOpt authors</author>
            <cYear>2016</cYear>
            <unstructured_citation>The GPyOpt authors. (2016). GPyOpt: A
Bayesian optimization framework in Python. GitHub.
https://github.com/SheffieldML/GPyOpt</unstructured_citation>
          </citation>
          <citation key="LindauerM2022jmlr">
            <article_title>SMAC3: A versatile Bayesian optimization
package for hyperparameter optimization</article_title>
            <author>Lindauer</author>
            <journal_title>Journal of Machine Learning
Research</journal_title>
            <issue>54</issue>
            <volume>23</volume>
            <cYear>2022</cYear>
            <unstructured_citation>Lindauer, M., Eggensperger, K.,
Feurer, M., Biedenkapp, A., Deng, D., Benjamins, C., Ruhkopf, T., Sass,
R., &amp; Hutter, F. (2022). SMAC3: A versatile Bayesian optimization
package for hyperparameter optimization. Journal of Machine Learning
Research, 23(54), 1–9.</unstructured_citation>
          </citation>
          <citation key="PaszkeA2019neurips">
            <article_title>PyTorch: An imperative style,
high-performance deep learning library</article_title>
            <author>Paszke</author>
            <journal_title>Advances in neural information processing
systems (NeurIPS)</journal_title>
            <volume>32</volume>
            <cYear>2019</cYear>
            <unstructured_citation>Paszke, A., Gross, S., Massa, F.,
Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,
N., Antiga, L., Desmaison, A., Köpf, A., Yang, E., DeVito, Z., Raison,
M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., … Chintala, S.
(2019). PyTorch: An imperative style, high-performance deep learning
library. Advances in Neural Information Processing Systems (NeurIPS),
32, 8026–8037.</unstructured_citation>
          </citation>
          <citation key="AbadiM2016osdi">
            <article_title>TensorFlow: A system for large-scale machine
learning</article_title>
            <author>Abadi</author>
            <journal_title>USENIX symposium on operating systems design
and implementation (OSDI)</journal_title>
            <cYear>2016</cYear>
            <unstructured_citation>Abadi, M., Barham, P., Chen, J.,
Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G.,
Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D.
G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., … Zheng, X.
(2016). TensorFlow: A system for large-scale machine learning. USENIX
Symposium on Operating Systems Design and Implementation (OSDI),
265–283.</unstructured_citation>
          </citation>
          <citation key="KnuddeN2017arxiv">
            <article_title>GPflowOpt: A Bayesian optimization library
using TensorFlow</article_title>
            <author>Knudde</author>
            <journal_title>arXiv preprint
arXiv:1711.03845</journal_title>
            <cYear>2017</cYear>
            <unstructured_citation>Knudde, N., van der Herten, J.,
Dhaene, T., &amp; Couckuyt, I. (2017). GPflowOpt: A Bayesian
optimization library using TensorFlow. arXiv Preprint
arXiv:1711.03845.</unstructured_citation>
          </citation>
          <citation key="BalandatM2020neurips">
            <article_title>BoTorch: A framework for efficient
Monte-Carlo Bayesian optimization</article_title>
            <author>Balandat</author>
            <journal_title>Advances in neural information processing
systems (NeurIPS)</journal_title>
            <volume>33</volume>
            <cYear>2020</cYear>
            <unstructured_citation>Balandat, M., Karrer, B., Jiang, D.
R., Daulton, S., Letham, B., Wilson, A. G., &amp; Bakshy, E. (2020).
BoTorch: A framework for efficient Monte-Carlo Bayesian optimization.
Advances in Neural Information Processing Systems (NeurIPS), 33,
21524–21538.</unstructured_citation>
          </citation>
          <citation key="KushnerHJ1964jbe">
            <article_title>A new method of locating the maximum point of
an arbitrary multipeak curve in the presence of noise</article_title>
            <author>Kushner</author>
            <journal_title>Journal of Basic Engineering</journal_title>
            <issue>1</issue>
            <volume>86</volume>
            <doi>10.1115/1.3653121</doi>
            <cYear>1964</cYear>
            <unstructured_citation>Kushner, H. J. (1964). A new method
of locating the maximum point of an arbitrary multipeak curve in the
presence of noise. Journal of Basic Engineering, 86(1), 97–106.
https://doi.org/10.1115/1.3653121</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
