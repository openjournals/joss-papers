<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5320</article-id>
<article-id pub-id-type="doi">10.21105/joss.05320</article-id>
<title-group>
<article-title>BayesO: A Bayesian optimization framework in
Python</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-1905-1399</contrib-id>
<name>
<surname>Kim</surname>
<given-names>Jungtaek</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7873-4616</contrib-id>
<name>
<surname>Choi</surname>
<given-names>Seungjin</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>University of Pittsburgh, USA</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Intellicode, South Korea</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-12-06">
<day>6</day>
<month>12</month>
<year>2022</year>
</pub-date>
<volume>8</volume>
<issue>90</issue>
<fpage>5320</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Bayesian optimization</kwd>
<kwd>global optimization</kwd>
<kwd>black-box optimization</kwd>
<kwd>optimization</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Bayesian optimization is a sample-efficient method for solving the
  optimization of a black-box function. In particular, it successfully
  shows its effectiveness in diverse applications such as hyperparameter
  optimization, automated machine learning, material design, sequential
  assembly, and chemical reaction optimization. In this paper we present
  an easy-to-use Bayesian optimization framework, referred to as
  <italic>BayesO</italic>, which is written in Python and licensed under
  the MIT license. To briefly introduce our software, we describe the
  functionality of BayesO and various components for software
  development.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Bayesian optimization
  (<xref alt="Brochu et al., 2010" rid="ref-BrochuE2010arxiv" ref-type="bibr">Brochu
  et al., 2010</xref>;
  <xref alt="Garnett, 2023" rid="ref-GarnettR2023book" ref-type="bibr">Garnett,
  2023</xref>;
  <xref alt="Shahriari et al., 2016" rid="ref-ShahriariB2016procieee" ref-type="bibr">Shahriari
  et al., 2016</xref>) is a sample-efficient method for solving the
  optimization of a black-box function <inline-formula><alternatives>
  <tex-math><![CDATA[f]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>:
  <named-content id="eqnU003Aglobal_opt" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[
      \mathbf{x}^\star = \underset{\mathbf{x} \in \mathcal{X}}{\mathrm{arg\,min}} f(\mathbf{x}) \quad \textrm{or} \quad \mathbf{x}^\star = \underset{\mathbf{x} \in \mathcal{X}}{\mathrm{arg\,max}} f(\mathbf{x}),]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msup><mml:mi>ùê±</mml:mi><mml:mo>‚ãÜ</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="0.167em"></mml:mspace><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>ùí≥</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mspace width="1.0em"></mml:mspace><mml:mtext mathvariant="normal">or</mml:mtext><mml:mspace width="1.0em"></mml:mspace><mml:msup><mml:mi>ùê±</mml:mi><mml:mo>‚ãÜ</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="0.167em"></mml:mspace><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mi>ùê±</mml:mi><mml:mo>‚àà</mml:mo><mml:mi>ùí≥</mml:mi></mml:mrow></mml:munder><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>ùê±</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></named-content>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{X} \subset \mathbb{R}^d]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ùí≥</mml:mi><mml:mo>‚äÇ</mml:mo><mml:msup><mml:mi>‚Ñù</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:math></alternatives></inline-formula>
  is a <inline-formula><alternatives>
  <tex-math><![CDATA[d]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>d</mml:mi></mml:math></alternatives></inline-formula>-dimensional
  space. In general, finding a solution <inline-formula><alternatives>
  <tex-math><![CDATA[\mathbf{x}^\star]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>ùê±</mml:mi><mml:mo>‚ãÜ</mml:mo></mml:msup></mml:math></alternatives></inline-formula>
  of <xref alt="Equation¬†1" rid="eqnU003Aglobal_opt">Equation¬†1</xref>,
  i.e., a global optimum on <inline-formula><alternatives>
  <tex-math><![CDATA[\mathcal{X}]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>ùí≥</mml:mi></mml:math></alternatives></inline-formula>,
  is time-consuming since we cannot employ any knowledge, e.g.,
  gradients and Hessians, in solving this problem. Compared to other
  possible approaches, e.g., random search and evolutionary algorithm,
  Bayesian optimization successfully shows its effectiveness by
  utilizing a probabilistic regression model and an acquisition
  function. In particular, the sample-efficient approach of our interest
  enables us to apply it in various real-world applications such as
  hyperparameter optimization
  (<xref alt="Snoek et al., 2012" rid="ref-SnoekJ2012neurips" ref-type="bibr">Snoek
  et al., 2012</xref>), automated machine learning
  (<xref alt="Feurer et al., 2015" rid="ref-FeurerM2015neurips" ref-type="bibr">Feurer
  et al., 2015</xref>,
  <xref alt="2022" rid="ref-FeurerM2022jmlr" ref-type="bibr">2022</xref>),
  neural architecture search
  (<xref alt="Kandasamy et al., 2018" rid="ref-KandasamyK2018neurips" ref-type="bibr">Kandasamy
  et al., 2018</xref>), material design
  (<xref alt="Frazier &amp; Wang, 2016" rid="ref-FrazierP2016ismdd" ref-type="bibr">Frazier
  &amp; Wang, 2016</xref>), free-electron laser configuration search
  (<xref alt="Duris et al., 2020" rid="ref-DurisJ2020phrl" ref-type="bibr">Duris
  et al., 2020</xref>), organic molecule synthesis
  (<xref alt="Korovina et al., 2020" rid="ref-KorovinaK2020aistats" ref-type="bibr">Korovina
  et al., 2020</xref>), sequential assembly
  (<xref alt="Kim et al., 2020" rid="ref-KimJ2020ml4eng" ref-type="bibr">Kim
  et al., 2020</xref>), and chemical reaction optimization
  (<xref alt="Shields et al., 2021" rid="ref-ShieldsBJ2021nature" ref-type="bibr">Shields
  et al., 2021</xref>).</p>
  <p>In this paper, we present an easy-to-use Bayesian optimization
  framework, referred to as <italic>BayesO</italic> (pronounced
  ‚Äúbayes-o‚Äù), to effortlessly utilize Bayesian optimization in the
  problems of interest to practitioners. Our BayesO is written in one of
  the most popular programming languages, Python, and licensed under the
  MIT license. Moreover, it provides various features including
  different types of input variables (e.g., vectors and sets
  (<xref alt="Kim et al., 2021" rid="ref-KimJ2021ml" ref-type="bibr">Kim
  et al., 2021</xref>)) and different surrogate models (e.g., Gaussian
  process regression
  (<xref alt="Rasmussen &amp; Williams, 2006" rid="ref-RasmussenCE2006book" ref-type="bibr">Rasmussen
  &amp; Williams, 2006</xref>) and
  Student-<inline-formula><alternatives>
  <tex-math><![CDATA[t]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
  process regression
  (<xref alt="Shah et al., 2014" rid="ref-ShahA2014aistats" ref-type="bibr">Shah
  et al., 2014</xref>)). Along with the description of such
  functionality, we cover various components for software development in
  the BayesO project. We hope that this BayesO project encourages
  researchers and practitioners to readily utilize the powerful
  black-box optimization technique in diverse academic and industrial
  fields.</p>
  <fig>
    <caption><p>Visualization of Bayesian optimization procedure. Given
    an objective function,
    <xref alt="Equation¬†2" rid="eqnU003Asimple">Equation¬†2</xref>
    (colored by turquoise) and four initial points (denoted as light
    blue <inline-formula><alternatives>
    <tex-math><![CDATA[\texttt{+}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtext mathvariant="monospace">+</mml:mtext></mml:math></alternatives></inline-formula>
    at iteration 1), a query point (denoted as pink
    <inline-formula><alternatives>
    <tex-math><![CDATA[\texttt{x}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mtext mathvariant="monospace">ùö°</mml:mtext></mml:math></alternatives></inline-formula>)
    is determined by constructing a surrogate model (colored by orange)
    and maximizing an acquisition function (colored by light green)
    every
    iteration.<styled-content id="figU003Abo_steps"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/bo_step_global_local_ei.png" />
  </fig>
</sec>
<sec id="bayesian-optimization">
  <title>Bayesian optimization</title>
  <p>As discussed in the work
  (<xref alt="Brochu et al., 2010" rid="ref-BrochuE2010arxiv" ref-type="bibr">Brochu
  et al., 2010</xref>;
  <xref alt="Garnett, 2023" rid="ref-GarnettR2023book" ref-type="bibr">Garnett,
  2023</xref>;
  <xref alt="Shahriari et al., 2016" rid="ref-ShahriariB2016procieee" ref-type="bibr">Shahriari
  et al., 2016</xref>), supposing that a target objective function is
  black-box, Bayesian optimization is an approach to optimizing the
  objective in a sample-efficient manner. It repeats three primary
  steps:</p>
  <list list-type="order">
    <list-item>
      <p>Building a probabilistic regression model, which is capable of
      estimating the degrees of exploration and exploitation;</p>
    </list-item>
    <list-item>
      <p>Optimizing an acquisition function, which is defined with the
      probabilistic regression model;</p>
    </list-item>
    <list-item>
      <p>Evaluating the target objective at a query point, which is
      determined by optimizing the acquisition function,</p>
    </list-item>
  </list>
  <p>until a predefined stopping criterion, e.g., an iteration budget or
  a budget of wall-clock time, is satisfied. Eventually, the best
  solution among the queries evaluated is selected by considering the
  function evaluations. As shown in
  <xref alt="[fig:bo_steps]" rid="figU003Abo_steps">[fig:bo_steps]</xref>,
  Bayesian optimization iteratively finds a candidate of global optimum,
  repeating the aforementioned steps. Note that, for this example, an
  objective function is
  <named-content id="eqnU003Asimple" content-type="equation"><disp-formula><alternatives>
  <tex-math><![CDATA[
      f(x) = 2 \sin(x) + 2 \cos(2x) + 0.05 x,]]></tex-math>
  <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>sin</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>cos</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mn>2</mml:mn><mml:mi>x</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.05</mml:mn><mml:mi>x</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></alternatives></disp-formula></named-content>
  where <inline-formula><alternatives>
  <tex-math><![CDATA[x \in [-5, 5]]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>x</mml:mi><mml:mo>‚àà</mml:mo><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:mo>‚àí</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></inline-formula>,
  and Gaussian process regression and expected improvement are used as a
  surrogate model and an acquisition function, respectively.</p>
  <p>Several related projects on Bayesian optimization have been
  proposed. GPyOpt
  (<xref alt="The GPyOpt authors, 2016" rid="ref-gpyopt2016software" ref-type="bibr">The
  GPyOpt authors, 2016</xref>), built on GPy
  (<xref alt="GPy, 2012" rid="ref-gpy2012software" ref-type="bibr">GPy,
  2012</xref>), has been implemented. SMAC3
  (<xref alt="Lindauer et al., 2022" rid="ref-LindauerM2022jmlr" ref-type="bibr">Lindauer
  et al., 2022</xref>) has been developed using random forests
  (<xref alt="Breiman, 2001" rid="ref-BreimanL2001ml" ref-type="bibr">Breiman,
  2001</xref>). Moreover, by making use of modern automatic
  differentiation tools, i.e., TensorFlow
  (<xref alt="Abadi et al., 2016" rid="ref-AbadiM2016osdi" ref-type="bibr">Abadi
  et al., 2016</xref>) and PyTorch
  (<xref alt="Paszke et al., 2019" rid="ref-PaszkeA2019neurips" ref-type="bibr">Paszke
  et al., 2019</xref>), GPflowOpt
  (<xref alt="Knudde et al., 2017" rid="ref-KnuddeN2017arxiv" ref-type="bibr">Knudde
  et al., 2017</xref>) and BoTorch
  (<xref alt="Balandat et al., 2020" rid="ref-BalandatM2020neurips" ref-type="bibr">Balandat
  et al., 2020</xref>) have been introduced.</p>
</sec>
<sec id="overview-of-bayeso">
  <title>Overview of BayesO</title>
  <fig>
    <caption><p>Logo of
    BayesO<styled-content id="figU003Alogo_bayeso"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/logo_bayeso_capitalized.png" />
  </fig>
  <p>In this section we cover the overview of BayesO including
  probabilistic regression models and acquisition functions, by
  referring to BayesO v0.5.4. For higher versions of BayesO, see
  official documentation. Note that the BayesO logo is shown in
  <xref alt="[fig:logo_bayeso]" rid="figU003Alogo_bayeso">[fig:logo_bayeso]</xref>.</p>
  <p>BayesO supports the following probabilistic regression models:</p>
  <list list-type="bullet">
    <list-item>
      <p>Gaussian process regression
      (<xref alt="Rasmussen &amp; Williams, 2006" rid="ref-RasmussenCE2006book" ref-type="bibr">Rasmussen
      &amp; Williams, 2006</xref>);</p>
    </list-item>
    <list-item>
      <p>Student-<inline-formula><alternatives>
      <tex-math><![CDATA[t]]></tex-math>
      <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
      process regression
      (<xref alt="Shah et al., 2014" rid="ref-ShahA2014aistats" ref-type="bibr">Shah
      et al., 2014</xref>);</p>
    </list-item>
    <list-item>
      <p>Random forest regression
      (<xref alt="Breiman, 2001" rid="ref-BreimanL2001ml" ref-type="bibr">Breiman,
      2001</xref>).</p>
    </list-item>
  </list>
  <p>Although random forest regression is not a probabilistic model
  inherently, we can compute its mean and variance functions as reported
  by Hutter et al.
  (<xref alt="2014" rid="ref-HutterF2014ai" ref-type="bibr">2014</xref>).</p>
  <p>We implement several acquisition functions:</p>
  <list list-type="bullet">
    <list-item>
      <p>pure exploration;</p>
    </list-item>
    <list-item>
      <p>pure exploitation;</p>
    </list-item>
    <list-item>
      <p>probability of improvement
      (<xref alt="Kushner, 1964" rid="ref-KushnerHJ1964jbe" ref-type="bibr">Kushner,
      1964</xref>);</p>
    </list-item>
    <list-item>
      <p>expected improvement
      (<xref alt="Jones et al., 1998" rid="ref-JonesDR1998jgo" ref-type="bibr">Jones
      et al., 1998</xref>);</p>
    </list-item>
    <list-item>
      <p>augmented expected improvement
      (<xref alt="Huang et al., 2006" rid="ref-HuangD2006jgo" ref-type="bibr">Huang
      et al., 2006</xref>);</p>
    </list-item>
    <list-item>
      <p>Gaussian process upper confidence bound
      (<xref alt="Srinivas et al., 2010" rid="ref-SrinivasN2010icml" ref-type="bibr">Srinivas
      et al., 2010</xref>).</p>
    </list-item>
  </list>
  <p>As a strategy to optimize an acquisition function, we suggest the
  following options: DIRECT
  (<xref alt="Jones et al., 1993" rid="ref-JonesDR1993jota" ref-type="bibr">Jones
  et al., 1993</xref>), CMA-ES
  (<xref alt="Hansen &amp; Ostermeier, 1997" rid="ref-HansenN1997eufit" ref-type="bibr">Hansen
  &amp; Ostermeier, 1997</xref>), and L-BFGS-B
  (<xref alt="Byrd et al., 1995" rid="ref-ByrdRH1995siamjsc" ref-type="bibr">Byrd
  et al., 1995</xref>). In addition, we also include Thompson sampling
  (<xref alt="Thompson, 1933" rid="ref-ThompsonWR1933biometrika" ref-type="bibr">Thompson,
  1933</xref>) in BayesO.</p>
  <p>Furthermore, to support an easy-to-use interface, we implement the
  wrappers of Bayesian optimization for particular scenarios:</p>
  <list list-type="bullet">
    <list-item>
      <p>a run with randomly-chosen initial inputs;</p>
    </list-item>
    <list-item>
      <p>a run with initial inputs provided;</p>
    </list-item>
    <list-item>
      <p>a run with initial inputs provided and their evaluations.</p>
    </list-item>
  </list>
</sec>
<sec id="software-development-for-bayeso">
  <title>Software development for BayesO</title>
  <p>To manage BayesO productively, we actively utilize external
  development management packages.</p>
  <list list-type="bullet">
    <list-item>
      <p>Code analysis: Our software is monitored and inspected to
      satisfy the code conventions predefined in our software.</p>
    </list-item>
    <list-item>
      <p>Type hints: As supported in Python 3, we provide type hints for
      any arguments.</p>
    </list-item>
    <list-item>
      <p>Unit tests: Unit tests for our software are included. We have
      achieved 100% coverage. In addition, unit tests for measuring
      execution time are also provided.</p>
    </list-item>
    <list-item>
      <p>Dependency: Our package depends on NumPy
      (<xref alt="Harris et al., 2020" rid="ref-HarrisCR2020nature" ref-type="bibr">Harris
      et al., 2020</xref>), SciPy
      (<xref alt="Virtanen et al., 2020" rid="ref-VirtanenP2020nm" ref-type="bibr">Virtanen
      et al., 2020</xref>), a quasi-Monte Carlo submodule in SciPy
      (<xref alt="Roy et al., 2023" rid="ref-RoyPT2023joss" ref-type="bibr">Roy
      et al., 2023</xref>), pycma
      (<xref alt="Hansen et al., 2019" rid="ref-HansenN2019software" ref-type="bibr">Hansen
      et al., 2019</xref>), and tqdm
      (<xref alt="The tqdm authors, 2016" rid="ref-tqdm2016software" ref-type="bibr">The
      tqdm authors, 2016</xref>).</p>
    </list-item>
    <list-item>
      <p>Installation: Our software is released via the Python Package
      Index (PyPI) meaning users can easily install BayesO into their
      environment.</p>
    </list-item>
    <list-item>
      <p>Documentation: We create official documentation with
      docstring.</p>
    </list-item>
  </list>
</sec>
<sec id="conclusion">
  <title>Conclusion</title>
  <p>In this work we have presented a Bayesian optimization framework,
  named BayesO. We hope that our project enables many researchers to
  suggest a new algorithm by modifying BayesO and many practitioners to
  utilize Bayesian optimization in their applications.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The BayesO project has been started when JK and SC were with
  POSTECH, and it has been mostly developed at POSTECH.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-BrochuE2010arxiv">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brochu</surname><given-names>E.</given-names></name>
        <name><surname>Cora</surname><given-names>V. M.</given-names></name>
        <name><surname>de Freitas</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</article-title>
      <source>arXiv preprint arXiv:1012.2599</source>
      <year iso-8601-date="2010">2010</year>
    </element-citation>
  </ref>
  <ref id="ref-ShahriariB2016procieee">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shahriari</surname><given-names>B.</given-names></name>
        <name><surname>Swersky</surname><given-names>K.</given-names></name>
        <name><surname>Wang</surname><given-names>Z.</given-names></name>
        <name><surname>Adams</surname><given-names>R. P.</given-names></name>
        <name><surname>de Freitas</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>Taking the human out of the loop: A review of Bayesian optimization</article-title>
      <source>Proceedings of the IEEE</source>
      <year iso-8601-date="2016">2016</year>
      <volume>104</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1109/JPROC.2015.2494218</pub-id>
      <fpage>148</fpage>
      <lpage>175</lpage>
    </element-citation>
  </ref>
  <ref id="ref-GarnettR2023book">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Garnett</surname><given-names>R.</given-names></name>
      </person-group>
      <source>Bayesian Optimization</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2023">2023</year>
    </element-citation>
  </ref>
  <ref id="ref-SnoekJ2012neurips">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Snoek</surname><given-names>J.</given-names></name>
        <name><surname>Larochelle</surname><given-names>H.</given-names></name>
        <name><surname>Adams</surname><given-names>R. P.</given-names></name>
      </person-group>
      <article-title>Practical Bayesian optimization of machine learning algorithms</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2012">2012</year>
      <volume>25</volume>
      <fpage>2951</fpage>
      <lpage>2959</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FeurerM2015neurips">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Klein</surname><given-names>A.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Springenberg</surname><given-names>J. T.</given-names></name>
        <name><surname>Blum</surname><given-names>M.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>Efficient and robust automated machine learning</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2015">2015</year>
      <volume>28</volume>
      <fpage>2962</fpage>
      <lpage>2970</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FeurerM2022jmlr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Falkner</surname><given-names>S.</given-names></name>
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>Auto-Sklearn 2.0: Hands-free AutoML via meta-learning</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>261</issue>
      <fpage>1</fpage>
      <lpage>61</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KandasamyK2018neurips">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kandasamy</surname><given-names>K.</given-names></name>
        <name><surname>Neiswanger</surname><given-names>W.</given-names></name>
        <name><surname>Schneider</surname><given-names>J.</given-names></name>
        <name><surname>P√≥czos</surname><given-names>B.</given-names></name>
        <name><surname>Xing</surname><given-names>E. P.</given-names></name>
      </person-group>
      <article-title>Neural architecture search with Bayesian optimisation and optimal transport</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2018">2018</year>
      <volume>31</volume>
      <fpage>2016</fpage>
      <lpage>2025</lpage>
    </element-citation>
  </ref>
  <ref id="ref-FrazierP2016ismdd">
    <element-citation publication-type="chapter">
      <person-group person-group-type="author">
        <name><surname>Frazier</surname><given-names>P. I.</given-names></name>
        <name><surname>Wang</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Bayesian optimization for materials design</article-title>
      <source>Information science for materials discovery and design</source>
      <publisher-name>Springer</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.1007/978-3-319-23871-5_3</pub-id>
      <fpage>45</fpage>
      <lpage>75</lpage>
    </element-citation>
  </ref>
  <ref id="ref-DurisJ2020phrl">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Duris</surname><given-names>J.</given-names></name>
        <name><surname>Kennedy</surname><given-names>D.</given-names></name>
        <name><surname>Hanuka</surname><given-names>A.</given-names></name>
        <name><surname>Shtalenkova</surname><given-names>J.</given-names></name>
        <name><surname>Edelen</surname><given-names>A.</given-names></name>
        <name><surname>Baxevanis</surname><given-names>P.</given-names></name>
        <name><surname>Egger</surname><given-names>A.</given-names></name>
        <name><surname>Cope</surname><given-names>T.</given-names></name>
        <name><surname>McIntire</surname><given-names>M.</given-names></name>
        <name><surname>Ermon</surname><given-names>S.</given-names></name>
        <name><surname>Ratner</surname><given-names>D.</given-names></name>
      </person-group>
      <article-title>Bayesian optimization of a free-electron laser</article-title>
      <source>Physical Review Letters</source>
      <year iso-8601-date="2020">2020</year>
      <volume>124</volume>
      <issue>12</issue>
      <pub-id pub-id-type="doi">10.1103/PhysRevLett.124.124801</pub-id>
      <fpage>124801</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-KorovinaK2020aistats">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Korovina</surname><given-names>K.</given-names></name>
        <name><surname>Xu</surname><given-names>S.</given-names></name>
        <name><surname>Kandasamy</surname><given-names>K.</given-names></name>
        <name><surname>Neiswanger</surname><given-names>W.</given-names></name>
        <name><surname>P√≥czos</surname><given-names>B.</given-names></name>
        <name><surname>Schneider</surname><given-names>J.</given-names></name>
        <name><surname>Xing</surname><given-names>E. P.</given-names></name>
      </person-group>
      <article-title>ChemBO: Bayesian optimization of small organic molecules with synthesizable recommendations</article-title>
      <source>Proceedings of the international conference on artificial intelligence and statistics (AISTATS)</source>
      <year iso-8601-date="2020">2020</year>
      <fpage>3393</fpage>
      <lpage>3403</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KimJ2020ml4eng">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Kim</surname><given-names>J.</given-names></name>
        <name><surname>Chung</surname><given-names>H.</given-names></name>
        <name><surname>Lee</surname><given-names>J.</given-names></name>
        <name><surname>Cho</surname><given-names>M.</given-names></name>
        <name><surname>Park</surname><given-names>J.</given-names></name>
      </person-group>
      <article-title>Combinatorial 3D shape generation via sequential assembly</article-title>
      <source>Neural information processing systems workshop on machine learning for engineering modeling, simulation, and design (ML4Eng)</source>
      <year iso-8601-date="2020">2020</year>
    </element-citation>
  </ref>
  <ref id="ref-ShieldsBJ2021nature">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Shields</surname><given-names>B. J.</given-names></name>
        <name><surname>Stevens</surname><given-names>J.</given-names></name>
        <name><surname>Li</surname><given-names>J.</given-names></name>
        <name><surname>Parasram</surname><given-names>M.</given-names></name>
        <name><surname>Damani</surname><given-names>F.</given-names></name>
        <name><surname>Alvarado</surname><given-names>J. I. M.</given-names></name>
        <name><surname>Janey</surname><given-names>J. M.</given-names></name>
        <name><surname>Adams</surname><given-names>R. P.</given-names></name>
        <name><surname>Doyle</surname><given-names>A. G.</given-names></name>
      </person-group>
      <article-title>Bayesian reaction optimization as a tool for chemical synthesis</article-title>
      <source>Nature</source>
      <year iso-8601-date="2021">2021</year>
      <volume>590</volume>
      <pub-id pub-id-type="doi">10.1038/s41586-021-03213-y</pub-id>
      <fpage>89</fpage>
      <lpage>96</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KimJ2021ml">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kim</surname><given-names>J.</given-names></name>
        <name><surname>McCourt</surname><given-names>M.</given-names></name>
        <name><surname>You</surname><given-names>T.</given-names></name>
        <name><surname>Kim</surname><given-names>S.</given-names></name>
        <name><surname>Choi</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>Bayesian optimization with approximate set kernels</article-title>
      <source>Machine Learning</source>
      <year iso-8601-date="2021">2021</year>
      <volume>110</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1007/s10994-021-05949-0</pub-id>
      <fpage>857</fpage>
      <lpage>879</lpage>
    </element-citation>
  </ref>
  <ref id="ref-RasmussenCE2006book">
    <element-citation publication-type="book">
      <person-group person-group-type="author">
        <name><surname>Rasmussen</surname><given-names>C. E.</given-names></name>
        <name><surname>Williams</surname><given-names>C. K. I.</given-names></name>
      </person-group>
      <source>Gaussian processes for machine learning</source>
      <publisher-name>MIT Press</publisher-name>
      <year iso-8601-date="2006">2006</year>
    </element-citation>
  </ref>
  <ref id="ref-ShahA2014aistats">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Shah</surname><given-names>A.</given-names></name>
        <name><surname>Wilson</surname><given-names>A. G.</given-names></name>
        <name><surname>Ghahramani</surname><given-names>Z.</given-names></name>
      </person-group>
      <article-title>Student-t processes as alternatives to Gaussian processes</article-title>
      <source>Proceedings of the international conference on artificial intelligence and statistics (AISTATS)</source>
      <year iso-8601-date="2014">2014</year>
      <fpage>877</fpage>
      <lpage>885</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HarrisCR2020nature">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harris</surname><given-names>C. R.</given-names></name>
        <name><surname>Millman</surname><given-names>K. J.</given-names></name>
        <name><surname>van der Walt</surname><given-names>S. J.</given-names></name>
        <name><surname>Gommers</surname><given-names>R.</given-names></name>
        <name><surname>Virtanen</surname><given-names>P.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Wieser</surname><given-names>E.</given-names></name>
        <name><surname>Taylor</surname><given-names>J.</given-names></name>
        <name><surname>Berg</surname><given-names>S.</given-names></name>
        <name><surname>Smith</surname><given-names>N. J.</given-names></name>
        <name><surname>Kern</surname><given-names>R.</given-names></name>
        <name><surname>Picus</surname><given-names>M.</given-names></name>
        <name><surname>Hoyer</surname><given-names>S.</given-names></name>
        <name><surname>van Kerkwijk</surname><given-names>M. H.</given-names></name>
        <name><surname>Brett</surname><given-names>M.</given-names></name>
        <name><surname>Haldane</surname><given-names>A.</given-names></name>
        <name><surname>del R√≠o</surname><given-names>J. F.</given-names></name>
        <name><surname>Wiebe</surname><given-names>M.</given-names></name>
        <name><surname>Peterson</surname><given-names>P.</given-names></name>
        <name><surname>G√©rard-Marchant</surname><given-names>P.</given-names></name>
        <name><surname>Sheppard</surname><given-names>K.</given-names></name>
        <name><surname>Reddy</surname><given-names>T.</given-names></name>
        <name><surname>Weckesser</surname><given-names>W.</given-names></name>
        <name><surname>Abbasi</surname><given-names>H.</given-names></name>
        <name><surname>Gohlke</surname><given-names>C.</given-names></name>
        <name><surname>Oliphant</surname><given-names>T. E.</given-names></name>
      </person-group>
      <article-title>Array programming with NumPy</article-title>
      <source>Nature</source>
      <year iso-8601-date="2020">2020</year>
      <volume>585</volume>
      <pub-id pub-id-type="doi">10.1038/s41586-020-2649-2</pub-id>
      <fpage>357</fpage>
      <lpage>362</lpage>
    </element-citation>
  </ref>
  <ref id="ref-VirtanenP2020nm">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Virtanen</surname><given-names>P.</given-names></name>
        <name><surname>Gommers</surname><given-names>R.</given-names></name>
        <name><surname>Oliphant</surname><given-names>T. E.</given-names></name>
        <name><surname>Haberland</surname><given-names>M.</given-names></name>
        <name><surname>Reddy</surname><given-names>T.</given-names></name>
        <name><surname>Cournapeau</surname><given-names>D.</given-names></name>
        <name><surname>Burovski</surname><given-names>E.</given-names></name>
        <name><surname>Peterson</surname><given-names>P.</given-names></name>
        <name><surname>Weckesser</surname><given-names>W.</given-names></name>
        <name><surname>Bright</surname><given-names>J.</given-names></name>
        <name><surname>van der Walt</surname><given-names>S. J.</given-names></name>
        <name><surname>Brett</surname><given-names>M.</given-names></name>
        <name><surname>Wilson</surname><given-names>J.</given-names></name>
        <name><surname>Millman</surname><given-names>K. J.</given-names></name>
        <name><surname>Mayorov</surname><given-names>N.</given-names></name>
        <name><surname>Nelson</surname><given-names>A. R. J.</given-names></name>
        <name><surname>Jones</surname><given-names>E.</given-names></name>
        <name><surname>Kern</surname><given-names>R.</given-names></name>
        <name><surname>Larson</surname><given-names>E.</given-names></name>
        <name><surname>Carey</surname><given-names>C. J.</given-names></name>
        <name><surname>Polat</surname><given-names>ƒ∞.</given-names></name>
        <name><surname>Feng</surname><given-names>Y.</given-names></name>
        <name><surname>Moore</surname><given-names>E. W.</given-names></name>
        <name><surname>VanderPlas</surname><given-names>J.</given-names></name>
        <name><surname>Laxalde</surname><given-names>D.</given-names></name>
        <name><surname>Perktold</surname><given-names>J.</given-names></name>
        <name><surname>Cimrman</surname><given-names>R.</given-names></name>
        <name><surname>Henriksen</surname><given-names>I.</given-names></name>
        <name><surname>Quintero</surname><given-names>E. A.</given-names></name>
        <name><surname>Harris</surname><given-names>C. R.</given-names></name>
        <name><surname>Archibald</surname><given-names>A. M.</given-names></name>
        <name><surname>Ribeiro</surname><given-names>A. H.</given-names></name>
        <name><surname>Pedregosa</surname><given-names>F.</given-names></name>
        <name><surname>van Mulbregt</surname><given-names>P.</given-names></name>
        <string-name>SciPy 1.0 Contributors</string-name>
      </person-group>
      <article-title>SciPy 1.0: Fundamental algorithms for scientific computing in Python</article-title>
      <source>Nature Methods</source>
      <year iso-8601-date="2020">2020</year>
      <volume>17</volume>
      <pub-id pub-id-type="doi">10.1038/s41592-019-0686-2</pub-id>
      <fpage>261</fpage>
      <lpage>272</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HansenN2019software">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hansen</surname><given-names>N.</given-names></name>
        <name><surname>Akimoto</surname><given-names>Y.</given-names></name>
        <name><surname>Baudis</surname><given-names>P.</given-names></name>
      </person-group>
      <article-title>CMA-ES/pycma on GitHub</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/CMA-ES/pycma</uri>
    </element-citation>
  </ref>
  <ref id="ref-BreimanL2001ml">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Breiman</surname><given-names>L.</given-names></name>
      </person-group>
      <article-title>Random forests</article-title>
      <source>Machine Learning</source>
      <year iso-8601-date="2001">2001</year>
      <volume>45</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1023/A:1010933404324</pub-id>
      <fpage>5</fpage>
      <lpage>32</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HutterF2014ai">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
        <name><surname>Xu</surname><given-names>L.</given-names></name>
        <name><surname>Hoos</surname><given-names>H. H.</given-names></name>
        <name><surname>Leyton-Brown</surname><given-names>K.</given-names></name>
      </person-group>
      <article-title>Algorithm runtime prediction: Methods &amp; evaluation</article-title>
      <source>Artificial Intelligence</source>
      <year iso-8601-date="2014">2014</year>
      <volume>206</volume>
      <pub-id pub-id-type="doi">10.1016/j.artint.2013.10.003</pub-id>
      <fpage>79</fpage>
      <lpage>111</lpage>
    </element-citation>
  </ref>
  <ref id="ref-JonesDR1998jgo">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jones</surname><given-names>D. R.</given-names></name>
        <name><surname>Schonlau</surname><given-names>M.</given-names></name>
        <name><surname>Welch</surname><given-names>W. J.</given-names></name>
      </person-group>
      <article-title>Efficient global optimization of expensive black-box functions</article-title>
      <source>Journal of Global Optimization</source>
      <year iso-8601-date="1998">1998</year>
      <volume>13</volume>
      <pub-id pub-id-type="doi">10.1023/A:1008306431147</pub-id>
      <fpage>455</fpage>
      <lpage>492</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ThompsonWR1933biometrika">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Thompson</surname><given-names>W. R.</given-names></name>
      </person-group>
      <article-title>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</article-title>
      <source>Biometrika</source>
      <year iso-8601-date="1933">1933</year>
      <volume>25</volume>
      <issue>3/4</issue>
      <pub-id pub-id-type="doi">10.1093/biomet/25.3-4.285</pub-id>
      <fpage>285</fpage>
      <lpage>294</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HuangD2006jgo">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Huang</surname><given-names>D.</given-names></name>
        <name><surname>Allen</surname><given-names>T. T.</given-names></name>
        <name><surname>Notz</surname><given-names>W. I.</given-names></name>
        <name><surname>Zheng</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>Global optimization of stochastic black-box systems via sequential kriging meta-models</article-title>
      <source>Journal of Global Optimization</source>
      <year iso-8601-date="2006">2006</year>
      <volume>34</volume>
      <pub-id pub-id-type="doi">10.1007/s10898-005-2454-3</pub-id>
      <fpage>441</fpage>
      <lpage>466</lpage>
    </element-citation>
  </ref>
  <ref id="ref-SrinivasN2010icml">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Srinivas</surname><given-names>N.</given-names></name>
        <name><surname>Krause</surname><given-names>A.</given-names></name>
        <name><surname>Kakade</surname><given-names>S.</given-names></name>
        <name><surname>Seeger</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Gaussian process optimization in the bandit setting: No regret and experimental design</article-title>
      <source>Proceedings of the international conference on machine learning (ICML)</source>
      <year iso-8601-date="2010">2010</year>
      <fpage>1015</fpage>
      <lpage>1022</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ByrdRH1995siamjsc">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Byrd</surname><given-names>R. H.</given-names></name>
        <name><surname>Lu</surname><given-names>P.</given-names></name>
        <name><surname>Nocedal</surname><given-names>J.</given-names></name>
        <name><surname>Zhu</surname><given-names>C.</given-names></name>
      </person-group>
      <article-title>A limited memory algorithm for bound constrained optimization</article-title>
      <source>SIAM Journal on Scientific Computing</source>
      <year iso-8601-date="1995">1995</year>
      <volume>16</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1137/0916069</pub-id>
      <fpage>1190</fpage>
      <lpage>1208</lpage>
    </element-citation>
  </ref>
  <ref id="ref-HansenN1997eufit">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Hansen</surname><given-names>N.</given-names></name>
        <name><surname>Ostermeier</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Convergence properties of evolution strategies with the derandomized covariance matrix adaptation: The (\mu/\mu_\mathrm{I}, \lambda)-CMA-ES</article-title>
      <source>Proceedings of the european congress on intelligent techniques and soft computing (EUFIT)</source>
      <year iso-8601-date="1997">1997</year>
      <fpage>650</fpage>
      <lpage>654</lpage>
    </element-citation>
  </ref>
  <ref id="ref-JonesDR1993jota">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Jones</surname><given-names>D. R.</given-names></name>
        <name><surname>Perttunen</surname><given-names>C. D.</given-names></name>
        <name><surname>Stuckman</surname><given-names>B. E.</given-names></name>
      </person-group>
      <article-title>Lipschitzian optimization without the Lipschitz constant</article-title>
      <source>Journal of Optimization Theory and Applications</source>
      <year iso-8601-date="1993">1993</year>
      <volume>79</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1007/BF00941892</pub-id>
      <fpage>157</fpage>
      <lpage>181</lpage>
    </element-citation>
  </ref>
  <ref id="ref-RoyPT2023joss">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Roy</surname><given-names>P. T.</given-names></name>
        <name><surname>Owen</surname><given-names>A. B.</given-names></name>
        <name><surname>Balandat</surname><given-names>M.</given-names></name>
        <name><surname>Haberland</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>Quasi-Monte Carlo methods in Python</article-title>
      <source>Journal of Open Source Software</source>
      <year iso-8601-date="2023">2023</year>
      <volume>8</volume>
      <issue>84</issue>
      <pub-id pub-id-type="doi">10.21105/joss.05309</pub-id>
      <fpage>5309</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tqdm2016software">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>The tqdm authors</string-name>
      </person-group>
      <article-title>tqdm</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <uri>https://github.com/tqdm/tqdm</uri>
    </element-citation>
  </ref>
  <ref id="ref-gpy2012software">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>GPy</string-name>
      </person-group>
      <article-title>GPy: A Gaussian process framework in Python</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2012">2012</year>
      <uri>https://github.com/SheffieldML/GPy</uri>
    </element-citation>
  </ref>
  <ref id="ref-gpyopt2016software">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>The GPyOpt authors</string-name>
      </person-group>
      <article-title>GPyOpt: A Bayesian optimization framework in Python</article-title>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <uri>https://github.com/SheffieldML/GPyOpt</uri>
    </element-citation>
  </ref>
  <ref id="ref-LindauerM2022jmlr">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lindauer</surname><given-names>M.</given-names></name>
        <name><surname>Eggensperger</surname><given-names>K.</given-names></name>
        <name><surname>Feurer</surname><given-names>M.</given-names></name>
        <name><surname>Biedenkapp</surname><given-names>A.</given-names></name>
        <name><surname>Deng</surname><given-names>D.</given-names></name>
        <name><surname>Benjamins</surname><given-names>C.</given-names></name>
        <name><surname>Ruhkopf</surname><given-names>T.</given-names></name>
        <name><surname>Sass</surname><given-names>R.</given-names></name>
        <name><surname>Hutter</surname><given-names>F.</given-names></name>
      </person-group>
      <article-title>SMAC3: A versatile Bayesian optimization package for hyperparameter optimization</article-title>
      <source>Journal of Machine Learning Research</source>
      <year iso-8601-date="2022">2022</year>
      <volume>23</volume>
      <issue>54</issue>
      <fpage>1</fpage>
      <lpage>9</lpage>
    </element-citation>
  </ref>
  <ref id="ref-PaszkeA2019neurips">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>A.</given-names></name>
        <name><surname>Gross</surname><given-names>S.</given-names></name>
        <name><surname>Massa</surname><given-names>F.</given-names></name>
        <name><surname>Lerer</surname><given-names>A.</given-names></name>
        <name><surname>Bradbury</surname><given-names>J.</given-names></name>
        <name><surname>Chanan</surname><given-names>G.</given-names></name>
        <name><surname>Killeen</surname><given-names>T.</given-names></name>
        <name><surname>Lin</surname><given-names>Z.</given-names></name>
        <name><surname>Gimelshein</surname><given-names>N.</given-names></name>
        <name><surname>Antiga</surname><given-names>L.</given-names></name>
        <name><surname>Desmaison</surname><given-names>A.</given-names></name>
        <name><surname>K√∂pf</surname><given-names>A.</given-names></name>
        <name><surname>Yang</surname><given-names>E.</given-names></name>
        <name><surname>DeVito</surname><given-names>Z.</given-names></name>
        <name><surname>Raison</surname><given-names>M.</given-names></name>
        <name><surname>Tejani</surname><given-names>A.</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>S.</given-names></name>
        <name><surname>Steiner</surname><given-names>B.</given-names></name>
        <name><surname>Fang</surname><given-names>L.</given-names></name>
        <name><surname>Bai</surname><given-names>J.</given-names></name>
        <name><surname>Chintala</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>PyTorch: An imperative style, high-performance deep learning library</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2019">2019</year>
      <volume>32</volume>
      <fpage>8026</fpage>
      <lpage>8037</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AbadiM2016osdi">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Abadi</surname><given-names>M.</given-names></name>
        <name><surname>Barham</surname><given-names>P.</given-names></name>
        <name><surname>Chen</surname><given-names>J.</given-names></name>
        <name><surname>Chen</surname><given-names>Z.</given-names></name>
        <name><surname>Davis</surname><given-names>A.</given-names></name>
        <name><surname>Dean</surname><given-names>J.</given-names></name>
        <name><surname>Devin</surname><given-names>M.</given-names></name>
        <name><surname>Ghemawat</surname><given-names>S.</given-names></name>
        <name><surname>Irving</surname><given-names>G.</given-names></name>
        <name><surname>Isard</surname><given-names>M.</given-names></name>
        <name><surname>Kudlur</surname><given-names>M.</given-names></name>
        <name><surname>Levenberg</surname><given-names>J.</given-names></name>
        <name><surname>Monga</surname><given-names>R.</given-names></name>
        <name><surname>Moore</surname><given-names>S.</given-names></name>
        <name><surname>Murray</surname><given-names>D. G.</given-names></name>
        <name><surname>Steiner</surname><given-names>B.</given-names></name>
        <name><surname>Tucker</surname><given-names>P.</given-names></name>
        <name><surname>Vasudevan</surname><given-names>V.</given-names></name>
        <name><surname>Warden</surname><given-names>P.</given-names></name>
        <name><surname>Wicke</surname><given-names>M.</given-names></name>
        <name><surname>Yu</surname><given-names>Y.</given-names></name>
        <name><surname>Zheng</surname><given-names>X.</given-names></name>
      </person-group>
      <article-title>TensorFlow: A system for large-scale machine learning</article-title>
      <source>USENIX symposium on operating systems design and implementation (OSDI)</source>
      <year iso-8601-date="2016">2016</year>
      <fpage>265</fpage>
      <lpage>283</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KnuddeN2017arxiv">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Knudde</surname><given-names>N.</given-names></name>
        <name><surname>van der Herten</surname><given-names>J.</given-names></name>
        <name><surname>Dhaene</surname><given-names>T.</given-names></name>
        <name><surname>Couckuyt</surname><given-names>I.</given-names></name>
      </person-group>
      <article-title>GPflowOpt: A Bayesian optimization library using TensorFlow</article-title>
      <source>arXiv preprint arXiv:1711.03845</source>
      <year iso-8601-date="2017">2017</year>
    </element-citation>
  </ref>
  <ref id="ref-BalandatM2020neurips">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Balandat</surname><given-names>M.</given-names></name>
        <name><surname>Karrer</surname><given-names>B.</given-names></name>
        <name><surname>Jiang</surname><given-names>D. R.</given-names></name>
        <name><surname>Daulton</surname><given-names>S.</given-names></name>
        <name><surname>Letham</surname><given-names>B.</given-names></name>
        <name><surname>Wilson</surname><given-names>A. G.</given-names></name>
        <name><surname>Bakshy</surname><given-names>E.</given-names></name>
      </person-group>
      <article-title>BoTorch: A framework for efficient Monte-Carlo Bayesian optimization</article-title>
      <source>Advances in neural information processing systems (NeurIPS)</source>
      <year iso-8601-date="2020">2020</year>
      <volume>33</volume>
      <fpage>21524</fpage>
      <lpage>21538</lpage>
    </element-citation>
  </ref>
  <ref id="ref-KushnerHJ1964jbe">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kushner</surname><given-names>H. J.</given-names></name>
      </person-group>
      <article-title>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise</article-title>
      <source>Journal of Basic Engineering</source>
      <year iso-8601-date="1964">1964</year>
      <volume>86</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1115/1.3653121</pub-id>
      <fpage>97</fpage>
      <lpage>106</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
