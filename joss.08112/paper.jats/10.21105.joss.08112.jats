<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">8112</article-id>
<article-id pub-id-type="doi">10.21105/joss.08112</article-id>
<title-group>
<article-title>pyCLINE: A Python package using the CLINE method for
discovery of nullcline structures in oscillatory
dynamics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9723-0176</contrib-id>
<name>
<surname>Prokop</surname>
<given-names>Bartosz</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-2788-1907</contrib-id>
<name>
<surname>Frolov</surname>
<given-names>Nikit</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-7290-9561</contrib-id>
<name>
<surname>Gelens</surname>
<given-names>Lendert</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Laboratory of Dynamics in Biological Systems, Department of
Cellular and Mollecular Medicine, KU Leuven</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-03-21">
<day>21</day>
<month>3</month>
<year>2025</year>
</pub-date>
<volume>10</volume>
<issue>113</issue>
<fpage>8112</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2025</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>nonlinear dynamics</kwd>
<kwd>dynamics</kwd>
<kwd>machine learning</kwd>
<kwd>data-driven methods</kwd>
<kwd>model identification</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Dynamical processes in physics, biology, chemistry, and
  engineering—such as planetary motion, climate variability, and cell
  cycle oscillations—are crucial to understanding complex systems.
  Traditionally, mathematical models describing these systems rely on
  differential equations derived from empirical data using established
  modeling principles and scientific intuition. However, the increasing
  availability of high-dimensional, complex datasets has rendered
  classical model derivation increasingly challenging.</p>
  <p>As a result, data-driven or machine learning methods emerged that
  are able to handle high-dimensional data sets. However existing
  methods either are limited by data quality or the interpretability of
  their results. Thus we developed the <bold>CLINE</bold>
  (<bold>C</bold>omputational <bold>L</bold>earning and
  <bold>I</bold>nference of <bold>N</bold>ullclin<bold>E</bold>s)
  (<xref alt="Prokop et al., 2025" rid="ref-Prokop2025" ref-type="bibr">Prokop
  et al., 2025</xref>) and introduce its Python implementation
  <monospace>pyCLINE</monospace> that allows to extract static to
  identify static phase-space structures without prior knowledge
  directly from time series data.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Machine learning and data-driven approaches have revolutionized the
  study of dynamical systems. Two primary methodologies exist:</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Black-box methods</bold> (e.g., neural networks)
      approximate system behavior but lack interpretability regarding
      underlying mechanisms.</p>
    </list-item>
    <list-item>
      <p><bold>White-box methods</bold> derive symbolic differential
      equations directly from data but require high-quality datasets to
      ensure accuracy
      (<xref alt="Prokop &amp; Gelens, 2024" rid="ref-Prokop2024" ref-type="bibr">Prokop
      &amp; Gelens, 2024</xref>).</p>
    </list-item>
  </list>
  <p>To bridge this gap, <bold>grey-box methods</bold> integrate the
  strengths of both approaches, handling large, structured datasets
  while preserving interpretability. Examples include Physics-Informed
  Neural Networks (PINNs)
  (<xref alt="Karniadakis et al., 2021" rid="ref-Karniadakis2021" ref-type="bibr">Karniadakis
  et al., 2021</xref>), Biology-Informed Neural Networks (BINNs)
  (<xref alt="Lagergren et al., 2020" rid="ref-Lagergren2020" ref-type="bibr">Lagergren
  et al., 2020</xref>), and Universal Differential Equations
  (<xref alt="Rackauckas et al., 2020" rid="ref-Rackauckas2020" ref-type="bibr">Rackauckas
  et al., 2020</xref>). However, most of these methods focus on
  forecasting rather than extracting fundamental structural properties
  of dynamical systems.</p>
  <p>To address this limitation, our method <bold>CLINE</bold> is able
  to extract static phase-space features, specifically the structure of
  nullclines, from time series data without forecasting.
  Understanding nullcline structure of a dynamical system provides
  several key benefits
  (<xref alt="Prokop et al., 2024" rid="ref-Prokop2024b" ref-type="bibr">Prokop
  et al., 2024</xref>):</p>
  <list list-type="bullet">
    <list-item>
      <p><bold>Comprehensive System Characterization:</bold> Nullclines
      fully describe the system’s steady-state behavior and provide
      richer insights than time series data alone.</p>
    </list-item>
    <list-item>
      <p><bold>Reduced Complexity for Symbolic Model
      Identification:</bold> Once nullcline structures are identified,
      symbolic equations can be inferred using sparse regression
      techniques, such as sparse identification of nonlinear dynamics
      (SINDy)
      (<xref alt="Brunton et al., 2016" rid="ref-Brunton2016" ref-type="bibr">Brunton
      et al., 2016</xref>) or symbolic regression (SR)
      (<xref alt="Schmidt &amp; Lipson, 2009" rid="ref-Schmidt2009" ref-type="bibr">Schmidt
      &amp; Lipson, 2009</xref>), with significantly lower computational
      complexity compared to direct time-series-based approaches.</p>
    </list-item>
    <list-item>
      <p><bold>Bias Reduction through Model-Free Inference:</bold>
      Unlike traditional white-box methods, CLINE does not rely on
      predefined candidate terms (e.g., library-based functions),
      minimizing biases in model formulation and increasing adaptability
      to diverse systems.</p>
    </list-item>
  </list>
  <p><monospace>pyCLINE</monospace> is a Python package that allows one
  to easily set up and use the CLINE method as explained and shown in
  Prokop et al.
  (<xref alt="2025" rid="ref-Prokop2025" ref-type="bibr">2025</xref>).
  It is based on the Python Torch implementation
  <monospace>pyTorch</monospace>
  (<xref alt="Paszke et al., 2019" rid="ref-Paszke2019" ref-type="bibr">Paszke
  et al., 2019</xref>) and enables rapid identification of nullcline
  structures from simulated or measured time series data. The
  implementation of <monospace>pyCLINE</monospace> can generate exemple
  data sets from scratch, correctly prepare data for training and set up
  the feed forward neural network for training.</p>
  <p><monospace>pyCLINE</monospace> was designed to be used by
  researchers experienced with the use of machine learning or laymen
  that are interested in applying the method to either different models
  or measured data. This allows for simple and fast implementation in
  many fields that are interested in discovering nullcline structures
  from measured data, that can help develop novel or confirm existing
  models of dynamical (oscillatory) systems.</p>
  <sec id="methodology">
    <title>Methodology</title>
    <p>The main aspects of the CLINE method are explained in Prokop et
    al.
    (<xref alt="2025" rid="ref-Prokop2025" ref-type="bibr">2025</xref>),
    nevertheless we provide a brief explanation of the method. In order
    to identify nullclines for a set of ordinary differential equations
    (ODEs) with system variables <inline-formula><alternatives>
    <tex-math><![CDATA[u]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>u</mml:mi></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[v]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>,
    we have to set the derivative to 0:</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[u_t = f(u,v) \rightarrow u_t = f(u,v)=0]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula>
    <disp-formula><alternatives>
    <tex-math><![CDATA[v_t = g(u,v) \rightarrow v_t = g(u,v)=0]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>→</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>The functions of <inline-formula><alternatives>
    <tex-math><![CDATA[f]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>f</mml:mi></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[g]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>g</mml:mi></mml:math></alternatives></inline-formula>
    are not known <italic>a priori</italic>. However, to learn the
    functions we can reformulate the nullcline equations to:</p>
    <p><disp-formula><alternatives>
    <tex-math><![CDATA[u = f^{-1}(v,u_t)\text{ or } v = f^{-1}(u,u_t)]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.333em"></mml:mspace><mml:mtext mathvariant="normal"> or </mml:mtext><mml:mspace width="0.333em"></mml:mspace></mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula>
    <disp-formula><alternatives>
    <tex-math><![CDATA[u = g^{-1}(v,v_t)\text{ or } v = g^{-1}(u,v_t)]]></tex-math>
    <mml:math display="block" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mrow><mml:mspace width="0.333em"></mml:mspace><mml:mtext mathvariant="normal"> or </mml:mtext><mml:mspace width="0.333em"></mml:mspace></mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow></mml:mrow></mml:math></alternatives></disp-formula></p>
    <p>Now we have to learn the inverse functions
    <inline-formula><alternatives>
    <tex-math><![CDATA[f^{-1}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>f</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[g^{-1}]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msup><mml:mi>g</mml:mi><mml:mrow><mml:mi>−</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></alternatives></inline-formula>
    which describe the relationship between the measured variables
    <inline-formula><alternatives>
    <tex-math><![CDATA[u]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>u</mml:mi></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[v]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>
    with additional derivative information
    <inline-formula><alternatives>
    <tex-math><![CDATA[u_t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    or <inline-formula><alternatives>
    <tex-math><![CDATA[v_t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
    As such, the target functions can be expressed as a feed-forward
    neural network with e.g. inputs <inline-formula><alternatives>
    <tex-math><![CDATA[u]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>u</mml:mi></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives>
    <tex-math><![CDATA[u_t]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>,
    to learn <inline-formula><alternatives>
    <tex-math><![CDATA[v]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>.</p>
    <p>After training, we can provide a set of
    <inline-formula><alternatives>
    <tex-math><![CDATA[u]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>u</mml:mi></mml:math></alternatives></inline-formula>
    together with <inline-formula><alternatives>
    <tex-math><![CDATA[u_t=0]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    (requirement for a nullcline) as inputs and learn the corresponding
    values of <inline-formula><alternatives>
    <tex-math><![CDATA[v]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mi>v</mml:mi></mml:math></alternatives></inline-formula>
    that describe <inline-formula><alternatives>
    <tex-math><![CDATA[u_t = f(u,v)=0]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:msub><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo stretchy="true" form="prefix">(</mml:mo><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="true" form="postfix">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>.
    As a result, we learn the structure of a nullcline in the phase
    space <inline-formula><alternatives>
    <tex-math><![CDATA[u,v]]></tex-math>
    <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>u</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
    to which other white-box methods can be applied to learn the
    symbolic equations, yet on a decisively simpler optimization problem
    then that on time series data.</p>
  </sec>
</sec>
<sec id="usage">
  <title>Usage</title>
  <p>The <monospace>pyCLINE</monospace> package can be downloaded and
  installed using <monospace>pip</monospace>:</p>
  <preformat>pip install pyCLINE</preformat>
  <p>The <monospace>pyCLINE</monospace> package includes three main
  modules (see
  <xref alt="[fig:method]" rid="figU003Amethod">[fig:method]</xref>):</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>pyCLINE.generate_data()</monospace>: This module
      generates data which has been used in Prokop et al.
      (<xref alt="2025" rid="ref-Prokop2025" ref-type="bibr">2025</xref>)
      along with many additional models that can be found under
      <monospace>pyCLINE.model()</monospace>.</p>
    </list-item>
    <list-item>
      <p><monospace>pyCLINE.recovery_methods.data_preparation()</monospace>:
      Splits and normalizes that data for training, with many more
      features for the user to change the data.</p>
    </list-item>
    <list-item>
      <p><monospace>pyCLINE.recovery_methods.nn_training()</monospace>:
      The <monospace>pyTorch</monospace> implementation that sets up the
      model and trains it.</p>
    </list-item>
  </list>
  <p>The <monospace>pyCLINE.model()</monospace> currently includes a set
  of different models:</p>
  <list list-type="bullet">
    <list-item>
      <p>FitzHugh-Nagumo model</p>
    </list-item>
    <list-item>
      <p>Bicubic model</p>
    </list-item>
    <list-item>
      <p>Gene expression model</p>
    </list-item>
    <list-item>
      <p>Glycolytic oscillation model</p>
    </list-item>
    <list-item>
      <p>Goodwin model</p>
    </list-item>
    <list-item>
      <p>Oregonator model</p>
    </list-item>
    <list-item>
      <p>Lorenz system</p>
    </list-item>
    <list-item>
      <p>Roessler system</p>
    </list-item>
    <list-item>
      <p>Delay oscillator (self-inhibitory gene)</p>
    </list-item>
  </list>
  <p>Some of the models are three-dimensional and can be used to further
  study the limitations of the method, when applied to higher
  dimensional systems.</p>
  <p>To demonstrate the method, we also provide
  <monospace>pyCLINE.example()</monospace> which contains full examples
  of how <monospace>pyCLINE</monospace> can be used. Here,
  <monospace>pyCLINE.example()</monospace> can be used to generate
  prediction data for four systems: The FitzHugh-Nagumo model with time
  scale separation variable <inline-formula><alternatives>
  <tex-math><![CDATA[\varepsilon=0.3]]></tex-math>
  <mml:math display="inline" xmlns:mml="http://www.w3.org/1998/Math/MathML"><mml:mrow><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
  (<monospace>FHN</monospace>), the Bicubic model
  (<monospace>Bicubic</monospace>), gene expression model
  (<monospace>GeneExpression</monospace>) and the delay oscillator model
  (<monospace>DelayOscillator</monospace>).</p>
  <fig>
    <caption><p>The method CLINE explained by using Figure 1 from Prokop
    et al.
    (<xref alt="2025" rid="ref-Prokop2025" ref-type="bibr">2025</xref>).
    In red the main modules of the <monospace>pyCLINE</monospace>
    package are shown.
    <styled-content id="figU003Amethod"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="introduction_manuscript_1.png" />
  </fig>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-Brunton2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Brunton</surname><given-names>Steven L</given-names></name>
        <name><surname>Proctor</surname><given-names>Joshua L</given-names></name>
        <name><surname>Kutz</surname><given-names>J Nathan</given-names></name>
      </person-group>
      <article-title>Discovering governing equations from data by sparse identification of nonlinear dynamical systems</article-title>
      <source>Proceedings of the National Academy of Sciences</source>
      <year iso-8601-date="2016-04">2016</year><month>04</month>
      <volume>113</volume>
      <issue>15</issue>
      <issn>0027-8424</issn>
      <pub-id pub-id-type="doi">10.1073/pnas.1517384113</pub-id>
      <fpage>3932</fpage>
      <lpage>3937</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Karniadakis2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Karniadakis</surname><given-names>George Em</given-names></name>
        <name><surname>Kevrekidis</surname><given-names>Ioannis G.</given-names></name>
        <name><surname>Lu</surname><given-names>Lu</given-names></name>
        <name><surname>Perdikaris</surname><given-names>Paris</given-names></name>
        <name><surname>Wang</surname><given-names>Sifan</given-names></name>
        <name><surname>Yang</surname><given-names>Liu</given-names></name>
      </person-group>
      <article-title>Physics-informed machine learning</article-title>
      <source>Nature Reviews Physics</source>
      <publisher-name>Springer Nature</publisher-name>
      <year iso-8601-date="2021-05">2021</year><month>05</month>
      <volume>3</volume>
      <issue>6</issue>
      <issn>2522-5820</issn>
      <pub-id pub-id-type="doi">10.1038/s42254-021-00314-5</pub-id>
      <fpage>422</fpage>
      <lpage>440</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Lagergren2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Lagergren</surname><given-names>John H.</given-names></name>
        <name><surname>Nardini</surname><given-names>John T.</given-names></name>
        <name><surname>Baker</surname><given-names>Ruth E.</given-names></name>
        <name><surname>Simpson</surname><given-names>Matthew J.</given-names></name>
        <name><surname>Flores</surname><given-names>Kevin B.</given-names></name>
      </person-group>
      <article-title>Biologically-informed neural networks guide mechanistic modeling from sparse experimental data</article-title>
      <source>PLOS Computational Biology</source>
      <person-group person-group-type="editor">
        <name><surname>Lavrik</surname><given-names>Inna</given-names></name>
      </person-group>
      <publisher-name>Public Library of Science</publisher-name>
      <year iso-8601-date="2020-12">2020</year><month>12</month>
      <volume>16</volume>
      <issue>12</issue>
      <issn>1553-7358</issn>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1008462</pub-id>
      <fpage>e1008462</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Prokop2024">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Prokop</surname><given-names>Bartosz</given-names></name>
        <name><surname>Gelens</surname><given-names>Lendert</given-names></name>
      </person-group>
      <article-title>From biological data to oscillator models using SINDy</article-title>
      <source>iScience</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2024-04">2024</year><month>04</month>
      <volume>27</volume>
      <issue>4</issue>
      <issn>2589-0042</issn>
      <pub-id pub-id-type="doi">10.1016/j.isci.2024.109316</pub-id>
      <fpage>109316</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Prokop2024b">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Prokop</surname><given-names>Bartosz</given-names></name>
        <name><surname>Frolov</surname><given-names>Nikita</given-names></name>
        <name><surname>Gelens</surname><given-names>Lendert</given-names></name>
      </person-group>
      <article-title>Enhancing model identification with SINDy via nullcline reconstruction</article-title>
      <source>Chaos: An Interdisciplinary Journal of Nonlinear Science</source>
      <publisher-name>AIP Publishing</publisher-name>
      <year iso-8601-date="2024-06">2024</year><month>06</month>
      <volume>34</volume>
      <issue>6</issue>
      <issn>1054-1500</issn>
      <pub-id pub-id-type="doi">10.1063/5.0199311</pub-id>
      <fpage>063135</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Prokop2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Prokop</surname><given-names>Bartosz</given-names></name>
        <name><surname>Billen</surname><given-names>Jimmy</given-names></name>
        <name><surname>Frolov</surname><given-names>Nikita</given-names></name>
        <name><surname>Gelens</surname><given-names>Lendert</given-names></name>
      </person-group>
      <article-title>Machine learning identifies nullclines in oscillatory dynamical systems</article-title>
      <source>Preprint at ArXiv</source>
      <year iso-8601-date="2025-03">2025</year><month>03</month>
      <uri>https://arxiv.org/abs/2503.16240v1 http://arxiv.org/abs/2503.16240</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2503.16240</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Rackauckas2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rackauckas</surname><given-names>Christopher</given-names></name>
        <name><surname>Ma</surname><given-names>Yingbo</given-names></name>
        <name><surname>Martensen</surname><given-names>Julius</given-names></name>
        <name><surname>Warner</surname><given-names>Collin</given-names></name>
        <name><surname>Zubov</surname><given-names>Kirill</given-names></name>
        <name><surname>Supekar</surname><given-names>Rohit</given-names></name>
        <name><surname>Skinner</surname><given-names>Dominic</given-names></name>
        <name><surname>Ramadhan</surname><given-names>Ali</given-names></name>
        <name><surname>Edelman</surname><given-names>Alan</given-names></name>
      </person-group>
      <article-title>Universal Differential Equations for Scientific Machine Learning</article-title>
      <source>Preprint at ArXiv</source>
      <year iso-8601-date="2020-01">2020</year><month>01</month>
      <uri>http://arxiv.org/abs/2001.04385</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2001.04385</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Paszke2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Gross</surname><given-names>Sam</given-names></name>
        <name><surname>Massa</surname><given-names>Francisco</given-names></name>
        <name><surname>Lerer</surname><given-names>Adam</given-names></name>
        <name><surname>Bradbury</surname><given-names>James</given-names></name>
        <name><surname>Chanan</surname><given-names>Gregory</given-names></name>
        <name><surname>Killeen</surname><given-names>Trevor</given-names></name>
        <name><surname>Lin</surname><given-names>Zeming</given-names></name>
        <name><surname>Gimelshein</surname><given-names>Natalia</given-names></name>
        <name><surname>Antiga</surname><given-names>Luca</given-names></name>
        <name><surname>Desmaison</surname><given-names>Alban</given-names></name>
        <name><surname>Köpf</surname><given-names>Andreas</given-names></name>
        <name><surname>Yang</surname><given-names>Edward</given-names></name>
        <name><surname>DeVito</surname><given-names>Zach</given-names></name>
        <name><surname>Raison</surname><given-names>Martin</given-names></name>
        <name><surname>Tejani</surname><given-names>Alykhan</given-names></name>
        <name><surname>Chilamkurthy</surname><given-names>Sasank</given-names></name>
        <name><surname>Steiner</surname><given-names>Benoit</given-names></name>
        <name><surname>Fang</surname><given-names>Lu</given-names></name>
        <name><surname>Bai</surname><given-names>Junjie</given-names></name>
        <name><surname>Chintala</surname><given-names>Soumith</given-names></name>
      </person-group>
      <article-title>PyTorch: An Imperative Style, High-Performance Deep Learning Library</article-title>
      <source>NeurIPS</source>
      <year iso-8601-date="2019-12">2019</year><month>12</month>
      <issue>NeurIPS</issue>
      <uri>http://arxiv.org/abs/1912.01703</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.1912.01703</pub-id>
      <fpage>8026</fpage>
      <lpage>8037</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Schmidt2009">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Schmidt</surname><given-names>Michael</given-names></name>
        <name><surname>Lipson</surname><given-names>Hod</given-names></name>
      </person-group>
      <article-title>Distilling Free-Form Natural Laws from Experimental Data</article-title>
      <source>Science</source>
      <publisher-name>American Association for the Advancement of Science</publisher-name>
      <year iso-8601-date="2009-04">2009</year><month>04</month>
      <volume>324</volume>
      <issue>5923</issue>
      <issn>0036-8075</issn>
      <uri>https://www.science.org/doi/abs/10.1126/science.1165893 https://www.science.org/doi/10.1126/science.1165893</uri>
      <pub-id pub-id-type="doi">10.1126/science.1165893</pub-id>
      <pub-id pub-id-type="pmid">19342586</pub-id>
      <fpage>81</fpage>
      <lpage>85</lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
