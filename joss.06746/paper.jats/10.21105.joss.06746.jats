<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">6746</article-id>
<article-id pub-id-type="doi">10.21105/joss.06746</article-id>
<title-group>
<article-title>DSSE: An environment for simulation of reinforcement
learning-empowered drone swarm maritime search and rescue
missions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" equal-contrib="yes" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-5943-0481</contrib-id>
<name>
<surname>Falcão</surname>
<given-names>Renato Laffranchi</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0005-1883-8703</contrib-id>
<name>
<surname>de Oliveira</surname>
<given-names>Jorás Custódio Campos</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0000-0056-4322</contrib-id>
<name>
<surname>Andrade</surname>
<given-names>Pedro Henrique Britto Aragão</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0008-1237-3353</contrib-id>
<name>
<surname>Rodrigues</surname>
<given-names>Ricardo Ribeiro</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-6263-121X</contrib-id>
<name>
<surname>Barth</surname>
<given-names>Fabrício Jailson</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-4387-0204</contrib-id>
<name>
<surname>Brancalion</surname>
<given-names>José Fernando Basso</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Insper, Brazil</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Embraer, Brazil</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2024-04-29">
<day>29</day>
<month>4</month>
<year>2024</year>
</pub-date>
<volume>9</volume>
<issue>99</issue>
<fpage>6746</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>PettingZoo</kwd>
<kwd>reinforcement learning</kwd>
<kwd>multi-agent</kwd>
<kwd>drone swarms</kwd>
<kwd>maritime search and rescue</kwd>
<kwd>shipwrecked people</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The goal of this project is to advance research in maritime search
  and rescue missions using Reinforcement Learning techniques. The
  software provides researchers with two distinct environments: one
  simulates shipwrecked people drifting with maritime currents, creating
  a stochastic setting for training and evaluating autonomous agents;
  the other features a realistic particle simulation for mapping and
  optimizing search area coverage by autonomous agents.</p>
  <p>Both environments adhere to open-source standards and offer
  extensive customization options, allowing users to tailor them to
  specific research needs. These tools enable Reinforcement Learning
  agents to learn efficient policies for locating shipwrecked
  individuals or maximizing search area coverage, thereby enhancing the
  effectiveness of maritime rescue operations.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Maritime navigation plays a crucial role across various domains,
  including leisure activities and commercial fishing. However, maritime
  transportation is particularly significant as it accounts for 80% to
  90% of global trade
  (<xref alt="Safety and Shipping Review, 2023" rid="ref-allianz" ref-type="bibr"><italic>Safety
  and Shipping Review</italic>, 2023</xref>). While maritime navigation
  is essential for global trade, it also poses significant safety risks,
  as evidenced by the World Health Organization’s report
  (<xref alt="Drowning, 2023" rid="ref-who" ref-type="bibr"><italic>Drowning</italic>,
  2023</xref>) of approximately 236,000 annual drowning deaths
  worldwide. Therefore, maritime safety is essential, demanding
  significant enhancements in search and rescue (SAR) missions. It is
  imperative that SAR missions minimize the search area and maximize the
  chances of locating the search object.</p>
  <p>To achieve this objective, traditional SAR operations have utilized
  path planning algorithms such as parallel sweep, expanding square, and
  sector searches
  (<xref alt="“Chapter 5. Search Techniques and Operations,” 2022" rid="ref-iamsar" ref-type="bibr">“Chapter
  5. Search Techniques and Operations,” 2022</xref>). However, these
  methods have not been optimal. Trummel &amp; Weisinger
  (<xref alt="Trummel &amp; Weisinger, 1986" rid="ref-trummel1986" ref-type="bibr">Trummel
  &amp; Weisinger, 1986</xref>) demonstrated that finding an optimal
  search path, where the agent must search all sub-areas using the
  shortest possible path, is NP-complete. Recent research, however,
  proposes a different approach using Reinforcement Learning (RL)
  algorithms instead of pre-determined search patterns
  (<xref alt="Ai et al., 2021" rid="ref-AI2021110098" ref-type="bibr">Ai
  et al., 2021</xref>;
  <xref alt="Wu et al., 2024" rid="ref-WU2024116403" ref-type="bibr">Wu
  et al., 2024</xref>). This is based on the belief that RL can develop
  new, more efficient search patterns tailored to specific applications.
  The hypothesis is that maximizing reward fosters generalization
  abilities, thereby creating powerful agents
  (<xref alt="Silver et al., 2021" rid="ref-SILVER2021103535" ref-type="bibr">Silver
  et al., 2021</xref>). Such advancements could potentially save more
  lives.</p>
  <p>The two primary metrics for evaluating an efficient search are
  coverage rate and time to detection. Coverage rate is the proportion
  of the search area covered by the search units over a specific period.
  Higher coverage rates typically indicate more effective search
  strategies. Time to detection is the time taken from the start of the
  search operation to the successful detection of the target. Minimizing
  this time is often a critical objective in SAR missions.</p>
  <p>Expanding on the state-of-the-art research presented by Ai et al.
  (<xref alt="2021" rid="ref-AI2021110098" ref-type="bibr">2021</xref>)
  and Wu et al.
  (<xref alt="2024" rid="ref-WU2024116403" ref-type="bibr">2024</xref>),
  this project introduces a unique simulation environment that has not
  been made available by other researchers. Additionally, this new
  environment enables experiments on search areas that are significantly
  larger than those used in existing research.</p>
</sec>
<sec id="functionality">
  <title>Functionality</title>
  <p>In order to contribute to research on the effectiveness of
  integrating RL techniques into SAR path planning, the Drone Swarm
  Search Environment (<monospace>DSSE</monospace>), distributed as a
  Python package, was designed to provide a training environment using
  the PettingZoo
  (<xref alt="J. Terry et al., 2021" rid="ref-terry2021pettingzoo" ref-type="bibr">J.
  Terry et al., 2021</xref>) interface. Its purpose is to facilitate the
  training and evaluation of single or multi-agent RL algorithms.
  Additionally, it has been included as a third-party environment in the
  official PettingZoo documentation
  (<xref alt="Jordan Terry et al., 2021" rid="ref-Terry_PettingZoo_Gym_for" ref-type="bibr">Jordan
  Terry et al., 2021</xref>).</p>
  <fig>
    <caption><p>Simulation environment showcasing the algorithm’s
    execution.<styled-content id="figU003Aexample"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="dsse-example.png" />
  </fig>
  <p>The environment depicted in
  <xref alt="[fig:example]" rid="figU003Aexample">[fig:example]</xref>
  comprises a grid, a probability matrix, drones, and an arbitrary
  number of persons-in-water (PIW). The movement of the PIW is
  influenced by, but not identical to, the dynamics of the probability
  matrix, which models the drift of sea currents impacting the PIW
  (<xref alt="Wu et al., 2023" rid="ref-WU2023113444" ref-type="bibr">Wu
  et al., 2023</xref>). The probability matrix itself is defined using a
  two-dimensional Gaussian distribution, which expands over time, thus
  broadening the potential search area. This expansion simulates the
  diffusion of the PIW, approximating the zone where drones are most
  likely to detect them. Moreover, the environment employs a reward
  function that incentivizes the speed of the search, rewarding the
  agents for shorter successful search durations.</p>
  <p>The package also includes a second environment option. Similar to
  the first, this alternative setup is designed for training agents, but
  with key differences in its objectives and mechanics. Unlike the first
  environment, which rewards agents for speed in their searches, this
  second option rewards agents that cover the largest area without
  repetition. It incorporates a trade-off by using a stationary
  probability matrix, but enhances the simulation with a more advanced
  Lagrangian particle model
  (<xref alt="Dagestad et al., 2018" rid="ref-gmd-11-1405-2018" ref-type="bibr">Dagestad
  et al., 2018</xref>) for pinpointing the PIW’s position. Moreover,
  this environment omits the inclusion of shipwrecked individuals,
  focusing instead on promoting research into how agents can learn to
  efficiently expand their search coverage over broader areas.</p>
  <p>Using this environment, any researcher or practitioner can write
  code and execute an agent’s training, such as the source code
  presented below.</p>
  <code language="python">from DSSE import DroneSwarmSearch

env = DroneSwarmSearch()

observations, info = env.reset()

rewards = 0
done = False
while not done:
    actions = policy(observations, env.get_agents())
    observations, rewards, terminations, truncations, infos = env.step(actions)
    done = any(terminations.values()) or any(truncations.values())</code>
  <p>The grid is divided into square cells, each representing a quadrant
  with sides measuring 130 meters in the real world. This correlation
  with real-world dimensions is crucial for developing agents capable of
  learning from realistic motion patterns. The drones, which are
  controlled by RL algorithms, serve as these agents. During the
  environment’s instantiation, users define the drones’ nominal speeds.
  These drones can move both orthogonally and diagonally across the
  grid, and they are equipped to search each cell for the presence of
  the PIW.</p>
  <p>Several works have been developed over the past few years to define
  better algorithms for the search and rescue of shipwrecks
  (<xref alt="Ai et al., 2021" rid="ref-AI2021110098" ref-type="bibr">Ai
  et al., 2021</xref>;
  <xref alt="Wu et al., 2024" rid="ref-WU2024116403" ref-type="bibr">Wu
  et al., 2024</xref>). However, no environment for agent training is
  made available publicly. For this reason, the development and
  provision of this environment as a Python library and open-source
  project are expected to have significant relevance to the machine
  learning community and ocean safety.</p>
  <p>This new library makes it possible to implement and evaluate new RL
  algorithms, such as Deep Q-Networks (DQN)
  (<xref alt="Mnih et al., 2015" rid="ref-dqn2015" ref-type="bibr">Mnih
  et al., 2015</xref>) and Proximal Policy Optimization (PPO)
  (<xref alt="Schulman et al., 2017" rid="ref-ppo2017" ref-type="bibr">Schulman
  et al., 2017</xref>), with little effort. Additionally, several
  state-of-the-art RL algorithms have already been implemented and are
  available
  (<xref alt="Rodrigues et al., 2024" rid="ref-algorithmsDSSE2024" ref-type="bibr">Rodrigues
  et al., 2024</xref>). An earlier iteration of this software was
  utilized in research that compared the Reinforce algorithm with the
  parallel sweep path planning algorithm
  (<xref alt="Abreu et al., 2023" rid="ref-dsse2023" ref-type="bibr">Abreu
  et al., 2023</xref>).</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-allianz">
    <element-citation>
      <article-title>Safety and shipping review</article-title>
      <publisher-name>Allianz Global Corporate &amp; Specialty</publisher-name>
      <year iso-8601-date="2023-05">2023</year><month>05</month>
      <uri>https://commercial.allianz.com/news-and-insights/reports/shipping-safety.html</uri>
      <fpage>4</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-who">
    <element-citation>
      <article-title>Drowning</article-title>
      <publisher-name>World Health Organization</publisher-name>
      <year iso-8601-date="2023">2023</year>
      <uri>https://www.who.int/news-room/fact-sheets/detail/drowning</uri>
    </element-citation>
  </ref>
  <ref id="ref-iamsar">
    <element-citation publication-type="chapter">
      <article-title>Chapter 5. Search techniques and operations</article-title>
      <source>International aeronautical and maritime search and rescue manual</source>
      <publisher-name>International Maritime Organization; International Civil Aviation Organization</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>II</volume>
      <isbn>9789280117356</isbn>
      <uri>https://store.icao.int/en/international-aeronautical-and-maritime-search-and-rescue-manual-volume-ii-mission-co-ordination-doc-9731-2</uri>
    </element-citation>
  </ref>
  <ref id="ref-trummel1986">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Trummel</surname><given-names>KE</given-names></name>
        <name><surname>Weisinger</surname><given-names>JR</given-names></name>
      </person-group>
      <article-title>The complexity of the optimal searcher path problem</article-title>
      <source>Operations Research</source>
      <publisher-name>INFORMS</publisher-name>
      <year iso-8601-date="1986">1986</year>
      <volume>34</volume>
      <issue>2</issue>
      <fpage>324</fpage>
      <lpage>327</lpage>
    </element-citation>
  </ref>
  <ref id="ref-terry2021pettingzoo">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Terry</surname><given-names>J</given-names></name>
        <name><surname>Black</surname><given-names>Benjamin</given-names></name>
        <name><surname>Grammel</surname><given-names>Nathaniel</given-names></name>
        <name><surname>Jayakumar</surname><given-names>Mario</given-names></name>
        <name><surname>Hari</surname><given-names>Ananth</given-names></name>
        <name><surname>Sullivan</surname><given-names>Ryan</given-names></name>
        <name><surname>Santos</surname><given-names>Luis S</given-names></name>
        <name><surname>Dieffendahl</surname><given-names>Clemens</given-names></name>
        <name><surname>Horsch</surname><given-names>Caroline</given-names></name>
        <name><surname>Perez-Vicente</surname><given-names>Rodrigo</given-names></name>
        <name><surname>Williams</surname><given-names>Niall</given-names></name>
        <name><surname>Lokesh</surname><given-names>Yashas</given-names></name>
        <name><surname>Ravi</surname><given-names>Praveen</given-names></name>
      </person-group>
      <article-title>PettingZoo: Gym for multi-agent reinforcement learning</article-title>
      <source>Advances in neural information processing systems</source>
      <person-group person-group-type="editor">
        <name><surname>Ranzato</surname><given-names>M.</given-names></name>
        <name><surname>Beygelzimer</surname><given-names>A.</given-names></name>
        <name><surname>Dauphin</surname><given-names>Y.</given-names></name>
        <name><surname>Liang</surname><given-names>P. S.</given-names></name>
        <name><surname>Vaughan</surname><given-names>J. Wortman</given-names></name>
      </person-group>
      <publisher-name>Curran Associates, Inc.</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>34</volume>
      <uri>https://proceedings.neurips.cc/paper_files/paper/2021/file/7ed2d3454c5eea71148b11d0c25104ff-Paper.pdf</uri>
      <fpage>15032</fpage>
      <lpage>15043</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Terry_PettingZoo_Gym_for">
    <element-citation publication-type="software">
      <person-group person-group-type="author">
        <name><surname>Terry</surname><given-names>Jordan</given-names></name>
        <name><surname>Black</surname><given-names>Benjamin</given-names></name>
        <name><surname>Grammel</surname><given-names>Nathaniel</given-names></name>
        <name><surname>Jayakumar</surname><given-names>Mario</given-names></name>
        <name><surname>Hari</surname><given-names>Ananth</given-names></name>
        <name><surname>Sullivan</surname><given-names>Ryan</given-names></name>
        <name><surname>Santos</surname><given-names>Luis</given-names></name>
        <name><surname>Perez</surname><given-names>Rodrigo</given-names></name>
        <name><surname>Horsch</surname><given-names>Caroline</given-names></name>
        <name><surname>Dieffendahl</surname><given-names>Clemens</given-names></name>
        <name><surname>Williams</surname><given-names>Niall</given-names></name>
        <name><surname>Lokesh</surname><given-names>Yashas</given-names></name>
      </person-group>
      <article-title>PettingZoo: Gym for multi-agent reinforcement learning</article-title>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/Farama-Foundation/PettingZoo</uri>
    </element-citation>
  </ref>
  <ref id="ref-AI2021110098">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Ai</surname><given-names>Bo</given-names></name>
        <name><surname>Jia</surname><given-names>Maoxin</given-names></name>
        <name><surname>Xu</surname><given-names>Hanwen</given-names></name>
        <name><surname>Xu</surname><given-names>Jiangling</given-names></name>
        <name><surname>Wen</surname><given-names>Zhen</given-names></name>
        <name><surname>Li</surname><given-names>Benshuai</given-names></name>
        <name><surname>Zhang</surname><given-names>Dan</given-names></name>
      </person-group>
      <article-title>Coverage path planning for maritime search and rescue using reinforcement learning</article-title>
      <source>Ocean Engineering</source>
      <year iso-8601-date="2021">2021</year>
      <volume>241</volume>
      <issn>0029-8018</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0029801821014220</uri>
      <pub-id pub-id-type="doi">10.1016/j.oceaneng.2021.110098</pub-id>
      <fpage>110098</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-WU2024116403">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Jie</given-names></name>
        <name><surname>Cheng</surname><given-names>Liang</given-names></name>
        <name><surname>Chu</surname><given-names>Sensen</given-names></name>
        <name><surname>Song</surname><given-names>Yanjie</given-names></name>
      </person-group>
      <article-title>An autonomous coverage path planning algorithm for maritime search and rescue of persons-in-water based on deep reinforcement learning</article-title>
      <source>Ocean Engineering</source>
      <year iso-8601-date="2024">2024</year>
      <volume>291</volume>
      <issn>0029-8018</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0029801823027877</uri>
      <pub-id pub-id-type="doi">10.1016/j.oceaneng.2023.116403</pub-id>
      <fpage>116403</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-SILVER2021103535">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Silver</surname><given-names>David</given-names></name>
        <name><surname>Singh</surname><given-names>Satinder</given-names></name>
        <name><surname>Precup</surname><given-names>Doina</given-names></name>
        <name><surname>Sutton</surname><given-names>Richard S.</given-names></name>
      </person-group>
      <article-title>Reward is enough</article-title>
      <source>Artificial Intelligence</source>
      <year iso-8601-date="2021">2021</year>
      <volume>299</volume>
      <issn>0004-3702</issn>
      <uri>https://doi.org/10.1016/j.artint.2021.103535</uri>
      <pub-id pub-id-type="doi">10.1016/j.artint.2021.103535</pub-id>
      <fpage>103535</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-gmd-11-1405-2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Dagestad</surname><given-names>K.-F.</given-names></name>
        <name><surname>Röhrs</surname><given-names>J.</given-names></name>
        <name><surname>Breivik</surname><given-names>Ø.</given-names></name>
        <name><surname>Ådlandsvik</surname><given-names>B.</given-names></name>
      </person-group>
      <article-title>OpenDrift v1.0: A generic framework for trajectory modelling</article-title>
      <source>Geoscientific Model Development</source>
      <year iso-8601-date="2018">2018</year>
      <volume>11</volume>
      <issue>4</issue>
      <uri>https://gmd.copernicus.org/articles/11/1405/2018/</uri>
      <pub-id pub-id-type="doi">10.5194/gmd-11-1405-2018</pub-id>
      <fpage>1405</fpage>
      <lpage>1420</lpage>
    </element-citation>
  </ref>
  <ref id="ref-dsse2023">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Abreu</surname><given-names>Leonardo D. M. de</given-names></name>
        <name><surname>Carrete</surname><given-names>Luis F. S.</given-names></name>
        <name><surname>Castanares</surname><given-names>Manuel</given-names></name>
        <name><surname>Damiani</surname><given-names>Enrico F.</given-names></name>
        <name><surname>Brancalion</surname><given-names>José Fernando B.</given-names></name>
        <name><surname>Barth</surname><given-names>Fabrício J.</given-names></name>
      </person-group>
      <article-title>Exploration and rescue of shipwreck survivors using reinforcement learning-empowered drone swarms</article-title>
      <publisher-name>Simpósio de Aplicações Operacionais em Áreas de Defesa (SIGE)</publisher-name>
      <publisher-loc>São José dos Campos, SP</publisher-loc>
      <year iso-8601-date="2023">2023</year>
      <issn>1983-7402</issn>
      <fpage>64</fpage>
      <lpage>69</lpage>
    </element-citation>
  </ref>
  <ref id="ref-algorithmsDSSE2024">
    <element-citation publication-type="manuscript">
      <person-group person-group-type="author">
        <name><surname>Rodrigues</surname><given-names>Ricardo Ribeiro</given-names></name>
        <name><surname>Oliveira</surname><given-names>Jorás Custódio Campos de</given-names></name>
        <name><surname>Andrade</surname><given-names>Pedro Henrique Britto Aragão</given-names></name>
        <name><surname>Falcão</surname><given-names>Renato Laffranchi</given-names></name>
      </person-group>
      <article-title>Algorithms for drone swarm search (DSSE)</article-title>
      <year iso-8601-date="2024">2024</year>
      <uri>https://github.com/pfeinsper/drone-swarm-search-algorithms</uri>
    </element-citation>
  </ref>
  <ref id="ref-dqn2015">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mnih</surname><given-names>Volodymyr</given-names></name>
        <name><surname>Kavukcuoglu</surname><given-names>Koray</given-names></name>
        <name><surname>Silver</surname><given-names>David</given-names></name>
        <name><surname>Rusu</surname><given-names>Andrei A</given-names></name>
        <name><surname>Veness</surname><given-names>Joel</given-names></name>
        <name><surname>Bellemare</surname><given-names>Marc G</given-names></name>
        <name><surname>Graves</surname><given-names>Alex</given-names></name>
        <name><surname>Riedmiller</surname><given-names>Martin</given-names></name>
        <name><surname>Fidjeland</surname><given-names>Andreas K</given-names></name>
        <name><surname>Ostrovski</surname><given-names>Georg</given-names></name>
        <name><surname>Petersen</surname><given-names>Stig</given-names></name>
        <name><surname>Beattie</surname><given-names>Charles</given-names></name>
        <name><surname>Sadik</surname><given-names>Amir</given-names></name>
        <name><surname>Antonoglou</surname><given-names>Ioannis</given-names></name>
        <name><surname>King</surname><given-names>Helen</given-names></name>
        <name><surname>Kumaran</surname><given-names>Dharshan</given-names></name>
        <name><surname>Wierstra</surname><given-names>Daan</given-names></name>
        <name><surname>Legg</surname><given-names>Shane</given-names></name>
        <name><surname>Hassabis</surname><given-names>Demis</given-names></name>
      </person-group>
      <article-title>Human-level control through deep reinforcement learning</article-title>
      <source>Nature</source>
      <year iso-8601-date="2015-02">2015</year><month>02</month>
      <volume>518</volume>
      <issue>7540</issue>
      <pub-id pub-id-type="doi">10.1038/nature14236</pub-id>
      <fpage>529</fpage>
      <lpage>533</lpage>
    </element-citation>
  </ref>
  <ref id="ref-ppo2017">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Schulman</surname><given-names>John</given-names></name>
        <name><surname>Wolski</surname><given-names>Filip</given-names></name>
        <name><surname>Dhariwal</surname><given-names>Prafulla</given-names></name>
        <name><surname>Radford</surname><given-names>Alec</given-names></name>
        <name><surname>Klimov</surname><given-names>Oleg</given-names></name>
      </person-group>
      <article-title>Proximal policy optimization algorithms</article-title>
      <year iso-8601-date="2017">2017</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1707.06347</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-WU2023113444">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wu</surname><given-names>Jie</given-names></name>
        <name><surname>Cheng</surname><given-names>Liang</given-names></name>
        <name><surname>Chu</surname><given-names>Sensen</given-names></name>
      </person-group>
      <article-title>Modeling the leeway drift characteristics of persons-in-water at a sea-area scale in the seas of china</article-title>
      <source>Ocean Engineering</source>
      <year iso-8601-date="2023">2023</year>
      <volume>270</volume>
      <issn>0029-8018</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0029801822027275</uri>
      <pub-id pub-id-type="doi">10.1016/j.oceaneng.2022.113444</pub-id>
      <fpage>113444</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
