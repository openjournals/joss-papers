<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">5819</article-id>
<article-id pub-id-type="doi">10.21105/joss.05819</article-id>
<title-group>
<article-title>SonoUno development: a User-Centered Sonification
software for data analysis</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0001-9528-5034</contrib-id>
<name>
<surname>Casado</surname>
<given-names>Johanna</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<name>
<surname>de la Vega</surname>
<given-names>Gonzalo</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author" equal-contrib="yes">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0919-2734</contrib-id>
<name>
<surname>García</surname>
<given-names>Beatriz</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-4"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Instituto en Tecnologías de Detección y Astropartículas
(CNEA, CONICET, UNSAM), Mendoza, Argentina</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Instituto de Bioingeniería, Facultad de Ingeniería,
Universidad de Mendoza, Argentina</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Independent Researcher, Argentina</institution>
</institution-wrap>
</aff>
<aff id="aff-4">
<institution-wrap>
<institution>Universidad Tecnológica Nacional - Regional Mendoza,
Argentina</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2023-06-14">
<day>14</day>
<month>6</month>
<year>2023</year>
</pub-date>
<volume>9</volume>
<issue>93</issue>
<fpage>5819</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>sonification</kwd>
<kwd>inclusion</kwd>
<kwd>astrophysics</kwd>
<kwd>open source</kwd>
<kwd>user-centered design</kwd>
<kwd>graphic user interface</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>In general, people think about information and nature based on a
  visual approach, but if we go back to our childhood and try to
  understand the world through our different senses, a new way of
  understanding the information arises. Sonification is a technique that
  uses non-speach audio to translate data into sound and it is a
  commonly-used approach to exploring astrophysical data through sensory
  inputs. This field has grown continuously over the recent years,
  especially since 1992 with the start of the International Community
  for Auditory Display (ICAD) conferences. In this contribution, a
  sonification software, sonoUno, and its application in astronomy are
  presented, without neglecting the analysis possibilities for data from
  various disciplines. The main distinguishing features of the
  development presented here are: it is user-centered from the
  beginning, with several contacts with end users along the design and a
  focus group test; it seeks a way to bring autonomy to its
  functionally-diverse users; and it presents a simple first framework,
  with the mathematical operations and settings in the menu. A
  description of the software and some cases of use are presented.</p>
</sec>
<sec id="introduction">
  <title>Introduction</title>
  <p>The use of sonification in astronomy has existed for years, some
  recent examples are a sonification of the zCOSMOS astronomical
  dataset, where the authors describe the dataset and the sonification
  strategy used
  (<xref alt="Bardelli et al., 2021" rid="ref-bardellietal2021" ref-type="bibr">Bardelli
  et al., 2021</xref>); the LightSound, an electronic device that allows
  the conversion of light into sound and it is used to observe eclipses
  (<xref alt="Bieryla et al., 2020" rid="ref-lightsound2020" ref-type="bibr">Bieryla
  et al., 2020</xref>); See-Through-Sound, a project dedicated to
  converting images into sound to allow visually impaired people to
  detect objects around them
  (<xref alt="Henriques et al., 2014" rid="ref-henriquesetal2014" ref-type="bibr">Henriques
  et al., 2014</xref>); R-Scuti, an audio-visual installation that
  proposes the conversion of astronomical data from the AAVSO (American
  Association of Variable Star Observers) database into an exhibition
  environment, using recordings of variable star observations
  (<xref alt="Laurentiz et al., 2021" rid="ref-laurentizetal2021" ref-type="bibr">Laurentiz
  et al., 2021</xref>); and an accessible science lab, research that
  highlights the need for multimodal approaches in teaching and learning
  environments, presenting principles for inclusive material design
  based on user-centered design and universal design for teaching
  environments
  (<xref alt="Reynaga-Peña &amp; Carmen López-Suero, 2020" rid="ref-reynagaetal2020" ref-type="bibr">Reynaga-Peña
  &amp; Carmen López-Suero, 2020</xref>).</p>
  <p>After an international sonification workshop held in August of
  2021, Zanella et al.
  (<xref alt="2022" rid="ref-zanella2022" ref-type="bibr">2022</xref>)
  created a repository of existing software as of December 2021 which
  contained 98 projects developed since 1962. Unfortunately many of them
  were no longer actively developed, lacked documentation and had no
  evidence of science applications. Almost 80% of the sonification
  projects have been carried out between 2011 and 2021. As of 2017, the
  date on which this sonification software development began, only 50%
  of the software included in the mentioned repository had started its
  development.</p>
  <p>In most cases, the sonification mapping of the dataset was defined
  by its creator and shared as a final product or even with some
  musicalization, not clearly devoted to research. The ICAD conferences
  intend to bring together multidisciplinary experts working in the
  field of sonification, in its repository there are a lot of works that
  Andreopoulou &amp; Goudarzi
  (<xref alt="2021" rid="ref-andreopoulouYgoudarzi2021" ref-type="bibr">2021</xref>)
  grouped into six categories: sonification methods, sonification
  tools/system, review/opinion, exploratory, perception/evaluation
  studies and other. This systematic review of the ICAD repository
  highlights a high percentage of papers devoted to sonification methods
  and tools, in contrast to a low percentage of papers devoted to design
  methodologies, perception studies, and evaluation methods.
  Fortunately, during The Audible Universe 2, a workshop organized by
  Lorentz Center<xref ref-type="fn" rid="fn1">1</xref>, the usefulness
  of evaluation methods and perception studies were pointed out
  (<xref alt="Misdariis et al., 2023" rid="ref-AU2023" ref-type="bibr">Misdariis
  et al., 2023</xref>).</p>
  <p>In this sense and taking into account the increasing examples of
  sonification, only some actual developments related to astronomy,
  astrophysics, and accessibility will be mentioned here. StarSound
  started its development at the same time as SonoUno; it offers a
  complete sound synthesizer customizable by the user. By nature,
  synthesizers in general present a lot of buttons and require setting a
  lot of variables, but paying attention to the accessibility of
  visually impaired people, StarSound allows setting them in a text
  field too. The IDATA project, beyond their numerous activities
  centered on accessible astronomical software and materials, launched
  the Afterglow Access Software, which allows users to open and produce
  sound from image files<xref ref-type="fn" rid="fn2">2</xref>.</p>
  <p>Some other developments center their efforts on sonification and
  don’t present a graphic user interface; this is the case of
  Soni-py<xref ref-type="fn" rid="fn3">3</xref>,
  Astronify<xref ref-type="fn" rid="fn4">4</xref> and
  STRAUSS<xref ref-type="fn" rid="fn5">5</xref>. Soni-py is a Python
  package devoted to the sonification of scatter plots; it presents
  examples and demos in its documentation. Astronify allows the
  sonification of any time series data, which means any data set where
  the first column was set as time and the second column as pitch; it is
  an active open-access development with proper documentation and
  examples with Light Curve.</p>
  <p>On the other hand, STRAUSS is a Python toolkit developed with the
  aim of improving the current visual display of data and accessibility
  using sonification. It offers the possibility of being used by people
  without knowledge of sound design or programming through predefined
  examples and default settings. However, it provides documentation that
  allows for more advanced usage, enabling the modification of sound
  parameters. One direct application of this development is in the
  “Audio Universe” project, in which they have recently published a tour
  of the solar system, for which they have used the STRAUSS
  tool<xref ref-type="fn" rid="fn6">6</xref>. Harrison et al.
  (<xref alt="2021" rid="ref-strauss2021" ref-type="bibr">2021</xref>)
  explains how this tour was developed using different instruments for
  each planet, along with sound configuration to represent the distance
  and position of each object.</p>
  <p>sonoUno is a sonification software to translate time series data
  into sound. It is an open-source program with a modular design that
  allows users to open different datasets and explore them through
  visual and auditory display, the last permitting them to adjust visual
  and sound settings to enhance their perception (a full view of the
  Graphic User Interface (GUI) is shown in
  <xref alt="[fig:fig1]" rid="figU003Afig1">[fig:fig1]</xref>). This
  project is user-centered from the beginning. To reach that goal we
  follow three main actions. First, the ISO 9241-171:2008 standard was
  used to analyze the accessibility of three previous software
  (<xref alt="Casado, Díaz-Merced, et al., 2022" rid="ref-ijskd2022" ref-type="bibr">Casado,
  Díaz-Merced, et al., 2022</xref>;
  <xref alt="García et al., 2019" rid="ref-ise2a2017" ref-type="bibr">García
  et al., 2019</xref>) and construct the first GUI mock-up. Second, a
  theoretical framework centered on visual disability was designed and
  applied to the software in development
  (<xref alt="Casado et al., 2019" rid="ref-casadoetal2019" ref-type="bibr">Casado
  et al., 2019</xref>); the consequent ISO analysis of SonoUno shows
  very good results. And finally, a focus group session was conducted
  with people with and without visual disabilities to test the first
  version of SonoUno. Some recommendations and updates arise from that
  study
  (<xref alt="Casado, García, et al., 2022" rid="ref-casadoFG2022" ref-type="bibr">Casado,
  García, et al., 2022</xref>). The first view of the GUI after all the
  updates is shown in
  <xref alt="[fig:fig2]" rid="figU003Afig2">[fig:fig2]</xref>.</p>
  <p>SonoUno is programmed purely in Python. Its modular design, with
  the use of agile methodology, allows programmers to divide the tasks,
  organize the job, and make the cooperative work between the team
  easier. In this case, the modules are Data Input, Data Output, Data
  Transformation, Sonification, Graphic User Interface Design, and Core.
  One important feature is the possibility of using each module alone
  without the need for a graphic user interface (GUI), so some users
  could import the sound module from bash to produce sonification
  without installing the graphic user interface library wxPython.</p>
  <p>Another important feature is the actual sound library and
  implementation on SonoUno. It produces sound from its sinusoidal
  parameters without any additional features, and each data point on the
  dataset produces a differentiated tone that faithfully represents the
  numerical data. In addition, the sonification of each point ensures a
  good correlation between the visual and sound representations and,
  consequently, a good match between the two human sensory styles. The
  principal aim os SonoUno is research through sonification; it is very
  important to understand and correlate sonification with visualization
  (the actual practice of research in astronomy) in conjunction with a
  deeper analysis of human sound perception.</p>
  <fig>
    <caption><p>Graphic User Interface with all the panels
    visible.<styled-content id="figU003Afig1"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/fig7.png" />
  </fig>
  <fig>
    <caption><p>The first framework of the GUI, the data deployed is a
    galaxy spectrum downloaded from the
    <ext-link ext-link-type="uri" xlink:href="https://skyserver.sdss.org/dr12/en/tools/explore/Summary.aspx?ra=179.689293428354&amp;dec=-0.454379056007667">SDSS
    database</ext-link>.<styled-content id="figU003Afig2"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/figures/fig6.png" />
  </fig>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Most tools that produce data sonification are centered on specific
  data and devoted to outreach. But sonoUno principal aim’s is research,
  and moreover, a multimodal approach to knowledge, allowing people with
  disabilities to explore scientific data and carry out reasearch.
  sonoUno has three main functionalities: graphic deployment,
  sonorization of the data, and use of mathematical functions. During
  our user tests it was observed that the tool allows for (a) deployment
  of data sets, (b) precise sonification to detects the data features,
  permitting: marking over the plot, changing intervals, volume, pitch,
  frequency and adapting the output to the perception of each user and
  (c) application of math functions. The output of the software (graph,
  sound, marks, and data plotted) can be saved and reopened for future
  or more detailed analysis, even using another tool. Finally, the
  graphic user interface was modified according to the user testing
  activities following a user-centered design framework from the
  beginning
  (<xref alt="Casado, García, et al., 2022" rid="ref-casadoFG2022" ref-type="bibr">Casado,
  García, et al., 2022</xref>).</p>
  <p>According to W. L. Díaz-Merced
  (<xref alt="2013" rid="ref-wandatesis2013" ref-type="bibr">2013</xref>),
  sonification could enhance the actual data display and the works of
  Casado et al. reinforce this claim
  (<xref alt="Casado et al., 2019" rid="ref-casadoetal2019" ref-type="bibr">Casado
  et al., 2019</xref>;
  <xref alt="Casado, García, et al., 2022" rid="ref-casadoFG2022" ref-type="bibr">Casado,
  García, et al., 2022</xref>). In addition, it was tested that
  astronomical datasets could be displayed in sonoUno in the same form
  as other visualization tools, with the benefit of sonification and
  allowing it to be used by people with different learning styles.</p>
</sec>
<sec id="cases-of-use">
  <title>Cases of use</title>
  <p>In addition to tests inside the research team, sonoUno has been
  used by other research groups with their own datasets. Some examples
  are detailed below.</p>
  <list list-type="bullet">
    <list-item>
      <p>The work of Carlos Morales Socorro with variables star, in CEP
      Las Palmas, Spain, where a training program using sonification was
      conducted, and a blind student could discover a variable star for
      the first time in
      history<xref ref-type="fn" rid="fn7">7</xref>.</p>
    </list-item>
    <list-item>
      <p>The ``Sensing the Dynamic Universe’’ project, led by a research
      group at Harvard
      (<xref alt="W. Díaz-Merced et al., 2019" rid="ref-wanda2019" ref-type="bibr">W.
      Díaz-Merced et al., 2019</xref>), displays a web page with
      information about variable star and uses sonoUno to generate the
      videos of data visualizations and
      sonification<xref ref-type="fn" rid="fn8">8</xref>. This project
      created a fork of the sonoUno code to make some
      updates<xref ref-type="fn" rid="fn9">9</xref>.</p>
    </list-item>
    <list-item>
      <p>The GitHub repository
      “SonoUno-Raspberry-Pi”<xref ref-type="fn" rid="fn10">10</xref> by
      Hartwell Fong shows the possibility of using sonoUno in a
      Raspberry Pi. This user also contacted us asking for the
      possibility to include some tactile technology in the software,
      that could be implemented in the near future.</p>
    </list-item>
    <list-item>
      <p>The Sound of BEARS<xref ref-type="fn" rid="fn11">11</xref> is a
      web page that uses sonoUno web version to display a video with the
      sonification of ALMA data, reinforcing the need to make astronomy
      more accessible.</p>
    </list-item>
  </list>
</sec>
<sec id="conclusions">
  <title>Conclusions</title>
  <p>The use of sound to communicate, detect or analyze data features
  has been used for many years, however, the use of sonification (the
  use of sound to represent data) formally began in the 90s. Maybe, it
  is too early to compare it with the visual display in the sense of the
  possibility to study data only through sonification, especially since
  we have used visual tools for centuries and learned about visual
  displays since elementary school. In contrast, the first approach to
  sound display usually occurs when people already have a degree or as a
  consequence of a disability. Perception studies and an earlier
  approximation to this technique are needed.</p>
  <p>It is important to mention that our experience shows that the use
  of more than one sense improves the detection of specific features in
  the data. In any case and in order to acquire a deep comprehension of
  multimodal analysis, training courses are needed. Besides, in order to
  know the sonification process better and investigate how it can be
  used in the research field, new sound perception experiments need to
  be done, with available tools like sonoUno and with different
  datasets.</p>
  <p>The authors hope to encourage more people to help with future steps
  and software maintenance, taking into account that this development is
  completely open source and the sonification technique is part of the
  new approach to the study of nature.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>The authors want to give their thanks to the people who tested the
  tool, participated in the training activities, and shared their
  experiences, ideas, suggestions, and comments. The contributions and
  feedback of Julieta Carricondo Robino, Aldana Palma, Carlos Morales
  Socorro, Richard Green, Poshak Gandhi, Gaston Jaren, Southampton
  Sight, students at the University of Mendoza and ITeDA, and many
  volunteers at the Focus Groups and beyond testing the software are
  deeply appreciated. In addition, thanks to IAU Office of Astronomy for
  Development-SAAO-South Africa and the University of Southampton for
  all their support to this development.</p>
  <p>This work was funded by the National Council of Scientific Research
  of Argentina (CONICET) and has been performed partially under the
  Project REINFORCE (GA 872859) with the support of the EC Research
  Innovation Action under the H2020 Programme SwafS-2019-1.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-wandatesis2013">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Díaz-Merced</surname><given-names>W. L.</given-names></name>
      </person-group>
      <article-title>Sound for the exploration of space physics data</article-title>
      <source>University of glasgow</source>
      <year iso-8601-date="2013">2013</year>
    </element-citation>
  </ref>
  <ref id="ref-wanda2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Díaz-Merced</surname><given-names>W.</given-names></name>
        <name><surname>Walker-Holmes</surname><given-names>M.</given-names></name>
        <name><surname>Hettlage</surname><given-names>C.</given-names></name>
        <name><surname>Green</surname><given-names>P.</given-names></name>
        <name><surname>Casado</surname><given-names>J.</given-names></name>
        <name><surname>García</surname><given-names>B.</given-names></name>
      </person-group>
      <article-title>A user centred inclusive web framework for astronomy</article-title>
      <source>Proceedings of the international astronomical union</source>
      <publisher-name>Cambridge University Press</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <pub-id pub-id-type="doi">10.1017/S1743921321000272</pub-id>
      <fpage>421</fpage>
      <lpage>422</lpage>
    </element-citation>
  </ref>
  <ref id="ref-bardellietal2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bardelli</surname><given-names>S.</given-names></name>
        <name><surname>Ferretti</surname><given-names>C.</given-names></name>
        <name><surname>Ludovico</surname><given-names>L. A.</given-names></name>
        <name><surname>Presti</surname><given-names>G.</given-names></name>
        <name><surname>Rinaldi</surname><given-names>M.</given-names></name>
      </person-group>
      <article-title>A sonification of the zCOSMOS galaxy dataset</article-title>
      <source>In international conference on human-computer interaction</source>
      <publisher-name>Springer, Cham</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.1007/978-3-030-77411-0_12</pub-id>
      <fpage>171</fpage>
      <lpage>188</lpage>
    </element-citation>
  </ref>
  <ref id="ref-lightsound2020">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Bieryla</surname><given-names>A.</given-names></name>
        <name><surname>Hyman</surname><given-names>S.</given-names></name>
        <name><surname>Davis</surname><given-names>D.</given-names></name>
      </person-group>
      <article-title>LightSound: A sonification tool for observing solar eclipses</article-title>
      <source>In american astronomical society meeting abstracts# 235</source>
      <year iso-8601-date="2020">2020</year>
      <fpage>203</fpage>
      <lpage>204</lpage>
    </element-citation>
  </ref>
  <ref id="ref-henriquesetal2014">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Henriques</surname><given-names>J. T.</given-names></name>
        <name><surname>Cavaco</surname><given-names>S.</given-names></name>
        <name><surname>Correia</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>See-through-sound: Transforming images into sonic representations to help the blind</article-title>
      <source>Journal of Information Technology Research (JITR)</source>
      <year iso-8601-date="2014">2014</year>
      <volume>7</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.4018/jitr.2014010105</pub-id>
      <fpage>59</fpage>
      <lpage>77</lpage>
    </element-citation>
  </ref>
  <ref id="ref-AU2023">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Misdariis</surname><given-names>N.</given-names></name>
        <name><surname>Pauletto</surname><given-names>S.</given-names></name>
        <name><surname>Bonne</surname><given-names>N.</given-names></name>
        <name><surname>Harrison</surname><given-names>C.</given-names></name>
        <name><surname>Meredith</surname><given-names>K.</given-names></name>
        <name><surname>Zanella</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>The audible universe workshop: An interdisciplinary approach to the design and evaluation of tools for astronomical data sonification</article-title>
      <source>Presented at the 28th international conference on auditory displays (ICAD 2023) - sonification for the masses, norrköping, sweden, 26–30 june 2023</source>
      <year iso-8601-date="2023">2023</year>
      <pub-id pub-id-type="doi">10.21785/icad2023.2124</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-laurentizetal2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Laurentiz</surname><given-names>S.</given-names></name>
        <name><surname>Bergantini</surname><given-names>L. P.</given-names></name>
        <name><surname>Venancio</surname><given-names>S.</given-names></name>
        <name><surname>Mayer</surname><given-names>B.</given-names></name>
        <name><surname>Carvalho</surname><given-names>M. A.</given-names></name>
        <name><surname>Vargas</surname><given-names>D.</given-names></name>
        <name><surname>Perissinotto</surname><given-names>P. M.</given-names></name>
        <name><surname>Policarpo</surname><given-names>C.</given-names></name>
      </person-group>
      <article-title>R scuti: Creative star data visualization</article-title>
      <source>International Journal of Creative Interfaces and Computer Graphics (IJCICG)</source>
      <year iso-8601-date="2021">2021</year>
      <volume>12</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.4018/IJCICG.2021010102</pub-id>
      <fpage>13</fpage>
      <lpage>26</lpage>
    </element-citation>
  </ref>
  <ref id="ref-reynagaetal2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Reynaga-Peña</surname><given-names>C. G.</given-names></name>
        <name><surname>Carmen López-Suero</surname><given-names>C. del</given-names></name>
      </person-group>
      <article-title>Strategies and technology aids for teaching science to blind and visually impaired students</article-title>
      <source>In User-Centered Software Development for the Blind and Visually Impaired: Emerging Research and Opportunities</source>
      <year iso-8601-date="2020">2020</year>
      <volume></volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.4018/978-1-5225-8539-8.ch002</pub-id>
      <fpage>26</fpage>
      <lpage>37</lpage>
    </element-citation>
  </ref>
  <ref id="ref-andreopoulouYgoudarzi2021">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Andreopoulou</surname><given-names>A.</given-names></name>
        <name><surname>Goudarzi</surname><given-names>V.</given-names></name>
      </person-group>
      <article-title>SONIFICATION FIRST: THE ROLE OF ICAD IN THE ADVANCEMENT OF SONIFICATION-RELATED RESEARCH</article-title>
      <source>In the 26th international conference on auditory display (ICAD), virtual conference</source>
      <year iso-8601-date="2021">2021</year>
      <pub-id pub-id-type="doi">10.21785/icad2021.031</pub-id>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ise2a2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>García</surname><given-names>B.</given-names></name>
        <name><surname>Díaz-Merced</surname><given-names>W.</given-names></name>
        <name><surname>Casado</surname><given-names>J.</given-names></name>
        <name><surname>Cancio</surname><given-names>A.</given-names></name>
      </person-group>
      <article-title>Evolving from xSonify: A new digital platform for sonorization</article-title>
      <source>EPJ web of conferences</source>
      <publisher-name>EDP Sciences</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>200</volume>
      <pub-id pub-id-type="doi">10.1051/epjconf/201920001013</pub-id>
      <fpage>01013</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-ijskd2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Casado</surname><given-names>J.</given-names></name>
        <name><surname>Díaz-Merced</surname><given-names>W.</given-names></name>
        <name><surname>García</surname><given-names>B.</given-names></name>
        <name><surname>Schneps</surname><given-names>M.</given-names></name>
        <name><surname>Kolemberg</surname><given-names>K.</given-names></name>
        <name><surname>Rychtarikova</surname><given-names>M.</given-names></name>
        <name><surname>Roozen</surname><given-names>N. B.</given-names></name>
      </person-group>
      <article-title>Sonification as a tool for data analysis: Usability and compliance evaluation study</article-title>
      <source>International Journal of Sociotechnology and Knowledge Development IJSKD</source>
      <year iso-8601-date="2022">2022</year>
      <volume>14</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.4018/IJSKD.299048</pub-id>
      <fpage>1</fpage>
      <lpage>27</lpage>
    </element-citation>
  </ref>
  <ref id="ref-casadoetal2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Casado</surname><given-names>J.</given-names></name>
        <name><surname>García</surname><given-names>B.</given-names></name>
        <name><surname>Díaz-Merced</surname><given-names>W. L.</given-names></name>
      </person-group>
      <article-title>Analysis of astronomical data through sonification: Reaching more inclusion for visually disable scientists</article-title>
      <source>Proceedings of the international astronomical union, (S358)</source>
      <publisher-name>Cambridge</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <fpage></fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-casadoFG2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Casado</surname><given-names>J.</given-names></name>
        <name><surname>García</surname><given-names>B.</given-names></name>
        <name><surname>Gandhi</surname><given-names>P.</given-names></name>
        <name><surname>Díaz-Merced</surname><given-names>W.</given-names></name>
      </person-group>
      <article-title>A new approach to sonification of astrophysical data: The user centred design of SonoUno</article-title>
      <source>American Journal of Astronomy and Astrophysics</source>
      <year iso-8601-date="2022">2022</year>
      <volume>9</volume>
      <issue>4</issue>
      <pub-id pub-id-type="doi">10.11648/j.ajaa.20210904.11</pub-id>
      <fpage>42</fpage>
      <lpage>51</lpage>
    </element-citation>
  </ref>
  <ref id="ref-zanella2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zanella</surname><given-names>A.</given-names></name>
        <name><surname>Harrison</surname><given-names>C. M.</given-names></name>
        <name><surname>Lenzi</surname><given-names>S.</given-names></name>
        <name><surname>Cooke</surname><given-names>J.</given-names></name>
        <name><surname>Damsma</surname><given-names>P.</given-names></name>
        <name><surname>Fleming</surname><given-names>S. W.</given-names></name>
      </person-group>
      <article-title>Sonification and sound design for astronomy research, education and public engagement</article-title>
      <source>Nature Astronomy</source>
      <year iso-8601-date="2022">2022</year>
      <volume></volume>
      <issue></issue>
      <pub-id pub-id-type="doi">10.1038/s41550-022-01721-z</pub-id>
      <fpage>1</fpage>
      <lpage>8</lpage>
    </element-citation>
  </ref>
  <ref id="ref-strauss2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Harrison</surname><given-names>C.</given-names></name>
        <name><surname>Trayford</surname><given-names>J.</given-names></name>
        <name><surname>Harrison</surname><given-names>L.</given-names></name>
        <name><surname>Bonne</surname><given-names>N.</given-names></name>
      </person-group>
      <article-title>Audio universe tour of the solar system: Using sound to make the universe more accessible</article-title>
      <source>Astronomy and Geophysics</source>
      <year iso-8601-date="2021">2021</year>
      <volume>63</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1093/astrogeo/atac027</pub-id>
      <fpage>2.38</fpage>
      <lpage>2.40</lpage>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p><ext-link ext-link-type="uri" xlink:href="https://www.lorentzcenter.nl/the-audible-universe-2.html">https://www.lorentzcenter.nl/the-audible-universe-2.html</ext-link></p>
  </fn>
  <fn id="fn2">
    <label>2</label><p><ext-link ext-link-type="uri" xlink:href="https://idataproject.org/resources/">https://idataproject.org/resources/</ext-link></p>
  </fn>
  <fn id="fn3">
    <label>3</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/lockepatton/sonipy">https://github.com/lockepatton/sonipy</ext-link></p>
  </fn>
  <fn id="fn4">
    <label>4</label><p><ext-link ext-link-type="uri" xlink:href="https://astronify.readthedocs.io/en/latest/">https://astronify.readthedocs.io/en/latest/</ext-link></p>
  </fn>
  <fn id="fn5">
    <label>5</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/james-trayford/strauss">https://github.com/james-trayford/strauss</ext-link></p>
  </fn>
  <fn id="fn6">
    <label>6</label><p>For more information, you can access the
    following link:
    <ext-link ext-link-type="uri" xlink:href="https://www.audiouniverse.org/">https://www.audiouniverse.org/</ext-link></p>
  </fn>
  <fn id="fn7">
    <label>7</label><p><ext-link ext-link-type="uri" xlink:href="https://astronomiayeducacion.org/taller-2-de-sonificacion-descubriendo-el-universo/">https://astronomiayeducacion.org/taller-2-de-sonificacion-descubriendo-el-universo/</ext-link></p>
  </fn>
  <fn id="fn8">
    <label>8</label><p><ext-link ext-link-type="uri" xlink:href="https://lweb.cfa.harvard.edu/sdu/rrlyrae.html">https://lweb.cfa.harvard.edu/sdu/rrlyrae.html</ext-link></p>
  </fn>
  <fn id="fn9">
    <label>9</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/joepalmo/sonoUno">https://github.com/joepalmo/sonoUno</ext-link></p>
  </fn>
  <fn id="fn10">
    <label>10</label><p><ext-link ext-link-type="uri" xlink:href="https://github.com/Physicslibrary/SonoUno-Raspberry-Pi">https://github.com/Physicslibrary/SonoUno-Raspberry-Pi</ext-link></p>
  </fn>
  <fn id="fn11">
    <label>11</label><p><ext-link ext-link-type="uri" xlink:href="https://stephenserjeant.github.io/sounds-of-bears/">https://stephenserjeant.github.io/sounds-of-bears/</ext-link></p>
  </fn>
</fn-group>
</back>
</article>
