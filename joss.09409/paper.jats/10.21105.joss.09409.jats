<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">9409</article-id>
<article-id pub-id-type="doi">10.21105/joss.09409</article-id>
<title-group>
<article-title>itwinai: A Python Toolkit for Scalable Scientific Machine
Learning on HPC Systems</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0008-5100-9300</contrib-id>
<name>
<surname>Bunino</surname>
<given-names>Matteo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0002-7971-2213</contrib-id>
<name>
<surname>Sæther</surname>
<given-names>Jarl Sondre</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-6691-2821</contrib-id>
<name>
<surname>Eickhoff</surname>
<given-names>Linus Maximilian</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0009-4804-4188</contrib-id>
<name>
<surname>Lappe</surname>
<given-names>Anna Elisa</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-3192-4260</contrib-id>
<name>
<surname>Tsolaki</surname>
<given-names>Kalliopi</given-names>
</name>
<email>kalliopi.tsolaki@cern.ch</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0006-4819-3229</contrib-id>
<name>
<surname>Verder</surname>
<given-names>Killian</given-names>
</name>
<email>killian.verder@cern.ch</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0001-9940-1167</contrib-id>
<name>
<surname>Mutegeki</surname>
<given-names>Henry</given-names>
</name>
<email>henry.mutegeki@cern.ch</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-9976-4420</contrib-id>
<name>
<surname>Machacek</surname>
<given-names>Roman</given-names>
</name>
<email>roman.machacek@cern.ch</email>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-0261-8392</contrib-id>
<name>
<surname>Girone</surname>
<given-names>Maria</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0009-0007-2245-9452</contrib-id>
<name>
<surname>Krochak</surname>
<given-names>Oleksandr</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3917-8407</contrib-id>
<name>
<surname>Rüttgers</surname>
<given-names>Mario</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0002-7069-4082</contrib-id>
<name>
<surname>Sarma</surname>
<given-names>Rakesh</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-3321-6599</contrib-id>
<name>
<surname>Lintermann</surname>
<given-names>Andreas</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>European Organization for Nuclear Research (CERN), Espl.
des Particules 1, 1217 Genève, Switzerland</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Forschungszentrum Jülich, Jülich Supercomputing Center,
Germany</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Data-Driven Fluid Engineering (DDFE) Laboratory, Inha
University, Incheon, Republic of Korea</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2025-08-16">
<day>16</day>
<month>8</month>
<year>2025</year>
</pub-date>
<volume>11</volume>
<issue>117</issue>
<fpage>9409</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2026</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Digital Twins</kwd>
<kwd>Distributed Training</kwd>
<kwd>Hyperparameter Optimization</kwd>
<kwd>High Performance Computing</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>The integration of Artificial Intelligence (AI) into scientific
  research has expanded significantly over the past decade, driven by
  the availability of large-scale datasets and Graphics Processing Units
  (GPUs), in particular at High Performance Computing (HPC) sites.</p>
  <p>However, many researchers face significant barriers when deploying
  AI workflows on HPC systems, as their heterogeneous nature forces
  scientists to focus on low-level implementation details rather than on
  their core research. At the same time, the researchers often lack
  specialized HPC/AI knowledge to implement their workflows
  efficiently.</p>
  <p>To address this, we present <monospace>itwinai</monospace>, a
  Python library that simplifies scalable AI on HPC. Its modular
  architecture and standard interface allow users to scale workloads
  efficiently from laptops to supercomputers, reducing implementation
  overhead and improving resource usage.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Integrating machine learning into scientific workflows on HPC
  systems remains complex. Researchers must often invest substantial
  effort to configure distributed training, manage hyperparameter
  optimization, and analyze performance, while adapting to varied system
  architectures.</p>
  <p><monospace>itwinai</monospace> is a Python library that streamlines
  this process by providing a unified framework for scalable AI
  workflows. It offers consistent interfaces for distributed training,
  supports large-scale hyperparameter optimization, and includes tools
  for profiling and code scalability analysis.</p>
  <p>Developed within the
  <ext-link ext-link-type="uri" xlink:href="https://www.intertwin.eu/">interTwin</ext-link>
  project
  (<xref alt="Manzi et al., 2025" rid="ref-manzi_intertwin_2025" ref-type="bibr">Manzi
  et al., 2025</xref>) to support Digital Twin applications in physics
  and environmental sciences, <monospace>itwinai</monospace> is designed
  to be extensible and reusable. By consolidating core functionality
  into a single framework, it lowers the technical barrier to HPC
  adoption and enables researchers to focus on scientific
  objectives.</p>
</sec>
<sec id="state-of-the-field">
  <title>State of the field</title>
  <p>Scalable AI workflows on HPC systems are typically assembled from
  multiple specialized tools. PyTorch-DDP
  (<xref alt="Li et al., 2020" rid="ref-torch_ddp" ref-type="bibr">Li et
  al., 2020</xref>), DeepSpeed
  (<xref alt="Rasley et al., 2020" rid="ref-deepspeed" ref-type="bibr">Rasley
  et al., 2020</xref>), Horovod
  (<xref alt="Sergeev &amp; Del Balso, 2018" rid="ref-horovod" ref-type="bibr">Sergeev
  &amp; Del Balso, 2018</xref>), and Ray
  (<xref alt="Moritz et al., 2018" rid="ref-rayU003A2018" ref-type="bibr">Moritz
  et al., 2018</xref>) provide distributed training backends; Ray Tune
  (<xref alt="Liaw et al., 2018" rid="ref-ray_tune" ref-type="bibr">Liaw
  et al., 2018</xref>) and Optuna
  (<xref alt="Akiba et al., 2019" rid="ref-optuna_2019" ref-type="bibr">Akiba
  et al., 2019</xref>) offer HPO frameworks; and TensorBoard
  (<xref alt="TensorBoard Contributors, 2025" rid="ref-tensorboard" ref-type="bibr">TensorBoard
  Contributors, 2025</xref>), MLflow
  (<xref alt="Zaharia et al., 2018" rid="ref-mlflow" ref-type="bibr">Zaharia
  et al., 2018</xref>), and Weights &amp; Biases
  (<xref alt="wandb Contributors, 2025" rid="ref-wandb" ref-type="bibr">wandb
  Contributors, 2025</xref>) support experiment tracking and
  visualization. HeAT
  (<xref alt="H. Developers, 2025" rid="ref-heat" ref-type="bibr">H.
  Developers, 2025</xref>), AI4HPC
  (<xref alt="A. Developers, 2025" rid="ref-ai4hpc" ref-type="bibr">A.
  Developers, 2025</xref>), and perun
  (<xref alt="Team, 2025" rid="ref-perun" ref-type="bibr">Team,
  2025</xref>) address, respectively, distributed tensor operations on
  HPC systems, Computational Fluid Dynamics (CFD)-oriented AI and HPO
  workflows on HPC systems, and performance or energy profiling, while
  interLink
  (<xref alt="Ciangottini et al., 2025" rid="ref-interlink" ref-type="bibr">Ciangottini
  et al., 2025</xref>) enables cloud to HPC offloading of containerized
  workloads.</p>
  <p><monospace>itwinai</monospace> combines these capabilities into a
  single, configurable library: it offers a plugin system for domain use
  cases, uniform configuration and logging interfaces, support for
  multiple distributed training and HPO backends, built-in profiling and
  scalability reporting, and portable deployment across laptops, cloud
  clusters, and SLURM-based HPC systems via cloud to HPC offloading.
  This makes it particularly suitable when users need to prototype and
  deploy AI workflows that should run with minimal changes on
  heterogeneous HPC infrastructures.</p>
</sec>
<sec id="package-features">
  <title>Package features</title>
  <p>The main features offered by the <monospace>itwinai</monospace>
  library are:</p>
  <p><bold>Configuration for reproducible AI workloads</bold>: a
  declarative, hierarchical, composable, and CLI-overrideable YAML-based
  configuration system that separates experimental parameters from
  implementation code.</p>
  <p><bold>Distributed training and inference</bold>: PyTorch-DDP
  (<xref alt="Li et al., 2020" rid="ref-torch_ddp" ref-type="bibr">Li et
  al., 2020</xref>), DeepSpeed
  (<xref alt="Rasley et al., 2020" rid="ref-deepspeed" ref-type="bibr">Rasley
  et al., 2020</xref>), Horovod
  (<xref alt="Sergeev &amp; Del Balso, 2018" rid="ref-horovod" ref-type="bibr">Sergeev
  &amp; Del Balso, 2018</xref>), and Ray
  (<xref alt="Moritz et al., 2018" rid="ref-rayU003A2018" ref-type="bibr">Moritz
  et al., 2018</xref>) distributed ML training frameworks are
  supported.</p>
  <p><bold>Hyperparameter optimization (HPO)</bold>: model performance
  can be improved by automatically traversing the hyperparameter
  space.</p>
  <p>Ray integration provides two HPO strategies: (i) assigning multiple
  workers to a single trial or (ii) running many trials concurrently
  (<xref alt="[fig:hpo]" rid="figU003Ahpo">[fig:hpo]</xref>).</p>
  <fig>
    <caption><p>Conceptual representation of an HPO workflow in
    itwinai.<styled-content id="figU003Ahpo"></styled-content></p></caption>
    <graphic mimetype="application" mime-subtype="pdf" xlink:href="hpo_itwinai_figure.pdf" />
  </fig>
  <p><bold>Profilers</bold>: <monospace>itwinai</monospace> integrates
  with multiple profilers, such as the <monospace>py-spy</monospace>
  profiler
  (<xref alt="Frederickson, 2018" rid="ref-py_spy" ref-type="bibr">Frederickson,
  2018</xref>) and the PyTorch Profiler, and also logs metrics about
  training time, GPU utilization, and GPU power consumption.</p>
  <p><bold>ML logs tracking</bold>: <monospace>itwinai</monospace>
  integrates with existing ML logging frameworks, such as TensorBoard
  (<xref alt="TensorBoard Contributors, 2025" rid="ref-tensorboard" ref-type="bibr">TensorBoard
  Contributors, 2025</xref>), MLflow
  (<xref alt="Zaharia et al., 2018" rid="ref-mlflow" ref-type="bibr">Zaharia
  et al., 2018</xref>), Weights &amp; Biases
  (<xref alt="wandb Contributors, 2025" rid="ref-wandb" ref-type="bibr">wandb
  Contributors, 2025</xref>), and yProvML
  (<xref alt="Padovani &amp; Fiore, 2025" rid="ref-yprovml" ref-type="bibr">Padovani
  &amp; Fiore, 2025</xref>) logger, and provides a unified interface
  across all of them through a thin abstraction layer.</p>
  <p><bold>Offloading to HPC systems and cloud</bold>: to benefit from
  both cloud and HPC, interLink
  (<xref alt="Ciangottini et al., 2025" rid="ref-interlink" ref-type="bibr">Ciangottini
  et al., 2025</xref>) is used, which is a lightweight component to
  enable seamless offloading of compute-intensive jobs from cloud to
  HPC, performing an automatic translation from Kubernetes pods to SLURM
  jobs.</p>
  <p><bold>Continuous integration and deployment</bold>:
  <monospace>itwinai</monospace> includes extensive tests (library and
  use cases). A
  <ext-link ext-link-type="uri" xlink:href="https://dagger.io/">Dagger</ext-link>
  pipeline builds containers on release, runs smoke tests on GitHub
  Actions (Azure runners: 4 CPUs, 16
  GB)<xref ref-type="fn" rid="fn1">1</xref>, offloads distributed tests
  to HPC systems via interLink, and publishes on success.</p>
</sec>
<sec id="use-case-integrations">
  <title>Use-case integrations</title>
  <p>There is a wide range of scientific use cases currently integrated
  with <monospace>itwinai</monospace> via its plug-in architecture.
  Earth-observation plugins cover hydrological forecasting, drought
  prediction, and climate/remote-sensing pipelines; physics plugins
  include high-energy physics, radio astronomy, lattice quantum
  chromodynamics (QCD), and gravitational-wave/glitch analysis.
  Packaging these as <monospace>itwinai</monospace> plugins enables
  reproducible, shareable workflows that run consistently on hardware
  ranging from personal computers to HPC systems. The full list of
  <monospace>itwinai</monospace> plugins can be found at
  <ext-link ext-link-type="uri" xlink:href="https://itwinai.readthedocs.io/latest/getting-started/plugins-list.html">this
  link</ext-link>.</p>
</sec>
<sec id="reproducibility">
  <title>Reproducibility</title>
  <p>The scalability results in this paper were obtained on the JUWELS
  Booster GPU partition at the Jülich Supercomputing Centre
  (<xref alt="Krause, 2019" rid="ref-JUWELS" ref-type="bibr">Krause,
  2019</xref>), a SLURM-managed system. For the Virgo scalability study
  (<xref alt="[fig:speedup]" rid="figU003Aspeedup">[fig:speedup]</xref>,
  <xref alt="[fig:compvsother]" rid="figU003Acompvsother">[fig:compvsother]</xref>),
  <monospace>itwinai</monospace> was driven by YAML configurations that
  specify dataset, model, optimizer, batch size, and number of epochs.
  The exact configuration files, SLURM scripts, and commands used to
  generate these figures are stored in the
  <monospace>itwinai</monospace> repository inside
  <ext-link ext-link-type="uri" xlink:href="https://github.com/interTwin-eu/itwinai/tree/joss/joss/reproducibility"><monospace>joss/reproducibility</monospace></ext-link>.
  The scalability-report and profiling workflow used to produce the
  plots is described in the documentation
  (<xref alt="itwinai Developers, 2025b" rid="ref-itwinai_docs_scalability" ref-type="bibr">itwinai
  Developers, 2025b</xref>,
  <xref alt="2025a" rid="ref-itwinai_docs_profiling" ref-type="bibr">2025a</xref>).</p>
  <p>The same scalability-report machinery applies to single- and
  multi-node training: on SLURM clusters, the configurations are
  launched via SLURM, while on a single node (CPU or GPU) they can be
  run with a local backend and reduced problem size. This allows readers
  with access to a comparable SLURM cluster to reproduce the multi-node
  results and readers without SLURM to run a smaller, single-node
  variant that produces reports and plots with the same structure as
  those shown in this paper.</p>
</sec>
<sec id="performance">
  <title>Performance</title>
  <p><monospace>itwinai</monospace> provides tools to assess scalability
  and diagnose bottlenecks, enabling efficient and accountable use of
  HPC resources. Two complementary components are provided: scalability
  report generation and profiling.</p>
  <sec id="scalability-report">
    <title>Scalability report</title>
    <p>For data-parallel training, adding more workers improves
    throughput, but as all-reduce communication costs grow,
    communication overhead eventually dominates, causing scaling to
    level-off or even decline. The report characterizes this trade-off
    across GPUs/nodes and backends, reporting wall-clock epoch time,
    relative speedup
    (<xref alt="[fig:speedup]" rid="figU003Aspeedup">[fig:speedup]</xref>),
    GPU utilization (0–100%), energy (Wh), and compute-versus-other
    time, including collective communication and memory operations
    (<xref alt="[fig:compvsother]" rid="figU003Acompvsother">[fig:compvsother]</xref>).
    Considered jointly, these metrics identify the most efficient
    configuration and distribution strategy, rather than relying on a
    single indicator.
    <xref alt="[fig:speedup]" rid="figU003Aspeedup">[fig:speedup]</xref>
    and
    <xref alt="[fig:compvsother]" rid="figU003Acompvsother">[fig:compvsother]</xref>
    show the scalability of the physics use case from
    INFN<xref ref-type="fn" rid="fn2">2</xref> targeting
    gravitational-wave analysis at the
    Virgo<xref ref-type="fn" rid="fn3">3</xref> interferometer
    (<xref alt="Tsolaki et al., 2025" rid="ref-tsolaki_2025_15120028" ref-type="bibr">Tsolaki
    et al., 2025</xref>)
    (<xref alt="Sæther et al., 2025" rid="ref-saether_scalabiliy_2025" ref-type="bibr">Sæther
    et al., 2025</xref>).</p>
    <fig>
      <caption><p>Relative speedup of average epoch time vs. number of
      workers for the Virgo use
      case.<styled-content id="figU003Aspeedup"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="svg+xml" xlink:href="virgo_relative_epoch_time_speedup.svg" />
    </fig>
    <fig>
      <caption><p>Proportion of time spent on computation versus other
      operations, such as collective communication, in the Virgo use
      case, broken down by number of workers and distributed framework.
      <styled-content id="figU003Acompvsother"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="svg+xml" xlink:href="virgo_computation_vs_other_plot.svg" />
    </fig>
  </sec>
  <sec id="addressing-bottlenecks-via-profiling">
    <title>Addressing bottlenecks via profiling</title>
    <p>To explain why performance degrades,
    <monospace>itwinai</monospace> integrates low-overhead, sample-based
    profiling (e.g., py-spy
    (<xref alt="Frederickson, 2018" rid="ref-py_spy" ref-type="bibr">Frederickson,
    2018</xref>)) and summarizes flame-graph data into actionable
    hotspots (e.g., data loading and I/O, kernel execution, host–device
    transfer, communication). These summaries guide targeted remedies
    such as adjusting batch size, data-loader parallelism, gradient
    accumulation, or backend/collective settings.</p>
  </sec>
</sec>
<sec id="outlook-and-future-developments">
  <title>Outlook and future developments</title>
  <p><monospace>itwinai</monospace> provides ready-to-use ML tools that
  are applicable across a wide range of scientific applications. The
  development of the library is continued through projects
  ODISSEE<xref ref-type="fn" rid="fn4">4</xref> and
  RI-SCALE<xref ref-type="fn" rid="fn5">5</xref>. The future
  developments include the integration of new scientific use cases,
  exploring additional parallelism approaches, integrating advanced user
  interfaces, and adding other EuroHPC systems and performance
  optimization features.</p>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>This work has been funded by the European Commission in the context
  of the interTwin project, with Grant Agreement Number 101058386. In
  interTwin, <monospace>itwinai</monospace> has been actively developed
  on the HPC systems at JSC, such as on the HDF-ML and JUWELS Booster
  systems, and using EuroHPC resources, such as on the Vega HPC system.
  <monospace>itwinai</monospace> is an open-source Python library
  primarily developed by CERN, in collaboration with Forschungszentrum
  Jülich (FZJ).</p>
</sec>
</body>
<back>
<ref-list>
  <title></title>
  <ref id="ref-rayU003A2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
        <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
        <name><surname>Wang</surname><given-names>Stephanie</given-names></name>
        <name><surname>Tumanov</surname><given-names>Alexey</given-names></name>
        <name><surname>Liaw</surname><given-names>Richard</given-names></name>
        <name><surname>Liang</surname><given-names>Eric</given-names></name>
        <name><surname>Elibol</surname><given-names>Melih</given-names></name>
        <name><surname>Yang</surname><given-names>Zongheng</given-names></name>
        <name><surname>Paul</surname><given-names>William</given-names></name>
        <name><surname>Jordan</surname><given-names>Michael I.</given-names></name>
        <name><surname>Stoica</surname><given-names>Ion</given-names></name>
      </person-group>
      <article-title>Ray: A distributed framework for emerging AI applications</article-title>
      <year iso-8601-date="2018">2018</year>
      <uri>https://arxiv.org/abs/1712.05889</uri>
    </element-citation>
  </ref>
  <ref id="ref-interlink">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Ciangottini</surname><given-names>D.</given-names></name>
        <name><surname>Bianchini</surname><given-names>G.</given-names></name>
        <name><surname>Spiga</surname><given-names>D.</given-names></name>
      </person-group>
      <article-title>interLink</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025-02">2025</year><month>02</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-07-27">2025</year><month>07</month><day>27</day></date-in-citation>
      <uri>https://github.com/interLink-hq/interLink</uri>
    </element-citation>
  </ref>
  <ref id="ref-optuna_2019">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Akiba</surname><given-names>Takuya</given-names></name>
        <name><surname>Sano</surname><given-names>Shotaro</given-names></name>
        <name><surname>Yanase</surname><given-names>Toshihiko</given-names></name>
        <name><surname>Ohta</surname><given-names>Takeru</given-names></name>
        <name><surname>Koyama</surname><given-names>Masanori</given-names></name>
      </person-group>
      <article-title>Optuna: A next-generation hyperparameter optimization framework</article-title>
      <source>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery and data mining</source>
      <publisher-name>ACM</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://doi.org/10.1145/3292500.3330701</uri>
      <pub-id pub-id-type="doi">10.1145/3292500.3330701</pub-id>
      <fpage>2623</fpage>
      <lpage>2631</lpage>
    </element-citation>
  </ref>
  <ref id="ref-heat">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Developers</surname><given-names>HeAT</given-names></name>
      </person-group>
      <article-title>HeAT</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-14">2025</year><month>08</month><day>14</day></date-in-citation>
      <uri>https://github.com/helmholtz-analytics/heat</uri>
    </element-citation>
  </ref>
  <ref id="ref-ai4hpc">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Developers</surname><given-names>AI4HPC</given-names></name>
      </person-group>
      <article-title>AI4HPC</article-title>
      <source>GitLab repository</source>
      <publisher-name>GitLab</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-14">2025</year><month>08</month><day>14</day></date-in-citation>
      <uri>https://gitlab.jsc.fz-juelich.de/CoE-RAISE/FZJ/ai4hpc/ai4hpc</uri>
    </element-citation>
  </ref>
  <ref id="ref-perun">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Team</surname><given-names>Helmholtz AI Energy</given-names></name>
      </person-group>
      <article-title>Perun</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-14">2025</year><month>08</month><day>14</day></date-in-citation>
      <uri>https://github.com/Helmholtz-AI-Energy/perun</uri>
    </element-citation>
  </ref>
  <ref id="ref-manzi_intertwin_2025">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Manzi</surname><given-names>Andrea</given-names></name>
        <name><surname>Bardaji</surname><given-names>Raul</given-names></name>
        <name><surname>Rodero</surname><given-names>Ivan</given-names></name>
        <name><surname>Moltó</surname><given-names>Germán</given-names></name>
        <name><surname>Fiore</surname><given-names>Sandro</given-names></name>
        <name><surname>Campos</surname><given-names>Isabel</given-names></name>
        <name><surname>Elia</surname><given-names>Donatello</given-names></name>
        <name><surname>Sarandrea</surname><given-names>Francesco</given-names></name>
        <name><surname>Millar</surname><given-names>A. Paul</given-names></name>
        <name><surname>Spiga</surname><given-names>Daniele</given-names></name>
        <name><surname>Bunino</surname><given-names>Matteo</given-names></name>
        <name><surname>Accarino</surname><given-names>Gabriele</given-names></name>
        <name><surname>Asprea</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Bernardo</surname><given-names>Samuel</given-names></name>
        <name><surname>Caballer</surname><given-names>Miguel</given-names></name>
        <name><surname>Chatzikyriakou</surname><given-names>Charis</given-names></name>
        <name><surname>Ciangottini</surname><given-names>Diego</given-names></name>
        <name><surname>Claus</surname><given-names>Michele</given-names></name>
        <name><surname>Cristofori</surname><given-names>Andrea</given-names></name>
        <name><surname>Donno</surname><given-names>Davide</given-names></name>
        <name><surname>Donno</surname><given-names>Emanuele</given-names></name>
        <name><surname>Ferrario</surname><given-names>Iacopo</given-names></name>
        <name><surname>Fronza</surname><given-names>Massimiliano</given-names></name>
        <name><surname>Jacob</surname><given-names>Alexander</given-names></name>
        <name><surname>Komijani</surname><given-names>Javad</given-names></name>
        <name><surname>Marinkovic</surname><given-names>Marina Krstic</given-names></name>
        <name><surname>Legger</surname><given-names>Federica</given-names></name>
        <name><surname>Palomo</surname><given-names>Ivan</given-names></name>
        <name><surname>Parcero</surname><given-names>Estíbaliz</given-names></name>
        <name><surname>Sarma</surname><given-names>Rakesh</given-names></name>
        <name><surname>Ray</surname><given-names>Gaurav Sinha</given-names></name>
        <name><surname>Vallero</surname><given-names>Sara</given-names></name>
        <name><surname>Zvolensky</surname><given-names>Juraj</given-names></name>
      </person-group>
      <article-title>interTwin: Advancing Scientific Digital Twins through AI, Federated Computing and Data</article-title>
      <source>Future Generation Computer Systems</source>
      <year iso-8601-date="2025">2025</year>
      <issn>0167-739X</issn>
      <uri>https://www.sciencedirect.com/science/article/pii/S0167739X25006065</uri>
      <pub-id pub-id-type="doi">10.1016/j.future.2025.108312</pub-id>
      <fpage>108312</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-tsolaki_2025_15120028">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Tsolaki</surname><given-names>Kalliopi</given-names></name>
        <name><surname>Vallecorsa</surname><given-names>Sofia</given-names></name>
        <name><surname>Vallero</surname><given-names>Sara</given-names></name>
        <name><surname>Asprea</surname><given-names>Lorenzo</given-names></name>
        <name><surname>Sarandrea</surname><given-names>Francesco</given-names></name>
        <name><surname>Komijani</surname><given-names>Javad</given-names></name>
        <name><surname>Ray</surname><given-names>Gaurav Sinha</given-names></name>
        <name><surname>Pidopryhora</surname><given-names>Yurii</given-names></name>
        <name><surname>Campos</surname><given-names>Isabel</given-names></name>
      </person-group>
      <article-title>interTwin D4.6 final architecture design of the DTs capabilities for high energy physics, radio astronomy and gravitational-wave astrophysics</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2025-04">2025</year><month>04</month>
      <uri>https://doi.org/10.5281/zenodo.15120028</uri>
      <pub-id pub-id-type="doi">10.5281/zenodo.15120028</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-saether_scalabiliy_2025">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Sæther</surname><given-names>Jarl Sondre</given-names></name>
        <name><surname>Bunino</surname><given-names>Matteo</given-names></name>
        <name><surname>Eickhoff</surname><given-names>Linus Maximilian</given-names></name>
      </person-group>
      <article-title>Scalability analysis of GlitchFlow with itwinai</article-title>
      <publisher-name>Zenodo</publisher-name>
      <year iso-8601-date="2025-08">2025</year><month>08</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-15">2025</year><month>08</month><day>15</day></date-in-citation>
      <uri>https://doi.org/10.5281/zenodo.16882390</uri>
    </element-citation>
  </ref>
  <ref id="ref-horovod">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sergeev</surname><given-names>Alexander</given-names></name>
        <name><surname>Del Balso</surname><given-names>Mike</given-names></name>
      </person-group>
      <article-title>Horovod: fast and easy distributed deep learning in TensorFlow</article-title>
      <source>arXiv:1802.05799</source>
      <year iso-8601-date="2018">2018</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1802.05799</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-deepspeed">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Rasley</surname><given-names>Jeff</given-names></name>
        <name><surname>Rajbhandari</surname><given-names>Samyam</given-names></name>
        <name><surname>Ruwase</surname><given-names>Olatunji</given-names></name>
        <name><surname>He</surname><given-names>Yuxiong</given-names></name>
      </person-group>
      <article-title>Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters</article-title>
      <source>Proc. 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.1145/3394486.3406703</pub-id>
      <fpage>3505</fpage>
      <lpage>3506</lpage>
    </element-citation>
  </ref>
  <ref id="ref-torch_ddp">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Li</surname><given-names>Shen</given-names></name>
        <name><surname>Zhao</surname><given-names>Yanli</given-names></name>
        <name><surname>Varma</surname><given-names>Rohan</given-names></name>
        <name><surname>Salpekar</surname><given-names>Omkar</given-names></name>
        <name><surname>Noordhuis</surname><given-names>Pieter</given-names></name>
        <name><surname>Li</surname><given-names>Teng</given-names></name>
        <name><surname>Paszke</surname><given-names>Adam</given-names></name>
        <name><surname>Smith</surname><given-names>Jeff</given-names></name>
        <name><surname>Vaughan</surname><given-names>Brian</given-names></name>
        <name><surname>Damania</surname><given-names>Pritam</given-names></name>
        <name><surname>others</surname></name>
      </person-group>
      <article-title>Pytorch Distributed: Experiences on accelerating data parallel training</article-title>
      <source>arXiv:2006.15704</source>
      <year iso-8601-date="2020">2020</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2006.15704</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-ray_tune">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Liaw</surname><given-names>Richard</given-names></name>
        <name><surname>Liang</surname><given-names>Eric</given-names></name>
        <name><surname>Nishihara</surname><given-names>Robert</given-names></name>
        <name><surname>Moritz</surname><given-names>Philipp</given-names></name>
        <name><surname>Gonzalez</surname><given-names>Joseph E.</given-names></name>
        <name><surname>Stoica</surname><given-names>Ion</given-names></name>
      </person-group>
      <article-title>Tune: A research platform for distributed model selection and training</article-title>
      <publisher-name>arXiv</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <uri>https://arxiv.org/abs/1807.05118</uri>
    </element-citation>
  </ref>
  <ref id="ref-mlflow">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zaharia</surname><given-names>Matei A.</given-names></name>
        <name><surname>Chen</surname><given-names>Andrew</given-names></name>
        <name><surname>Davidson</surname><given-names>Aaron</given-names></name>
        <name><surname>Ghodsi</surname><given-names>Ali</given-names></name>
        <name><surname>Hong</surname><given-names>Sue Ann</given-names></name>
        <name><surname>Konwinski</surname><given-names>Andy</given-names></name>
        <name><surname>Murching</surname><given-names>Siddharth</given-names></name>
        <name><surname>Nykodym</surname><given-names>Tomas</given-names></name>
        <name><surname>Ogilvie</surname><given-names>Paul</given-names></name>
        <name><surname>Parkhe</surname><given-names>Mani</given-names></name>
        <name><surname>Xie</surname><given-names>Fen</given-names></name>
        <name><surname>Zumar</surname><given-names>Corey</given-names></name>
      </person-group>
      <article-title>Accelerating the Machine Learning Lifecycle with MLflow</article-title>
      <source>IEEE Data Eng. Bull.</source>
      <year iso-8601-date="2018">2018</year>
      <volume>41</volume>
      <uri>http://sites.computer.org/debull/A18dec/p39.pdf</uri>
      <fpage>39</fpage>
      <lpage>45</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wandb">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>wandb Contributors</string-name>
      </person-group>
      <article-title>Weights &amp; Biases</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-04-25">2025</year><month>04</month><day>25</day></date-in-citation>
      <uri>https://github.com/wandb/wandb</uri>
    </element-citation>
  </ref>
  <ref id="ref-tensorboard">
    <element-citation>
      <person-group person-group-type="author">
        <string-name>TensorBoard Contributors</string-name>
      </person-group>
      <article-title>TensorBoard</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-04-25">2025</year><month>04</month><day>25</day></date-in-citation>
      <uri>https://github.com/tensorflow/tensorboard</uri>
    </element-citation>
  </ref>
  <ref id="ref-yprovml">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Padovani</surname><given-names>G.</given-names></name>
        <name><surname>Fiore</surname><given-names>S.</given-names></name>
      </person-group>
      <article-title>yProvML</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2025-04">2025</year><month>04</month>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-04-25">2025</year><month>04</month><day>25</day></date-in-citation>
      <uri>https://github.com/HPCI-Lab/yProvML</uri>
    </element-citation>
  </ref>
  <ref id="ref-py_spy">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Frederickson</surname><given-names>B.</given-names></name>
      </person-group>
      <article-title>py-spy: Sampling profiler for Python programs</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-07-30">2025</year><month>07</month><day>30</day></date-in-citation>
      <uri>https://github.com/benfred/py-spy</uri>
    </element-citation>
  </ref>
  <ref id="ref-JUWELS">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Krause</surname><given-names>Dorian</given-names></name>
      </person-group>
      <article-title>JUWELS: Modular tier-0/1 supercomputer at the Jülich Supercomputing Centre</article-title>
      <source>J. of Large-Scale Research Facilities</source>
      <year iso-8601-date="2019">2019</year>
      <volume>5</volume>
      <issue>A135</issue>
      <pub-id pub-id-type="doi">10.17815/jlsrf-5-171</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-itwinai_docs_scalability">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Developers</surname></name>
      </person-group>
      <article-title>Scalability report</article-title>
      <source>itwinai documentation</source>
      <publisher-name>Read the Docs</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-14">2025</year><month>08</month><day>14</day></date-in-citation>
      <uri>https://itwinai.readthedocs.io/v0.3.4/how-it-works/scalability-report/scalability_report.html</uri>
    </element-citation>
  </ref>
  <ref id="ref-itwinai_docs_profiling">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Developers</surname></name>
      </person-group>
      <article-title>Profiling overview</article-title>
      <source>itwinai documentation</source>
      <publisher-name>Read the Docs</publisher-name>
      <year iso-8601-date="2025">2025</year>
      <date-in-citation content-type="access-date"><year iso-8601-date="2025-08-14">2025</year><month>08</month><day>14</day></date-in-citation>
      <uri>https://itwinai.readthedocs.io/v0.3.4/tutorials/profiling/profiling-overview.html</uri>
    </element-citation>
  </ref>
</ref-list>
<fn-group>
  <fn id="fn1">
    <label>1</label><p>GitHub hosted runners define the type of machine
    that will process a job in your workflow.
    <ext-link ext-link-type="uri" xlink:href="https://docs-internal.github.com/en/actions/how-tos/write-workflows/choose-where-workflows-run/choose-the-runner-for-a-job?utm_source=chatgpt.com">Find
    more here</ext-link> (Accessed on 2025-08-14).</p>
  </fn>
  <fn id="fn2">
    <label>2</label><p>Istituto Nazionale di Fisica Nucleare
    <ext-link ext-link-type="uri" xlink:href="https://www.infn.it/en/">infn.it</ext-link>
    (Accessed on 2025-08-14).</p>
  </fn>
  <fn id="fn3">
    <label>3</label><p>Virgo Collaboration
    <ext-link ext-link-type="uri" xlink:href="https://www.virgo-gw.eu/">www.virgo-gw.eu</ext-link>
    (Accessed on 2025-08-14).</p>
  </fn>
  <fn id="fn4">
    <label>4</label><p>Online Data Intensive Solutions for Science in
    the Exabytes Era (ODISSEE):
    <ext-link ext-link-type="uri" xlink:href="https://www.odissee-project.eu/">odissee-project.eu</ext-link>
    (Accessed on 2025-08-14).</p>
  </fn>
  <fn id="fn5">
    <label>5</label><p>RI-SCALE project:
    <ext-link ext-link-type="uri" xlink:href="https://www.riscale.eu/">riscale.eu</ext-link>
    (Accessed on 2025-08-14).</p>
  </fn>
</fn-group>
</back>
</article>
