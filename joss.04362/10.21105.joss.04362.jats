<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4362</article-id>
<article-id pub-id-type="doi">10.21105/joss.04362</article-id>
<title-group>
<article-title>Pose2Sim: An open-source Python package for multiview
markerless kinematics</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0002-6891-8331</contrib-id>
<name>
<surname>Pagnon</surname>
<given-names>David</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-4518-8479</contrib-id>
<name>
<surname>Domalain</surname>
<given-names>Mathieu</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-0810-5187</contrib-id>
<name>
<surname>Reveret</surname>
<given-names>Lionel</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Laboratoire Jean Kuntzmann, Université Grenoble Alpes, 700
avenue Centrale, 38400 Saint Martin d’Hères, France</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Institut Pprime, 2 Bd des Frères Lumière, 86360
Chasseneuil-du-Poitou, France</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Inria Grenoble Rhône-Alpes, 38330 Montbonnot-Saint-Martin,
France</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<volume>7</volume>
<issue>77</issue>
<fpage>4362</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>python</kwd>
<kwd>markerless kinematics</kwd>
<kwd>motion capture</kwd>
<kwd>sports performance analysis</kwd>
<kwd>openpose</kwd>
<kwd>opensim</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p><monospace>Pose2Sim</monospace> provides a workflow for 3D
  markerless kinematics, as an alternative to the more usual
  marker-based motion capture methods. <monospace>Pose2Sim</monospace>
  stands for “OpenPose to OpenSim”, as it uses
  <monospace>OpenPose</monospace> inputs (2D coordinates obtained from
  multiple videos) and leads to an <monospace>OpenSim</monospace> result
  (full-body 3D joint angles).</p>
  <p>The repository presents a framework for:</p>
  <list list-type="bullet">
    <list-item>
      <p>Detecting 2D joint coordinates from videos, e.g. via
      <monospace>OpenPose</monospace>
      (<xref alt="Cao et al., 2019" rid="ref-Cao_2019" ref-type="bibr">Cao
      et al., 2019</xref>)</p>
    </list-item>
    <list-item>
      <p>Calibrating cameras</p>
    </list-item>
    <list-item>
      <p>Detecting the person of interest</p>
    </list-item>
    <list-item>
      <p>Triangulating 2D joint coordinates and storing them as 3D
      positions in a .trc file</p>
    </list-item>
    <list-item>
      <p>Filtering these calculated 3D positions</p>
    </list-item>
    <list-item>
      <p>Scaling and running inverse kinematics via
      <monospace>OpenSim</monospace>
      (<xref alt="Delp et al., 2007" rid="ref-Delp_2007" ref-type="bibr">Delp
      et al., 2007</xref>;
      <xref alt="Seth, 2018" rid="ref-Seth_2018" ref-type="bibr">Seth,
      2018</xref>), in order to obtain full-body 3D joint angles</p>
    </list-item>
  </list>
  <p>Each task is easily customizable, and requires only moderate Python
  skills. <monospace>Pose2Sim</monospace> is accessible at
  <ext-link ext-link-type="uri" xlink:href="https://github.com/perfanalytics/pose2sim">https://github.com/perfanalytics/pose2sim</ext-link>.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>For the last few decades, marker-based kinematics has been
  considered the best choice for the analysis of human movement, when
  regarding the trade-off between ease of use and accuracy. However, a
  marker-based system is hard to set up outdoors or in context, and it
  requires placing markers on the body, which can hinder natural
  movement
  (<xref alt="Colyer et al., 2018" rid="ref-Colyer_2018" ref-type="bibr">Colyer
  et al., 2018</xref>).</p>
  <p>The emergence of markerless kinematics opens up new possibilities.
  Indeed, the interest in deep-learning pose estimation neural networks
  has been growing fast since 2015
  (<xref alt="Zheng et al., 2022" rid="ref-Zheng_2022" ref-type="bibr">Zheng
  et al., 2022</xref>), which makes it now possible to collect accurate
  and reliable kinematic data without the use of physical markers.
  <monospace>OpenPose</monospace>, for example, is a widespread
  open-source software which provides 2D joint coordinate estimates from
  videos. These coordinates can then be triangulated in order to produce
  3D positions. Aside from <monospace>Pose2Sim</monospace>, a number a
  tools are available for such triangulation: the experimental
  <monospace>OpenPose 3D reconstruction module</monospace>
  (<xref alt="Hidalgo, 2021" rid="ref-Hidalgo_2021" ref-type="bibr">Hidalgo,
  2021</xref>), the <monospace>FreeMoCap</monospace> Python and Blender
  toolbox
  (<xref alt="Matthis &amp; Cherian, 2022" rid="ref-Matthis_2022" ref-type="bibr">Matthis
  &amp; Cherian, 2022</xref>), and the <monospace>pose3d</monospace>
  Matlab toolbox
  (<xref alt="Sheshadri et al., 2020" rid="ref-Sheshadri_2020" ref-type="bibr">Sheshadri
  et al., 2020</xref>). Yet, when it comes to the biomechanical analysis
  of human motion, it is often more useful to obtain joint angles than
  joint center positions in space. Joint angles allow for better
  comparison among trials and individuals, and they represent the first
  step for other analyses such as inverse dynamics.</p>
  <p><monospace>OpenSim</monospace> is another widespread open-source
  software which helps compute 3D joint angles, usually from marker
  coordinates. It lets scientists define a detailed musculoskeletal
  model, scale it to individual subjects, and perform inverse kinematics
  with customizable biomechanical constraints. It provides other
  features such as net calculation of joint moments or resolution of
  individual muscle forces, although this is beyond the scope of our
  contribution.</p>
  <p>So far, little work has been done towards obtaining 3D angles from
  multiple views
  (<xref alt="Zheng et al., 2022" rid="ref-Zheng_2022" ref-type="bibr">Zheng
  et al., 2022</xref>). However, three software applications are worth
  mentioning. <monospace>Anipose</monospace>
  (<xref alt="Karashchuk et al., 2021" rid="ref-Karashchuk_2021" ref-type="bibr">Karashchuk
  et al., 2021</xref>) proposes a Python open-source framework which
  allows for joint angle estimation with spatio-temporal constraints,
  but it is primarily designed for animal motion analysis.
  <monospace>Theia3D</monospace>
  (<xref alt="Kanko et al., 2021" rid="ref-Kanko_2021" ref-type="bibr">Kanko
  et al., 2021</xref>) is a software application for human gait
  markerless kinematics. Although the GUI is more user friendly, it is
  neither open-source nor customizable. <monospace>OpenCap</monospace>
  (<xref alt="Uhlrich et al., 2022" rid="ref-Uhlrich_2022" ref-type="bibr">Uhlrich
  et al., 2022</xref>) has recently been released, and offers a
  user-friendly web application working with low-cost hardware. It
  predicts the coordinates of 43 anatomical markers from 20 triangulated
  keypoints, and imports them in <monospace>OpenSim</monospace>.
  However, the source code has not yet been released.</p>
  <p>The goal of <monospace>Pose2Sim</monospace> is to build a bridge
  between the communities of computer vision and biomechanics, by
  providing a simple and open-source pipeline connecting the two
  aforementioned state-of-the-art tools: <monospace>OpenPose</monospace>
  and <monospace>OpenSim</monospace>. The whole workflow runs from any
  video cameras, on any computer, equipped with any operating system
  (although <monospace>OpenSim</monospace> has to be compiled from
  source on Linux.) <monospace>Pose2Sim</monospace> has already been
  used and tested in a number of situations (walking, running, cycling,
  dancing, balancing, swimming, boxing), and published in peer-reviewed
  scientific publications assessing its robustness
  (<xref alt="Pagnon et al., 2021" rid="ref-Pagnon_2021" ref-type="bibr">Pagnon
  et al., 2021</xref>) and accuracy
  (<xref alt="Pagnon et al., 2022" rid="ref-Pagnon_2022" ref-type="bibr">Pagnon
  et al., 2022</xref>). Its results for inverse kinematics were deemed
  good when compared to marker-based ones, with errors generally below
  4.0° across several activities, on both lower and on upper limbs. The
  combination of its ease of use, customizable parameters, and high
  robustness and accuracy makes it promising, especially for
  “in-the-wild” sports movement analysis.</p>
</sec>
<sec id="pose2sim-workflow">
  <title>Pose2Sim workflow</title>
  <p><monospace>Pose2Sim</monospace> connects two of the most widely
  recognized (and open-source) softwares in their respective fields:</p>
  <list list-type="bullet">
    <list-item>
      <p><monospace>OpenPose</monospace>
      (<xref alt="Cao et al., 2019" rid="ref-Cao_2019" ref-type="bibr">Cao
      et al., 2019</xref>), a 2D human pose estimation neural
      network</p>
    </list-item>
    <list-item>
      <p><monospace>OpenSim</monospace>
      (<xref alt="Delp et al., 2007" rid="ref-Delp_2007" ref-type="bibr">Delp
      et al., 2007</xref>), a 3D biomechanics analysis software</p>
    </list-item>
  </list>
  <p>The workflow is organized as follows
  (<xref alt="Figure 1" rid="figU003Apipeline">Figure 1</xref>):</p>
  <list list-type="order">
    <list-item>
      <p>Preliminary <monospace>OpenPose</monospace>
      (<xref alt="Cao et al., 2019" rid="ref-Cao_2019" ref-type="bibr">Cao
      et al., 2019</xref>) 2D keypoints detection</p>
    </list-item>
    <list-item>
      <p><monospace>Pose2Sim</monospace> core, including 4 customizable
      steps:</p>
      <list list-type="order">
        <list-item>
          <p>Camera calibration</p>
        </list-item>
        <list-item>
          <p>2D tracking of the person of interest</p>
        </list-item>
        <list-item>
          <p>3D keypoint triangulation</p>
        </list-item>
        <list-item>
          <p>3D coordinate filtering</p>
        </list-item>
      </list>
    </list-item>
    <list-item>
      <p>A full-body <monospace>OpenSim</monospace>
      (<xref alt="Delp et al., 2007" rid="ref-Delp_2007" ref-type="bibr">Delp
      et al., 2007</xref>) skeletal model with
      <monospace>OpenPose</monospace> keypoints is provided, as well as
      scaling and inverse kinematics setup files.</p>
    </list-item>
  </list>
  <fig>
    <caption><p>Pose2Sim full pipeline: (1) OpenPose 2D keypoint
    detection; (2.1) Camera calibration; (2.1–2.4) Tracking of the
    person of interest, Triangulation of keypoint coordinates, and
    Filtering; (3) Constraining the 3D coordinates to an individually
    scaled, physically consistent OpenSim skeletal
    model.<styled-content id="figU003Apipeline"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/Pipeline.png" xlink:title="" />
  </fig>
</sec>
<sec id="pose2sim-method-details">
  <title>Pose2Sim method details</title>
  <p><monospace>Pose2Sim</monospace> is meant to be as fully and easily
  configurable as possible, by editing the ‘User/Config.toml’ file.
  Optional tools are also provided for extending its usage
  (<xref alt="Figure 2" rid="figU003Autilities">Figure 2</xref>).</p>
  <sec id="project">
    <title>Project</title>
    <p>The user can specify the project path and folder names, the video
    frame rate, and the range of analyzed frames.</p>
    <fig>
      <caption><p>The Pose2Sim workflow, along with some optional
      utilities provided in the
      package.<styled-content id="figU003Autilities"></styled-content></p></caption>
      <graphic mimetype="image" mime-subtype="jpeg" xlink:href="media/Pose2Sim_workflow_utilities.jpg" xlink:title="" />
    </fig>
  </sec>
  <sec id="d-keypoint-detection">
    <title>2D keypoint detection</title>
    <p>It is possible to define which 2D pose estimation model is used.
    This choice will affect how keypoint indices will be mapped to model
    markers in <monospace>OpenSim</monospace>, corresponding to
    anatomical landmarks or joint centers. Only 21 of the 25 keypoints
    detected by the default <monospace>OpenPose</monospace> models are
    tracked, since eye and ear keypoints would be redundant in the
    determination of the head orientation.</p>
    <p>The <monospace>OpenPose</monospace> BODY_25B experimental model
    is recommended, as it is as fast as the standard BODY_25 model,
    while being more accurate
    (<xref alt="Hidalgo, 2019" rid="ref-Hidalgo_2019" ref-type="bibr">Hidalgo,
    2019</xref>). Non-<monospace>OpenPose</monospace> models can also be
    chosen, whether they are human (such as the AlphaPose one
    (<xref alt="Fang et al., 2017" rid="ref-Fang_2017" ref-type="bibr">Fang
    et al., 2017</xref>)), or animal (such as any
    <monospace>DeepLabCut</monospace> model trained by the user
    (<xref alt="Mathis et al., 2018" rid="ref-Mathis_2018" ref-type="bibr">Mathis
    et al., 2018</xref>)). Two optional standalone scripts are also
    provided if the user desires a visual display of the resulting 2D
    pose estimation
    (<xref alt="Figure 2" rid="figU003Autilities">Figure 2</xref>).</p>
  </sec>
  <sec id="camera-calibration">
    <title>Camera calibration</title>
    <p>The user can indicate whether cameras are going to be calibrated
    with a checkerboard, or if a preexisting calibration file (such as
    one provided by a Qualisys system) will simply be converted.</p>
    <p>If checkerboard calibration is chosen, the number of corners and
    the size of the squares have to be specified. In this case, the
    operator needs to take at least 10 pictures or one video per camera
    of the checkerboard, covering as much as the field of view as
    possible, with different orientations. Corners are then detected and
    refined with OpenCV. Detected corners can optionally be displayed
    for verification. Each camera is finally calibrated using OpenCV
    with an algorithm based on
    (<xref alt="Zhang, 2000" rid="ref-Zhang_2000" ref-type="bibr">Zhang,
    2000</xref>). The user can choose the index of the image which they
    want to be used as a reference for calculating extrinsic parameters.
    Residual calibration errors are given, and stored in a log file.</p>
  </sec>
  <sec id="tracking-the-person-of-interest">
    <title>Tracking the person of interest</title>
    <p>One needs to differentiate the people in the background from the
    actual subject. The tracking step examines all possible
    triangulations of a chosen keypoint among all detected persons, and
    reprojects them on the image planes. The triangulation with the
    smallest reprojection error is considered to be the one associated
    with the right person on all cameras. If the reprojection error is
    above a predefined threshold, the process is repeated after taking
    off one, or several cameras. This happens, for example, if the
    person of interest has exited the field of a camera, while another
    person is still in the background.</p>
    <p>We recommend choosing the neck point or one of the hip points. In
    most cases they are the least likely to move out of the cameras’
    views.</p>
  </sec>
  <sec id="triangulation">
    <title>Triangulation</title>
    <p><monospace>Pose2Sim</monospace> triangulation is robust, largely
    because instead of using classic Direct Linear Transform (DLT)
    (<xref alt="Hartley &amp; Sturm, 1997" rid="ref-Hartley_1997" ref-type="bibr">Hartley
    &amp; Sturm, 1997</xref>), we propose a weighted DLT, i.e., a
    triangulation procedure where each <monospace>OpenPose</monospace>
    keypoint coordinate is weighted with its confidence score
    (<xref alt="Pagnon et al., 2021" rid="ref-Pagnon_2021" ref-type="bibr">Pagnon
    et al., 2021</xref>).</p>
    <p>Other parameters can be specified, such as:</p>
    <list list-type="bullet">
      <list-item>
        <p>The minimum likelihood (given by
        <monospace>OpenPose</monospace> for each detected keypoint)
        below which a 2D point will not be taken into account for
        triangulation.
        </p>
      </list-item>
      <list-item>
        <p>The maximum in reprojection error above which triangulation
        results will not be accepted. This can happen if
        <monospace>OpenPose</monospace> provides a bad 2D keypoint
        estimate, or if the person of interest leaves the camera field.
        Triangulation will then be tried again on all subsets of all
        cameras minus one. If the best of the resulting reprojection
        errors is below the threshold, it is retained. If it is still
        above the threshold, one more camera is excluded.
        </p>
      </list-item>
      <list-item>
        <p>The minimum number of “good” cameras (i.e., cameras remaining
        after the last two steps) required for triangulating a keypoint.
        If there are not enough cameras left, the 3D keypoint is dropped
        for this frame.</p>
      </list-item>
    </list>
    <p>Once all frames are triangulated, the ones with missing keypoint
    coordinates are interpolated. The interpolation method can also be
    chosen from among linear, slinear, quadratic, and cubic. The mean
    reprojection error over all frames is given for each point and saved
    to a log file, as well as the number of cameras excluded to reach
    the demanded thresholds. The resulting 3D coordinates are formatted
    as a .trc file, which can be read by
    <monospace>OpenSim</monospace>.</p>
  </sec>
  <sec id="filtering-and-other-operations">
    <title>Filtering and other operations</title>
    <p>Different filters can be chosen, and their parameters can be
    adjusted. The user can choose a zero-phase low-pass Butterworth
    filter
    (<xref alt="Butterworth, 1930" rid="ref-Butterworth_1930" ref-type="bibr">Butterworth,
    1930</xref>) that they can apply either on keypoint positions or on
    their speeds, a LOESS filter
    (<xref alt="Cleveland, 1981" rid="ref-Cleveland_1981" ref-type="bibr">Cleveland,
    1981</xref>), a Gaussian filter, or a median filter. Waveforms
    before and after filtering can be displayed and compared.</p>
    <p>If needed, other standalone tools are provided to further work on
    the .trc 3D coordinate files
    (<xref alt="Figure 2" rid="figU003Autilities">Figure 2</xref>).
    Among others, it is possible to undersample a file from a higher to
    a lower framerate, or to convert a file from Z-up to Y-up axis
    convention. The resulting 3D coordinates can be plotted for
    verification. Additionally, a tool is provided to detect gait events
    from point coordinates, according to the equations given by
    (<xref alt="Zeni Jr et al., 2008" rid="ref-Zeni_2008" ref-type="bibr">Zeni
    Jr et al., 2008</xref>).</p>
  </sec>
  <sec id="opensim-scaling-and-inverse-kinematics">
    <title>OpenSim scaling and inverse kinematics</title>
    <p>The main contribution of this software is to build a bridge
    between <monospace>OpenPose</monospace> and
    <monospace>OpenSim</monospace>. The latter allows for much more
    accurate and robust results
    (<xref alt="Pagnon et al., 2022" rid="ref-Pagnon_2022" ref-type="bibr">Pagnon
    et al., 2022</xref>), since it constrains kinematics to an
    individually scaled and physically accurate skeletal model. Bones
    are constrained to a constant length, and joints to coherent angle
    limits.</p>
    <p>The provided model is adapted from the human gait full-body model
    (<xref alt="Rajagopal et al., 2016" rid="ref-Rajagopal_2016" ref-type="bibr">Rajagopal
    et al., 2016</xref>) and the lifting full-body model
    (<xref alt="Beaucage-Gauvreau et al., 2019" rid="ref-Beaucage_2019" ref-type="bibr">Beaucage-Gauvreau
    et al., 2019</xref>). The first one has a better definition of the
    knee joint: abduction/adduction and internal/external rotation
    angles are constrained to the flexion/extension angle. The latter
    has a better definition of the spine: each lumbar vertebra is
    constrained to the next one, which makes it possible for the spine
    to bend in a coherent way with only a few tracked keypoints, without
    having to make it a rigid single bone. Combining those two models
    allows for ours to be as versatile as possible. Hand movements are
    locked, because the standard <monospace>OpenPose</monospace> models
    don’t provide any hand detection.</p>
    <p>This model also takes into account systematic labelling errors in
    <monospace>OpenPose</monospace>
    (<xref alt="Needham et al., 2021" rid="ref-Needham_2021" ref-type="bibr">Needham
    et al., 2021</xref>), and offsets model markers as regards true
    joint centers accordingly. Unlike in marker-based capture, and
    despite the aforementioned systematic errors, keypoints detection
    hardly depends on the subject, the operator, nor the context. For
    this reason, the scaling and the inverse kinematic steps are
    straightforward, and the provided setup files require little to no
    adjusting.</p>
  </sec>
</sec>
<sec id="acknowledgements">
  <title>Acknowledgements</title>
  <p>We acknowledge the dedicated people involved in the many major
  software programs and packages used by
  <monospace>Pose2Sim</monospace>, such as
  <monospace>Python</monospace>, <monospace>OpenPose</monospace>,
  <monospace>OpenSim</monospace>, <monospace>OpenCV</monospace>
  (<xref alt="Bradski, 2000" rid="ref-Bradski_2000" ref-type="bibr">Bradski,
  2000</xref>), among others.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-Beaucage_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Beaucage-Gauvreau</surname><given-names>Erica</given-names></name>
        <name><surname>Robertson</surname><given-names>William SP</given-names></name>
        <name><surname>Brandon</surname><given-names>Scott CE</given-names></name>
        <name><surname>Fraser</surname><given-names>Robert</given-names></name>
        <name><surname>Freeman</surname><given-names>Brian JC</given-names></name>
        <name><surname>Graham</surname><given-names>Ryan B</given-names></name>
        <name><surname>Thewlis</surname><given-names>Dominic</given-names></name>
        <name><surname>Jones</surname><given-names>Claire F</given-names></name>
      </person-group>
      <article-title>Validation of an OpenSim full-body model with detailed lumbar spine for estimating lower lumbar spine loads during symmetric and asymmetric lifting tasks</article-title>
      <source>Computer methods in biomechanics and biomedical engineering</source>
      <publisher-name>Taylor &amp; Francis</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>22</volume>
      <issue>5</issue>
      <pub-id pub-id-type="doi">10.1080/10255842.2018.1564819</pub-id>
      <fpage>451</fpage>
      <lpage>464</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Bradski_2000">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Bradski</surname><given-names>G.</given-names></name>
      </person-group>
      <article-title>The OpenCV library</article-title>
      <source>Dr. Dobb’s Journal of Software Tools</source>
      <year iso-8601-date="2000">2000</year>
    </element-citation>
  </ref>
  <ref id="ref-Butterworth_1930">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Butterworth</surname><given-names>Stephen</given-names></name>
      </person-group>
      <article-title>On the theory of filter amplifiers</article-title>
      <source>Wireless Engineer</source>
      <year iso-8601-date="1930">1930</year>
      <volume>7</volume>
      <issue>6</issue>
      <fpage>536</fpage>
      <lpage>541</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cao_2019">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cao</surname><given-names>Zhe</given-names></name>
        <name><surname>Hidalgo</surname><given-names>Gines</given-names></name>
        <name><surname>Simon</surname><given-names>Tomas</given-names></name>
        <name><surname>Wei</surname><given-names>Shih-En</given-names></name>
        <name><surname>Sheikh</surname><given-names>Yaser</given-names></name>
      </person-group>
      <article-title>OpenPose: Realtime multi-person 2D pose estimation using part affinity fields</article-title>
      <source>IEEE transactions on pattern analysis and machine intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <volume>43</volume>
      <issue>1</issue>
      <uri>https://arxiv.org/abs/1611.08050</uri>
      <pub-id pub-id-type="doi">10.1109/TPAMI.2019.2929257</pub-id>
      <fpage>172</fpage>
      <lpage>186</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Cleveland_1981">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Cleveland</surname><given-names>William S</given-names></name>
      </person-group>
      <article-title>LOWESS: A program for smoothing scatterplots by robust locally weighted regression</article-title>
      <source>American Statistician</source>
      <year iso-8601-date="1981">1981</year>
      <volume>35</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.2307/2683591</pub-id>
      <fpage>54</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Colyer_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Colyer</surname><given-names>Steffi L</given-names></name>
        <name><surname>Evans</surname><given-names>Murray</given-names></name>
        <name><surname>Cosker</surname><given-names>Darren P</given-names></name>
        <name><surname>Salo</surname><given-names>Aki IT</given-names></name>
      </person-group>
      <article-title>A review of the evolution of vision-based motion analysis and the integration of advanced computer vision methods towards developing a markerless system</article-title>
      <source>Sports medicine-open</source>
      <publisher-name>SpringerOpen</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>4</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1186/s40798-018-0139-y</pub-id>
      <fpage>1</fpage>
      <lpage>15</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Delp_2007">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Delp</surname><given-names>Scott L</given-names></name>
        <name><surname>Anderson</surname><given-names>Frank C</given-names></name>
        <name><surname>Arnold</surname><given-names>Allison S</given-names></name>
        <name><surname>Loan</surname><given-names>Peter</given-names></name>
        <name><surname>Habib</surname><given-names>Ayman</given-names></name>
        <name><surname>John</surname><given-names>Chand T</given-names></name>
        <name><surname>Guendelman</surname><given-names>Eran</given-names></name>
        <name><surname>Thelen</surname><given-names>Darryl G</given-names></name>
      </person-group>
      <article-title>OpenSim: Open-source software to create and analyze dynamic simulations of movement</article-title>
      <source>IEEE transactions on biomedical engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2007">2007</year>
      <volume>54</volume>
      <issue>11</issue>
      <uri>https://ieeexplore.ieee.org/abstract/document/4352056</uri>
      <pub-id pub-id-type="doi">10.1109/TBME.2007.901024</pub-id>
      <fpage>1940</fpage>
      <lpage>1950</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Fang_2017">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Fang</surname><given-names>Hao-Shu</given-names></name>
        <name><surname>Xie</surname><given-names>Shuqin</given-names></name>
        <name><surname>Tai</surname><given-names>Yu-Wing</given-names></name>
        <name><surname>Lu</surname><given-names>Cewu</given-names></name>
      </person-group>
      <article-title>RMPE: Regional multi-person pose estimation</article-title>
      <source>ICCV</source>
      <year iso-8601-date="2017">2017</year>
      <uri>https://ieeexplore.ieee.org/document/8237518</uri>
      <pub-id pub-id-type="doi">10.1109/ICCV.2017.256</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Hartley_1997">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hartley</surname><given-names>Richard I</given-names></name>
        <name><surname>Sturm</surname><given-names>Peter</given-names></name>
      </person-group>
      <article-title>Triangulation</article-title>
      <source>Computer vision and image understanding</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="1997">1997</year>
      <volume>68</volume>
      <issue>2</issue>
      <pub-id pub-id-type="doi">10.1006/cviu.1997.0547</pub-id>
      <fpage>146</fpage>
      <lpage>157</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Hidalgo_2019">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hidalgo</surname><given-names>Ginés</given-names></name>
      </person-group>
      <article-title>OpenPose experimental models</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2019">2019</year>
      <uri>https://github.com/CMU-Perceptual-Computing-Lab/openpose_train/tree/master/experimental_models#body_25b-model---option-2-recommended</uri>
    </element-citation>
  </ref>
  <ref id="ref-Hidalgo_2021">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Hidalgo</surname><given-names>Ginés</given-names></name>
      </person-group>
      <article-title>OpenPose 3D reconstruction module</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <uri>https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/advanced/3d_reconstruction_module.md</uri>
    </element-citation>
  </ref>
  <ref id="ref-Kanko_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Kanko</surname><given-names>Robert M</given-names></name>
        <name><surname>Laende</surname><given-names>Elise K</given-names></name>
        <name><surname>Davis</surname><given-names>Elysia M</given-names></name>
        <name><surname>Selbie</surname><given-names>W Scott</given-names></name>
        <name><surname>Deluzio</surname><given-names>Kevin J</given-names></name>
      </person-group>
      <article-title>Concurrent assessment of gait kinematics using marker-based and markerless motion capture</article-title>
      <source>Journal of biomechanics</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>127</volume>
      <uri>https://doi.org/10.1016/j.jbiomech.2021.110665</uri>
      <pub-id pub-id-type="doi">10.1016/j.jbiomech.2021.110665</pub-id>
      <fpage>110665</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Karashchuk_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Karashchuk</surname><given-names>Pierre</given-names></name>
        <name><surname>Rupp</surname><given-names>Katie L</given-names></name>
        <name><surname>Dickinson</surname><given-names>Evyn S</given-names></name>
        <name><surname>Walling-Bell</surname><given-names>Sarah</given-names></name>
        <name><surname>Sanders</surname><given-names>Elischa</given-names></name>
        <name><surname>Azim</surname><given-names>Eiman</given-names></name>
        <name><surname>Brunton</surname><given-names>Bingni W</given-names></name>
        <name><surname>Tuthill</surname><given-names>John C</given-names></name>
      </person-group>
      <article-title>Anipose: A toolkit for robust markerless 3D pose estimation</article-title>
      <source>Cell reports</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>36</volume>
      <issue>13</issue>
      <uri>https://doi.org/10.1016/j.celrep.2021.109730</uri>
      <pub-id pub-id-type="doi">10.1016/j.celrep.2021.109730</pub-id>
      <fpage>109730</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Mathis_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Mathis</surname><given-names>Alexander</given-names></name>
        <name><surname>Mamidanna</surname><given-names>Pranav</given-names></name>
        <name><surname>Cury</surname><given-names>Kevin M</given-names></name>
        <name><surname>Abe</surname><given-names>Taiga</given-names></name>
        <name><surname>Murthy</surname><given-names>Venkatesh N</given-names></name>
        <name><surname>Mathis</surname><given-names>Mackenzie Weygandt</given-names></name>
        <name><surname>Bethge</surname><given-names>Matthias</given-names></name>
      </person-group>
      <article-title>DeepLabCut: Markerless pose estimation of user-defined body parts with deep learning</article-title>
      <source>Nature neuroscience</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2018">2018</year>
      <volume>21</volume>
      <issue>9</issue>
      <uri>https://www.nature.com/articles/s41593-018-0209-y</uri>
      <pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id>
      <fpage>1281</fpage>
      <lpage>1289</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Matthis_2022">
    <element-citation>
      <person-group person-group-type="author">
        <name><surname>Matthis</surname><given-names>Jonathan Samir</given-names></name>
        <name><surname>Cherian</surname><given-names>Aaron</given-names></name>
      </person-group>
      <article-title>FreeMoCap: A free, open source markerless motion capture system</article-title>
      <source>GitHub repository</source>
      <publisher-name>GitHub</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <uri>https://github.com/freemocap/freemocap</uri>
    </element-citation>
  </ref>
  <ref id="ref-Needham_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Needham</surname><given-names>Laurie</given-names></name>
        <name><surname>Evans</surname><given-names>Murray</given-names></name>
        <name><surname>Cosker</surname><given-names>Darren P</given-names></name>
        <name><surname>Wade</surname><given-names>Logan</given-names></name>
        <name><surname>McGuigan</surname><given-names>Polly M</given-names></name>
        <name><surname>Bilzon</surname><given-names>James L</given-names></name>
        <name><surname>Colyer</surname><given-names>Steffi L</given-names></name>
      </person-group>
      <article-title>The accuracy of several pose estimation methods for 3D joint centre localisation</article-title>
      <source>Scientific reports</source>
      <publisher-name>Nature Publishing Group</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>11</volume>
      <issue>1</issue>
      <pub-id pub-id-type="doi">10.1038/s41598-021-00212-x</pub-id>
      <fpage>1</fpage>
      <lpage>11</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Pagnon_2021">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pagnon</surname><given-names>David</given-names></name>
        <name><surname>Domalain</surname><given-names>Mathieu</given-names></name>
        <name><surname>Reveret</surname><given-names>Lionel</given-names></name>
      </person-group>
      <article-title>Pose2Sim: An end-to-end workflow for 3D markerless sports kinematics—part 1: robustness</article-title>
      <source>Sensors</source>
      <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>21</volume>
      <issue>19</issue>
      <uri>https://www.mdpi.com/1424-8220/21/19/6530</uri>
      <pub-id pub-id-type="doi">10.3390/s21196530</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Pagnon_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Pagnon</surname><given-names>David</given-names></name>
        <name><surname>Domalain</surname><given-names>Mathieu</given-names></name>
        <name><surname>Reveret</surname><given-names>Lionel</given-names></name>
      </person-group>
      <article-title>Pose2Sim: An end-to-end workflow for 3D markerless sports kinematics—part 2: accuracy</article-title>
      <source>Sensors</source>
      <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
      <year iso-8601-date="2022">2022</year>
      <volume>22</volume>
      <issue>7</issue>
      <uri>https://www.mdpi.com/1424-8220/22/7/2712</uri>
      <pub-id pub-id-type="doi">10.3390/s22072712</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-Rajagopal_2016">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Rajagopal</surname><given-names>Apoorva</given-names></name>
        <name><surname>Dembia</surname><given-names>Christopher L</given-names></name>
        <name><surname>DeMers</surname><given-names>Matthew S</given-names></name>
        <name><surname>Delp</surname><given-names>Denny D</given-names></name>
        <name><surname>Hicks</surname><given-names>Jennifer L</given-names></name>
        <name><surname>Delp</surname><given-names>Scott L</given-names></name>
      </person-group>
      <article-title>Full-body musculoskeletal model for muscle-driven simulation of human gait</article-title>
      <source>IEEE transactions on biomedical engineering</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2016">2016</year>
      <volume>63</volume>
      <issue>10</issue>
      <pub-id pub-id-type="doi">10.1109/tbme.2016.2586891</pub-id>
      <fpage>2068</fpage>
      <lpage>2079</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Seth_2018">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Seth</surname><given-names>Jennifer L. AND Uchida</given-names><suffix>Ajay AND Hicks</suffix></name>
      </person-group>
      <article-title>OpenSim: Simulating musculoskeletal dynamics and neuromuscular control to study human and animal movement</article-title>
      <source>PLOS Computational Biology</source>
      <publisher-name>Public Library of Science</publisher-name>
      <year iso-8601-date="2018-07">2018</year><month>07</month>
      <volume>14</volume>
      <issue>7</issue>
      <uri>https://doi.org/10.1371/journal.pcbi.1006223</uri>
      <pub-id pub-id-type="doi">10.1371/journal.pcbi.1006223</pub-id>
      <fpage>1</fpage>
      <lpage>20</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Sheshadri_2020">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Sheshadri</surname><given-names>Swathi</given-names></name>
        <name><surname>Dann</surname><given-names>Benjamin</given-names></name>
        <name><surname>Hueser</surname><given-names>Timo</given-names></name>
        <name><surname>Scherberger</surname><given-names>Hansjoerg</given-names></name>
      </person-group>
      <article-title>3D reconstruction toolbox for behavior tracked with multiple cameras</article-title>
      <source>Journal of Open Source Software</source>
      <publisher-name>The Open Journal</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <volume>5</volume>
      <issue>45</issue>
      <uri>https://doi.org/10.21105/joss.01849</uri>
      <pub-id pub-id-type="doi">10.21105/joss.01849</pub-id>
      <fpage>1849</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Uhlrich_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Uhlrich</surname><given-names>Scott D.</given-names></name>
        <name><surname>Falisse</surname><given-names>Antoine</given-names></name>
        <name><surname>Kidziński</surname><given-names>Łukasz</given-names></name>
        <name><surname>Muccini</surname><given-names>Julie</given-names></name>
        <name><surname>Ko</surname><given-names>Michael</given-names></name>
        <name><surname>Chaudhari</surname><given-names>Akshay S.</given-names></name>
        <name><surname>Hicks</surname><given-names>Jennifer L.</given-names></name>
        <name><surname>Delp</surname><given-names>Scott L.</given-names></name>
      </person-group>
      <article-title>OpenCap: 3D human movement dynamics from smartphone videos</article-title>
      <publisher-name>bioRxiv</publisher-name>
      <year iso-8601-date="2022-07">2022</year><month>07</month>
      <uri>https://www.biorxiv.org/content/10.1101/2022.07.07.499061v1</uri>
      <pub-id pub-id-type="doi">10.1101/2022.07.07.499061</pub-id>
      <fpage>2022.07.07.499061</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-Zeni_2008">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zeni Jr</surname><given-names>JA</given-names></name>
        <name><surname>Richards</surname><given-names>JG</given-names></name>
        <name><surname>Higginson</surname><given-names>JS2384115</given-names></name>
      </person-group>
      <article-title>Two simple methods for determining gait events during treadmill and overground walking using kinematic data</article-title>
      <source>Gait &amp; posture</source>
      <publisher-name>Elsevier</publisher-name>
      <year iso-8601-date="2008">2008</year>
      <volume>27</volume>
      <issue>4</issue>
      <uri>https://doi.org/10.1016/j.gaitpost.2007.07.007</uri>
      <pub-id pub-id-type="doi">10.1016/j.gaitpost.2007.07.007</pub-id>
      <fpage>710</fpage>
      <lpage>714</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Zhang_2000">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zhang</surname><given-names>Zhengyou</given-names></name>
      </person-group>
      <article-title>A flexible new technique for camera calibration</article-title>
      <source>IEEE Transactions on pattern analysis and machine intelligence</source>
      <publisher-name>IEEE</publisher-name>
      <year iso-8601-date="2000">2000</year>
      <volume>22</volume>
      <issue>11</issue>
      <pub-id pub-id-type="doi">10.1109/34.888718</pub-id>
      <fpage>1330</fpage>
      <lpage>1334</lpage>
    </element-citation>
  </ref>
  <ref id="ref-Zheng_2022">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Zheng</surname><given-names>Ce</given-names></name>
        <name><surname>Wu</surname><given-names>Wenhan</given-names></name>
        <name><surname>Yang</surname><given-names>Taojiannan</given-names></name>
        <name><surname>Zhu</surname><given-names>Sijie</given-names></name>
        <name><surname>Chen</surname><given-names>Chen</given-names></name>
        <name><surname>Liu</surname><given-names>Ruixu</given-names></name>
        <name><surname>Shen</surname><given-names>Ju</given-names></name>
        <name><surname>Kehtarnavaz</surname><given-names>Nasser</given-names></name>
        <name><surname>Shah</surname><given-names>Mubarak</given-names></name>
      </person-group>
      <article-title>Deep learning-based human pose estimation: A survey</article-title>
      <source>arXiv</source>
      <year iso-8601-date="2022">2022</year>
      <uri>https://doi.org/10.48550/arXiv.2012.13392</uri>
      <pub-id pub-id-type="doi">10.48550/arXiv.2012.13392</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
