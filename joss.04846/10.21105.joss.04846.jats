<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4846</article-id>
<article-id pub-id-type="doi">10.21105/joss.04846</article-id>
<title-group>
<article-title>giotto-deep: A Python Package for Topological Deep
Learning</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<contrib-id contrib-id-type="orcid">0000-0001-9416-9090</contrib-id>
<name>
<surname>Caorsi</surname>
<given-names>Matteo</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="corresp" rid="cor-1"><sup>*</sup></xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Reinauer</surname>
<given-names>Raphael</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Berkouk</surname>
<given-names>Nicolas</given-names>
</name>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>L2F SA, Rue du Centre 9, Saint-Sulpice, 1025,
CH</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Ecole Polytechnique Fédérale de Lausanne (EPFL), Laboratory
for topology and neuroscience, Lausanne, 1015, CH</institution>
</institution-wrap>
</aff>
</contrib-group>
<author-notes>
<corresp id="cor-1">* E-mail: <email></email></corresp>
</author-notes>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2022-09-30">
<day>30</day>
<month>9</month>
<year>2022</year>
</pub-date>
<volume>7</volume>
<issue>79</issue>
<fpage>4846</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>topological data analysis</kwd>
<kwd>deep learning</kwd>
<kwd>persformer</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Topological data analysis (TDA) has already provided many novel
  insights into machine learning
  (<xref alt="Carrière et al., 2020" rid="ref-carriere2020perslay" ref-type="bibr">Carrière
  et al., 2020</xref>) due to its capabilities of synthesizing the shape
  information into a multiset of points in two dimensions: the
  persistence diagrams
  (<xref alt="Wasserman, 2016" rid="ref-wasserman2016topological" ref-type="bibr">Wasserman,
  2016</xref>). Furthermore, many researchers in the field hope to give
  new insights into deep-learning models by applying TDA techniques to
  study the models’ weights, the activation values in the different
  layers, and their evolution during the training phase
  (<xref alt="Naitzat et al., 2020" rid="ref-naitzat2020topology" ref-type="bibr">Naitzat
  et al., 2020</xref>). Orthogonally, TDA techniques have been used as
  feature engineering tools to extract novel information from the data,
  which are then used as standard features in a machine learning
  pipeline, with significant success in many fields
  (<xref alt="Hensel et al., 2021" rid="ref-hensel2021survey" ref-type="bibr">Hensel
  et al., 2021</xref>).</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p><monospace>giotto-deep</monospace> is a deep-learning Python
  package that seamlessly integrates topological data analysis models
  and data structures. Indeed, persistence diagrams (the core
  descriptors of topological data analysis) are intrinsically sets, and
  therefore require specific methods to be manipulated as tensors, and
  analyzed by neural networks. The library is founded on a PyTorch core
  due to the extensive use of the library in the machine learning
  community. The <monospace>giotto-deep</monospace> package was designed
  with usability in mind and provides a class-based interface to fast
  implementations of standard machine learning tasks, such as data
  preprocessing, model building, model training and validation,
  reporting and logging, as well as image classification, Q&amp;A,
  translation, persistence diagram vectorization (via Persformer
  (<xref alt="Reinauer et al., 2021" rid="ref-reinauer2021persformer" ref-type="bibr">Reinauer
  et al., 2021</xref>)). Additionally, giotto-deep supports more
  advanced tasks such as (distributed and multi-pod) hyperparameter
  optimization through its seamless integration with optuna
  (<xref alt="Akiba et al., 2019" rid="ref-akiba2019optuna" ref-type="bibr">Akiba
  et al., 2019</xref>).</p>
  <p><monospace>giotto-deep</monospace> has been designed to be used by
  mathematics researchers and by machine learning engineers. The
  combination of speed, versatility, design, and support for TDA data
  structures in <monospace>giotto-deep</monospace> will enable exciting
  scientific explorations of the behavior of deep learning models,
  hopefully shedding new light on the generalisability and robustness of
  such complex and powerful models. In summary,
  <monospace>giotto-deep</monospace> is a powerful, easy-to-use tool
  that will help to incorporate topological data into machine learning
  models with little effort.</p>
  <fig>
    <caption><p>Simplified architecture
    diagram.<styled-content id="figU003Aarch"></styled-content></p></caption>
    <graphic mimetype="image" mime-subtype="png" xlink:href="media/arch_dgm.png" xlink:title="" />
  </fig>
  <p>The <monospace>giotto-deep</monospace> architecture is schematized
  in figure <xref alt="Figure 1" rid="figU003Aarch">Figure 1</xref>.</p>
  <p>The hyperparameter searches (HPO) can also be distributed on a
  <monospace>kubernetes</monospace> cluster using
  <ext-link ext-link-type="uri" xlink:href="https://python-rq.org">python
  RQ</ext-link>, speeding up the computation: this is an essential
  aspect when dealing with large models and large hyperparameter
  searches. Many topological computations in
  <monospace>giotto-deep</monospace> are performed by
  <monospace>giotto-tda</monospace>
  (<xref alt="Tauzin et al., 2021" rid="ref-tauzin2021giotto" ref-type="bibr">Tauzin
  et al., 2021</xref>).</p>
  <p><monospace>giotto-deep</monospace> handles the whole pipeline: from
  data preprocessing up to the hyperparameter search, the
  <monospace>k-fold</monospace> cross-validation, and the deployment of
  the models. We provide various preprocessing and training pipelines
  already implemented, but we invite users to extend and improve them.
  The eventual goal is to create a readable code base that is easy to
  learn and simple to implement so that the contribution of new features
  would be naturally encouraged. Additionally, we provide classical TDA
  datasets in a dedicated dataset cloud; any user can access and
  download the dataset from the cloud fully automated from the
  <monospace>giotto-deep</monospace>.</p>
</sec>
<sec id="research-projects-using-giotto-deep">
  <title>Research projects using
  <monospace>giotto-deep</monospace></title>
  <p>The current most relevant scientific application of this software
  is the Persformer: a novel algorithm to automatize the persistence
  diagrams vectorization
  (<xref alt="Reinauer et al., 2021" rid="ref-reinauer2021persformer" ref-type="bibr">Reinauer
  et al., 2021</xref>). We also showcased in Jupyter notebook how to
  apply the library to get the results of
  (<xref alt="Perez &amp; Reinauer, 2022" rid="ref-perez2022topological" ref-type="bibr">Perez
  &amp; Reinauer, 2022</xref>) to extract topological information from
  the attention scores of the BERT-Transformer model.</p>
</sec>
<sec id="acknowledgments">
  <title>Acknowledgments</title>
  <p>The authors would like to acknowledge the financial support of the
  Swiss federation: Innosuisse project 41665.1 IP-ICT.</p>
</sec>
</body>
<back>
<ref-list>
  <ref id="ref-reinauer2021persformer">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Reinauer</surname><given-names>Raphael</given-names></name>
        <name><surname>Caorsi</surname><given-names>Matteo</given-names></name>
        <name><surname>Berkouk</surname><given-names>Nicolas</given-names></name>
      </person-group>
      <article-title>Persformer: A transformer architecture for topological machine learning</article-title>
      <source>arXiv preprint arXiv:2112.15210</source>
      <year iso-8601-date="2021">2021</year>
    </element-citation>
  </ref>
  <ref id="ref-carriere2020perslay">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Carrière</surname><given-names>Mathieu</given-names></name>
        <name><surname>Chazal</surname><given-names>Frédéric</given-names></name>
        <name><surname>Ike</surname><given-names>Yuichi</given-names></name>
        <name><surname>Lacombe</surname><given-names>Théo</given-names></name>
        <name><surname>Royer</surname><given-names>Martin</given-names></name>
        <name><surname>Umeda</surname><given-names>Yuhei</given-names></name>
      </person-group>
      <article-title>Perslay: A neural network layer for persistence diagrams and new graph topological signatures</article-title>
      <source>International conference on artificial intelligence and statistics</source>
      <publisher-name>PMLR</publisher-name>
      <year iso-8601-date="2020">2020</year>
      <fpage>2786</fpage>
      <lpage>2796</lpage>
    </element-citation>
  </ref>
  <ref id="ref-wasserman2016topological">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Wasserman</surname><given-names>Larry</given-names></name>
      </person-group>
      <article-title>Topological data analysis</article-title>
      <source>arXiv preprint arXiv:1609.08227</source>
      <year iso-8601-date="2016">2016</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.1609.08227</pub-id>
    </element-citation>
  </ref>
  <ref id="ref-naitzat2020topology">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Naitzat</surname><given-names>Gregory</given-names></name>
        <name><surname>Zhitnikov</surname><given-names>Andrey</given-names></name>
        <name><surname>Lim</surname><given-names>Lek-Heng</given-names></name>
      </person-group>
      <article-title>Topology of deep neural networks.</article-title>
      <source>J. Mach. Learn. Res.</source>
      <year iso-8601-date="2020">2020</year>
      <volume>21</volume>
      <issue>184</issue>
      <fpage>1</fpage>
      <lpage>40</lpage>
    </element-citation>
  </ref>
  <ref id="ref-akiba2019optuna">
    <element-citation publication-type="paper-conference">
      <person-group person-group-type="author">
        <name><surname>Akiba</surname><given-names>Takuya</given-names></name>
        <name><surname>Sano</surname><given-names>Shotaro</given-names></name>
        <name><surname>Yanase</surname><given-names>Toshihiko</given-names></name>
        <name><surname>Ohta</surname><given-names>Takeru</given-names></name>
        <name><surname>Koyama</surname><given-names>Masanori</given-names></name>
      </person-group>
      <article-title>Optuna: A next-generation hyperparameter optimization framework</article-title>
      <source>Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</source>
      <year iso-8601-date="2019">2019</year>
      <fpage>2623</fpage>
      <lpage>2631</lpage>
    </element-citation>
  </ref>
  <ref id="ref-tauzin2021giotto">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Tauzin</surname><given-names>Guillaume</given-names></name>
        <name><surname>Lupo</surname><given-names>Umberto</given-names></name>
        <name><surname>Tunstall</surname><given-names>Lewis</given-names></name>
        <name><surname>Pérez</surname><given-names>Julian Burella</given-names></name>
        <name><surname>Caorsi</surname><given-names>Matteo</given-names></name>
        <name><surname>Medina-Mardones</surname><given-names>Anibal M</given-names></name>
        <name><surname>Dassatti</surname><given-names>Alberto</given-names></name>
        <name><surname>Hess</surname><given-names>Kathryn</given-names></name>
      </person-group>
      <article-title>Giotto-tda:: A topological data analysis toolkit for machine learning and data exploration.</article-title>
      <source>J. Mach. Learn. Res.</source>
      <year iso-8601-date="2021">2021</year>
      <volume>22</volume>
      <issue>39</issue>
      <fpage>1</fpage>
      <lpage>6</lpage>
    </element-citation>
  </ref>
  <ref id="ref-hensel2021survey">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Hensel</surname><given-names>Felix</given-names></name>
        <name><surname>Moor</surname><given-names>Michael</given-names></name>
        <name><surname>Rieck</surname><given-names>Bastian</given-names></name>
      </person-group>
      <article-title>A survey of topological machine learning methods</article-title>
      <source>Frontiers in Artificial Intelligence</source>
      <publisher-name>Frontiers Media SA</publisher-name>
      <year iso-8601-date="2021">2021</year>
      <volume>4</volume>
      <fpage>681108</fpage>
      <lpage></lpage>
    </element-citation>
  </ref>
  <ref id="ref-perez2022topological">
    <element-citation publication-type="article-journal">
      <person-group person-group-type="author">
        <name><surname>Perez</surname><given-names>Ilan</given-names></name>
        <name><surname>Reinauer</surname><given-names>Raphael</given-names></name>
      </person-group>
      <article-title>The topological BERT: Transforming attention into topology for natural language processing</article-title>
      <source>arXiv preprint arXiv:2206.15195</source>
      <year iso-8601-date="2022">2022</year>
      <pub-id pub-id-type="doi">10.48550/arXiv.2206.15195</pub-id>
    </element-citation>
  </ref>
</ref-list>
</back>
</article>
