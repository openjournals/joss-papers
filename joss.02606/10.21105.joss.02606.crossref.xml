<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>9a904837f5fde3f7db112ab1f0181e27</doi_batch_id>
    <timestamp>20211019072203</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>10</month>
          <year>2021</year>
        </publication_date>
        <journal_volume>
          <volume>6</volume>
        </journal_volume>
        <issue>66</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>TorchGAN: A Flexible Framework for GAN Training and Evaluation</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Avik</given_name>
            <surname>Pal</surname>
            <ORCID>http://orcid.org/0000-0002-3938-7375</ORCID>
          </person_name>
          <person_name sequence="additional" contributor_role="author">
            <given_name>Aniket</given_name>
            <surname>Das</surname>
          </person_name>
        </contributors>
        <publication_date>
          <month>10</month>
          <day>19</day>
          <year>2021</year>
        </publication_date>
        <pages>
          <first_page>2606</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.02606</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.5281/zenodo.5575758”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/2606</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.02606</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.02606</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.02606.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <unstructured_citation>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks, Radford, Alec and Metz, Luke and Chintala, Soumith, 2016, 1511.06434, arXiv, cs.LG, 2016 International Conference on Learning Representations</unstructured_citation>
          </citation>
          <citation key="ref2">
            <unstructured_citation>Generative Adversarial Nets, Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua, Advances in neural information processing systems, 2672–2680, 2014</unstructured_citation>
          </citation>
          <citation key="ref3">
            <unstructured_citation>BEGAN: Boundary Equilibrium Generative Adversarial Networks, Berthelot, David and Schumm, Thomas and Metz, Luke, 2017, 1703.10717, arXiv, cs.LG</unstructured_citation>
          </citation>
          <citation key="ref4">
            <unstructured_citation>Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron, Improved Training of Wasserstein GANs, 2017, 9781510860964, Curran Associates Inc., Red Hook, NY, USA, Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms., Proceedings of the 31st International Conference on Neural Information Processing Systems, 5769–5779, 11, Long Beach, California, USA, NIPS’17</unstructured_citation>
          </citation>
          <citation key="ref5">
            <unstructured_citation>Conditional Generative Adversarial Nets, Mirza, Mehdi and Osindero, Simon, 2014, 1411.1784, arXiv, cs.LG</unstructured_citation>
          </citation>
          <citation key="ref6">
            <doi>10.1109/iccv.2017.244</doi>
          </citation>
          <citation key="ref7">
            <doi>10.1109/cvpr.2017.19</doi>
          </citation>
          <citation key="ref8">
            <unstructured_citation>Progressive Growing of GANs for Improved Quality, Stability, and Variation, Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko, International Conference on Learning Representations, 2018, https://openreview.net/forum?id=Hk99zCeAb</unstructured_citation>
          </citation>
          <citation key="ref9">
            <unstructured_citation>Large Scale GAN Training for High Fidelity Natural Image Synthesis, Brock, Andrew and Donahue, Jeff and Simonyan, Karen, International Conference on Learning Representations, 2019, https://openreview.net/forum?id=B1xsqj09Fm</unstructured_citation>
          </citation>
          <citation key="ref10">
            <unstructured_citation>Adversarial feature matching for text generation, Zhang, Yizhe and Gan, Zhe and Fan, Kai and Chen, Zhi and Henao, Ricardo and Shen, Dinghan and Carin, Lawrence, Proceedings of the 34th International Conference on Machine Learning-Volume 70, 4006–4015, 2017, JMLR. org</unstructured_citation>
          </citation>
          <citation key="ref11">
            <unstructured_citation>Real-valued (medical) time series generation with recurrent conditional gans, Esteban, Cristóbal and Hyland, Stephanie L and Rätsch, Gunnar, arXiv preprint arXiv:1706.02633, 2017</unstructured_citation>
          </citation>
          <citation key="ref12">
            <unstructured_citation>Learning Multiple Layers of Features from Tiny Images, Krizhevsky, Alex, 2009, Citeseer</unstructured_citation>
          </citation>
          <citation key="ref13">
            <unstructured_citation>Tulyakov, Sergey and Liu, Ming-Yu and Yang, Xiaodong and Kautz, Jan, MoCoGAN: Decomposing Motion and Content for Video Generation, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), jun, 2018, 6</unstructured_citation>
          </citation>
          <citation key="ref14">
            <unstructured_citation>Efficient Video Generation on Complex Datasets, Clark, Aidan and Donahue, Jeff and Simonyan, Karen, 2019, 1907.06571, arXiv, cs.CV</unstructured_citation>
          </citation>
          <citation key="ref15">
            <unstructured_citation>Generative Multi-Adversarial Networks, Durugkar, Ishan and Gemp, Ian and Mahadevan, Sridhar, International Conference on Learning Representations, 2016, https://openreview.net/forum?id=Byk-VI9eg</unstructured_citation>
          </citation>
          <citation key="ref16">
            <unstructured_citation>Shor, Joel, Tensorflow GAN, 2017, GitHub, GitHub repository, https://github.com/tensorflow/models/tree/master/research/gan, f96099da87c1d6a00b67cab499660253de75a672</unstructured_citation>
          </citation>
          <citation key="ref17">
            <unstructured_citation>Community, KyperGAN, HyperGAN, 2016, GitHub, GitHub repository, https://github.com/HyperGAN/HyperGAN, 5869e4798a6d13ca6bd8f18b87465826d70be922</unstructured_citation>
          </citation>
          <citation key="ref18">
            <unstructured_citation>Raunak Sinha, Naveen Panwar, Anush Sankaran, IBM GAN Toolkit, 2018, GitHub, GitHub repository, https://https://github.com/IBM/gan-toolkit, 7f868208bd5016ab890f6dfcaf7d1ecef15b4c95</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
