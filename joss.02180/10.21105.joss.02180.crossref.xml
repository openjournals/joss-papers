<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/4.4.0" xmlns:ai="http://www.crossref.org/AccessIndicators.xsd" xmlns:rel="http://www.crossref.org/relations.xsd" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" version="4.4.0" xsi:schemaLocation="http://www.crossref.org/schema/4.4.0 http://www.crossref.org/schemas/crossref4.4.0.xsd">
  <head>
    <doi_batch_id>9e79f56a913abf395b5b5eab18e1a1ab</doi_batch_id>
    <timestamp>20200619104717</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>06</month>
          <year>2020</year>
        </publication_date>
        <journal_volume>
          <volume>5</volume>
        </journal_volume>
        <issue>50</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Sensie: Probing the sensitivity of neural networks</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Colin</given_name>
            <surname>Jacobs</surname>
            <ORCID>http://orcid.org/0000-0003-4239-4055</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>06</month>
          <day>19</day>
          <year>2020</year>
        </publication_date>
        <pages>
          <first_page>2180</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.02180</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">“https://doi.org/10.5281/zenodo.3894083”</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/2180</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.02180</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.02180</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">http://www.theoj.org/joss-papers/joss.02180/10.21105.joss.02180.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="ref1">
            <doi>10.1007/978-3-319-44781-0_8</doi>
          </citation>
          <citation key="ref2">
            <doi>10.3847/1538-4365/ab26b6</doi>
          </citation>
          <citation key="ref3">
            <unstructured_citation>Deep inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, Deep inside Convolutional Networks, Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew, 2013, https://arxiv.org/abs/1312.6034, 2017-04-19, /home/coljac/Zotero/storage/8G575WF5/Simonyan et al_2013_Deep inside convolutional networks.pdf;/home/coljac/Zotero/storage/88P8VFPR/1312.html, arXiv preprint arXiv:1312.6034</unstructured_citation>
          </citation>
          <citation key="ref4">
            <unstructured_citation>SmoothGrad: Removing Noise by Adding Noise, SmoothGrad, Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin, 2017, jun, http://arxiv.org/abs/1706.03825, 2019-10-17, Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results., arXiv, 1706.03825, arxiv, /home/coljac/Zotero/storage/IFBTUGJH/IFBTUGJH.pdf;/home/coljac/Zotero/storage/GYQFIQ7T/1706.html, arXiv:1706.03825 [cs, stat], Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning, cs, stat, 6</unstructured_citation>
          </citation>
          <citation key="ref5">
            <unstructured_citation>Visualizing and Understanding Convolutional Networks, Computer Vision – ECCV 2014, Zeiler, Matthew D. and Fergus, Rob, Fleet, David and Pajdla, Tomas and Schiele, Bernt and Tuytelaars, Tinne, 2014, 8689, 818–833, Springer International Publishing, Cham, http://link.springer.com/10.1007/978-3-319-10590-1_53, 2016-02-19, /home/coljac/Zotero/storage/2S76DX3R/Zeiler_Fergus_2014_Visualizing and Understanding Convolutional Networks.pdf, 978-3-319-10589-5 978-3-319-10590-1, convnets,machine learning,toread</unstructured_citation>
          </citation>
          <citation key="ref6">
            <unstructured_citation>keras-vis, Kotikalapudi, Raghavendra and contributors, 2017, GitHub, ://github.com/raghakot/keras-vis</unstructured_citation>
          </citation>
          <citation key="ref7">
            <unstructured_citation>iNNvestigate Neural Networks!, Alber, Maximilian and Lapuschkin, Sebastian and Seegerer, Philipp and Hägele, Miriam and Schütt, Kristof T. and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert and Dähne, Sven and Kindermans, Pieter-Jan, 2018, aug, http://arxiv.org/abs/1808.04260, 2019-10-23, In recent years, deep neural networks have revolutionized many application domains of machine learning and are key components of many critical decision or predictive processes. Therefore, it is crucial that domain specialists can understand and analyze actions and pre- dictions, even of the most complex neural network architectures. Despite these arguments neural networks are often treated as black boxes. In the attempt to alleviate this short- coming many analysis methods were proposed, yet the lack of reference implementations often makes a systematic comparison between the methods a major effort. The presented library iNNvestigate addresses this by providing a common interface and out-of-the- box implementation for many analysis methods, including the reference implementation for PatternNet and PatternAttribution as well as for LRP-methods. To demonstrate the versatility of iNNvestigate, we provide an analysis of image classifications for variety of state-of-the-art neural network architectures., arXiv, 1808.04260, arxiv, /home/coljac/Zotero/storage/IT36VRJ6/Alber et al. - 2018 - iNNvestigate neural networks!.pdf;/home/coljac/Zotero/storage/M2EJG9GQ/1808.html, arXiv:1808.04260 [cs, stat], Computer Science - Machine Learning,Statistics - Machine Learning, cs, stat, 8</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
