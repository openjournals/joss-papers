<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Publishing DTD v1.2 20190208//EN"
                  "JATS-publishing1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
<journal-title>Journal of Open Source Software</journal-title>
<abbrev-journal-title>JOSS</abbrev-journal-title>
</journal-title-group>
<issn publication-format="electronic">2475-9066</issn>
<publisher>
<publisher-name>Open Journals</publisher-name>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="publisher-id">4045</article-id>
<article-id pub-id-type="doi">10.21105/joss.04045</article-id>
<title-group>
<article-title>Octopus Sensing: A Python library for human behavior
studies</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0002-6082-9772</contrib-id>
<name>
<surname>Saffaryazdi</surname>
<given-names>Nastaran</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0001-6482-3944</contrib-id>
<name>
<surname>Gharibnavaz</surname>
<given-names>Aidin</given-names>
</name>
<xref ref-type="aff" rid="aff-3"/>
</contrib>
<contrib contrib-type="author">
<contrib-id contrib-id-type="orcid">0000-0003-4172-6759</contrib-id>
<name>
<surname>Billinghurst</surname>
<given-names>Mark</given-names>
</name>
<xref ref-type="aff" rid="aff-1"/>
<xref ref-type="aff" rid="aff-2"/>
</contrib>
<aff id="aff-1">
<institution-wrap>
<institution>Empathic Computing Laboratory, Auckland Bioengineering
Institute, University of Auckland</institution>
</institution-wrap>
</aff>
<aff id="aff-2">
<institution-wrap>
<institution>Empathic Computing Laboratory, University of South
Australia</institution>
</institution-wrap>
</aff>
<aff id="aff-3">
<institution-wrap>
<institution>Independent Researcher</institution>
</institution-wrap>
</aff>
</contrib-group>
<pub-date date-type="pub" publication-format="electronic" iso-8601-date="2021-12-01">
<day>1</day>
<month>12</month>
<year>2021</year>
</pub-date>
<volume>7</volume>
<issue>71</issue>
<fpage>4045</fpage>
<permissions>
<copyright-statement>Authors of papers retain copyright and release the
work under a Creative Commons Attribution 4.0 International License (CC
BY 4.0)</copyright-statement>
<copyright-year>2022</copyright-year>
<copyright-holder>The article authors</copyright-holder>
<license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/4.0/">
<license-p>Authors of papers retain copyright and release the work under
a Creative Commons Attribution 4.0 International License (CC BY
4.0)</license-p>
</license>
</permissions>
<kwd-group kwd-group-type="author">
<kwd>Python</kwd>
<kwd>Javascript</kwd>
<kwd>Human-Computer-Interaction(HCI)</kwd>
<kwd>Human behavior research</kwd>
<kwd>Physiological Signals</kwd>
<kwd>Electroencephalography</kwd>
<kwd>Multimodal Sensors</kwd>
<kwd>Synchronous data acquisition</kwd>
<kwd>Data visuaization</kwd>
<kwd>Affective Computing</kwd>
<kwd>Experimental design</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="summary">
  <title>Summary</title>
  <p>Designing user studies and collecting data is critical to exploring
  and automatically recognizing human behavior. It is currently possible
  to use a range of sensors to capture heart rate, brain activity, skin
  conductance, and a variety of different physiological cues
  (<xref alt="Seneviratne et al., 2017" rid="ref-seneviratne2017survey" ref-type="bibr">Seneviratne
  et al., 2017</xref>). These data can be combined to provide
  information about a user’s emotional state
  (<xref alt="Dzedzickis et al., 2020" rid="ref-dzedzickis2020human" ref-type="bibr">Dzedzickis
  et al., 2020</xref>;
  <xref alt="Egger et al., 2019" rid="ref-egger2019emotion" ref-type="bibr">Egger
  et al., 2019</xref>), cognitive load
  (<xref alt="Mangaroska et al., 2022" rid="ref-mangaroska2022exploring" ref-type="bibr">Mangaroska
  et al., 2022</xref>;
  <xref alt="Vanneste et al., 2021" rid="ref-vanneste2021towards" ref-type="bibr">Vanneste
  et al., 2021</xref>), or other factors. However, even when data are
  collected correctly, synchronizing data from multiple sensors is
  time-consuming and prone to errors. Failure to record and synchronize
  data is likely to result in errors in analysis and results, as well as
  the need to repeat the time-consuming experiments several times. To
  overcome these challenges, <monospace>Octopus Sensing</monospace>
  facilitates synchronous data acquisition from various sources and
  provides some utilities for designing user studies, real-time
  monitoring, and offline data visualization. The primary aim of
  <monospace>Octopus Sensing</monospace> is to provide a simple
  scripting interface so that people with basic or no software
  development skills can define sensor-based experiment scenarios with
  less effort.</p>
</sec>
<sec id="statement-of-need">
  <title>Statement of need</title>
  <p>Several changes occur in the body and mind due to various internal
  and external stimuli. Nowadays, researchers use various sensors to
  measure and monitor these responses to determine an individual’s state
  (<xref alt="Chen et al., 2021" rid="ref-chen2021physiological" ref-type="bibr">Chen
  et al., 2021</xref>;
  <xref alt="Kreibig, 2010" rid="ref-kreibig2010autonomic" ref-type="bibr">Kreibig,
  2010</xref>;
  <xref alt="Sun et al., 2020" rid="ref-sun2020multimodal" ref-type="bibr">Sun
  et al., 2020</xref>) and to assist patients
  (<xref alt="Hassouneh et al., 2020" rid="ref-hassouneh2020development" ref-type="bibr">Hassouneh
  et al., 2020</xref>) or monitor mental health
  (<xref alt="Jiang et al., 2020" rid="ref-jiang2020snapshot" ref-type="bibr">Jiang
  et al., 2020</xref>). Monitoring and analyzing human responses can be
  used to improve social interactions
  (<xref alt="Hossain &amp; Gedeon, 2019" rid="ref-hossain2019observers" ref-type="bibr">Hossain
  &amp; Gedeon, 2019</xref>;
  <xref alt="Verschuere et al., 2006" rid="ref-verschuere2006psychopathy" ref-type="bibr">Verschuere
  et al., 2006</xref>) and improve quality of life by creating
  intelligent devices such as Intelligent Tutoring Systems
  (<xref alt="Dewan et al., 2019" rid="ref-dewan2019engagement" ref-type="bibr">Dewan
  et al., 2019</xref>), creating adaptive systems
  (<xref alt="Aranha et al., 2019" rid="ref-aranha2019adapting" ref-type="bibr">Aranha
  et al., 2019</xref>), or creating interactive robots and virtual
  characters
  (<xref alt="Hong et al., 2021" rid="ref-hong2021multimodal" ref-type="bibr">Hong
  et al., 2021</xref>;
  <xref alt="Val-Calvo et al., 2020" rid="ref-val2020affective" ref-type="bibr">Val-Calvo
  et al., 2020</xref>).</p>
  <p>Researchers have recently attempted to gain a deeper understanding
  of humans by simultaneously studying physiological and behavioral
  changes in the human body
  (<xref alt="Koelstra et al., 2011" rid="ref-koelstra2011deap" ref-type="bibr">Koelstra
  et al., 2011</xref>;
  <xref alt="Shu et al., 2018" rid="ref-shu2018review" ref-type="bibr">Shu
  et al., 2018</xref>). Acquiring and analyzing data from different
  sources with various hardware and software is complex, time-consuming,
  and challenging. Additionally, human error can easily affect
  synchronously recording data in multiple formats. These tasks decrease
  the pace of progress in human-computer interaction and human behavior
  research.</p>
  <p>There are only a few frameworks that support synchronous data
  acquisition and design.
  <ext-link ext-link-type="uri" xlink:href="https://imotions.com/">iMotions</ext-link>
  has developed software for integrating and synchronizing data
  recording through a wide range of various sensors and devices. Despite
  having many great features, iMotions is commercial software and not
  open-source. In contrast, there are a few open-source programs for
  conducting human studies.
  <ext-link ext-link-type="uri" xlink:href="https://www.psychopy.org/">Psychopy</ext-link>
  (<xref alt="Peirce et al., 2019" rid="ref-peirce2019psychopy2" ref-type="bibr">Peirce
  et al., 2019</xref>) is a powerful open-source, cross-platform
  software that is mainly used for building experiments’ pipelines in
  behavioral science with visual and auditory stimuli. It can also
  record data from a few devices and send triggers to them. Another
  effort in this area is
  <ext-link ext-link-type="uri" xlink:href="http://labstreaminglayer.org/">LabStreamingLayer
  (LSL) LabRecorder</ext-link>. Although LSL LabRecorder provides
  synchronized, multimodal streaming through a wide range of devices, an
  extra application still needs to be run for acquiring data from each
  sensor separately.</p>
  <p><monospace>Octopus Sensing</monospace> is a lightweight open-source
  multi-platform library that facilitates synchronous data acquisition
  from various sources through a unified interface and could be easily
  extended to process and analyze data in real-time. We designed the
  <monospace>Octopus Sensing</monospace> library to minimize the effect
  of network failure in synchronous data streaming and reduce the number
  of applications that we should run for data streaming through
  different devices. Rather than creating a standalone software or
  framework, we created a library that could be easily integrated with
  other applications. <monospace>Octopus Sensing</monospace> provides a
  real-time monitoring system for illustrating and monitoring signals
  remotely using a web-based platform. The system also offers offline
  data visualization to see various human responses simultaneously.</p>
</sec>
<sec id="overview">
  <title>Overview</title>
  <p><monospace>Octopus Sensing</monospace> is a tool to help in running
  scientific experiments that involve recording data synchronously from
  multiple sources. It can simultaneously collect data from various
  devices such as
  <ext-link ext-link-type="uri" xlink:href="https://openbci.com/">OpenBCI
  EEG headset</ext-link>,
  <ext-link ext-link-type="uri" xlink:href="https://shimmersensing.com">Shimmer3
  sensor</ext-link>, camera, and audio-recorder without running another
  software for data recording. Data recording can be started, stopped,
  and triggered synchronously across all devices through a unified
  interface.</p>
  <p>The main features of <monospace>Octopus Sensing</monospace> are
  that it:</p>
  <list list-type="bullet">
    <list-item>
      <p>manages data recording from multiple sources using a simple
      unified interface,</p>
    </list-item>
    <list-item>
      <p>minimizes human errors from manipulating data in synchronous
      data collection,</p>
    </list-item>
    <list-item>
      <p>provides some utilities for designing studies like showing
      different stimuli or designing questionnaires,</p>
    </list-item>
    <list-item>
      <p>offers a monitoring interface that prepares and visualizes
      collected data in real-time, and</p>
    </list-item>
    <list-item>
      <p>provides offline visualization of data from multiple sources
      simultaneously.</p>
    </list-item>
  </list>
</sec>
<sec id="methodology">
  <title>Methodology</title>
  <p><monospace>Octopus Sensing</monospace> synchronizes data recording
  by using <monospace>multiprocessing</monospace> in Python. By
  instantiating the <monospace>Device</monospace> class,
  <monospace>Octopus Sensing</monospace> creates a process for the
  device. Each device’s process has three threads:
  <monospace>data acquisition</monospace> thread,
  <monospace>trigger handling and data recording</monospace> thread, and
  <monospace>real-time data</monospace> thread. The
  <monospace>data acquisition</monospace> thread is responsible for
  acquiring data from a sensor. The
  <monospace>trigger handling and data recording</monospace> thread
  handles trigger messages through a message queue for synchronous data
  recording. It also records data in a file or files. The
  <monospace>real-time data</monospace> thread listens on a queue for
  requests and returns the last three seconds of the recorded data in
  the same queue. This data is being used in real-time monitoring and
  can be used for real-time processing and creating real-time feedback
  in the future. The <monospace>Device Coordinator</monospace> sends
  different triggers such as the start of recording or end of recording
  to different devices by putting the message in all devices’ trigger
  queues at the same time. The <monospace>Device Coordinator</monospace>
  can also send the trigger over the network for devices that are not
  embedded in the <monospace>Octopus Sensing</monospace>. The following
  image shows the overall view of the main components of the
  <monospace>Octopus Sensing</monospace> and their relations.
  -<inline-graphic mimetype="image" mime-subtype="png" xlink:href="media/OCS-diagram.png" /></p>
</sec>
<sec id="research-perspective">
  <title>Research perspective</title>
  <p>We used <monospace>Octopus Sensing</monospace> to design several
  human emotion recognition user studies. We developed a user study
  using <monospace>Octopus Sensing</monospace> for recording facial
  video, brain activity, and physiological signals during a watching
  video task. The recorded data was used to make multimodal emotion
  recognition models
  (<xref alt="Saffaryazdi, Wasim, et al., 2022" rid="ref-Saffaryazdi2022emotion" ref-type="bibr">Saffaryazdi,
  Wasim, et al., 2022</xref>). This scenario which is common in
  physiological emotion recognition studies has been included in the
  repository as an example and explained in the tutorial. In another
  study, we collected multimodal data during face-to-face conversations
  to make models for emotion recognition during interactive tasks
  (<xref alt="Saffaryazdi, Goonesekera, et al., 2022" rid="ref-Saffaryazdi2022conv" ref-type="bibr">Saffaryazdi,
  Goonesekera, et al., 2022</xref>). We developed this user study’s
  scenario in Python using the <monospace>Octopus Sensing</monospace>
  library and conducted the study only by running our developed program,
  without running any other software or any supervision for data
  recording or data synchronization.</p>
  <p>This tool can be used to build real-time data processing systems to
  recognize emotions, stress, cognitive load, or analyze human behavior.
  Our final goal is to extend its capabilities to provide real-time
  emotion recognition using multimodal data. Furthermore, we plan to
  integrate it with <monospace>Psychopy</monospace> in the future and
  combine multimodal data collection and monitoring with
  <monospace>Psychopy</monospace> features when designing scenarios.
  Additionally, we plan to support LSL in the future. By supporting LSL,
  other applications that already support LSL could work with
  <monospace>Octopus Sensing</monospace>.</p>
</sec>
<sec id="acknowledgement">
  <title>Acknowledgement</title>
  <p>We acknowledge the
  <ext-link ext-link-type="uri" xlink:href="http://empathiccomputing.org/">Empatic
  Computing Laboratory (ECL)</ext-link> for financial support and for
  providing feedback, and Professor Suranga Nanayakkara for
  encouragement and feedback.</p>
</sec>
</body>
<back>
<ref-list>
  <ref-list>
    <ref id="ref-kreibig2010autonomic">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Kreibig</surname><given-names>Sylvia D</given-names></name>
        </person-group>
        <article-title>Autonomic nervous system activity in emotion: A review</article-title>
        <source>Biological psychology</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2010">2010</year>
        <volume>84</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1016/j.biopsycho.2010.03.010</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-chen2021physiological">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Chen</surname><given-names>Kuan-Hua</given-names></name>
          <name><surname>Brown</surname><given-names>Casey L</given-names></name>
          <name><surname>Wells</surname><given-names>Jenna L</given-names></name>
          <name><surname>Rothwell</surname><given-names>Emily S</given-names></name>
          <name><surname>Otero</surname><given-names>Marcela C</given-names></name>
          <name><surname>Levenson</surname><given-names>Robert W</given-names></name>
          <name><surname>Fredrickson</surname><given-names>Barbara L</given-names></name>
        </person-group>
        <article-title>Physiological linkage during shared positive and shared negative emotion.</article-title>
        <source>Journal of Personality and Social Psychology</source>
        <publisher-name>American Psychological Association</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>121</volume>
        <issue>5</issue>
        <pub-id pub-id-type="doi">10.1037/pspi0000337</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-sun2020multimodal">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Sun</surname><given-names>Yanjia</given-names></name>
          <name><surname>Ayaz</surname><given-names>Hasan</given-names></name>
          <name><surname>Akansu</surname><given-names>Ali N</given-names></name>
        </person-group>
        <article-title>Multimodal affective state assessment using fNIRS+ EEG and spontaneous facial expression</article-title>
        <source>Brain Sciences</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>10</volume>
        <issue>2</issue>
        <pub-id pub-id-type="doi">10.3390/brainsci10020085</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hassouneh2020development">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hassouneh</surname><given-names>Aya</given-names></name>
          <name><surname>Mutawa</surname><given-names>AM</given-names></name>
          <name><surname>Murugappan</surname><given-names>M</given-names></name>
        </person-group>
        <article-title>Development of a real-time emotion recognition system using facial expressions and EEG based on machine learning and deep neural network methods</article-title>
        <source>Informatics in Medicine Unlocked</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>20</volume>
        <pub-id pub-id-type="doi">10.1016/j.imu.2020.100372</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-verschuere2006psychopathy">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Verschuere</surname><given-names>Bruno</given-names></name>
          <name><surname>Crombez</surname><given-names>Geert</given-names></name>
          <name><surname>Koster</surname><given-names>Ernst</given-names></name>
          <name><surname>Uzieblo</surname><given-names>Katarzyna</given-names></name>
        </person-group>
        <article-title>Psychopathy and physiological detection of concealed information: A review</article-title>
        <source>Psychologica Belgica</source>
        <publisher-name>Ubiquity Press</publisher-name>
        <year iso-8601-date="2006">2006</year>
        <volume>46</volume>
        <issue>1-2</issue>
        <pub-id pub-id-type="doi">10.5334/pb-46-1-2-99</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hossain2019observers">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hossain</surname><given-names>Md Zakir</given-names></name>
          <name><surname>Gedeon</surname><given-names>Tom</given-names></name>
        </person-group>
        <article-title>Observers’ physiological measures in response to videos can be used to detect genuine smiles</article-title>
        <source>International Journal of Human-Computer Studies</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>122</volume>
        <pub-id pub-id-type="doi">10.1016/j.ijhcs.2018.10.003</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dewan2019engagement">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dewan</surname><given-names>M Ali Akber</given-names></name>
          <name><surname>Murshed</surname><given-names>Mahbub</given-names></name>
          <name><surname>Lin</surname><given-names>Fuhua</given-names></name>
        </person-group>
        <article-title>Engagement detection in online learning: A review</article-title>
        <source>Smart Learning Environments</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>6</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1186/s40561-018-0080-z</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-val2020affective">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Val-Calvo</surname><given-names>Mikel</given-names></name>
          <name><surname>Álvarez-Sánchez</surname><given-names>José Ramón</given-names></name>
          <name><surname>Ferrández-Vicente</surname><given-names>José Manuel</given-names></name>
          <name><surname>Fernández</surname><given-names>Eduardo</given-names></name>
        </person-group>
        <article-title>Affective robot story-telling human-robot interaction: Exploratory real-time emotion estimation analysis using facial expressions and physiological signals</article-title>
        <source>IEEE Access</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>8</volume>
        <pub-id pub-id-type="doi">10.1109/ACCESS.2020.3007109</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-hong2021multimodal">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Hong</surname><given-names>Alexander</given-names></name>
          <name><surname>Lunscher</surname><given-names>Nolan</given-names></name>
          <name><surname>Hu</surname><given-names>Tianhao</given-names></name>
          <name><surname>Tsuboi</surname><given-names>Yuma</given-names></name>
          <name><surname>Zhang</surname><given-names>Xinyi</given-names></name>
          <name><surname>Franco dos Reis Alves</surname><given-names>Silas</given-names></name>
          <name><surname>Nejat</surname><given-names>Goldie</given-names></name>
          <name><surname>Benhabib</surname><given-names>Beno</given-names></name>
        </person-group>
        <article-title>A multimodal emotional human–robot interaction architecture for social robots engaged in bidirectional communication</article-title>
        <source>IEEE Transactions on Cybernetics</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>51</volume>
        <issue>12</issue>
        <pub-id pub-id-type="doi">10.1109/TCYB.2020.2974688</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-shu2018review">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Shu</surname><given-names>Lin</given-names></name>
          <name><surname>Xie</surname><given-names>Jinyan</given-names></name>
          <name><surname>Yang</surname><given-names>Mingyue</given-names></name>
          <name><surname>Li</surname><given-names>Ziyi</given-names></name>
          <name><surname>Li</surname><given-names>Zhenqi</given-names></name>
          <name><surname>Liao</surname><given-names>Dan</given-names></name>
          <name><surname>Xu</surname><given-names>Xiangmin</given-names></name>
          <name><surname>Yang</surname><given-names>Xinyi</given-names></name>
        </person-group>
        <article-title>A review of emotion recognition using physiological signals</article-title>
        <source>Sensors</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2018">2018</year>
        <volume>18</volume>
        <issue>7</issue>
        <pub-id pub-id-type="doi">10.3390/s18072074</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-koelstra2011deap">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Koelstra</surname><given-names>Sander</given-names></name>
          <name><surname>Muhl</surname><given-names>Christian</given-names></name>
          <name><surname>Soleymani</surname><given-names>Mohammad</given-names></name>
          <name><surname>Lee</surname><given-names>Jong-Seok</given-names></name>
          <name><surname>Yazdani</surname><given-names>Ashkan</given-names></name>
          <name><surname>Ebrahimi</surname><given-names>Touradj</given-names></name>
          <name><surname>Pun</surname><given-names>Thierry</given-names></name>
          <name><surname>Nijholt</surname><given-names>Anton</given-names></name>
          <name><surname>Patras</surname><given-names>Ioannis</given-names></name>
        </person-group>
        <article-title>Deap: A database for emotion analysis; using physiological signals</article-title>
        <source>IEEE transactions on affective computing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2011">2011</year>
        <volume>3</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1109/T-AFFC.2011.15</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Saffaryazdi2022conv">
      <element-citation publication-type="paper-conference">
        <person-group person-group-type="author">
          <name><surname>Saffaryazdi</surname><given-names>Nastaran</given-names></name>
          <name><surname>Goonesekera</surname><given-names>Yenushka</given-names></name>
          <name><surname>Saffaryazdi</surname><given-names>Nafiseh</given-names></name>
          <name><surname>Hailemariam</surname><given-names>Nebiyou Daniel</given-names></name>
          <name><surname>Temesgen</surname><given-names>Ebasa Girma</given-names></name>
          <name><surname>Nanayakkara</surname><given-names>Suranga</given-names></name>
          <name><surname>Broadbent</surname><given-names>Elizabeth</given-names></name>
          <name><surname>Billinghurst</surname><given-names>Mark</given-names></name>
        </person-group>
        <article-title>Emotion recognition in conversations using brain and physiological signals</article-title>
        <source>27th international conference on intelligent user interfaces</source>
        <publisher-name>Association for Computing Machinery</publisher-name>
        <publisher-loc>New York, NY, USA</publisher-loc>
        <year iso-8601-date="2022">2022</year>
        <isbn>9781450391443</isbn>
        <pub-id pub-id-type="doi">10.1145/3490099.3511148</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-Saffaryazdi2022emotion">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Saffaryazdi</surname><given-names>Nastaran</given-names></name>
          <name><surname>Wasim</surname><given-names>Syed Talal</given-names></name>
          <name><surname>Dileep</surname><given-names>Kuldeep</given-names></name>
          <name><surname>Farrokhinia</surname><given-names>Alireza</given-names></name>
          <name><surname>Nanayakkara</surname><given-names>Suranga</given-names></name>
          <name><surname>Broadbent</surname><given-names>Elizabeth</given-names></name>
          <name><surname>Billinghurst</surname><given-names>Mark</given-names></name>
        </person-group>
        <article-title>Using facial micro-expressions in combination with EEG and physiological signals for emotion recognition</article-title>
        <publisher-name>Manuscript is under review</publisher-name>
        <year iso-8601-date="2022">2022</year>
      </element-citation>
    </ref>
    <ref id="ref-egger2019emotion">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Egger</surname><given-names>Maria</given-names></name>
          <name><surname>Ley</surname><given-names>Matthias</given-names></name>
          <name><surname>Hanke</surname><given-names>Sten</given-names></name>
        </person-group>
        <article-title>Emotion recognition from physiological signal analysis: A review</article-title>
        <source>Electronic Notes in Theoretical Computer Science</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>343</volume>
        <pub-id pub-id-type="doi">10.1016/j.entcs.2019.04.009</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-vanneste2021towards">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Vanneste</surname><given-names>Pieter</given-names></name>
          <name><surname>Raes</surname><given-names>Annelies</given-names></name>
          <name><surname>Morton</surname><given-names>Jessica</given-names></name>
          <name><surname>Bombeke</surname><given-names>Klaas</given-names></name>
          <name><surname>Van Acker</surname><given-names>Bram B</given-names></name>
          <name><surname>Larmuseau</surname><given-names>Charlotte</given-names></name>
          <name><surname>Depaepe</surname><given-names>Fien</given-names></name>
          <name><surname>Van den Noortgate</surname><given-names>Wim</given-names></name>
        </person-group>
        <article-title>Towards measuring cognitive load through multimodal physiological data</article-title>
        <source>Cognition, Technology &amp; Work</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2021">2021</year>
        <volume>23</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.1007/s10111-020-00641-0</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-mangaroska2022exploring">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Mangaroska</surname><given-names>Katerina</given-names></name>
          <name><surname>Sharma</surname><given-names>Kshitij</given-names></name>
          <name><surname>Gašević</surname><given-names>Dragan</given-names></name>
          <name><surname>Giannakos</surname><given-names>Michail</given-names></name>
        </person-group>
        <article-title>Exploring students’ cognitive and affective states during problem solving through multimodal data: Lessons learned from a programming activity</article-title>
        <source>Journal of Computer Assisted Learning</source>
        <publisher-name>Wiley Online Library</publisher-name>
        <year iso-8601-date="2022">2022</year>
        <volume>38</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.1111/jcal.12590</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-dzedzickis2020human">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Dzedzickis</surname><given-names>Andrius</given-names></name>
          <name><surname>Kaklauskas</surname><given-names>Artūras</given-names></name>
          <name><surname>Bucinskas</surname><given-names>Vytautas</given-names></name>
        </person-group>
        <article-title>Human emotion recognition: Review of sensors and methods</article-title>
        <source>Sensors</source>
        <publisher-name>Multidisciplinary Digital Publishing Institute</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>20</volume>
        <issue>3</issue>
        <pub-id pub-id-type="doi">10.3390/s20030592</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-seneviratne2017survey">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Seneviratne</surname><given-names>Suranga</given-names></name>
          <name><surname>Hu</surname><given-names>Yining</given-names></name>
          <name><surname>Nguyen</surname><given-names>Tham</given-names></name>
          <name><surname>Lan</surname><given-names>Guohao</given-names></name>
          <name><surname>Khalifa</surname><given-names>Sara</given-names></name>
          <name><surname>Thilakarathna</surname><given-names>Kanchana</given-names></name>
          <name><surname>Hassan</surname><given-names>Mahbub</given-names></name>
          <name><surname>Seneviratne</surname><given-names>Aruna</given-names></name>
        </person-group>
        <article-title>A survey of wearable devices and challenges</article-title>
        <source>IEEE Communications Surveys &amp; Tutorials</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2017">2017</year>
        <volume>19</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1109/COMST.2017.2731979</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-aranha2019adapting">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Aranha</surname><given-names>Renan Vinicius</given-names></name>
          <name><surname>Corrêa</surname><given-names>Cléber Gimenez</given-names></name>
          <name><surname>Nunes</surname><given-names>Fátima LS</given-names></name>
        </person-group>
        <article-title>Adapting software with affective computing: A systematic review</article-title>
        <source>IEEE Transactions on Affective Computing</source>
        <publisher-name>IEEE</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>12</volume>
        <issue>4</issue>
        <pub-id pub-id-type="doi">10.1109/TAFFC.2019.2902379</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-jiang2020snapshot">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Jiang</surname><given-names>Yingying</given-names></name>
          <name><surname>Li</surname><given-names>Wei</given-names></name>
          <name><surname>Hossain</surname><given-names>M Shamim</given-names></name>
          <name><surname>Chen</surname><given-names>Min</given-names></name>
          <name><surname>Alelaiwi</surname><given-names>Abdulhameed</given-names></name>
          <name><surname>Al-Hammadi</surname><given-names>Muneer</given-names></name>
        </person-group>
        <article-title>A snapshot research and implementation of multimodal information fusion for data-driven emotion recognition</article-title>
        <source>Information Fusion</source>
        <publisher-name>Elsevier</publisher-name>
        <year iso-8601-date="2020">2020</year>
        <volume>53</volume>
        <pub-id pub-id-type="doi">10.1016/j.inffus.2019.06.019</pub-id>
      </element-citation>
    </ref>
    <ref id="ref-peirce2019psychopy2">
      <element-citation publication-type="article-journal">
        <person-group person-group-type="author">
          <name><surname>Peirce</surname><given-names>Jonathan</given-names></name>
          <name><surname>Gray</surname><given-names>Jeremy R</given-names></name>
          <name><surname>Simpson</surname><given-names>Sol</given-names></name>
          <name><surname>MacAskill</surname><given-names>Michael</given-names></name>
          <name><surname>Höchenberger</surname><given-names>Richard</given-names></name>
          <name><surname>Sogo</surname><given-names>Hiroyuki</given-names></name>
          <name><surname>Kastman</surname><given-names>Erik</given-names></name>
          <name><surname>Lindeløv</surname><given-names>Jonas Kristoffer</given-names></name>
        </person-group>
        <article-title>PsychoPy2: Experiments in behavior made easy</article-title>
        <source>Behavior research methods</source>
        <publisher-name>Springer</publisher-name>
        <year iso-8601-date="2019">2019</year>
        <volume>51</volume>
        <issue>1</issue>
        <pub-id pub-id-type="doi">10.3758/s13428-018-01193-y</pub-id>
      </element-citation>
    </ref>
  </ref-list>
</ref-list>
</back>
</article>
