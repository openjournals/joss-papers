<?xml version="1.0" encoding="UTF-8"?>
<doi_batch xmlns="http://www.crossref.org/schema/5.3.1"
           xmlns:ai="http://www.crossref.org/AccessIndicators.xsd"
           xmlns:rel="http://www.crossref.org/relations.xsd"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           version="5.3.1"
           xsi:schemaLocation="http://www.crossref.org/schema/5.3.1 http://www.crossref.org/schemas/crossref5.3.1.xsd">
  <head>
    <doi_batch_id>20220323T195622-7c13643b85a9c59c7d15ef073a89a5fd47dc3d72</doi_batch_id>
    <timestamp>20220323195622</timestamp>
    <depositor>
      <depositor_name>JOSS Admin</depositor_name>
      <email_address>admin@theoj.org</email_address>
    </depositor>
    <registrant>The Open Journal</registrant>
  </head>
  <body>
    <journal>
      <journal_metadata>
        <full_title>Journal of Open Source Software</full_title>
        <abbrev_title>JOSS</abbrev_title>
        <issn media_type="electronic">2475-9066</issn>
        <doi_data>
          <doi>10.21105/joss</doi>
          <resource>https://joss.theoj.org/</resource>
        </doi_data>
      </journal_metadata>
      <journal_issue>
        <publication_date media_type="online">
          <month>03</month>
          <year>2022</year>
        </publication_date>
        <journal_volume>
          <volume>7</volume>
        </journal_volume>
        <issue>71</issue>
      </journal_issue>
      <journal_article publication_type="full_text">
        <titles>
          <title>Octopus Sensing: A Python library for human behavior
studies</title>
        </titles>
        <contributors>
          <person_name sequence="first" contributor_role="author">
            <given_name>Nastaran</given_name>
            <surname>Saffaryazdi</surname>
            <ORCID>https://orcid.org/0000-0002-6082-9772</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Aidin</given_name>
            <surname>Gharibnavaz</surname>
            <ORCID>https://orcid.org/0000-0001-6482-3944</ORCID>
          </person_name>
          <person_name sequence="additional"
                       contributor_role="author">
            <given_name>Mark</given_name>
            <surname>Billinghurst</surname>
            <ORCID>https://orcid.org/0000-0003-4172-6759</ORCID>
          </person_name>
        </contributors>
        <publication_date>
          <month>03</month>
          <day>23</day>
          <year>2022</year>
        </publication_date>
        <pages>
          <first_page>4045</first_page>
        </pages>
        <publisher_item>
          <identifier id_type="doi">10.21105/joss.04045</identifier>
        </publisher_item>
        <ai:program name="AccessIndicators">
          <ai:license_ref applies_to="vor">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="am">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
          <ai:license_ref applies_to="tdm">http://creativecommons.org/licenses/by/4.0/</ai:license_ref>
        </ai:program>
        <rel:program>
          <rel:related_item>
            <rel:description>Software archive</rel:description>
            <rel:inter_work_relation relationship-type="references" identifier-type="doi">10.5281/zenodo.6350710</rel:inter_work_relation>
          </rel:related_item>
          <rel:related_item>
            <rel:description>GitHub review issue</rel:description>
            <rel:inter_work_relation relationship-type="hasReview" identifier-type="uri">https://github.com/openjournals/joss-reviews/issues/4045</rel:inter_work_relation>
          </rel:related_item>
        </rel:program>
        <doi_data>
          <doi>10.21105/joss.04045</doi>
          <resource>https://joss.theoj.org/papers/10.21105/joss.04045</resource>
          <collection property="text-mining">
            <item>
              <resource mime_type="application/pdf">https://joss.theoj.org/papers/10.21105/joss.04045.pdf</resource>
            </item>
          </collection>
        </doi_data>
        <citation_list>
          <citation key="kreibig2010autonomic">
            <article_title>Autonomic nervous system activity in emotion:
A review</article_title>
            <author>Kreibig</author>
            <journal_title>Biological psychology</journal_title>
            <issue>3</issue>
            <volume>84</volume>
            <doi>10.1016/j.biopsycho.2010.03.010</doi>
            <cYear>2010</cYear>
            <unstructured_citation>Kreibig, S. D. (2010). Autonomic
nervous system activity in emotion: A review. Biological Psychology,
84(3), 394–421.
https://doi.org/10.1016/j.biopsycho.2010.03.010</unstructured_citation>
          </citation>
          <citation key="chen2021physiological">
            <article_title>Physiological linkage during shared positive
and shared negative emotion.</article_title>
            <author>Chen</author>
            <journal_title>Journal of Personality and Social
Psychology</journal_title>
            <issue>5</issue>
            <volume>121</volume>
            <doi>10.1037/pspi0000337</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Chen, K.-H., Brown, C. L., Wells, J.
L., Rothwell, E. S., Otero, M. C., Levenson, R. W., &amp; Fredrickson,
B. L. (2021). Physiological linkage during shared positive and shared
negative emotion. Journal of Personality and Social Psychology, 121(5),
1029. https://doi.org/10.1037/pspi0000337</unstructured_citation>
          </citation>
          <citation key="sun2020multimodal">
            <article_title>Multimodal affective state assessment using
fNIRS+ EEG and spontaneous facial expression</article_title>
            <author>Sun</author>
            <journal_title>Brain Sciences</journal_title>
            <issue>2</issue>
            <volume>10</volume>
            <doi>10.3390/brainsci10020085</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Sun, Y., Ayaz, H., &amp; Akansu, A.
N. (2020). Multimodal affective state assessment using fNIRS+ EEG and
spontaneous facial expression. Brain Sciences, 10(2), 85.
https://doi.org/10.3390/brainsci10020085</unstructured_citation>
          </citation>
          <citation key="hassouneh2020development">
            <article_title>Development of a real-time emotion
recognition system using facial expressions and EEG based on machine
learning and deep neural network methods</article_title>
            <author>Hassouneh</author>
            <journal_title>Informatics in Medicine
Unlocked</journal_title>
            <volume>20</volume>
            <doi>10.1016/j.imu.2020.100372</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Hassouneh, A., Mutawa, A., &amp;
Murugappan, M. (2020). Development of a real-time emotion recognition
system using facial expressions and EEG based on machine learning and
deep neural network methods. Informatics in Medicine Unlocked, 20,
100372.
https://doi.org/10.1016/j.imu.2020.100372</unstructured_citation>
          </citation>
          <citation key="verschuere2006psychopathy">
            <article_title>Psychopathy and physiological detection of
concealed information: A review</article_title>
            <author>Verschuere</author>
            <journal_title>Psychologica Belgica</journal_title>
            <issue>1-2</issue>
            <volume>46</volume>
            <doi>10.5334/pb-46-1-2-99</doi>
            <cYear>2006</cYear>
            <unstructured_citation>Verschuere, B., Crombez, G., Koster,
E., &amp; Uzieblo, K. (2006). Psychopathy and physiological detection of
concealed information: A review. Psychologica Belgica, 46(1-2), 99.
https://doi.org/10.5334/pb-46-1-2-99</unstructured_citation>
          </citation>
          <citation key="hossain2019observers">
            <article_title>Observers’ physiological measures in response
to videos can be used to detect genuine smiles</article_title>
            <author>Hossain</author>
            <journal_title>International Journal of Human-Computer
Studies</journal_title>
            <volume>122</volume>
            <doi>10.1016/j.ijhcs.2018.10.003</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Hossain, M. Z., &amp; Gedeon, T.
(2019). Observers’ physiological measures in response to videos can be
used to detect genuine smiles. International Journal of Human-Computer
Studies, 122, 232–241.
https://doi.org/10.1016/j.ijhcs.2018.10.003</unstructured_citation>
          </citation>
          <citation key="dewan2019engagement">
            <article_title>Engagement detection in online learning: A
review</article_title>
            <author>Dewan</author>
            <journal_title>Smart Learning Environments</journal_title>
            <issue>1</issue>
            <volume>6</volume>
            <doi>10.1186/s40561-018-0080-z</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Dewan, M. A. A., Murshed, M., &amp;
Lin, F. (2019). Engagement detection in online learning: A review. Smart
Learning Environments, 6(1), 1.
https://doi.org/10.1186/s40561-018-0080-z</unstructured_citation>
          </citation>
          <citation key="val2020affective">
            <article_title>Affective robot story-telling human-robot
interaction: Exploratory real-time emotion estimation analysis using
facial expressions and physiological signals</article_title>
            <author>Val-Calvo</author>
            <journal_title>IEEE Access</journal_title>
            <volume>8</volume>
            <doi>10.1109/ACCESS.2020.3007109</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Val-Calvo, M., Álvarez-Sánchez, J.
R., Ferrández-Vicente, J. M., &amp; Fernández, E. (2020). Affective
robot story-telling human-robot interaction: Exploratory real-time
emotion estimation analysis using facial expressions and physiological
signals. IEEE Access, 8, 134051–134066.
https://doi.org/10.1109/ACCESS.2020.3007109</unstructured_citation>
          </citation>
          <citation key="hong2021multimodal">
            <article_title>A multimodal emotional human–robot
interaction architecture for social robots engaged in bidirectional
communication</article_title>
            <author>Hong</author>
            <journal_title>IEEE Transactions on
Cybernetics</journal_title>
            <issue>12</issue>
            <volume>51</volume>
            <doi>10.1109/TCYB.2020.2974688</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Hong, A., Lunscher, N., Hu, T.,
Tsuboi, Y., Zhang, X., Franco dos Reis Alves, S., Nejat, G., &amp;
Benhabib, B. (2021). A multimodal emotional human–robot interaction
architecture for social robots engaged in bidirectional communication.
IEEE Transactions on Cybernetics, 51(12), 5954–5968.
https://doi.org/10.1109/TCYB.2020.2974688</unstructured_citation>
          </citation>
          <citation key="shu2018review">
            <article_title>A review of emotion recognition using
physiological signals</article_title>
            <author>Shu</author>
            <journal_title>Sensors</journal_title>
            <issue>7</issue>
            <volume>18</volume>
            <doi>10.3390/s18072074</doi>
            <cYear>2018</cYear>
            <unstructured_citation>Shu, L., Xie, J., Yang, M., Li, Z.,
Li, Z., Liao, D., Xu, X., &amp; Yang, X. (2018). A review of emotion
recognition using physiological signals. Sensors, 18(7), 2074.
https://doi.org/10.3390/s18072074</unstructured_citation>
          </citation>
          <citation key="koelstra2011deap">
            <article_title>Deap: A database for emotion analysis; using
physiological signals</article_title>
            <author>Koelstra</author>
            <journal_title>IEEE transactions on affective
computing</journal_title>
            <issue>1</issue>
            <volume>3</volume>
            <doi>10.1109/T-AFFC.2011.15</doi>
            <cYear>2011</cYear>
            <unstructured_citation>Koelstra, S., Muhl, C., Soleymani,
M., Lee, J.-S., Yazdani, A., Ebrahimi, T., Pun, T., Nijholt, A., &amp;
Patras, I. (2011). Deap: A database for emotion analysis; using
physiological signals. IEEE Transactions on Affective Computing, 3(1),
18–31. https://doi.org/10.1109/T-AFFC.2011.15</unstructured_citation>
          </citation>
          <citation key="Saffaryazdi2022conv">
            <article_title>Emotion recognition in conversations using
brain and physiological signals</article_title>
            <author>Saffaryazdi</author>
            <journal_title>27th international conference on intelligent
user interfaces</journal_title>
            <doi>10.1145/3490099.3511148</doi>
            <isbn>9781450391443</isbn>
            <cYear>2022</cYear>
            <unstructured_citation>Saffaryazdi, N., Goonesekera, Y.,
Saffaryazdi, N., Hailemariam, N. D., Temesgen, E. G., Nanayakkara, S.,
Broadbent, E., &amp; Billinghurst, M. (2022). Emotion recognition in
conversations using brain and physiological signals. 27th International
Conference on Intelligent User Interfaces, 229–242.
https://doi.org/10.1145/3490099.3511148</unstructured_citation>
          </citation>
          <citation key="Saffaryazdi2022emotion">
            <article_title>Using facial micro-expressions in combination
with EEG and physiological signals for emotion
recognition</article_title>
            <author>Saffaryazdi</author>
            <cYear>2022</cYear>
            <unstructured_citation>Saffaryazdi, N., Wasim, S. T.,
Dileep, K., Farrokhinia, A., Nanayakkara, S., Broadbent, E., &amp;
Billinghurst, M. (2022). Using facial micro-expressions in combination
with EEG and physiological signals for emotion recognition. Under
review.</unstructured_citation>
          </citation>
          <citation key="egger2019emotion">
            <article_title>Emotion recognition from physiological signal
analysis: A review</article_title>
            <author>Egger</author>
            <journal_title>Electronic Notes in Theoretical Computer
Science</journal_title>
            <volume>343</volume>
            <doi>10.1016/j.entcs.2019.04.009</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Egger, M., Ley, M., &amp; Hanke, S.
(2019). Emotion recognition from physiological signal analysis: A
review. Electronic Notes in Theoretical Computer Science, 343, 35–55.
https://doi.org/10.1016/j.entcs.2019.04.009</unstructured_citation>
          </citation>
          <citation key="vanneste2021towards">
            <article_title>Towards measuring cognitive load through
multimodal physiological data</article_title>
            <author>Vanneste</author>
            <journal_title>Cognition, Technology &amp;
Work</journal_title>
            <issue>3</issue>
            <volume>23</volume>
            <doi>10.1007/s10111-020-00641-0</doi>
            <cYear>2021</cYear>
            <unstructured_citation>Vanneste, P., Raes, A., Morton, J.,
Bombeke, K., Van Acker, B. B., Larmuseau, C., Depaepe, F., &amp; Van den
Noortgate, W. (2021). Towards measuring cognitive load through
multimodal physiological data. Cognition, Technology &amp; Work, 23(3),
567–585.
https://doi.org/10.1007/s10111-020-00641-0</unstructured_citation>
          </citation>
          <citation key="mangaroska2022exploring">
            <article_title>Exploring students’ cognitive and affective
states during problem solving through multimodal data: Lessons learned
from a programming activity</article_title>
            <author>Mangaroska</author>
            <journal_title>Journal of Computer Assisted
Learning</journal_title>
            <issue>1</issue>
            <volume>38</volume>
            <doi>10.1111/jcal.12590</doi>
            <cYear>2022</cYear>
            <unstructured_citation>Mangaroska, K., Sharma, K., Gašević,
D., &amp; Giannakos, M. (2022). Exploring students’ cognitive and
affective states during problem solving through multimodal data: Lessons
learned from a programming activity. Journal of Computer Assisted
Learning, 38(1), 40–59.
https://doi.org/10.1111/jcal.12590</unstructured_citation>
          </citation>
          <citation key="dzedzickis2020human">
            <article_title>Human emotion recognition: Review of sensors
and methods</article_title>
            <author>Dzedzickis</author>
            <journal_title>Sensors</journal_title>
            <issue>3</issue>
            <volume>20</volume>
            <doi>10.3390/s20030592</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Dzedzickis, A., Kaklauskas, A., &amp;
Bucinskas, V. (2020). Human emotion recognition: Review of sensors and
methods. Sensors, 20(3), 592.
https://doi.org/10.3390/s20030592</unstructured_citation>
          </citation>
          <citation key="seneviratne2017survey">
            <article_title>A survey of wearable devices and
challenges</article_title>
            <author>Seneviratne</author>
            <journal_title>IEEE Communications Surveys &amp;
Tutorials</journal_title>
            <issue>4</issue>
            <volume>19</volume>
            <doi>10.1109/COMST.2017.2731979</doi>
            <cYear>2017</cYear>
            <unstructured_citation>Seneviratne, S., Hu, Y., Nguyen, T.,
Lan, G., Khalifa, S., Thilakarathna, K., Hassan, M., &amp; Seneviratne,
A. (2017). A survey of wearable devices and challenges. IEEE
Communications Surveys &amp; Tutorials, 19(4), 2573–2620.
https://doi.org/10.1109/COMST.2017.2731979</unstructured_citation>
          </citation>
          <citation key="aranha2019adapting">
            <article_title>Adapting software with affective computing: A
systematic review</article_title>
            <author>Aranha</author>
            <journal_title>IEEE Transactions on Affective
Computing</journal_title>
            <issue>4</issue>
            <volume>12</volume>
            <doi>10.1109/TAFFC.2019.2902379</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Aranha, R. V., Corrêa, C. G., &amp;
Nunes, F. L. (2019). Adapting software with affective computing: A
systematic review. IEEE Transactions on Affective Computing, 12(4),
883–899.
https://doi.org/10.1109/TAFFC.2019.2902379</unstructured_citation>
          </citation>
          <citation key="jiang2020snapshot">
            <article_title>A snapshot research and implementation of
multimodal information fusion for data-driven emotion
recognition</article_title>
            <author>Jiang</author>
            <journal_title>Information Fusion</journal_title>
            <volume>53</volume>
            <doi>10.1016/j.inffus.2019.06.019</doi>
            <cYear>2020</cYear>
            <unstructured_citation>Jiang, Y., Li, W., Hossain, M. S.,
Chen, M., Alelaiwi, A., &amp; Al-Hammadi, M. (2020). A snapshot research
and implementation of multimodal information fusion for data-driven
emotion recognition. Information Fusion, 53, 209–221.
https://doi.org/10.1016/j.inffus.2019.06.019</unstructured_citation>
          </citation>
          <citation key="peirce2019psychopy2">
            <article_title>PsychoPy2: Experiments in behavior made
easy</article_title>
            <author>Peirce</author>
            <journal_title>Behavior research methods</journal_title>
            <issue>1</issue>
            <volume>51</volume>
            <doi>10.3758/s13428-018-01193-y</doi>
            <cYear>2019</cYear>
            <unstructured_citation>Peirce, J., Gray, J. R., Simpson, S.,
MacAskill, M., Höchenberger, R., Sogo, H., Kastman, E., &amp; Lindeløv,
J. K. (2019). PsychoPy2: Experiments in behavior made easy. Behavior
Research Methods, 51(1), 195–203.
https://doi.org/10.3758/s13428-018-01193-y</unstructured_citation>
          </citation>
        </citation_list>
      </journal_article>
    </journal>
  </body>
</doi_batch>
